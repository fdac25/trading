,Link,Title,Time,Author,Body
0,https://www.forbes.com/sites/solrashidi/2025/10/09/openais-strategic-shift-what-the-amd-and-nvidia-partnerships-signal/,OpenAI’s Strategic Shift: What The AMD And Nvidia Partnerships Signal,"Oct 09, 2025, 09:00am EDT",Sol Rashidi,"AMD, Advanced Micro Devices saw a 23.71% surge in its share price this week, driven by a high-profile partnership with OpenAI that could fundamentally reshape the competitive landscape in AI computing infrastructure. The deal, centered around AMD’s graphics processing units (GPUs), will supply OpenAI with an initial 1-gigawatt of computing power beginning in the second half of 2026. What’s interesting about this partnership is, this isn’t just another chip supply agreement—it appears to be a calculated strategic move that gives OpenAI options for up to 160 million shares of AMD common stock, a stake potentially worth billions. This announcement comes  just two weeks after OpenAI inked a $100 billion equity and data deal with Nvidia.
This marks a clear shift in OpenAI’s ambitions to vertically integrate compute, data, and model training at unprecedented scale.
According to sources close to the matter, OpenAI will tap AMD’s Instinct MI300X accelerators, which have emerged as viable competitors to Nvidia’s H100 in certain high-throughput AI workloads. The company will gain access to a gigawatt-scale infrastructure footprint by late 2026—a move likely to insulate it from compute shortages that have plagued the AI industry over the past 18 months.
While the dollar figure remains undisclosed, the scale of the deal—coupled with the 160 million share option—signals a valuation easily in the multibillion-dollar range. For perspective, AMD’s total market capitalization currently sits north of $180 billion, and the share options represent nearly 9% of the company’s outstanding shares.
What this new structure is telling us is OpenAI isn’t just a customer—it’s positioning itself as a strategic partner, if not an indirect stakeholder. The option to own nearly a tenth of AMD gives OpenAI substantial negotiating leverage and aligns both companies’ incentives to optimize AMD’s chips for future GPT models.
OpenAI is not a traditional enterprise customer, it appears to be morphing into a power broker within the semiconductor ecosystem through the following avenues:
For AMD, the deal is a validation of its long-term strategy to challenge Nvidia’s AI dominance. Its MI300X chips, launched in late 2023, have seen gradual uptake in hyperscale environments. But with OpenAI’s endorsement, AMD now has a marquee customer that could catalyze broader adoption.  Expect AMD’s software ecosystem—to receive increased investment, while new benchmarking metrics may emerge to compare GPT training performance across vendors.
For Nvidia, the deal is less a loss and more a wake-up call. While it still dominates AI acceleration, OpenAI’s move signals that no vendor is too big to compete with. It’s also a hedge against Nvidia’s own ambitions in AI services and inference, which increasingly overlap with OpenAI’s.
For the broader market, this marks a splintering of the AI hardware stack. Rather than standardizing on a single vendor, the next generation of AI firms—spurred by OpenAI’s model—may adopt multi-vendor architectures, mixing AMD, Nvidia, and even custom silicon like Google’s TPUs.
The AMD partnership is more than a procurement deal—it’s frankly a masterclass in strategic positioning. OpenAI has quietly built a playbook based on ecosystem control, upstream equity leverage, and industrial-scale deployment, setting the tone for the next phase of AI’s evolution.  By aligning itself with both Nvidia and AMD—and demanding favorable terms via options and equity—OpenAI ensures both security and influence over its most critical dependency: compute.
In the words of Peter Drucker, “The best way to predict the future is to create it.” OpenAI isn’t waiting for the AI infrastructure to evolve—it appears to be building it, one chip deal at a time."
1,https://www.forbes.com/sites/greatspeculations/2025/10/07/nvidia-stock-2x-to-350/,Nvidia Stock 2x To $350?,"Oct 07, 2025, 05:30am EDT",Trefis Team,"Will Nvidia stock (NASDAQ: NVDA) reach $350 in the upcoming years? This likelihood is more than mere speculation. In the last six months alone, Nvidia’s share price has almost doubled, soaring from around $94 per share to nearly $185. At first sight, trading at approximately 42x consensus adjusted FY’26 earnings and 62x for FY’25, the stock may seem expensive. However, considering Nvidia's remarkable earnings growth (with EPS anticipated to increase by nearly 50% this year), its leading role in accelerated computing, and the substantial potential for AI adoption, the valuation begins to appear more justifiable. CEO Jensen Huang foresees that spending on AI infrastructure could rise to $4 trillion by the decade's conclusion, and if artificial general intelligence (AGI) materializes even partially, the need for high-performance computing could soar. Below, we illustrate a possible path indicating how Nvidia’s revenue, profitability, and valuation multiples could elevate its stock to over $350 per share, nearing a $10 trillion market cap.
That being said, if you seek an upside with less volatility than holding an individual stock like NVDA, consider the High Quality Portfolio. It has comfortably outperformed its benchmark—a combination of the S&P 500, Russell, and S&P MidCap indexes—and has achieved returns exceeding 91% since its inception. Why is that? As a group, HQ Portfolio stocks provided better returns with less risk versus the benchmark index; less of a roller-coaster ride, as evident in HQ Portfolio performance metrics.
Nvidia’s revenues have nearly doubled over the past 12 months, while achieving an average annual growth rate of about 69% over the previous three years, and this momentum could persist. If Nvidia increases its sales at an average annual rate of approximately 60% for the next two years and around 45% in the third year—its revenues could rise from about $131 billion in FY’25 to roughly $486 billion by FY’28, or nearly 3.7x.
Several trends could continue to fuel Nvidia’s growth. While the initial AI models implemented by companies like OpenAI in 2022 were mainly text-based, AI is increasingly evolving to be multimodal—handling speech, images, video, and even 3D—which necessitates greater computing power and a higher quantity of GPU shipments. Activity in AI deal-making shows no signs of deceleration. In the past three months, Nvidia has completed some of its most significant AI-related agreements, including a $100 billion investment and partnership with OpenAI, covering the provision of advanced AI chips and an equity stake, a $6.3 billion cloud capacity agreement with CoreWeave, and Microsoft’s $19.4 billion contract with Nebius, which secures access to nearly 100,000 of Nvidia’s latest GPU chips to boost its AI capabilities. Separately, see How SOUN Stock Falls To $2?
The next significant tech revolution could be AGI—Artificial General Intelligence—and Nvidia is positioned at the core of it. Though the timeline is uncertain, the objective is to enable AI to reason, plan, and learn new tasks without requiring retraining. Artificial General Intelligence could conduct scientific research, generate innovative insights, or design complete products independently. This potential could unlock breakthroughs and innovations across various industries, with some models suggesting that AGI could elevate global GDP growth from low single-digit percentages to over 20% annually. [1] As the world approaches AGI, Nvidia stands to gain significantly. AGI will require immense computing power to train and operate, far exceeding current AI models, and Nvidia’s GPUs are the gold standard for these high-performance functions.
Combining this robust revenue growth with the fact that Nvidia’s margins (net income, or profits after all expenses and taxes, expressed as a percentage of revenues) are on an upward trend—they increased from approximately 25% in FY’19 to around 51% in FY’25 as the company benefited from enhanced economies of scale and a more favorable product mix biased toward complex data center products. Software-related sales are also on the rise. We can assume that margins will remain stable at current levels since Nvidia's introduction of pricier high-end products, such as the latest Blackwell chips, is offset by potentially increased costs and competition in the low-end market from competitors like AMD. With margins staying steady and revenue increasing 3.7x, we could witness earnings rise 3.7x.
If earnings increase by 3.7x, the PE multiple will decline by 3.7x to approximately 18x, assuming the stock price remains constant. But that’s precisely what Nvidia investors are betting will not occur. If earnings grow 3.7x over the next few years, instead of the trailing PE decreasing from around 62x currently to roughly 18x, a situation where the PE metric remains around 32x appears quite probable. For context, Apple—a company experiencing low single-digit growth—trades at 30x. This scenario could mean Nvidia's stock could grow by approximately 1.9x within the next three years or so, a tangible possibility, leading to a share price exceeding $350. What about the timeline for this high-return scenario? In practice, it won’t significantly matter whether it takes three years or four, as long as Nvidia continues on this revenue growth trajectory, coupled with margins holding steady, the stock price could respond accordingly.
As always, investing in a single company stock carries risk. Consider the Trefis Reinforced Value (RV) Portfolio, which has outperformed its all-cap benchmark (a blend of the S&P 500, S&P MidCap, and Russell 2000) to deliver strong returns for investors. The quarterly rebalanced mix of large-, mid-, and small-cap RV Portfolio stocks provides a flexible approach to capitalize on bullish markets while minimizing drawdowns, as shown in RV Portfolio performance metrics.

"
2,https://www.forbes.com/sites/devpatnaik/2025/10/06/the-lesson-from-intel-and-nvidia-choose-action-over-anxiety/,The Lesson from Intel And Nvidia: Choose Action Over Anxiety,"Oct 06, 2025, 04:42pm EDT",Dev Patnaik,"If I told you back in 2010 that a scrappy maker of graphics chips for video games would bail out Intel, you’d have laughed me out of the room.
And yet, that’s what happened last month. On September 18, Nvidia announced a $5 billion investment in Intel, tossing a lifeline to the struggling icon and instantly becoming one of its largest shareholders.
The moment was surreal for those of us who’ve spent time in Silicon Valley. Fifteen years ago, Intel was untouchable. It had a near-monopoly on central processing units, the brains inside every personal computer on the planet. It commanded almost 15% of global semiconductor revenue. It even billed itself as “putting the silicon in Silicon Valley.” At that time, Intel’s market cap was ten times bigger than Nvidia’s. Sure, mobile computing was chipping away at its dominance. But still…this was Intel.
And yet, Nvidia’s $5 billion power play wasn’t a sudden disruption. It’s the culmination of a story that’s been unfolding for two decades. The signs were always there…if you were willing to look.
Back in the early 2000s, Nvidia was a niche player beloved by hardcore gamers. Its GPUs were great for rendering graphics, but not much else. Then, in 2006, Nvidia released the Compute Unified Device Architecture. This software toolkit lets engineers stitch GPUs together into parallel computing workhorses. And while some execs at Intel took notice, most saw it as a mere curiosity.
Three years later, a team of Stanford researchers proved otherwise. They published a paper entitled “Large Scale Deep Unsupervised Learning Using Graphics Processors”. The paper showed that GPUs could crush CPUs at deep learning tasks. Suddenly, a lot of work in AI research wasn’t theoretical anymore—it was feasible. Nvidia moved quickly to support this new market for its chips. Intel continued to focus on CPUs.
And then the bomb went off.
In 2017, researchers at Google invented the Transformer. In a groundbreaking paper entitled “Attention is All You Need"", the team proposed a new deep-learning architecture that was far more powerful than older recurrent neural networks. That breakthrough would power large language models and launch the modern Age of Artificial Intelligence. Nvidia was ready. Intel wasn’t.
Over the course of twenty years, Intel execs watched as the world changed around them. By 2016, they were publicly acknowledging the rising importance of AI and GPUs. They recognized that machine learning would be a driving force in their data center and cloud businesses. But they never broke from the gravitational pull of their CPU business. For them, AI was a side hustle.
Meanwhile, Intel started to stumble in manufacturing, losing ground to TSMC and Samsung. By contrast, Nvidia remained “fabless,” focusing on semiconductor design and relying on TSMC to build its chips. The rest is history.
It’s tempting to treat Intel’s failure as an industry-specific cautionary tale. It isn’t. Too many leaders fall into the same trap. They notice something shifting at the edges—new technology, changing customer behavior—and it sparks anxiety. But instead of acting, they retreat into the comfort of day-to-day operations.
I once met a food company exec who bragged about his consumer insights team. “There isn’t a single industry trend in the last twenty years that we haven’t seen coming,” he crowed. That didn’t strike me as a good thing. After all, if he’d seen it all coming, why didn’t he do something about it?
At the turn of the century, the term “information overload” was widely used to describe the feeling of being overwhelmed by excessive data, numerous signals, and complexity. Interestingly, it’s not a term you hear much these days, even though the wave of information overwhelm has turned into a tsunami. Perhaps we’ve just gotten used to things.
My theory is that we’ve traded information overload for something even more disorienting: crisis overload. If you’re a business leader, you’re used to facing some new issue every month or so. Except nowadays, you’re getting a new crisis every three days. Politically driven policy chaos, market disruptions, regulatory changes, technological shifts, and supply chain breakdowns—the pace and scale of overwhelm have risen drastically, creating a perpetual state of anxiety.
To be sure, spotting trends is important. But it’s only the first step in moving from anxiety to action.
Most large companies are pretty good at identifying and forecasting trends. They employ sophisticated monitoring systems, subscribe to syndicated research, and track technological, economic, and social shifts. But if you’re buying the same reports as everyone else, you’ll have the same view as everyone else. The real work is investing in original research and developing your own point of view on how the world is changing. But that’s table stakes.
Fewer companies get beyond trend forecasting to actual scenario planning. That’s where you play these trends out to what the world might look like in five or ten years. In 2010, Intel should have been imagining a world where GPUs revolutionized computation and AI progress accelerated. Scenario planning should make you uncomfortable. That’s the point. If your low-grade anxiety doesn’t turn into real terror about what could upend you, you aren’t doing it right.
A very small number of companies move beyond scenarios to actual wargaming. This is where leadership teams role-play direct competitors and rivals. Competitors fight the same game; rivals change the game entirely. It’s not just about imagining how you’ll respond—it’s about stepping into the shoes of the players who might destroy you. Done well, wargaming can turn your terror into a playbook for action. It can open your eyes to ""no-regrets moves"": strategies that will strengthen your position regardless of which future materializes. Intel’s leadership team should have regularly held wargame sessions where one group represented Nvidia, another played ARM, another acted as a Chinese startup, and another embodied an entirely new computational paradigm.
Tommy Lasorda once said there are three kinds of people: those who make it happen, those who watch it happen, and those who wonder what happened. Intel’s decline is a story of watching. For two decades, its leaders spotted every trend, wrote every report, nodded sagely at every forecast—and remained spectators to their own disruption.
It’s entirely understandable to be feeling some anxiety right now. We live in anxious times. But don’t stay that way. In times like these, the companies that win are those that turn anxiety into terror, and terror into action. When you see the future coming at you, don’t just track it or talk about it. Step into the arena, feel the fear, and start swinging. The future won’t wait—and neither should you."
3,https://www.forbes.com/sites/petercohan/2025/09/24/nvidia-stock-up-1124-other-winners-and-whether-to-buy-nvda/,"Nvidia Stock Up 1,124%. Other Winners And Whether To Buy $NVDA","Sep 24, 2025, 01:31pm EDT",Peter Cohan,"Nvidia's $100B investment in OpenAI, linked to $350B in chip sales, raises circular financing concerns, drawing parallels to Cisco's risky dot-com era vendor lending.OpenAI's $1 trillion data center ambition heavily relies on Nvidia's investment, securing vital chip supplies for AI growth, though potentially at lower margins for Nvidia.The AI hardware sector, led by Nvidia, is experiencing a significant boom, with its segment of the Generative AI Stock Index surging 403%, driving overall market growth.

Nvidia shares are up 29% this year and 1,124% since the beginning of 2023. The company’s $100 billion investment in OpenAI raised questions about whether that rally would continue, according to the New York Times.
The most fundamental question is this: Is Nvidia using its own money – investments in Open AI, CoreWeave and others – to fuel growing demand for the company’s graphics processing units?
This brings back memories of the dot-com boom when Cisco Systems provided financing to startups to fuel growth in demand for the company’s networking equipment. Through its Cisco Capital arm, the company lent billions to money-losing telecom firms to buy Cisco’s products.
In the late 1990s, Cisco regularly reported 40% to 60% revenue growth and by 2000, 10% of the company’s $20 billion in revenue was attributable to such financing and leasing deals. When the dot-bubble burst in 2001, Cisco set aside $900 million in reserves for bad loans to those customers, reported the Los Angeles Times, and tightened its lending standards.
To be sure, if Nvidia's investments do not pay off, the GPU designer will not need to write off bad loans as Cisco did – but it could mark down the value of those investments on its balance sheet.
In the meantime, all of these investments make Nvidia and Open AI the lead companies in a V-shaped forward migration of AI technology providers.
Other segments of the industry – including chip makers and their suppliers, data center technology and energy providers, database companies, and many others, noted my book Brain Rush – are benefiting from this ongoing boom as evidenced by the 157% rise in my Generative AI Stock Index (compared to a 116% rise in the Nasdaq).
OpenAI is planning to build $1 trillion worth of data centers – $300 billion worth will be supplied with help from Oracle and another $100 billion from the Nvidia deal, reported the Wall Street Journal.
""Everything starts with compute,"" OpenAI CEO Sam Altman said in a statement. ""Compute infrastructure will be the basis for the economy of the future, and we will utilize what we're building with Nvidia to both create new AI breakthroughs and empower people and businesses with them at scale.""
If OpenAI – which last fall said it expected to lose $44 billion through 2029, noted the Journal – can find the cash to finance these data centers, the industry will keep expanding.
That may keep shares of Nvidia, Oracle, and many other participants in the AI ecosystem rising. If not, look out below.
An Nvidia spokesperson declined my request to comment.
Nvidia plans to invest $100 billion in OpenAI in $10 billion chunks. For every gigawatt of Nvidia-supported data centers OpenAI builds, Nvidia will invest $10 billion. For every $10 billion Nvidia invests, OpenAI will spend $35 billion on Nvidia chips, according to New Street Research analyst Pierre Ferragu whose work was featured in The Economist. OpenAI will pay for the chips through a blend of 71% in cash and 29% stock, noted NewStreet Research.
One worrisome aspect of this deal is its lower margins for Nvidia. “That arrangement reduces Nvidia’s typical margins for cutting-edge chips,” wrote the Journal, while the chip designer will benefit from ensuring steady GPU demand.
This deal also lowers OpenAI’s cost of capital due to Nvidia’s relatively sterling credit rating. Prior to this deal, OpenAI bought thousands of Nvidia’s chips through cloud service providers and neo-clouds – which buy chips and develop data center clusters – “renting them out at a premium,” the Journal reported.
These data center builders borrow money to finance their operations. Data center deals associated with money-losing companies such as OpenAI were financed at interest rates as high as 15%; whereas the interest rates for deals backed by Microsoft ranged from 6% to 9%, the Journal wrote.
Nvidia – which generated $72 billion in free cash flow over the last four quarters – seems to have ample interest free funds for financing its Open AI deal. It remains to be seen whether the chip designer will fund all $100 billion of its OpenAI commitment.
Mark Twain reportedly said history does not repeat itself, but sometimes it rhymes. Will Nvidia’s huge bets on money-losing companies such as OpenAI rhyme with Cisco’s aggressive lending to unprofitable telecom service providers?
It is too early to tell. However, Cisco’s lending during the dot-com boom “highlights the lengths to which one of the darlings of Wall Street would go to keep reporting revenue increases,” noted the Los Angeles Times.
For Cisco, a key question was whether the company booked revenue for these financing deals too soon. In some of the company’s deals, such as those of American Metrocomm and Digital Broadband Communications, Cisco recorded sales only when the customers began repaying the loans, the Los Angeles Times wrote.
In other cases, such as Rhythms NetConnections  – an Englewood, Colo. based Internet access provider owned by a then-major Cisco shareholder Bill Stensrud – “Cisco booked sales at the outset -- even before a penny had been collected and even when the customers were teetering toward a bankruptcy filing,” reported the Los Angeles Times.
Rhythms was unprofitable – reporting a loss of $36 million on $1 million in 1998 sales. But Cisco shipped $20 million of its equipment to Rhythms prior to the company’s April 1999 IPO. By August 2001, Rhythms had filed for bankruptcy leaving $30 million in unpaid debt to Cisco, according to the Los Angeles Times.
By 2003, Cisco was chastened. “Cisco recognized that there were risks involved” in its earlier strategy, the company said in a statement to the Los Angeles Times, “but the decision to proceed was made for sound business reasons and based on analysis of the facts that existed at the time.”
An economist who studied the telecom industry and reviewed documents about Cisco’s lending practices reached a sobering conclusion.
By making loans to so many distressed companies, “these guys were operating under a fallacy,” Massachusetts Institute of Technology economist Jerry Hausman told the Los Angeles Times. For a company to increase sales this way, he added, it’s like “you’re pumping yourself up on steroids,” Hausman added.
The takeaway? Investors should keep a close watch on how much of Nvidia’s revenue is coming from customers in which the chip designer has invested. Moreover, it’s worth investigating the accounting policies for recognizing revenue from such deals and quantifying their value on Nvidia’s balance sheet.
When Nvidia announced its investment in OpenAI, shares in other technology providers rose as well. Other stocks – notably Taiwan Semiconductor Manufacturing, memory supplier SK Hynix, STMicro, Infineon and BE Semiconductor – gained ground, reported CNBC.
My Generative AI Stock Index, about which I wrote in a June 2014 Forbes post, aims to capture the stock market performance of companies in the following industry segments:
Overall, the GAISI has risen 157% between the beginning of January 2023 – the month ChatGPT reached 100 million subscribers – and September 24, 2025. Here is the average return for the four segments of the industry value network during that period:
The top performing industry segment was generative AI hardware – of which Nvidia (+1,124%)  and liquid cooling provider Vertiv (1,012%) – were the leading stocks. The worst performing sector was AI consulting – of which Gartner dropped most – down 23%.
Analysts gave mixed opinions about Nvidia’s investment. The good news is the investment assures things get built; the bad news is 'circularity’ – which I interpret to mean Nvidia is buying its own products through a third-party.
“On the one hand this helps OpenAI deliver on what are some very aspirational goals for compute infrastructure, and helps Nvidia ensure that that stuff gets built,” Bernstein analyst Stacy Rasgon told Reuters. “On the other hand the 'circular’ concerns have been raised in the past, and this will fuel them further,” Rasgon added.
Several analysts see the deal adding between $400 billion and $500 billion to Nvidia’s revenue. For example, “We estimate this deal ultimately translates into ~$400B of NVDA revenue over a multi-year period,” UBS analyst Timothy Arcuri said in a note Monday, reported Barron’s.
Another analyst shares Rasgon’s concerns about circularity and sees both Nvidia and OpenAI being better off. “So Nvidia has the demand for the chips, but are also investing, hoping that OpenAI itself becomes a good investment,” Creative Strategies CEO Ben Bajarin told the Times.
Circularity has led to problems in prior tech booms. If OpenAI can keep convincing investors to provide more capital, Nvidia has a chance of reporting better-than-expected results. Otherwise, AI could become a bubble that bursts."
4,https://www.forbes.com/sites/saibala/2025/09/23/nvidia-powered-diligent-robotics-is-optimizing-last-mile-tasks-in-healthcare/,Nvidia-Powered Diligent Robotics Is Optimizing ‘Last-Mile’ Tasks In Healthcare,"Sep 23, 2025, 08:30am EDT","Dr. Sai Balasubramanian, M.D., J.D.","With rapid advancements in artificial intelligence and technology, there are numerous opportunities to improve key aspects of the healthcare value chain, ranging from workflow automation to supply chain optimization and increased patient engagement. Diligent Robotics is working on an often overlooked portion of the healthcare industry: last mile operations.
Specifically, the company’s flagship robot, Moxi, is helping medical teams with routine tasks that would otherwise require dedicated human focus and time, such as transporting goods, delivering lab samples, and distributing medical supplies or medications, all while seamlessly integrating itself into existing spaces and workflows.
Moxi was built with functionality and ease-of-use as priorities, with a robotic arm to aid with intricate tasks such as navigating elevators and doors, multiple safety sensors and a storage box to provide ample space to carry payloads across physical spaces. Beyond just the physical form, Moxi is powered by cutting-edge artificial intelligence, designed to display social intelligence (e.g., the robot will automatically detect a busy elevator and navigate to prevent bumping into others) and continuously learn from its users. This also helps the bot solve for logistical concerns, such as space constraints, navigating around immovable objects, and the adapting to dynamic environments.
Importantly, Diligent is working closely with Nvidia to power its intelligence platform by using state-of-the-art robotics training hardware. Nvidia, perhaps most well known for GPUs and the significant part the company has played in the recent AI boom, is a novel innovator in the robotics space. The company’s Jetson platform for edge AI and robotics enables advanced generative AI solutions to be deployed at scale.
David Niewolny, senior director and global head of business development for healthcare at Nvidia, discusses how the world is entering into a new era of robotics and edge computing, enabling massive potential. In the context of Moxi, Niewolny enthusiastically explains that “there is huge opportunity to be able to significantly improve workflows and there are so many other opportunities in the healthcare landscape outside of just traditional clinical workflows.” Last mile delivery and easing routine tasks is one such non-traditional opportunity.
Why is all of this important?
The entire field of humanoid robots is taking off rapidly, as companies are eager to help humans augment their workflows and daily routines with intelligent hardware that can actively learn. Take for example Tesla’s Optimus robot, which is meant to be a “bi-pedal, autonomous humanoid robot capable of performing unsafe, repetitive or boring tasks.” Or another example is Figure.AI’s humanoid bot, which is being built with the intention of addressing labor shortages throughout a variety of industries. While these have incredible potential, they are also being proposed as generalized models for generalized applications.
Diligent, on the other hand, is currently focused on addressing a very real need of the hour: a crumbling healthcare system.
In healthcare, labor shortages are escalating quickly. Specifically, nursing shortages are one of the hardest problems that hospitals face today, and are often the key reason why hospitals cannot see patients in a timely manner. Even if a hospital has beds available, nurses have to actually staff those beds for patients to be admitted, often making them the rate-limiting step for patient care. The American Nursing Association explains that “staffing solutions must evolve to cope with the full weight of the health care system, and flexible staffing plans are a huge factor in successfully accomplishing this goal.” Thanks to significant leaps in technology, flexible solutions can now include advancements in robotics.
The problem that Moxi aims to solve is not just about helping an organization be technology forward; it is about helping nurses and medical teams practice at the top of their licenses and spend time with patients rather than doing otherwise routine tasks. The real return on investment is better patient outcomes and increased access to care."
5,https://www.forbes.com/sites/greatspeculations/2025/09/23/mrvl-stock-vs-nvidia/,MRVL Stock vs. NVIDIA,"Sep 23, 2025, 10:15am EDT",Trefis Team,"Marvell Technology stock recently jumped 12% in a single week, driven by growing optimism about its custom AI accelerators and optical sensitivity chips. This performance raises a key question for investors: how does Marvell now compare to its peer, NVIDIA (NASDAQ: NVDA)?
NVIDIA presents superior revenue growth in key periods, enhanced profitability, and a comparatively lower valuation compared to Marvell Technology, indicating that investing in NVDA may be more advantageous.
See how Marvell’s financials compare with its peers, including NVIDIA.
MRVL designs and markets analog, mixed-signal, digital, and embedded integrated circuits, providing Ethernet solutions and storage controllers for HDDs and SSDs that support multiple system interfaces. NVDA supplies graphics, computing, and networking solutions for gaming, visualization, data centers, and automotive sectors, alongside a strategic partnership with Kroger Co.
See further revenue details:
See further margin details:
However, do these figures provide the entire picture? Read Buy or Sell NVDA Stock to determine if NVIDIA’s advantage is sustainable or if Marvell Technology still has strategies to deploy (see Buy or Sell MRVL Stock).
This is one perspective on evaluating stocks. If you seek an upside with less volatility than holding an individual stock, consider the High Quality Portfolio. It has comfortably outperformed its benchmark—a combination of the S&P 500, Russell, and S&P MidCap indexes—and has achieved returns exceeding 91% since its inception. Why is that? As a group, HQ Portfolio stocks provided better returns with less risk versus the benchmark index; less of a roller-coaster ride, as evident in HQ Portfolio performance metrics.
Regardless of how appealing the numbers are, stock investing is never without its bumps. There’s risk involved that must be considered. Read NVDA Dip Buyer Analyses and MRVL Dip Buyer Analyses to see how these stocks experienced declines and recoveries in past occurrences.
Investing in a single stock without comprehensive analysis can be risky. Consider the Trefis Reinforced Value (RV) Portfolio, which has outperformed its all-cap stocks benchmark (combination of the S&P 500, S&P mid-cap, and Russell 2000 benchmark indices) to produce strong returns for investors. Why is that? The quarterly rebalanced mix of large-, mid-, and small-cap RV Portfolio stocks provided a responsive way to make the most of upbeat market conditions while limiting losses when markets head south, as detailed in RV Portfolio performance metrics."
6,https://www.forbes.com/sites/iainmartin/2025/09/22/nvidia-keeps-minting-new-coreweave-style-ai-data-center-unicorns/,Nvidia Keeps Minting New Coreweave-Style AI Data Center Unicorns,"Sep 22, 2025, 01:41pm EDT",Iain Martin,"“I’ve never seen a startup take off like that before.” That was Nvidia’s cofounder and CEO Jensen Huang’s take on Nscale, a tiny London-based startup that pivoted from mining crypto to winning deals to power OpenAI’s data centers in just two years.
Nscale has emerged as one of the biggest winners from the $200 billion of deals announced last week as part of President Donald Trump’s state visit to the United Kingdom. The startup, which is one of a new crop of “neocloud” companies like Coreweave that run the GPU-powered datacenters underpinning the current AI boom, landed a $500 million investment last week from Nvidia, and a deal with the chip giant and OpenAI to build one of its massive “Stargate” data centers in the U.K.
As Huang took to the stage with British Prime Minister Keir Starmer in London Thursday night he name checked Nscale’s CEO Josh Payne. “Three months ago, when I met Josh, they were zero billion dollars [revenue]. Today, Josh is zero billion dollars [revenue], but with potential,” said Huang. “In just three months, we have helped Josh set up Nscale and they are underway to scaling up to 300,000 GPUs around the world…and 60,000 here in the U.K.”
That grand a buildout would put the little-known startup on par with Nasdaq-listed CoreWeave which claimed to be operating some 250,000 in GPUs in its March IPO prospectus. Coreweave’s stock market value has leapt to $60 billion since then, and just six years after its own pivot from crypto mining.
Huang appears to have even more ambitious plans for Nscale. In a video shared on Linkedin last week, he handed Nscale’s CEO a bottle of Johnnie Walker whisky he said was signed with the quip: “Zero to $50 billion in 18 months.” Nvidia and Nscale declined to comment on the timeframe for the startup to deploy 300,000 GPUs, hit this valuation, or the current valuation at the time of this $500 million investment.
In an August interview with Forbes, Nscale CEO Payne declined to say how many GPUs the company had currently deployed but said it had 40 megawatts of AI capacity online. That’s far less than rival Coreweave which has 1.3 gigawatts of capacity and announced plans for a U.K. datacenter of its own last week. Meanwhile Amsterdam-listed operator Nebius said it had plans to sign deals to access over 1 gigawatts of electricity by the end of 2026.
Nvidia’s Huang has emerged as a key booster of the companies trying to meet the seemingly limitless demand for compute from the AI sector. With origin stories in crypto, many of these firms might have seemed unlikely partners for the world's most valuable company a few years ago. But now they could be critical to its future. Tech giants like Google, Amazon and Meta are huge customers for Nvidia but also have advanced programs to design and build their AI chips which one day could rival, or replace, Nvidia’s. Neoclouds help hedge Nvidia’s reliance on sales to the existing giants of cloud computing.
Nscale’s Payne told Forbes in August that it also got its start in mining cryptocurrencies. The company in August 2023 was “spun out” of an equally obscure Australian crypto miner Arkon Energy that had flirted with a SPAC listing. After the split, Nscale was left with a data center in a remote northern Norwegian town, Glomfjord, that like Coreweave and Crusoe, was retooled from mining crypto to powering AI startups. Arkon’s website is now offline and Nscale spokesperson Tom Broughton of the Hoffman Agency said that it now remained only as a holding company for Nscale stock.
“I look forward to making a fortune off you,”
With demand for AI data centers booming, Nscale raised $155 million from investors in December 2024 with some of the financing coming from the listing of convertible notes on the International Stock Exchange Group, a backwater stock market based on Guernsey, a tiny island in the English Channel. The company hired data center veterans with experience from Iron Mountain, Google and Amazon Web Services along with Imran Shafi, the British government’s former director of AI opportunities.
In July, Nscale broke into the big leagues with a deal to build OpenAI’s first “Stargate” AI data center in Norway’s Arctic circle as part of a deal with Norwegian billionaire Kjell Inge Røkke’s Aker industrial conglomerate. “We operate in Norway exclusively today. We’re doing deployments in about five countries over the space of the next six months and our intention over the next two years is to be in about 15 European countries,” Payne told Forbes in August.
Currently Nscale’s website only has details of its Glomfjord data center, which it says operates with just 30 megawatts of capacity. “Nscale has multiple data centres including Nscale owned and colocated. Due to security and client sensitivities, we do not share the details of our full global footprint,” said Broughton on behalf of Nscale.
In addition to its deal with OpenAI to build a 30,000 GPU “Stargate” facility in the U.K., Nscale separately signed a deal for Microsoft to lease an unfinished 50 megawatt data centre in south west England it had only bought months earlier. These projects are a fraction of the size of the original “Stargate” project, a 4.5 gigawatt data center, that OpenAI is building in Abilene, Texas, with Oracle and Crusoe.
Despite its Australian origins, and backing from mostly American “opportunistic hedge fund investors,” Nscale is pitching itself as an European “sovereign” alternative to Silicon Valley’s hyper-scalers. “Europe lacks AI infrastructure that’s European sovereign, that’s something we’re working fiercely to solve,” said Payne.
Nscale wasn’t the only tech bet that Huang talked up on his U.K. tour. The Nvidia cofounder said he planned to invest over $2.7 billion in digital bank Revolut, self-driving car Wayve and other British startups. “I look forward to making a fortune off you,” he told founders at the London event.

"
7,https://www.forbes.com/sites/antoniopequenoiv/2025/09/22/nvidiaopenai-deal-chipmaker-investing-100-billion-in-chatgpt-maker/,Nvidia/OpenAI Deal: Chipmaker Investing 100 Billion In ChatGPT Maker,"Sep 22, 2025, 02:31pm EDT",Antonio Pequeño IV,"Chip designer Nvidia will invest up to $100 billion in OpenAI, the companies announced Monday, creating a partnership that will focus on the development of artificial intelligence data centers and AI superintelligence.
OpenAI will use at least 10 gigawatts worth of Nvidia systems for infrastructure used to train AI models (comparatively, 10 gigawatts could roughly power five Hoover Dams).
As the Nvidia systems are deployed, the chip designer will invest $100 billion into OpenAI, with the first gigawatt of Nvidia’s tech slated to go online in the second half of 2026.
Nvidia CEO Jensen Huang told investors last month the construction of one gigawatt of data center capacity costs between $50 billion and $60 billion, with Nvidia’s chips and systems accounting for about $35 billion of that cost.
Nvidia shares are up 3.6% to $41.89 as of Monday afternoon, nearing their highest point of the year.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
$500 billion. That is how much OpenAI could be valued following an upcoming $6 billion secondary share selloff, which could make the AI darling the world’s most valuable private company.
Training and running AI models requires energy-intensive infrastructure, and with an increasing number of OpenAI users (700 million weekly) alongside competition from Elon Musk’s xAI and other firms, OpenAI has to go big on datacenters to meet demand. OpenAI CEO Sam Altman said in Monday’s announcement, “Compute infrastructure will be the basis for the economy of the future” as OpenAI looks to one day deploy AI superintelligence, a hypothetical AI system with intelligence beyond the scope of any human.
Nvidia joins Microsoft as one of a few companies providing services and products to OpenAI in addition to investing in the startup. Microsoft has invested some $13 billion into OpenAI and is expected to receive an equity stake in the AI giant when it transitions into a for-profit corporation. OpenAI is still leading the valuation race among competitors. Elon Musk’s xAI will reportedly raise $10 billion in a funding round that will value the company at $200 billion. If it secures the funds, xAI will be valued above Anthropic’s $183 billion.
Trump Says Licenses Should ‘Maybe’ Be Pulled From Networks He Claims Are Critical Of Him (Forbes)
‘Blatant Censorship’: Late Night Hosts React To Kimmel Being Taken Off Air (Forbes)"
8,https://www.forbes.com/sites/paulocarvao/2025/09/22/ai-red-lines-nvidia-and-openai-100b-push-and-uns-global-warning/,Nvidia/OpenAI $100 Billion Deal Fuels AI As UN Calls For Red Lines,"Sep 22, 2025, 09:23pm EDT",Paulo Carvão,"Nvidia’s $100 billion investment in OpenAI made headlines Monday, along with a U.N. General Assembly petition demanding global rules to guard against dangerous AI uses.
The two developments are just the latest moves in a years-long debate that started shortly after ChatGPT’s launch, when a group of researchers issued an open letter calling for a pause in AI development.
The question remains: accelerate or pause AI? What most miss, however, is that it’s not an either-or. The goal should be to balance speed and security.
Nvidia and OpenAI are joining forces on what, according to Nvidia founder and CEO Jensen Huang, is “the biggest AI infrastructure project in history.” The two companies announced plans for OpenAI to build at least 10 gigawatts of Nvidia-powered computing systems, an expansion on par with constructing several of the world’s largest data centers. Nvidia is committing up to $100 billion to the effort, with the first facilities expected to begin operating in the second half of 2026.
Sam Altman, CEO of OpenAI, said, “building this infrastructure is critical to everything we want to do. This is the fuel that we need to drive improvement, drive better models, drive revenue, drive everything,” a statement emblematic of the industry’s accelerationist bias.
The partnership aims to ease access to the massive computing power needed to train advanced models, one of the biggest constraints in AI today. By substantially expanding its hardware base, OpenAI hopes to push forward capabilities like advanced reasoning, multimodal processing that blends text and images and systems capable of sustaining deeper interactions and handling extensive documents. Both companies argue that this will lower the cost of delivering intelligence while accelerating the transition from experimental research to broad commercial use.
But at what cost?
At the opening of the U.N. General Assembly, Nobel laureate Maria Ressa stood before world leaders with a petition signed by more than 200 prominent figures, including former heads of state, scientists and technologists, urging the creation of binding global “red lines” for artificial intelligence. Backed by over 70 organizations, the initiative calls for an enforceable international agreement by 2026 to ban certain high-risk applications of AI, from deepfake impersonations and self-replicating systems to mass surveillance and autonomous weapons.
Their message is clear: without global rules, AI’s most dangerous uses could spiral beyond any single nation’s control. The letter highlights that “AI could soon far surpass human capabilities and escalate risks such as engineered pandemics, widespread disinformation, large-scale manipulation of individuals including children, national and international security concerns, mass unemployment, and systematic human rights violations.”
“For thousands of years, humans have learned—sometimes the hard way—that powerful technologies can have dangerous as well as beneficial consequences. With AI, we may not get a chance to learn from our mistakes, because AI is the first technology that can make decisions by itself, invent new ideas by itself, and escape our control. Humans must agree on clear red lines for AI before the technology reshapes society beyond our understanding and destroys the foundations of our humanity,” said Yuval Noah Harari, historian, philosopher and the 2014 bestselling author of Sapiens: A Brief History of Humankind.
Stuart Russell, another signatory of the red lines letter and distinguished professor of computer science at the University of California, Berkeley, captured the concerns animating the initiative: “The development of highly capable AI could be the most significant event in human history. It is imperative that world powers act decisively to ensure it is not the last.”
The industry announcement reflects AI’s rapid acceleration and the race to push its capabilities further; the call at the United Nations highlights the parallel urgency to define guardrails before those capabilities run unchecked. Together, the two capture the dual reality of AI in 2025, staggering investment and innovation on one side, and mounting global pressure for rules and restraint on the other.
The temptation is to view these announcements as opposite poles: industry bent on acceleration and policymakers calling for restraint. But framing it as a binary, pause or accelerate, misses the real challenge. As I wrote in March 2023 in “Do Not Pause AI, Accelerate Ethics,” this framing is a false dichotomy. The world does not face a choice between unleashing AI’s potential and safeguarding humanity from its risks. We must do both in tandem.
The key lies in incentives. Industry will continue to race ahead if the rewards for speed outweigh the costs of failure. Policy, market structures and public trust mechanisms must realign those incentives so that companies compete not only on the scale of their models but also on their safety, transparency and accountability. If directed wisely, the same competitive forces that drive innovation can just as powerfully drive responsibility.
AI has the potential to expand productivity, accelerate discovery and solve problems once thought intractable. But it also threatens to spread disinformation, deepen inequality and destabilize societies. The choice is not between acceleration and pause. It is whether we can align incentives so that speed and safety move together, ensuring AI’s benefits outweigh its risks."
9,https://www.forbes.com/sites/daniellechemtob/2025/09/19/forbes-daily-intel-shares-spike-after-deal-with-rival-nvidia/,Forbes Daily: Intel Shares Spike After Deal With Rival Nvidia,"Sep 19, 2025, 07:55am EDT",Danielle Chemtob,"andForbes Daily,
Forbes Staff.
Fans have long complained about the pains of buying tickets to their favorite shows on sites like Ticketmaster and Live Nation. Now, the FTC is stepping in.
The agency filed a lawsuit against the two ticketing giants on Thursday, alleging they deceived artists and consumers by allowing resellers to purchase large numbers of tickets and charge higher prices on the secondary market. The FTC said the companies failed to stop the reselling practices because they allow the firms to “triple dip,” initially collecting fees from the mass ticket buyers, then again from the buyers when they resell tickets, and finally from the consumers who purchase the resale tickets.
It’s the latest in a slew of recent regulatory problems for the industry, and comes after the Justice Department sued to break up Live Nation last year.
Nvidia is making a $5 billion investment in rival chipmaker Intel, leading Intel’s shares to jump 23% Thursday on the heels of the Trump Administration’s recent announcement that it would take a 10% stake in the company. The firms will collaborate to develop custom data center and personal computer products, as Nvidia’s stock purchase will give it around a 4% stake in the struggling company.
President Donald Trump said that “maybe” licenses should be taken away from TV networks with shows critical of him, his latest suggestion about deplatforming critics after ABC’s removal of Jimmy Kimmel Live! from the air. Meanwhile, legal experts have raised concerns that the Trump Administration unconstitutionally influenced the show’s suspension through “jawboning,” a practice in which officials use strongman tactics to try and indirectly silence critics or influence private companies’ actions.
President Donald Trump continued his efforts to fire Federal Reserve Governor Lisa Cook on Thursday, asking the Supreme Court to step in and pause a lower court ruling that blocked the termination. Two federal courts have now ruled that Cook can only be fired for an issue related to her job performance, and recent reports have suggested she did not falsify details in her mortgage applications as the Trump Administration has claimed.
Volvo isn’t a giant in the auto industry, but its sustainability ambitions are: By the end of the decade, the Swedish carmaker wants 35% of all content used to make a new vehicle to be recycled. Even in the midst of the Trump Administration’s upheaval of the U.S. auto industry, carmakers are competing globally to show consumers they’re making big changes to cut carbon pollution and go electric.
A week after its billion-dollar fundraise, AI data storage and analytics company Databricks said Thursday it is launching an accelerator program for seed and pre-seed stage AI startups that use its tools and platform. The company, now valued at $100 billion, wants to go beyond standard venture capital, offering startups mentorship from its executives and helping them grow.
The Trump Administration offered few details on the substance of the TikTok deal, but Chinese officials said it would involve a new U.S. TikTok entity licensing algorithms from ByteDance—an arrangement that suggests TikTok will still rely, at least in part, on its Chinese parent company. That’s causing bipartisan alarm in Congress, as the law requiring ByteDance to sell TikTok or face a ban placed explicit restrictions on any operational ties to ByteDance after a sale.
Tom’s Watch Bar has expanded to 18 locations in six years, despite competing with established sports bar chains like Buffalo Wild Wings, Beef ‘O’ Brady’s and Walk-On’s Sports Bistreaux. Its strategy is to cater to fans on their way into or out of stadiums: Of Tom’s 18 locations, 15 are adjacent to a stadium or arena.
The AI revolution hasn’t materialized on Main Street just yet. Three business owners who spoke with Forbes said they are largely using the technology when it comes bundled inside the software they already use, or as a faster way to search the internet. Some said it helps them speed up routine tasks, like responding to Google reviews.
As CEO and cofounder of the consumer packaged goods spinoff of the iconic high-end restaurant Carbone, Eric Skae has the task of selling more affordable access.
With its $7 to $11 jarred sauce, Carbone Fine Food has become the fastest-growing pasta sauce brand in America—with sales expected to reach $100 million by the end of this year, nearly doubling 2024’s performance.
But Skae, who previously ran the retail business for another New York red sauce cathedral—Rao’s in East Harlem—and led its 2017 acquisition, is enjoying his second act. Carbone Fine Food’s 18 sauces, from arrabiata to pizza sauce, are now sold across 27,000 stores in America and he’s hungry for more expansion. “This will be the No. 1 brand in America,” says Skae, who believes sales will one day reach $1 billion.
For the past year, around 154,000 new households purchased a jar of Carbone sauce each month. Carbone Fine Food’s household penetration is still small, at around 2.7%, but Skae says he is closing the gap with Rao’s, which has 23.7% of the market share. “It’s me against Rao’s,” Skae adds.
The tomato sauce aisle was already fairly crowded before Carbone joined the fray—with $4 billion in annual sales and more than 500 brands sold nationwide. The bevy of startups that have launched in the past few years include a hot sauce extension from Truff (founded by two Forbes 30 Under 30 alums), a line from the New York City restaurant Rubirosa, as well as the Cavu-backed Sauz with $21.8 million in investor funding.
WHY IT MATTERS “Brands are trending towards becoming more premium, and cleaner ingredient lists like Carbone Fine Food and Rao’s have are driving this trend,” says Forbes staff writer Chloe Sorvino.
MORE Meet The New Big Cheese At The World’s Largest Mozzarella Maker
Since Meta banned a surveillance firm called Cobwebs from gathering intelligence across its platforms in 2021, ICE has spent millions on the company’s tools. Critics say the software enables warrantless online surveillance:
Over $5 million: The amount ICE has spent on its tools, including a $2 million purchase this week
$35,000 a year: The amount law enforcement in Virginia spends to access a Cobwebs tool for “AI face detection and AI face search”
‘Nothing new here’: “For years law enforcement across the nation has leveraged technological innovation to fight crime. ICE is no different,” ICE spokesperson Mike Alvarez told Forbes
The IRS is continuing to roll out drafts of some 2026 tax forms, and there will be a few key updates for those who fill out a W-9, which is used by independent contractors. New instructions on the draft form clarify that sole proprietors of businesses must use their Social Security number and disregarded entities must use the owner’s taxpayer identification number. There’s also a new category of exempt recipient for sales of digital assets.

New York Attorney General Letitia James is accusing a business of bribing an auditor to reduce its tax bill. What kind of business is it?
A. Casino
B. Strip club
C. Marijuana dispensary
D. Liquor store
Check your answer.
Thanks for reading! This edition of Forbes Daily was edited by Sarah Whitmire and Chris Dobstaff."
10,https://www.forbes.com/sites/digital-assets/2025/09/19/intel-stock-surges-25-as-ai-alliance-with-nvidia-confirms-americas-chip-strategy/,Intel Stock Surges 25% As AI Alliance With Nvidia Confirms America’s Chip Strategy,"Sep 19, 2025, 11:12am EDT",Clem Chambers,"I’ve argued all year that Intel wasn’t a dead dog; it was a strategic asset. Back then, I wrote that the Trump administration’s policy push and capital commitment to on-shore chipmaking made failure politically and economically “impossible.” That wasn’t bravado: it was reading the map.
I explained the thesis in a five-minute rant on YouTube on August 15.
Why does any of this matter? Because semiconductors are the beating heart of modern industry. The U.S.-China contest is no longer academic or diplomatic; it’s industrial. Most advanced fabs sit in Taiwan and China, and that concentration is a single point of failure for everything from phones to fighter jets. The moment geopolitics turns “spicy”, supply chains fracture, and companies that rely on offshore fabrication – think the current Apple configuration – have to scramble to adapt or fail. U.S. policy is now explicitly about re-industrialization: rebuild “the workshop of the world” onshore, and you regain leverage. That is the context in which Intel’s strategic role becomes obvious.
This is a chart I drew in a YouTube video on August 23:
Here we are now:
Intel is one of the only large-scale chipmakers with the current capacity and footprint to be a backbone of on-shore fabrication in the U.S. and Europe. It takes years to build factories that can make chips. For years, the market treated Intel as a stodgy legacy business. I took a contrarian view: Intel is a $53 billion manufacturer of some of the most technically complex artifacts homo sapiens have ever made. Its challenge was – and is – execution; its opportunity is geopolitics and AI. Those two forces change the valuation landscape overnight.
Evidence arrived fast: first, it was a SoftBank investment; then a U.S. government buy-in; then, on September 18, 2025, Intel and Nvidia announced a collaboration on AI infrastructure and solutions. Intel’s share price exploded, up 25% in a single session. That move crystallizes the narrative: Intel is now squarely in the AI infrastructure story, not just a commodity fabs operator.
When the narrative changes, so do the hedges. For years, funds shorted Intel as a hedge against Nvidia or other long tech positions. That market structure kept Intel low – you might say artificially – but the market does what it does because it knows best. However, when the underlying strategic game changes, when governments underwrite capacity and AI demand explodes, those hedges unwind and reprice the business.
Look at the valuation gap. Intel trades at roughly 2x price-to-sales. Compare that with AMD at roughly 10x, ARM 35x, and Nvidia 33x. Even modest multiple convergence delivers substantial upside. At 4x sales, Intel doubles; at the “new normal” of 10x, it becomes transformational wealth creation. That’s the dream, but it’s backed by a shift in macro policy and industrial dynamics, as well as the fundamentals of clearly stated U.S. policies.
This is not merely a trade idea; it’s a thesis based on the strategic reasoning unfolding everywhere, from shipbuilding to strategic minerals. The U.S. needs secure onshore fabs. The strategy aims at investing in capacity, supply-chain resilience, and aspects like rare earth access. Intel sits at the intersection of two mega-trends: the AI revolution and the U.S.-China clash of superpowers. The result is a rare asymmetric payoff: a beaten-down giant with structural cash flows, on the front line of national policy, and with a valuation gap large enough to trigger a rapid re-rating.
When Intel was $20, I sounded crazy. Now the White House is talking about industrial policy, and the market is finally catching up. Something for nervous investors to remember: when governments decide a sector is strategic, markets follow. And when markets follow, old hedges break and new winners appear.
“Look, I’ve made some great investments in my life – the BEST. And Intel? Intel is one of the greatest. Absolutely phenomenal…”
Who might say that? The question for me is not who, but when."
11,https://www.forbes.com/sites/tiriasresearch/2025/09/19/nvidias-ai-factory-vision-comes-into-focus-with-rubin-cpx/,Nvidia’s AI Factory Vision Comes Into Focus With Rubin CPX,"Sep 19, 2025, 02:25am EDT",Jim McGregor,"Correction, Sept. 24: This article has been edited to correct details about the flow of data through a system containing both a Rubin AI and Rubin CPX GPUs.
At the InfraAI Global Summit’25, Nvidia announced a new member to its upcoming Vera Rubin data center AI product family. The Rubin CPX will complement the standard Rubin AI Graphics Processing Unit (GPU) in providing high-value inference content generation at a more cost-efficient price. More importantly, it fits into the data center infrastructure Nvidia has designed for a multi-AI GPU data center.
Tirias Research has consulted for Nvidia and other AI companies mentioned in this article.
Tirias Research has long forecasted the need for a variety of AI inference accelerators from companies like AMD, Intel, Nvidia and anyone else developing AI semiconductor solutions. Like any other data center workload, no two AI models are the same. As consumers and enterprises adopt AI and AI models continue to evolve, there will be an opportunity to optimize the hardware around an AI model or groups of models. However, GPUs will remain one of the best solutions for both AI training and AI inference processing for two key reasons, which Nvidia is building upon with the Rubin CPX announcement.
The first reason is the nature of the semiconductor industry. The tech industry swings like a pendulum. When new technology is introduced, there is a period of rapid innovation, or in the case of AI, daily innovation. When the pace of innovation slows, standards emerge. At this point, it makes sense to consider optimizing a functional task into a dedicated chip known as an application-specific integrated circuit (ASIC). In many cases, that function may eventually be integrated into a host processor like a Central Processing Unit (CPU) or GPU. However, developing a custom chip or functional block can take three or more years. With new models and ways to process these models changing rapidly, the GPU is a more practical solution than an ASIC for most IA applications.
The second reason is the ability of GPUs to be partitioned to handle multiple AI models concurrently. There is a myth that a transition from AI training to AI inference is coming in the near future. With the deployment of models like OpenAI’s ChatGPT models, Google’s Gemini, Microsoft’s Copilot, DeepSeek’s R and V series models, Anthropic’s Claude, Perplexity AI and countless others, the vast majority of AI processing across the industry is already inference processing. If such a line existed, it would have been crossed several years ago. With the programmable efficiency of AI GPUs and the buildout of GPU-enabled data centers, the vast majority of AI workloads, especially generative AI and agentic AI, are running on GPUs because they are the most efficient option.
At GTC 2025, Nvidia introduced several key technologies for building AI-centric data centers. These included the NVL144 rack design, KV Cache, Dynamo, data center blueprints and enhancements to the company’s NVLink, Spectrum-X, and Quantum-X networking technologies. KV cache allows for the storage of computed key and value tensors to be used in subsequent AI generation and between GPUs. Dynamo is an open-source inference framework for planning and routing AI workloads in the data center, essentially an data center workload orchestrator. The NVL144 rack design and Nvidia networking technologies form the infrastructure of the data center. And the data center blueprints running on Omniverse provide a digital twin for the design, construction, and operation of an AI data center, or AI factory as Nvidia refers to them. Now, Nvidia has introduced the Rubin CPX, an AI GPU inference accelerator optimized to do specific functions exceptionally well. With Rubin CPX, Nvidia takes another step in designing an AI factory that can be optimized for specific AI functions.
Nvidia refers to Rubin CPX as a context inference accelerator designed for very complex AI tasks, such as millions of lines of software development, hours of video generation, and deep research. The Rubin CPX works in conjunction with the Vera CPU and Rubin AI GPU. The Rubin CPX ingests large volumes of contextual data to generate the first token and KV cache, which require high compute performance. Then, the Rubin AI GPU receives KV cache to begin generating the subsequent output or content. This generational phase is more reliant on memory and networking bandwidth, while the contextual phase is dependent on high compute. As a result, the Rubin CPX, while built on the same Rubin AI GPU architecture, is designed differently than the Rubin AI GPU, with 128GB of GDDR7 memory plus hardware encode and decode engines to support video generation. The Rubin CPX is capable of 30 petaFLOPs of performance using the NVFP4 data format and a 3x increase in attention acceleration compared to the GB300 NVL72, and of for processing a one-million-plus-token context window. The memory and single-die Rubin CPX architecture changes result in a reduction of approximately 20 petaFLOPS of overall performance versus Rubin AI GPU but an increase in contextual token generation efficiency.
Nvidia plans to offer the Rubin CPX integrated into a single rack with the Vera CPU and Rubin AI GPU called the Vera Rubin NVL144 CPX, and as a separate accelerator rack to the standard Vera Rubin NVL144 rack. The Vera Rubin NVL144 CPX rack will be configured with 36 Vera CPUs, 144 Rubin AI GPUs, and 144 Rubin CPXs with 100 TB of high-speed memory and 1.7 PB/s of memory bandwidth. The result is eight exaFLOPs of NVFP4 performance, a 7.5x increase over the GB300 NVL72 rack. According to Nvidia, a $100 million CAPEX investment could result in up to a $5 billion return, a 30x to 50x return on investment (ROI). The dual rack solution will offer the same performance with an additional 50 TB of memory.
The Rubin CPX is an AI GPU inference accelerator platform focused on high-end generational applications. We will likely see other versions of the Nvidia AI GPU architectures that concentrate on different segments of AI processing, such as smaller AI models, in the future. We could even see various versions of the CPX solutions optimized for even more specific applications. AI is not a single uniform workload, and optimizing the accelerator is just one step in the process. More importantly, Nvidia continues to focus on the entire data center as a single system to ensure that all potential performance bottlenecks are addressed, resulting in the highest possible performance efficiency and ROI.
A common question is whether the industry needs an annual cadence for new AI GPUs. At this point, the answer is that it needs new AI GPUs every year just to keep pace with the innovation in AI. Additionally, it requires optimized GPUs for the various types of AI workloads."
12,https://www.forbes.com/sites/karlfreund/2025/09/19/cadence-design-and-nvidia-team-to-create-ai-data-center-digital-twin/,Cadence Design And Nvidia Team To Create AI Data Center Digital Twin,"Sep 19, 2025, 11:42am EDT",Karl Freund,"The data center business is in a bit of a panic. Demand is skyrocketing. Rack power requirements have increased from ~12KW per rack to over 125 KW in just the last year. Now they are preparing for a Gigawatt rack in the next two to three years (Nvidia Rubin Ultra).  When the power goes up, the cooling demands do as well. Enter liquid cooling, with cooling distribution units demanding space where the computer racks used to go. The modern AI factory doesn’t look like any data center we know.
And there is no way the complex interactions of all the mordern data center components behaviour, power, and cooling can be simulatred by humans, especially as workload demands in AI can raise or drop power dramatically every second.
Enter AI and Digital Twins.
I’ve covered Digital Twins and Nvidia Omniverse and have  also covered the Cadence Reality Digital Twins Platform here on Forbes and in a white paper I co-authored with Dr. Jonathon Koomey. There is no question in my mind that this type of simulator will become an absolute requirement for future data centers, especially AI Factories.
Jensen Huang said at the Cadence Live event last year that Nvidia uses the Cadence Reality platform to simulate Nvidia’s own supercomputing data centers. Cadence has adopted Nvidia Omniverse as the Digital Twin collaboration platform. (Disclosure: Cadence Design and Nvidia, like many companies in the semiconductor space, are clients of Cambrian-AI Research.)
One of the disclosures at the recent AI Infra Summit in Sunnyvale, Ca., was that while Cadence has engaged some 99% of the semiconductor industry with simulation and AI, only 20% of data center systems are currently being simulated before construction or in operation. As this infrastructure prepares for the future of AI, its a pretty safe bet that this number will increase dramatically over the next three to five years. After that, Cadence believes its has more opportunities in simulating drug discovery.
At the Summit, Nvidia and Cadence announced they have developed a comprehensive physics model to enable simulation of a DGX SuperPOD with GB200 GPUs. This work brings the Cadence Reality DC platform to contain over 14,000 devices (servers, networking, storage, power, cooling, etc.)  from over 750 vendors in its library of reference designs and workflows. This database  enables simulation of customized accurate operational behaviour of the air- and water-cooled components in the AI Factory, resulting in faster design, lower risk, better operational efficiency of Gigawatt AI Factories
Adding support for the DGX SuperPOD to Cadence Reality DC is an important piece of the puzzle, but is only the start of a more comprehensive capability that will extend to NVL72, Rubin, and other roadmap technologies that populate the AI Factory.
In a few years, designers will wonder how the early days of AI data centers was ever possible without rigorous physics-based simulation.
Disclosures: This article expresses the opinions of the author and is not to be taken as advice to purchase from or invest in the companies mentioned. My firm, Cambrian-AI Research, is fortunate to have many semiconductor firms as our clients, including Baya Systems BrainChip, Cadence, Cerebras Systems, D-Matrix, Esperanto, Flex, Groq, IBM, Intel, Micron, NVIDIA, Qualcomm, Graphcore, SImA.ai, Synopsys, Tenstorrent, Ventana Microsystems, and scores of investors. I have no investment positions in any of the companies mentioned in this article. For more information, please visit our website at https://cambrian-AI.com."
13,https://www.forbes.com/sites/timbajarin/2025/09/18/the-nvidia-intel-partnership-signals-a-fundamental-market-realignment/,The Nvidia-Intel Partnership Signals A Fundamental Market Realignment,"Sep 18, 2025, 04:38pm EDT",Tim Bajarin,"The recently announced Nvidia-Intel partnership represents more than a simple investment deal. It’s a strategic recalibration that reveals the current power dynamics in the semiconductor industry and hints at where both companies see their future competitive advantages.
Nvidia’s $5 billion investment in Intel may seem modest against Nvidia's current market capitalization, but the symbolic weight is enormous. For Intel, facing well-documented financial pressures and foundry challenges, this capital injection provides crucial breathing room. However, the real story lies not in the dollars but in what each company is positioning itself to gain strategically.
Perhaps most telling is Intel's decision to adopt Nvidia's proprietary NVLINK protocols. This move effectively signals Intel's acknowledgment that its direct competition with Nvidia in large-scale AI infrastructure has reached a strategic dead end. Rather than continuing to burn resources in an increasingly difficult battle, Intel is pivoting to where it can still win: AI inference, edge computing, and the vast installed base of x86 systems.
This move isn't surrender—it's strategic repositioning. Intel retains significant advantages in areas where power efficiency, cost optimization, and integration with existing infrastructure matter more than raw AI training performance.
For Nvidia, this partnership solves a different problem entirely. While the company dominates AI training and high-end inference, it has faced challenges penetrating the broader PC and small server markets where Intel's ecosystem relationships run deep. By partnering rather than competing directly in these segments, Nvidia gains access to distribution channels and market segments that would have required years of investment to develop independently.
This partnership suggests we're entering a new phase of semiconductor industry evolution, where pure competition gives way to strategic specialization. Rather than every major player attempting to build comprehensive, competing stacks, we're seeing recognition that partnership can create more value than head-to-head competition in every segment.
The implications extend beyond these two companies. AMD, Qualcomm, and other players will need to reassess their strategies in light of this new Intel-Nvidia axis, potentially accelerating similar partnership discussions across the industry.
The success of this arrangement will ultimately depend on execution. Intel's ability to leverage this partnership to rebuild its foundry capabilities and market position, and Nvidia's capacity to extend its AI leadership into new market segments without diluting its core advantages.
What's clear is that both companies have made a calculated bet that collaboration will deliver better outcomes than continued direct competition. In an industry where the costs of staying competitive continue to escalate, that may prove to be the most brilliant strategic move either company has made in years.
Disclosure:  Nvidia, Intel, Qualcomm and AMD subscribe to Creative Strategies research reports along with many other high tech companies around the world."
14,https://www.forbes.com/sites/siladityaray/2025/09/18/nvidia-is-investing-5-billion-in-rival-intel/,"Nvidia Will Invest $5 Billion In Rival Chipmaker Intel, Following $10 Billion Deal With U.S. Government","Sep 18, 2025, 08:11am EDT",Siladitya Ray,"andZachary Folk,
Forbes Staff.
Nvidia announced it will acquire $5 billion worth of chipmaking rival Intel’s shares and the two companies will collaborate to develop custom data center and personal computer products, boosting Intel’s stock as Wall Street indexes hit record highs Thursday.
In a statement, Nvidia said the companies will work together to combine Nvidia’s advanced AI and computer chips with Intel’s x86 CPU architecture, which powers most desktops, servers and laptops around the world.
As part of the deal, Intel will build new consumer chips that integrate its x86 CPUs with Nvidia’s RTX GPU chips and this will be used to power “a wide range of PCs.”
Intel will also make a custom x86 CPU for use in datacenters that Nvidia will integrate into its AI machines and sell to its customers.
As part of the deal, Nvidia will purchase around $5 billion worth of Intel’s common stock, at $23.28 per share—lower than Intel’s closing price of $24.90 on Wednesday—granting it a 4% stake in the struggling chipmaker.
The two companies will also work to integrate NVIDIA’s NVLink—which allows the AI giant to connect multiple GPUs together—with Intel’s chips.
Speaking at an online press conference on Thursday afternoon, Nvidia CEO Jensen Huang said the investment “reflects how excited we are about this partnership.”
The Trump administration previously took a 10% stake in Intel—but Huang said the government “had no involvement in this partnership.”
“They would have been very supportive, of course,” Huang added, claiming that Commerce Secretary Howard Lutnick was “very excited” about the partnership after they spoke about it.
The Nasdaq climbed 1.2% to reach a new record high on Thursday in the hours after the deal was announced, while the S&P 500 and the Dow Jones Industrial Average rose .7%  and .5% respectively.

Nvidia’s share prices rose steadily over the course of the morning, and were up 3.7% by Thursday afternoon. Intel surged as high as 29%—its single biggest day gain since October 1987. Rival AMD’s shares are down around 2.7% as the deal likely dents a critical moat the chipmaker had over Nvidia. Only AMD and Intel are licensed to manufacture x86 chips, however, this deal allows Nvidia’s powerful GPUs—which are used to power AI applications and graphically intensive tasks—to be integrated with Intel’s x86 CPUs.
Nvidia’s investment comes weeks after the Trump administration announced the U.S. government would take a 10% stake in Intel in a $10 billion deal—turning the government into the struggling chipmaker’s third-largest shareholder. The deal converted roughly $8.9 billion in grants from former President Joe Biden’s CHIPS Act into nonvoting shares. The chipmaker was flailing before the rare intervention from the government, as well as another $2 billion investment from Japanese tech investment bank SoftBank announced days earlier. It was also a reversal of fortunes for Intel’s CEO Lip-Bu Tan, who President Donald Trump previously criticized as “highly CONFLICTED” after facing questions from Senate Republicans over his alleged ties to companies connected to the Chinese Communist Party. Trump called for Tan’s resignation in August, just weeks before announcing the deal."
15,https://www.forbes.com/sites/johnkoetsier/2025/09/16/humanoid-robotics-company-raises-1-billion-for-nvidia-chips-ai-data-collection-production/,"Humanoid Robotics Company Raises $1 Billion For Nvidia Chips, AI Data Collection, Production","Sep 16, 2025, 01:07pm EDT",John Koetsier,"Humanoid robotics company Figure AI has raised “more than $1 billion” in a series C financing round, the company said Tuesday. The goal: expand robot production, build out the company’s Nvidia GPU infrastructure to accelerate training and simulation and expand data collection of humans working and living.
Ultimately, that means manufacturing the shippable hardware, or robots, building the AI engine that will make its robots smart, and capturing the training data its AI engine will require.
“Figure’s goal is to solve general robots,” CEO Brett Adcock said Tuesday in a YouTube video. “For the first time in history, the right technologies exist to make that possible.”
Figure has extremely high ambitions. Its robot, Figure 02, was just the second humanoid robot to get a paying job late last year. Earlier this year, Adcock announced that the company had a goal of shipping 100,000 humanoid robots over the next four years and added that Figure’s customer list included “one of the biggest U.S. companies.”
Those ambitions don’t end at the warehouse or factory. Figure regularly shares videos of its robots working in kitchens, serving drinks, loading a dishwasher, folding clothes and doing other domestic tasks. Core to both is Helix AI, the intelligence that Figure is building into each of its robots. Helix AI gives the Figure robots adaptable real-world intelligence, enabling them to understand what objects are even if they’ve never seen them before, and enabling smart or reasonable actions based on that knowledge.
That’s core to shipping useful humanoid robots that don’t have to be explicitly trained on every tiny edge case of helping or working in a home or factory.
“This is a really hard problem,” Adcock says. But, he adds, the “team is in place, the robots are built, and the path ahead is clear.”
A billion-plus dollars in funding will help, of course. Intel, Nvidia, LG, Salesforce, Qualcomm and T-Mobile all participated in the round via their investment arms, but the funding round was led by Parkway Venture Capital.
Figure is not the only humanoid robotics company to raise a billion dollars lately. China-based UBTech reportedly raised that amount earlier this month, although the company has not confirmed it. UBtech has the distinction of having won the largest publicly known contract for humanoid robots at 90.5115 million yuan, or $12.7 million USD, from Miyi Automotive Technology Co.
The future for humanoid robots could be bright. The potential market is $40 trillion of physical labor that is done every year, or half of global domestic product.
There are some naysayers, however. Bren Pierce, robotics OG and CEO of Kinisi Robotics, is at least partially one of them.
“What are the legs useful for,” he asked on a recent TechFirst podcast with me. “Obviously you’re going to have legged robots … [but] are we actually missing one of the key features to enable them, which is the AI?”
His argument: legs are flashy, but wheels work just fine in factories and warehouses, and are potentially much better for heavy loads and long battery life. Also, hands are the real challenge.
All that said, dozens of manufacturers are charging full speed ahead into fully humanoid robots, and cashing in along the way.
In a market with dozens if not hundreds of competitors, Figure sees itself as the market leader.
“This milestone is critical to unlocking the next stage of growth for humanoid robots, scaling out our AI platform Helix and BotQ manufacturing,” Adcock said in a statement. ""Support from new partners, alongside the continued backing of our existing investors, reflects both Figure's position as the market leader and a shared belief in a future where this technology becomes a natural part of daily life.""
With all this investment, that could be sooner than we realize."
16,https://www.forbes.com/sites/siladityaray/2025/09/15/china-says-nvidia-violated-antitrust-law-chipmakers-shares-slip-15-in-premarket/,"China Says Nvidia Violated Antitrust Law, Chipmaker’s Shares Slip 1.5% In Premarket","Sep 15, 2025, 07:50am EDT",Siladitya Ray,"China's top competition regulator on Monday said a preliminary investigation found chipmaker Nvidia was violating the country’s antitrust laws, in an announcement that could further inflame tensions between the U.S. and China at a time when officials from both countries are meeting to discuss a potential trade deal in Madrid.
In a brief announcement, China’s State Administration for Market Regulation said it has determined after a “preliminary investigation” that Nvidia “has violated the Anti-Monopoly Law of the People's Republic of China.”
The agency said the probe was opened after the chipmaking giant allegedly violated conditions outlined in the agency’s approval of its 2020 acquisition of networking devices maker Mellanox Technologies.
The regulator said it has decided to conduct “further investigation into the matter in accordance with the law,” without mentioning any timeline or additional details.
The antitrust regulator had first mentioned plans to investigate Nvidia’s $6.9 billion acquisition of Mellanox in December last year.
Nvidia’s shares are down more than 1.5% in premarket trading to $175.11 early on Monday.
$17 billion. That is the amount of revenue Nvidia generated from China in its last fiscal year, which ended in January, around 13% of the company’s total sales. In May this year, Nvidia CEO Jensen Huang told the Stratechery Podcast that he estimates Nvidia could earn $15 billion from sales of its H20 AI chips to China.
The announcement of the expanded antitrust probe comes on the same day a U.S. contingent led by Treasury Secretary Scott Bessent is meeting with Chinese officials in Madrid for trade talks. Speaking to reporters ahead of Monday’s talks, Bessent said both sides are “very close” to an agreement regarding the fate of the social media platform TikTok, but Beijing’s approval of any such deal could be linked to its demands for trade concessions. “Our Chinese counterparts have come with a very aggressive ask. We will see if we can get there. At present, we are not willing to sacrifice national security for a social media app.” The deadline for TikTok to secure a U.S. buyer is set to expire on Wednesday, but Trump has indicated he may extend it once again. On Sunday, Trump told reporters, “We're negotiating TikTok right now…We may let it die, or we may, I don't know, it depends, up to China…It doesn't matter too much. I'd like to do it for the kids that like it.”
In recent months, President Donald Trump has threatened to retaliate against regulatory efforts by foreign countries targeting U.S. tech giants. Earlier this month, Trump attacked the European Union after it announced it was hitting Google with a $3.5 billion fine. In response, the president claimed on Truth Social that with this fine, the bloc was “effectively taking money that would otherwise go to American Investments and Jobs.”"
17,https://www.forbes.com/sites/greatspeculations/2025/09/12/how-amd-nvidia-broadcom-can-ride-oracles-455b-cloud-surge/,"How AMD, Nvidia, Broadcom Can Ride Oracle’s $455B Cloud Surge","Sep 12, 2025, 05:30am EDT",Trefis Team,"On Monday, during its earnings announcement, Oracle (NYSE:ORCL) delivered a striking revelation that surprised even the most optimistic analysts. The company’s remaining performance obligations (RPO) for its cloud division surged 359% year-over-year, reaching $455 billion, compared to only $138 billion in the previous quarter. This astonishing increase caused Oracle’s stock to rise nearly 36% in Wednesday’s trading, marking its largest single-day gain in many years. Consider RPO as Oracle’s guaranteed revenue stream—contracts that have already been finalized and are just pending execution. This is not speculative thinking or hopeful forecasts; it represents money that customers have already committed. CEO Safra Catz stated that Oracle anticipates securing several additional multi-billion-dollar customers soon, pushing its RPO beyond the half-trillion-dollar threshold within months. Here’s a brief overview of how the major AI silicon players will reap benefits from Oracle’s significant initiative. How Oracle Stock Surges 3x To $900?
For Oracle, turning these obligations into revenue hinges on the speed at which it can expand its infrastructure. This necessitates electricity, regulatory approvals, and high-performance GPUs, all of which are still in persistent global shortage. The corporation also increased its capital expenditure projection to $35 billion for fiscal 2026—a 65% rise that highlights the urgency of its AI infrastructure expansion. It’s no surprise that this exceptional guidance caused shares of Nvidia, Broadcom, and AMD to climb between 2% and 10%, as these chip manufacturers form the foundation of nearly every AI implementation. Each additional Oracle contract should correspond to increased demand for their semiconductors. Furthermore, in contrast to cloud competitors like Google, AWS, or Microsoft, Oracle is not developing its own bespoke AI chips. Instead, it has relied on GPUs from Nvidia (NASDAQ:NVDA) and AMD (NASDAQ:AMD).
Oracle maintains a robust partnership with AI compute standard bearer Nvidia, which provides most of the GPUs currently utilized in Oracle Cloud Infrastructure. Oracle stands out among hyperscalers by offering so-called “bare metal” instances alongside Nvidia GPUs. This affords customers direct, exclusive access to high-end physical chips like A10, A100, H100, H200, and the flagship Blackwell chips, eliminating virtualization overhead. Such a setup enables businesses to optimize performance for model training and inference. Oracle’s AI supercomputers deploy these GPUs in densely packed, liquid-cooled data centers engineered for energy efficiency and throughput. This close collaboration allows Oracle to cater to some of the most sophisticated AI applications globally at scale. Nvidia GPUs have been the backbone of Oracle’s AI infrastructure, and this partnership may prove vital as Oracle carries out its extensive AI contracts in the upcoming years. Nvidia Stock To Fall 50% As AI Cycle Turns?
Oracle has also been diversifying away from solely relying on Nvidia. In June, Oracle and AMD revealed that the AMD Instinct MI355X GPUs would be available on OCI, providing more than double the price-performance of the previous generation. Oracle plans to deploy as many as 131,072 MI355X GPUs, empowering enterprises to build, train, and perform inference on massive AI models with competitive economics. This collaboration guarantees that AMD will secure a significant portion of Oracle’s AI expenditures. The MI355X, based on AMD’s new CDNA 4 architecture and TSMC’s 3nm process, marks AMD’s most significant advancement yet in AI accelerators. For Oracle, this strategy broadens its hardware alternatives, diminishes reliance on a single supplier, and is expected to enhance negotiation leverage with Nvidia for GPUs, thereby controlling overall spending.
Oracle’s revenue outlook is increasingly linked to AI inference, which involves executing trained models at scale for real-time user queries. Buy or Sell AVGO Stock at $360? As AI transitions from development to widespread use, inference workloads are likely to significantly exceed training requirements, resulting in a recurring, utility-like demand stream. This is precisely where Broadcom and its custom application-specific integrated circuits (ASICs) could emerge as substantial beneficiaries. While GPUs are predominant in training, inference at scale is expected to increasingly favor custom chips that provide better cost and energy efficiency. While Oracle still primarily utilizes GPUs for AI tasks, it appears that ASICs will undoubtedly play a more substantial role moving forward. This trend helps clarify why Broadcom’s stock surged nearly 10% on Wednesday—surpassing the gains of Nvidia and AMD, which increased by 2% to 4%—as investors speculate that Broadcom’s custom chips will capture part of Oracle’s workload.
The Trefis High Quality (HQ) Portfolio, comprising 30 stocks, has a demonstrated track record of consistently outperforming its benchmark, including the S&P 500, Russell, and S&P midcap. What drives this? Collectively, HQ Portfolio stocks have produced superior returns with lower risk compared to the benchmark index, delivering a smoother experience, as reflected in HQ Portfolio performance metrics."
18,https://www.forbes.com/sites/greatspeculations/2025/09/11/google-tpus-vs-nvidia-gpus/,Google TPUs Vs Nvidia GPUs,"Sep 11, 2025, 05:54am EDT",Trefis Team,"Google has begun placing its Tensor Processing Units (TPUs) in data centres run by smaller cloud providers that have long relied on Nvidia's GPUs. This is more than a product rollout—it's a calculated move in the AI infrastructure chess game.
The effects extend across the AI ecosystem, creating both opportunity and uncertainty for investors. While individual AI names like Google and Nvidia could see heightened volatility as the landscape shifts, those seeking exposure with less single-stock risk might prefer diversified approaches. The High Quality Portfolio, for example, has comfortably outperformed its blended benchmark—a mix of the S&P 500, Russell, and S&P MidCap indexes—and has delivered returns exceeding 91% since inception, letting investors participate in these structural shifts across multiple quality companies instead of betting on a single winner. Separately, see – How Oracle Stock Surges 3x To $900?
TPUs are Google's purpose-built AI chips designed specifically for machine-learning tasks. While Nvidia's GPUs are versatile general-purpose processors, TPUs act like specialized scalpels optimized for AI workloads. The latest Ironwood chip delivers 42.5 exaflops of compute per pod with more than 9,200 chips per unit—“more than 10x improvement” over Google's previous generation.
Also, take a look at – How Does Google Stock Rise 2x?
This puts Nvidia in an uncomfortable spot:
Still, Nvidia enjoys a massive ecosystem and deep relationships. The key question: Can Google win meaningful share before Nvidia responds?
This is not just about chips—it’s about control of the AI infrastructure stack. Google is effectively telling the market, “You don’t need Nvidia’s ecosystem anymore.” Yet in reducing dependence on Nvidia, customers may increase dependence on Google’s technology.
For buyers, the trade-off is clear: would you rather be locked into Nvidia or Google?
Separately, look at – Are Nvidia Investors Ignoring This Big Risk?
Google’s custom TPU push is a bid to challenge Nvidia’s dominance. But this isn’t a simple one-on-one duel. With companies like Broadcom, AMD, and Marvell advancing their own chips, AI hardware is now a multi-player race.
That’s good for the industry—more competition, more choice, and potentially lower costs. For investors, it signals a heating AI infrastructure market with no guaranteed single winner.
Nvidia isn’t finished, but the competition is tougher than ever.
While the AI arms race intensifies, you could explore the Trefis Reinforced Value (RV) Portfolio, which has outperformed its all-cap benchmark (a combination of the S&P 500, S&P MidCap, and Russell 2000) to deliver strong returns. Why? The quarterly rebalanced mix of large-, mid-, and small-cap RV Portfolio stocks provided a responsive way to benefit from upbeat markets while limiting losses when conditions turned, as shown in RV Portfolio performance metrics."
19,https://www.forbes.com/sites/karlfreund/2025/09/09/nvidia-announces-rubin-cpx-gpu-to-speed-long-context-ai/,Nvidia Announces Rubin CPX GPU To Speed Long-Context AI,"Sep 09, 2025, 11:00am EDT",Karl Freund,"In an industry-first, Nvidia has announced a new GPU, the Rubin CPX, to offload the compute-intensive “context processing” off another GPU. Yep, now, for some AI, you will need two GPUs to achieve maximize performance and profit.  I would be surprised if the competition doesn’t follow suit; the benefits are tremendous. (Nvidia, like many other semiconductor firms, is a client of my company, Cambrian-AI Research.)
Rubin CPX is designed to handle very long input to LLMs, over 1 million tokens. Not many applications needs such a long context to be encoded for AI processing. But those that do desperately need a better hardware platform that can handle the job; encoding is an extremely compute-intensive process. Modern GPUs are designed for the memory- and network-bound generation phase of LLMs, with expensive  HBM memory that isn’t needed for decoding.  As Nvidia has been explaining the different needs of these two phases over the last couple years, and highlighted the benefits of disaggregating inference to different GPUs in their MLPerf announcement, many of us began wondering when someone would build a solution tailored for the pre-fill job. Nvidia did just that with CPX, but you may have to wait a year to get it.
Nvidia estimates that some 20% of AI applications are waiting for the emergence of the first token (Time to First Token, or TTFT) while the GPUs crunch on the decoding work. That can take perhaps five to 10 minutes for 100,000 lines of code. For multi-frame, multi-second videos, pre-processing and per-frame embedding increases latency rapidly; 10–20 seconds or longer is common, varying with video length and LLM capabilities. That is why video LLMs typically are only used today to create short clips.
And as the chart above contends, an AI Factory’s profit increases with performance. Even if the competition were to give away their GPUs for free, today’s GB200 NVL72 can increased token profit by near four fold over the free competition.  And one should assume an even better ROI with Blackwell Ultra and Rubin next year.  Of course, it will be even better when you add the new CPX to a rack of Rubin GPUs.
If you use the Blackwell GPUs in today’s rack more intelligently, dividing the context and generation across different GPUs, you can increase the performance by three fold with the same cost and energy profile. Now, if you add a GPU that is optimized for long-context decoding, lowering cost by using lex expensive memory, and increasing the attention acceleration by another 3X, the total inference performance can increase by another factor of three.
Nvidia plans to make the Rubin CPX available in two forms.  For new installations requiring long-context AI, the Vera Rubin NVL144 CPX adds the CPX chips onto the compute tray housing the Vera CPU and the Rubin GPU, tripling performance of next year’s Vera Rubin.
But hey!  You just payed $3M for a shiny new NVL144!  No worries. Nvidia will sell you a separate rack full of the right amount of CPX nodes to attach to your Rubin rack. This will increase performance of the Vera Rubin rack from 3.6 Exaflops to 8 EF, and supports up to 150TB of fast GDDR7 memory.
Here’s Nvidia’s updated roadmap through Feynman in 2028. While Nvidia did not announce that the Rubin CPX would give rise to a Rubin Ultra CPX, it can probably be assumed. Nvidia announces products over a year out these days, as data center operators need to plan for future upgrades and expansions. For example, now planners can make room for a CPX rack next to the Rubin racks installed before CPX availability.
This announcement represents a major milestone in the software and hardware needed to efficiently process inference queries, disaggregating inference processing into two workloads with a GPU tailored for each in the case of long-context windows greater than one million tokens. Others like Google and AMD will certainly evaluate the methods used here, and decide if their customers would benefit.
Disclosures: This article expresses the opinions of the author and is not to be taken as advice to purchase from or invest in the companies mentioned. My firm, Cambrian-AI Research, is fortunate to have many semiconductor firms as our clients, including Baya Systems BrainChip, Cadence, Cerebras Systems, D-Matrix, Esperanto, Flex, Groq, IBM, Intel, Micron, NVIDIA, Qualcomm, Graphcore, SImA.ai, Synopsys, Tenstorrent, Ventana Microsystems, and scores of investors. I have no investment positions in any of the companies mentioned in this article. For more information, please visit our website at https://cambrian-AI.com.

"
20,https://www.forbes.com/sites/greatspeculations/2025/09/05/nvidia-stock-to-fall-50-as-ai-cycle-turns/,Nvidia Stock To Fall 50% As AI Cycle Turns?,"Sep 05, 2025, 05:00am EDT",Trefis Team,"Nvidia (NASDAQ: NVDA) has indisputably led the AI boom. Its GPUs are considered the gold standard for training extensive AI models, resulting in sales growth from $27 billion in FY'23 to an anticipated $200 billion this fiscal year. In addition to selling the highest performance chips, the company's CUDA software ecosystem has effectively retained customers. While Nvidia is expected to continue its reign as the leader in AI hardware for years ahead, its stock valuation of nearly 40x forward earnings signifies not only its leadership but also expectations of ongoing, multi-year growth.
This renders it vulnerable: even a slight decrease in demand or a structural shift in the AI lifecycle – from focusing on training workloads to inference – could undermine investor confidence and cause significant declines. Historical events highlight this risk – following the Covid-era surge in GPU demand for gaming and cryptocurrency, inflation and reduced demand led Nvidia shares to plummet almost 66% from peak to trough, in contrast to a mere 25% decline for the S&P 500. This is important with current market context – see S&P 500 Index To Crash 8%?. Nvidia’s volatility suggests a similar drop could occur if the current AI growth phase slows. Of course, if you are looking for an upside with reduced volatility compared to holding an individual stock, consider the High Quality portfolio (HQ). It has consistently outperformed its benchmark that includes all 3: S&P 500, Russell, and S&P Mid-Cap indexes—and has achieved returns surpassing 91% since its inception, as evident in HQ performance metrics.
Over the past two years, companies have invested vast resources into developing AI models. Training these enormous models usually requires a concentrated effort that demands significant computing power, with Nvidia being the largest beneficiary, as its GPUs are widely seen as the fastest and most efficient for these operations.
However, the AI landscape may be evolving. Incremental performance improvements are waning as models become larger, while access to high-quality training data is becoming a limiting factor – much of what is readily available online has already been utilized in current models. Collectively, these factors imply that the most demanding phase of AI training might start to plateau. Adding to the uncertainty, the economics of the GPU market remains challenging, as numerous Nvidia customers continue to struggle to yield significant returns on their substantial AI investments. Despite that, Nvidia stock has been strong - see NVDA Dip Buyer Analyses to examine how the stock has bounced back from sharp dips in the past.
In contrast, inference involves applying trained models to new data in real-time and at scale. It is less intensive per task but occurs continually across millions of users and applications. As AI evolves, a larger portion of value creation could shift from training to inference. The challenge for Nvidia is that its growth has been primarily linked to training, where its high-end GPUs have a stronghold. Inference presents an opportunity for more mid-performance and cost-effective chip alternatives, as well as specialized offerings. Here’s a glimpse at some of the main competitors in the inference sector.
AMD has notably trailed Nvidia during the initial phase of AI development, but it could become a significant competitor to Nvidia in inference. Its chips are becoming increasingly competitive in performance while providing cost and memory advantages. Not all organizations require or can afford Nvidia’s premier GPUs. Many are likely to choose older Nvidia models or budget-friendly alternatives such as AMD’s (NASDAQ:AMD) MI series, which delivers solid performance for inference and fine-tuning of models. See How AMD stock surges to $330.
ASICs or Application-Specific Integrated Circuits are also gaining momentum. Unlike GPUs, which are flexible and programmable, ASICs are designed for a specific task, making them more cost- and power-efficient for inference workloads. The cryptocurrency sector offers a precedent: Bitcoin mining began with GPUs but swiftly transitioned to ASICs once scale and efficiency became vital. A similar trend could emerge in AI inference. Two companies likely to benefit from this shift are Marvell and Broadcom, both of which possess expertise in creating custom silicon for hyperscalers.
U.S. Big Tech players such as Amazon (NASDAQ:AMZN), Alphabet, and Meta are all designing AI chips. Amazon has focused on training-oriented chips, Meta began with inference and is expanding into training, while Google accommodates both with its TPU (tensor processing unit) lineup. For these hyperscalers, the goal is not necessarily to outdo Nvidia in the marketplace but to reduce costs, enhance bargaining power, and manage supply for their expansive cloud ecosystems. Over time, this translates to diminished incremental demand for Nvidia’s GPUs. In Q2, Nvidia reported that just two of its clients accounted for approximately 39% of total revenue, and it is highly plausible that these were major U.S. tech firms. This concentration makes Nvidia significantly more susceptible. If hyperscalers increasingly turn to proprietary silicon, even minor changes in purchasing behavior could lead to substantial revenue implications.
Chinese Players In China, firms like Alibaba (NYSE:BABA), Baidu, and Huawei are enhancing their AI chip initiatives. Reports last week indicated that Alibaba plans to introduce a new inference chip for its cloud division. The strategy is twofold: to facilitate inference at scale using its own technology stack, and to ensure a reliable supply of semiconductors in light of U.S. export restrictions. Currently, Nvidia’s GPUs are still anticipated to serve as the backbone of Alibaba’s AI training operations, but inference is poised to become the long-term growth driver for the firm.
Nvidia’s position is still robust due to its established ecosystem, substantial R&D investments, and supremacy in training. However, inference is expected to become the next growth engine for AI hardware, and the competitive landscape is significantly more crowded. Even a slight dip in growth could heavily impact the stock, considering how much future performance is already factored in. For investors, the critical question is whether Nvidia’s growth trajectory can align with the elevated expectations set by the market. If the economics of inference prove less advantageous than those of training, the stock could still face a “valuation reset” despite retaining its technological leadership.
The Trefis High Quality (HQ) Portfolio, which comprises 30 stocks, has a history of consistently outperforming its benchmark, inclusive of the S&P 500, Russell, and S&P midcap. What accounts for this? As a collective, HQ Portfolio stocks have delivered superior returns with reduced risk compared to the benchmark index; a smoother ride, as evident in HQ Portfolio performance metrics."
21,https://www.forbes.com/sites/johnwerner/2025/09/03/nvidia-and-todays-chip-landscape--what-is-google-doing/,Nvidia And Today’s Chip Landscape — What Is Google Doing?,"Sep 03, 2025, 11:31am EDT",John Werner,"After the dominant U.S. tech company with the largest market cap lost around 5% over the last couple of weeks, analysts are trying to figure out why, and what this augers for the company’s financial future. Much of the recent coverage cites wishy-washy export signals as having a major effect on Nvidia’s stock with the on-again, off-again nature of H20 sales to the Middle Kingdom shaking things up. But what about competition from U.S. companies?
To be clear, Nvidia isn’t going anywhere – its hold on the AI hardware market is dominant, with Blackwell and H100s and AI accelerators par excellence, the firm is towering above the crowd. But that said, there are signs of competition, and some of them are coming from unlikely places.
There’s Intel and its line of Gaudi chips, named after famed architect Antoni Gaudi and his flamboyant style of building. Experts explain that the Gaudi chips are not competitive with Nvidia’s GPUs in terms of power – rather, Gaudi is for cost-conscious customers looking for a pared-down chip at lower expense.
Another contender, though, is not a traditional chip maker.
It seems that Google is continuing to develop its own Axion line of processors running in what the company calls a “Titanium” ecosystem – a framework for hardware performance that sort of sits in the tech stack next to the processors.
Now, there’s not a lot of public information about what Google is doing with Axion online – at least not this year. There was an explosion of articles in late 2024 about Axion’s origin, and how it could threaten Nvidia’s monolithic dominance. But one of the very few indicators that this is still in play is a new article at The Information suggesting that Google people might want some kind of parley with Nvidia to talk about things.
There is, however, some other suggestion that Google might also be making inroads with its TPUs.
“While Chinese rivals have gotten some notice for recent efforts to compete with Nvidia, investors may want to pay more attention to a major domestic rival that doesn't get much buzz for its work in semiconductors,” writes Britney Nguyen Sept. 2. “Alphabet Inc. has closed the gap in the past year to become ‘the best alternative’ to Nvidia.”
Citing input from D.A. Davidson analysts, Nguyen theorizes how a “spinoff” of the AI accelerator division could be lucrative for the parent company. Specifically, Nguyen describes market interest in Google's sixth-generation Trillium TPUs, which were made generally available in December, and the seventh-generation Ironwood TPUs, which debuted this spring. Noting Anthropic’s hiring of TPU engineers, Nguyen suggests that Google is gaining ground in taking business away from other vendors.
So the TPUs are one alternative, and Axion is another. It’s helpful to note, though, that the TPUs are only made for Google’s own ecosystem, and not competitive across the entire market.
In the stock market, there’s no indicator that any competition is driving Nvidia stock down right now. After that share price cut, which seems generally related to export policies, the stock has gained about 1% in recent cycles. The suggestion of more robust domestic competition is a longer-term concept. When those in tech media report that Elon Musk and xAI might want to buy TPUs, or that a selection of customers are choosing some non-Blackwell chip builds, they’re pondering a longer time horizon, where the only constant in this kind of fast-paced market is change.
“There's little argument that chipmaker Nvidia has been the biggest beneficiary of the advent of artificial intelligence (AI),” wrote James Brumley at Motley Fool last week in a piece that I think articulates this well. “Its processors are the centerpiece of most of the world's AI computing platforms, and artificial intelligence-related products now account for the vast majority of Nvidia's sales. … the next five years aren't likely to look quite like the last five, though. The AI industry is maturing, and so are Nvidia's competitors. Indeed, even AI data center operators are now looking for very specific solutions for their unique needs. Names other than Nvidia will benefit from this ongoing evolution.”
Neither Intel nor Google features in Brumley’s recent analysis. Instead, he mentions chip rival Qualcomm, and two other lesser-known companies: Marvell, with its CXL (Compute Express Link) motherboard extenders, and NXP Semiconductors, which he describes this way:
“(NXP’s) i.MX RT700 mini neural processing unit paired with NXP's eIQ Neutron NPU AI/ML accelerator turns wearables, smart home devices, and even portable medical tech into true AI ‘edge computing’ products. At the same time, the company makes advanced driver assistance systems equipment, industrial automation solutions, and more.”
So – some of the change is based on new models and ways of building hardware systems. We know that edge computing is taking over as engineers find feasible ways to run more powerful LLMs on unconnected devices. Maybe the new world of the future will be reliant on vibrant unit-to-unit decentralized networks – and Nvidia’s brighter moments will be associated with obsolete top-down data center design. But data centers are still popping up, so again, this is a longer term anticipation.
We’ll be seeing more analysis of these trends in conferences and events planned in the MIT community through the fall. We’re closing out a year that brought us more than most of us could have imagined in terms of tech advances. What will the next five or ten years look like?"
22,https://www.forbes.com/sites/bill_stone/2025/08/31/nvidia--magnificent-7-too-hot-to-handle/,NVIDIA & Magnificent 7: Too Hot To Handle?,"Aug 31, 2025, 07:00am EDT",Bill Stone,"As the dominant supplier of artificial intelligence (AI) chips, NVIDIA (NVDA) has become the largest company in the world and within the S&P 500 index. Following NVIDIA’s earnings last week, it’s an appropriate time to revisit the attractiveness of large technology stocks with a special emphasis on NVIDIA.
The Magnificent 7, comprising Microsoft (MSFT), Meta Platforms (META), Amazon.com (AMZN), Apple (AAPL), NVIDIA (NVDA), Alphabet (GOOGL), and Tesla (TSLA), collectively account for nearly 34% of the S&P 500’s market capitalization. Eight of the ten largest companies in the S&P 500 are technology companies, including Broadcom (AVGO). Berkshire Hathaway (BRK/A, BRK/B) and JPMorgan Chase (JPM) are the only non-technology companies in the top ten.
Investors already appreciate the strong fundamentals of the Magnificent 7, and NVIDIA in particular, with stock returns far outstripping those of the S&P 500. The Magnificent 7 and NVIDIA have achieved total returns of 280% and 1,093%, respectively, since the start of the ChatGPT era at the end of 2022, while the S&P 500 has returned 75%.
Based on the past twelve months of earnings and consensus earnings estimates for the next twelve months, the price-to-earnings ratios for all the Magnificent 7 stocks are above the S&P 500. While this ratio indicates that most of these companies have a higher valuation on both measures, it doesn’t answer whether other factors justify that premium.
Another way to value companies is by their multiple revenues, which is called the price-to-sales ratio. All the Magnificent 7 are valued at price-to-sales ratios above the S&P 500, with NVIDIA at an almost twenty-seven times price-to-sales ratio.
Historical annualized growth in sales over the last three and five years has been impressive for most of the group, but downright amazing for NVIDIA.
Likewise, historical earnings per share growth has generally been impressive and exceptional, as seen in cases like NVIDIA.
The five-year annualized earnings per share growth rate of the Magnificent 7 demonstrates just how unique these companies have been when compared to the stock market as a whole.
Future earnings are the most crucial criterion when considering whether to buy or hold a stock. Consensus forecasts from Wall Street analysts still evidence significant optimism about NVIDIA’s earnings growth prospects, with 39% annualized growth expected from the world’s largest company. Separately, the sustainable growth rate for the group supports the future growth story. The sustainable growth rate theoretically indicates how fast a company can grow without borrowing additional funds while maintaining its existing capital structure.
The profitability metrics of most of the Magnificent 7 are impressive. Gross margins represent the profitability after all direct expenses, specifically the cost of goods sold, have been deducted from sales. Operating margins measure how much a company earns on a dollar of sales after paying for the variable expenses, or the percentage of sales converted into operating income. NVIDIA converted almost seventy cents of every sales dollar into operating profit over the last year. While NVIDIA’s gross margin was about 70% over the previous year, it improved to 75% in the most recent quarter.
Just looking at a simple high price-to-earnings ratio can be deceiving when analyzing a company with high returns on invested capital (ROIC). It is crucial to consider the return generated on the capital needed to fund a business, since a company with a sustainably higher ROIC should sell at a valuation premium.  To put some of the impressive return on invested capital figures for the Magnificent 7 into proper perspective, the ROIC for the S&P 500 is around 8%.
Furthermore, most companies in the Magnificent 7 generate substantial amounts of free cash flow, despite investing in growing businesses. The free cash flow margin indicates the percentage of revenue that the company can utilize or distribute to shareholders as free cash flow. NVIDIA is exceptional, with almost forty-four cents of every dollar in sales converted to free cash flow.
The free cash flow yield refers to the free cash flow per share generated by a business, expressed as a percentage of its stock price. Value investors, such as Warren Buffett, value a company based on the cash that can be distributed from a business over its lifetime, discounted back to the present. Free cash flow can be a good measure of the money available to owners. A free cash flow yield below that of the S&P 500, such as the Magnificent 7 and NVIDIA, can be seen as investors already pricing in superior growth and durability of free cash flow relative to the market.
NVIDIA is particularly notable given the massive increase in its market capitalization, which now exceeds $4.2 trillion, driven by the artificial intelligence spending wave. The consumer introduction of ChatGPT as an artificial intelligence tool in late 2022 coincided with a sharp rise in NVIDIA’s valuation, as the company became the dominant supplier of AI semiconductors.
There has been some concern about NVIDIA’s relatively meager beat of consensus earnings estimates last week and less exceptional guidance than has been typical. There were zero sales to China in the quarter due to geopolitical tensions, which made for more challenging comparisons. For example, data center sales, which were impacted by China, declined by 1% quarter-over-quarter, but were up about 12% quarter-over-quarter excluding China, according to TD Cowen. Despite NVIDIA’s less optimistic forward guidance than was hoped, a resumption in sales to China could add $2 to $5 billion in sales.
In the short term, NVIDIA’s dominance in high-end artificial intelligence computing appears untouchable, as its next-generation platform is on pace for meaningful production next year. Furthermore, the company continues to benefit from AI demand outstripping supply, alleviating concerns about potential pricing pressures.
It appears likely that AI will be a transformative tool for businesses and households, generating substantial economic benefits. Although it is still early, indications suggest that artificial intelligence adoption is continuing at a brisk pace.
The capital spending (capex) by AI-leaders, Alphabet, Amazon, Meta Platforms, and Microsoft, to support complex computations needed to support AI, has seen a massive rise since 2023. NVIDIA reaped significant revenue from this spending as the leading chip provider for AI.
Estimates are that capital spending by these four AI leaders will exceed $400 billion by 2026. What if demand for AI services doesn’t meet expectations or the firms are unable to monetize the services at a profitable enough level? Capital expenditures (capex) budgets can indeed be slashed if needed. This downside risk isn’t a forecast, but it must be considered when evaluating NVIDIA and all AI-connected technology stocks. Notably, these sizable capital expenditures are more sustainable than some past instances, because the companies are highly profitable and can self-fund rather than depending on the charity of lenders or additional capital.
Though it is nowhere in sight, one must always be wary of some competing technology eventually displacing NVIDIA as a dominant leader. Capitalism subjects companies to intense competition in their pursuit of profits, and the profits generated by technology leaders make them an attractive target.
The bullish case for large tech and NVIDIA is bolstered by the rise of artificial intelligence (AI). The mega-cap technology stocks, represented by the Magnificent 7, continue to produce earnings and free cash flow at impressive rates. Large technology stocks typically exhibit exceptional profitability compared to the average stock and should therefore trade at a premium valuation. The rise in the stock prices of the Magnificent 7, including NVIDIA, has been supported by robust fundamentals and not just a promise of future profitability.
Investors buy or hold companies for their future profits, not their past performance. The past is only significant to the extent that it informs the firm’s future prospects. While one needs to remain mindful of risk, the robust adoption rate of artificial intelligence, combined with companies’ ability to self-fund the massive costs required to build AI infrastructure, implies that the end is not yet here. Investors should be mindful of the risk associated with concentrated holdings, given the remarkable growth of some of these technology leaders.
Disclosure: Glenview Trust may hold stocks mentioned in this article within its recommended investment strategies."
23,https://www.forbes.com/sites/johnwerner/2025/08/30/china-blocks-import-of-nvidia-h20s/,China Blocks Import Of Nvidia H20s,"Aug 30, 2025, 11:11am EDT",John Werner,"There’s a new problem for Jensen Huang and the people at Nvidia, where a powerful American tech firm is going all-in on AI. The problem, according to recent reports, is that China is rethinking its import protocols for what Nvidia is selling, proving that, when it comes to Sino-U.S. trade policy, two can play at that game.
I’ll explain: earlier in the year, the U.S. had been banning Nvidia exports to China, even after the company had come up with a dedicated H20 chip that allowed Nvidia to forego offering China the biggest and best new tech. And the previous U.S. administration had eventually cleared those for export.
Then the Trump white house created a new requirement for an export license, cutting of that sale pipeline, and Nvidia was once again under the same pressure. Huang worked his magic, and soon, the H20s were allowed to ship to China again.
Prior to that change, Huang had cited a $4.5 billion write-down due to lack of access to the Chinese market. It’s worth mentioning, though, that even without the Chinese market, Nvidia had posted 69% year-over-year revenue growth to $44 billion for Q1, exceeding analysts’ estimates, with a corresponding 4% stock increase.
So now, it seems, it’s the Chinese who are getting cold feet about buying the same chips.
Recent reports indicate the Chinese have been wanting to create a national security review to evaluate the chip sales in question, and that they are worried about the chips having “backdoors,” even though Huang has assured them that this is not the case. Chinese officials have apparently recently asked companies like ByteDance, Alibaba and Tencent to stop buying the H20s. CNBC coverage Aug. 21 confirms that Nvidia has asked Amkor Technology and Samsung Electronics to stop H20-related production, due to the Chinese actions, citing news from The Information.
“A separate report from Reuters, citing sources, said that Nvidia had asked Foxconn — officially known as Hon Hai — to suspend work related to the H20s,” wrote Dylan Butts. “Foxconn did not immediately respond to a request for comment.”
If Jensen Huang has told the Chinese that there are no backdoors or tracking and locating gear in the chips, why would officials there be wary?
It turns out that this spring, according to AP reports, Rep. Bill Huizenga, R.-Michigan, and Rep. Bill Foster, D.-Illinois, introduced the Chip Security Act that would require the more powerful American chips to be outfitted with “security mechanisms” that sound a lot like what the Chinese are worried about. The stated purpose was to detect “smuggling or exploitation.”
The fact that the bill has stalled in Congress doesn’t alleviate Chinese fears.
So now, the shoe is on the other foot, and Nvidia is back in the proverbial doghouse.
Realistically, Nvidia is still making money: it’s the country’s biggest tech firm by market cap, even with Huawei able to make chips for the Chinese. But in past comments, Huang had explained to Trump and others how selling chips to China is good for America. He also cited efforts to develop an energy plan to power data centers.
""In order for this industry to thrive, we need to build these systems of course, but we also need a progressive growth and industry-oriented energy policy, which this president has really put his weight behind and I really appreciate that,"" Huang said back in April. And presumably, he’d be making the same points now, including that we should be selling hardware in China.
However, that’s if the Chinese will let Nvidia peddle its products there. Huang has admitted that Chinese firms have the capacity to build.
And then there’s the kind of international tension that gets amped up by words. According to reporting at Motley Fool this week, one of the major steps leading to wider Chinese import restrictions of the H20s was remarks by U.S. Commerce Secretary Howard Lutnick, who said publicly on American tv that Chinese companies would only get Nvidia's ""fourth best"" chip.
“You want to sell the Chinese enough that they get addicted to the American technology stack,” Lutnick reportedly said, triggering a response from a country that is developing its own tech stack.
If that’s the reason for the Chinese reaction, it’s unlikely that causal statecraft is going to reverse the momentum on the siloing of U.S./China processor trade.
Stay tuned."
24,https://www.forbes.com/sites/johnwerner/2025/08/29/after-earnings-nvidia-powers-ahead-on-robotics-and-automation/,"After Earnings, Nvidia Powers Ahead On Robotics And Automation","Aug 29, 2025, 04:52pm EDT",John Werner,"First of all, following the numbers for the past quarter on Nvidia’s scoreboard, analysts are considering the results a mixed bag, with some more impressed by revenue and growth than others.
By any account, Nvidia is a big company, and it would be hard for anyone not to qualify its trajectory over the last few years as a success. After betting big on AI hardware, Nvidia rocketed to the top of the American tech market leaderboard. But the “what have you done for me lately” mindset often raises its head with quarterly reviews, so the iterative progress of Jensen Huang and company can be seeb through various lenses.
Coverage at Investopedia points out that while some American bankers, like Morgan Stanley, are properly excited about Q3, other parties, HSBC as named, are more concerned about the ban on China’s importing of Nvidia H20 chips. Certainly, the future of that market is a wild card, but there’s a growing consensus that even if China won’t buy what Nvidia is selling, the company will still be okay.
In fact, a closer look shows that Nvidia’s posted third-quarter revenue of $54 billion, plus or minus 2%, beats analyst projections of $53.8 billion, even without the missed Chinese H20 sales built in. So if sales of the chip resume there, that’s, as they say, gravy.
Part of what Nvidia is doing, besides conquering the hardware market with its Blackwell architecture and becoming a top chip vendor to mega-clients, is developing a spectrum of new technologies around automation. In other words, this isn’t just a hardware firm anymore. Nvidia is cultivating a set of solutions for robotics and self-driving vehicles that are going to add flavor and dimension to the company’s footprint worldwide.
There’s Nvidia Drive AV, a full-stack setup for self-driving cars, and Jetson AGX Thor, agentic tech that I covered last week. These, in addition to the Nvidia Halos AI safety program and Nvidia Cosmos world foundation models, are poised to give the company a lot more relevance in a developing market that represents big changes for global society.
So what is all of this stuff?
NVIDIA DRIVE AV is the company’s all-in-one software platform for self-driving cars. It combines vision, prediction, planning, and control, all in one system, with lidar, sensors, and other gear tied to robust AI decision-making capabilities.
This might be a good place to break down an industry term often used here: sensor fusion.
What does this mean?
Sensor fusion is the process of combining data from multiple sensors—like cameras, radar, lidar, and ultrasonic detectors. That’s all. You could call it joint sensor management, or sensor aggregation, but for some reason, the term “fusion” has become an industry standard, presumably because you’re bring data from multiple sensors together and “fusing” it into a coherent whole. This is a major part of how stakeholders are making self-driving solutions safer for users.
Anyway, that’s one of Nvidia’s big contributions in this area. But the company is making big inroads into non-vehicular robotics, too.
This robotics application is something fans call a “powerhouse” – with 128 GB RAM and the capacity for up to 2070 FP4 TFLOPS of AI compute, the model towers over the previous iteration, Orin, and proponents foresee that Nvidia will be competitive in helping to deliver to us the next generation of robot butlers. And yes, it’s powered by Blackwell.
Other robotics people outside of Nvidia’s stable are paying attention.
“(Thor) offers the computational horsepower and energy efficiency necessary to develop and scale the next generation of AI‑powered robots … transforming how we move and manage goods globally,” said Tye Brady, Chief Technologist at Amazon Robotics, in response to the unveiling of this technology.
Then there’s Halos, a program aimed at making sure that all of these autonomous vehicles will operate safety on the road.
Halos is accredited by the ANSI National Accreditation Board (ANAB), a U.S. organization that evaluates and verifies compliance with rigorous international standards for self-driving cars, trucks, buses, etc.
The program is also supported by the Nvidia AI Systems Inspection Lab, where people work on testing, validating, and certifying AI-powered systems like self-driving cars and advanced robots. Engineers run deep simulations to see how the AI behaves in tricky driving or robotics scenarios. They also test “edge cases” — which you also might call “black swan events” – where challenges like sudden obstacles, unpredictable drivers, or sensor failures might jeopardize the security that the systems were built to sustain.
Let’s not forget about Nvidia Cosmos. This project is not aimed at self-driving cars: it’s more focused on world simulations with robust models. However, it does tie back into autonomous vehicle safety: Nvidia Cosmos creates ultra-realistic, physics-aware video simulations for AI training. That means it’s working on the premise of helping the applications to “see” objects, evaluate landscapes, and understand context, all things that can be applied to either vehicle automation or mobile robotics.
That’s a bit of a survey of some of the major things going on at Nvidia as outsiders read the tea leaves from recent earnings releases. As for whether Nvidia can break back into the Chinese market, we’ll just have to see. This isn’t even the first time this year that the international sales of H20s have been in question. And there’s a lot of geopolitical context. The market doesn’t operate in a vacuum – and as a global society, we also have to figure out how we feel about the rapid advancements in AI that might lead us toward “the singularity.” Stay tuned."
25,https://www.forbes.com/sites/petercohan/2025/08/28/nvidia-stock-down-12-why-growth-may-slow-and-why-not-to-buy-nvda/,Nvidia Stock Down 1.2%. Why Growth May Slow And Why Not To Buy $NVDA,"Aug 28, 2025, 01:18pm EDT",Peter Cohan,"Nvidia's Q2 2026 results beat overall expectations, but data center revenue fell short, primarily due to the lack of sales in China.The company's revenue and stock price growth rates are decelerating, with future explosive growth challenged by market saturation and elusive AI investment returns.Re-entry into the Chinese market offers significant revenue upside, but remains subject to geopolitical uncertainties.
Nvidia stock fell 1.2% Wednesday after reporting record sales, falling short on data-center revenue, and predicting cooler growth, according to the Wall Street Journal.
Does this slight dip represent a buying opportunity? If Nvidia can accelerate revenue growth, its share price will rise. Here are three reasons that may not happen:
Nvidia is optimistic about the company’s future. “This year is a record-breaking year,” Nvidia CEO Jensen Huang said, reported the Journal. “I expect next year to be a record-breaking year” as well, he added.
With the exception of its data center business, Nvidia beat expectations in Q2 2026 and issued slower Q3 growth guidance – still ahead of analyst forecasts. Overall, the chip designer reported “better-than-expected earnings and revenue on Wednesday, and said sales growth this quarter will remain above 50%,” according to CNBC.
Here are the key numbers:
The absence of revenue from China affected Nvidia’s data center results. The company generated $33.8 billion in GPU data center sales – a 1% decline from the first quarter. The company sold $4 billion fewer H20 chips – which are designated for the Chinese market, Nvidia chief financial officer Colleen Kress said in a statement.
Nvidia is bullish on its latest GPUs – the Blackwell line. Their revenue increased 17% between the first and second quarters – indicating “demand is extraordinary,” Nvidia CEO Jensen Huang told the Journal. In May, Nvidia said Blackwell – which accounted for roughly 70% of data center revenue – reached $27 billion in sales, reported CNBC.
The company sees tremendous growth ahead. The largest AI companies will spend $3 trillion to $4 trillion by 2030, based on their current level of capital expenditures, and Nvidia could capture “as much as 70% of that revenue,” Huang said, according to the Journal.
Will Nvidia’s efforts to revive sales in China succeed? The answer could depend on political considerations. During the quarter, after Huang met with President Donald Trump, Nvidia hinted it may get U.S. licenses to ship the H20 chip to China.
Indeed the president “changed his mind in August and allowed H20 sales to resume after Huang visited the White House and pledged to give the federal government a 15% cut of AI-chip revenue earned in China,” the Journal reported.
Meanwhile, Nvidia took $4.5 billion in writedowns because of the lack of sales and said the H20 would have added $8 billion in second-quarter had the company been able to sell them, CNBC noted. Kress told investors Nvidia could generate between $2 billion and $5 billion in H20 revenue during the current quarter if the geopolitical environment permits.
Nvidia’s stock price growth depends on the company’s ability to grow faster than investors expect. The company’s growth and stock price have slowed down in parallel.
How so? In 2023, Nvidia revenue was up 126%, according to Macrotrends, while the stock soared 230%, noted Google Finance. In 2024, the company’s revenue increased 113% while its stock price rose 194%. In the latest quarter, the company’s revenues increased 56% and the stock has risen 29% so far this year.
This suggests the stock will keep going up more slowly unless Nvidia can grow much faster. However, if the company resumed growing at 100% a year for the next decade, revenue would soar from $176 billion in the last year to $360 trillion in 2036.
Sadly, such growth is impossible unless Nvidia targets new – much larger markets. After all, the global market for semiconductors is around $600 billion – much smaller than the gigantic 2036 revenue figure.
Will Nvidia’s data center growth decelerate significantly? The answer depends on whether companies remain as eager as they now appear to be to use AI chatbots.
For instance, enterprise technology spending grew 75% to $4.9 trillion in 2024 while large technology companies boosted AI capital expenditures from $245 billion in 2024 to $320 billion in 2025, according to Computerworld.
Unfortunately, these capital expenditures have not yielded profits. That’s because 95% of companies surveyed received no return on their AI investment. Only 5% of companies generated returns on AI – which averaged 3.7 times what they invested, noted a report from the MIT NANDA Institute.
If the economy contracts due to rising tariff-induced inflation and return on AI remains small, will companies keep spending so much on the technology? If companies cut back on their AI spending, it is only a matter of time before such a slowdown reduces Nvidia’s data center revenue growth even more?
Investors see some upside in the stock. Nvidia’s shares could rise about 14% to reach the average price target of $205 per share set by 31 analysts covered by TipRanks.
In my view the biggest potential for upside would happen if the Chinese market reopens for Nvidia. Unfortunately, that market could continue to be a political football.
Unless Nvidia can find new growth opportunities, the best days of owning its stock may be in the rear view mirror."
26,https://www.forbes.com/sites/tylerroush/2025/08/27/nvidia-earnings-beats-wall-street-expectations-and-sets-another-quarterly-sales-record/,Nvidia Earnings Beat Wall Street Expectations And Set Another Quarterly Sales Record,"Aug 27, 2025, 04:34pm EDT",Ty Roush,"The world’s most valuable company reported second-quarter earnings Wednesday that beat Wall Street’s expectations yet again, with investors seeking clues into how Nvidia fared with challenges in the China market.
Nvidia reported $46.7 billion in second-quarter revenue, surpassing the chip designer’s sales record of $44.1 billion in the previous quarter and besting economist forecasts of $46.05 billion, according to estimates compiled by FactSet.
A bulk of Nvidia’s top-line came from nearly $41.1 billion in data center revenue, which Morgan Stanley analyst Joseph Moore wrote Monday would drive Nvidia’s business over the next year, behind predictions of $41.3 billion.
The AI firm brought in adjusted earnings per share of $1.05 and $25.78 billion net income, besting Wall Street’s projections of $1.01 EPS and $24.7 billion, respectively.
Nvidia’s top- and bottom-line figures account for annual increases of 6% and 42%, respectively trumping the year-earlier totals of $30 billion and $0.67 EPS.
Nvidia shares briefly fell 4% in after-hours trading following the release of the company’s earnings report. The stock hit an intraday high of $184.12 earlier this month, while Nvidia shares traded around $181 before market close Wednesday.
Nvidia will host a call with investors at 5 p.m. EDT. Some analysts are looking for answers from CEO Jensen Huang about the Chinese government pressuring domestic companies not to buy the company’s H20 AI processors over national security concerns, leading Nvidia reportedly to halt production. Morgan Stanley believes this “remains an important issue to resolve long term,” though it would likely be resolved “with both countries incentivized to enable access to U.S. hardware,” Moore wrote. Kevin Cassidy, an analyst for Rosenblatt Securities, wrote his firm believes Nvidia will introduce a “China-specific” alternative to the AI chip, which could “command a higher price point, better margins and offer a more competitive solution.”
WedBush Securities, in a note Wednesday led by Seth Basham, said Nvidia is one of 30 companies that will “define the future of the AI revolution over the coming years.” The brokerage dispelled fears Nvidia and other AI firms would be the victims of a broader “stock market bubble,” adding, “The potential for AI to drive productivity and economic growth dwarfs past technologies.”
As the last of the “Magnificent Seven” companies to report Q2 earnings this year, Nvidia closes out a three-month period of growing competition, specifically in China. Chinese semiconductor firm Cambricon reported a record $402 million profit over the first half of 2025, though Wall Street remains optimistic for Nvidia’s growth prospects while backed by demand for AI technology and data centers. Analysts for Evercore ISI wrote earlier this month they believed rising cloud demand at companies like Amazon indicates AI has “hit a tipping point at enterprises,” as Nvidia’s chips are the “solution of choice” for training AI language models. The brokerage noted Nvidia remains its “top pick” in the near term and lifted its price target for Nvidia’s stock to $214 from $190.
S&P 500 And Nasdaq Set New Record Highs—Here’s Why Wall Street Expects Rally Could Climb Further (Forbes)"
27,https://www.forbes.com/sites/davealtavilla/2025/08/26/nvidia-jetson-agx-thor-dev-kit-raises-the-robotics-bar-with-blackwell/,Nvidia Jetson AGX Thor Dev Kit Raises The Robotics Bar With Blackwell,"Aug 26, 2025, 11:46am EDT",Dave Altavilla,"Nvidia’s Jetson line-up has long been the company’s proving ground for embedded AI computing at the edge, especially in robotics, industrial automation and autonomous vehicles. With the new Jetson AGX Thor developer kit, Nvidia is introducing a platform targeted at advancing machine learning in an arena where “physical AI”—robots, autonomous machines, and sensor-rich devices—must process vast amounts of data in real time.
At the core of the dev kit is the company’s Jetson T5000 module, built on Nvidia’s Blackwell GPU architecture. It integrates 2,560 Blackwell CUDA cores, 96 fifth-generation Tensor cores, and a 14-core Arm Neoverse CPU with a healthy cache memory complement. The system also features 128 GB of LPDDR5x memory and 1 TB of NVMe storage, creating a compact but well-provisioned development platform. Networking and I/O options are extensive as well, with multiple 25 GbE links, a 5 GbE port, PCIe Gen5 lanes, and a mix of display and USB connectivity.
On paper, this is more than a robotics brain and central nervous system, it is closer to a low power data-center node engineered for deployment in edge environments. Jetson Thor’s performance profile suggests it was designed with humanoid robotics, multi-sensor fusion, and advanced machine vision at the forefront. However, unlike desktop GPUs, the platform is delivered in a tightly integrated, industrial form factor that developers can treat as a turnkey development environment.
Nvidia has learned over years of building Jetson platforms that hardware without a robust, coherent software ecosystem is of limited value. With Thor, the company continues to emphasize developer readiness. The kit ships with support for JetPack, which provides an Ubuntu Linux environment, Nvidia CUDA libraries, and a host of pre-built AI frameworks.
Beyond this baseline, Nvidia offers domain-specific stacks that give the hardware practical utility. The company’s Isaac SDK remains central to robotics developers, providing simulation, navigation, and manipulation tools. Metropolis targets advanced machine vision and smart city applications, while Holoscan focuses on streaming sensor data for medical and industrial use cases. Together, these SDKs signal that Nvidia is striving for Thor to be a cross-domain platform rather than a niche robotics controller.
There is also the integration of Isaac GR00T foundation models, aimed at humanoid robotics and generative AI workflows. While still in their early stages, these tools reflect Nvidia’s strategy to move beyond raw compute and into the higher-level frameworks developers increasingly look for. Jetson AGX Thor’s carrier board and SDK Manager make setup relatively straightforward, so teams can begin experimenting without a long ramp-up period. And let’s not forget Cosmos, which you can think of as a virtual, multiverse playground for robot training. There are a few start-ups and even bellwethers like Qualcomm that are making significant strides in industrial automation and surveillance, but Nvidia’s consistent investment in developer tools is arguably one of the Jetson platform’s strongest differentiators.
Of course, Nvidia’s claims are bold about Jetson AGX Thor performance goals. The Blackwell GPU at its core is capable of roughly 2,070 TFLOPS at FP4 precision and 1,035 TFLOPS at FP8. Compared to the outgoing Jetson AGX Orin’s 275 TOPS (INT8), this represents a substantial leap in raw throughput. This is significant as it marks the first time the company has equipped its latest Jetson platform with its latest GPU architecture, unifying Blackwell technology across its entire hardware product stack. Nvidia characterizes Thor’s performance lift at about 7.5× in AI compute performance, though that’s specifically with Nvidia’s NVFP4 lower precision, efficient inference floating point format. Nvidia notes NVFP4 is currently as accurate, if not slightly better in certain cases, than INT8 precision.
Memory bandwidth also plays a critical role. With 128 GB of LPDDR5x and a 256-bit memory interface delivering 273 GB/s, the platform can accommodate large models and multi-modal workloads without resorting to external storage as often. This matters for real-time sensor data collection and fusion, where latency and bandwidth can be bottlenecks.
In addition, the T5000’s Blackwell GPU now supports Multi-Instance GPU partitioning, allowing up to seven isolated, simultaneous GPU instances, which adds significant flexibility. With MIG, developers can allocate different partitions to separate workloads—say, vision, natural language, and path planning—without incurring the performance penalties of context switching.
Networking performance is another highlight. Four 25 GbE interfaces and a 5 GbE port give Thor the kind of throughput typically reserved for rackmount servers. For multi-camera arrays or fleets of sensors generating video and lidar data, this capability is not just useful, it’s true differentiation.
Still, the kit operates within a power envelope of 40 to 130 Watts. This makes it deployable in embedded scenarios where a standard server-class GPU would be impractical. However, it may not be the most power efficient platform for all use cases. Qualcomm, for instance, leans heavily into performance per watt with its Snapdragon Ride and RB platforms.  Jetson Thor offers efficiency improvements over Orin, but its main appeal remains outright compute density rather than minimal power draw.
The Jetson AGX Thor Developer Kit illustrates Nvidia’s strategic focus on owning the high-performance segment of the embedded AI and robotics market. At $3,499, it is clearly not priced for small-scale tinkerers. Instead, it is targeted at research institutions, robotics startups and enterprises pursuing advanced automation projects.
In that sense, Thor fits into a growing competitive landscape where edge AI hardware is bifurcating: one branch pushing extreme efficiency (Qualcomm, MediaTek, and other AR or RISC-V-based SoCs), and the other emphasizing performance headroom (Nvidia, AMD in emerging edge GPU and FPGA-powered technologies). Nvidia is betting that serious robotics and physical AI initiatives will prioritize performance and ecosystem maturity over absolute power efficiency or lower cost.
From a market perspective, Thor provides developers with strong tools to build systems that blur the line between the data center and the edge. Warehouse robots, industrial inspection bots, drones, medical imaging systems, and autonomous vehicles are all potential beneficiaries. But adoption will depend on whether developers find its substantial compute capacity necessary or whether lower-cost platforms suffice for their use cases.
Another consideration is timing. The robotics and humanoid robot markets remain in their early phases. While companies like Figure AI, Boston Dynamics, Agility Robotics, and others are experimenting with humanoids, practical deployments at scale are still years away. In the meantime, Thor is likely to establish a beachhead in industrial automation robotics, where the return on investment is more near term.
In the broader competitive picture, Thor underscores Nvidia’s approach: dominate the high end, secure developer mindshare with robust tools, and let the ecosystem drive adoption. This leaves room for others to compete on power efficiency, price, or integration into consumer devices, though in premium, high performance developer kits, Nvidia continues to set the pace.
Ultimately, the Nvidia Jetson AGX Thor Developer Kit signals that the company now intends to also be the default platform for physical AI at the edge. And it’s a big market opportunity, even if it takes 3 - 5 years to fully materialize. For developers with workloads that genuinely need this level of performance, Jetson AGX Thor offers one of the most capable and complete solutions available today.
Dave co-founded and is principal analyst at HotTech Vision And Analysis, a tech industry analyst firm specializing in consulting, test validation and go-to-market strategies for major chip and system OEMs. Like all analyst firms, HTVA provides paid services, research and consulting to many chip manufacturers and system OEMs, including companies mentioned in this article. However, this does not influence his unbiased, objective coverage."
28,https://www.forbes.com/sites/janakirammsv/2025/08/26/nvidias-jetson-thor-delivers-a-data-center-class-ai-punch-to-robotics/,Nvidia’s Jetson Thor Delivers A Data Center-Class AI Punch To Robotics,"Aug 26, 2025, 10:17am EDT",Janakiram MSV,"In a bid to accelerate autonomous machines, Nvidia has released Jetson AGX Thor, its most powerful embedded AI computer for robots and physical devices. This new “robot brain” dramatically boosts on-board computing, offering up to 7.5x the AI performance of the current Jetson AGX Orin module at 3.5x better energy efficiency. With Jetson Thor, Robots that once relied on cloud processing or multiple chips can now perceive and make decisions in real time on a single compact module. Early adopters like Amazon’s warehouse robotics division and Boston Dynamics are already integrating Thor, aiming to enable more intelligent, independent machines. It’s a timely leap as industries race to deploy smarter robots across manufacturing, logistics, healthcare and more.
The Jetson Thor platform is rolling out just as demand surges for robots that can handle complex tasks in unpredictable environments. Nvidia announced general availability of the Jetson AGX Thor developer kit in August 2025, positioning it to power “millions of robots across industries” from logistics and retail to agriculture and healthcare. Jetson Thor becomes the cornerstone for the next generation of physical AI, meaning AI embodied in physical systems like robots and autonomous vehicles.
By delivering unprecedented on-device compute, Thor addresses one of robotics’ biggest hurdles: enabling real-time, intelligent interaction with the physical world. Companies care because this capability can unlock new automation and efficiency. A factory robot, for example, could visually inspect products, understand verbal instructions, and coordinate with other machines simultaneously, all without offloading processing to the cloud. This level of autonomy has been hard to achieve until now. Thanks to Jetson Thor, the standards are now even higher. Organizations that adopt it could significantly advance with more capable robots, while those that don’t might find themselves falling behind in the race for automation progress.
Roughly the size of a small textbook, Jetson Thor packs the kind of computing muscle once confined to data centers. It features Nvidia’s latest Blackwell GPU architecture alongside a 14-core Arm CPU, fused on a system-on-module built for edge deployment. The module delivers up to 2,070 FP4 teraflops of AI compute within a 130 W power envelope, effectively condensing server-grade AI horsepower into a form factor for robots and autonomous machines.
In plain terms, it can perform on the order of two quadrillion operations per second using new low-precision math techniques, a massive jump from the ~0.25 quadrillion (275 TOPS) that the previous Jetson AGX Orin could achieve. The device boasts 128 GB of high-speed LPDDR5x memory (double the Orin’s capacity) and support for high-bandwidth I/O like 25 Gbps Ethernet and PCIe Gen5, enabling robots to ingest and process a torrent of sensor data in real time. Crucially, Thor’s gains aren’t just about raw speed – they’re about concurrency. The design allows multiple advanced AI models to run at once without straining resources. Its GPU can even be partitioned into parallel instances to dedicate compute to different tasks simultaneously. This means a single Thor module can handle a robot’s vision processing, natural language comprehension, and motion planning in parallel, where earlier systems would struggle or require multiple units. Compared to Jetson AGX Orin’s 12 CPU cores and 64 GB RAM, Thor’s 14 cores and 128 GB memory, combined with its new GPU, represent a generational jump in capability,  albeit with a trade-off of higher power draw (up to ~120–130 W, about double that of Orin’s max).
Nvidia is effectively betting that many robotics applications are ready to accommodate a bit more power and heat in exchange for supercomputer-level on-board AI. For edge use cases that can supply the power and cooling, Jetson Thor opens the door to running complex models like large vision transformers or even language models directly on a robot. For example, a humanoid assistant could use Thor to interpret voice commands with a large language model while simultaneously navigating its environment via real-time 3D vision, while running all models on-device, with minimal latency or dependence on connectivity.
Jetson Thor’s introduction is poised to reshape the competitive dynamics in robotics computing. Nvidia’s Jetson platform already dominates high-end robotics and autonomous machine development, with over 2 million developers in its ecosystem and more than 7,000 companies using the prior Orin-based modules.
Jetson Thor extends that lead at a time when few alternatives can match its blend of performance and maturity. Rival edge-AI offerings from the likes of Intel or Qualcomm currently operate at a much lower scale. Even automotive-grade chips or AI accelerators can’t yet deliver this level of integrated compute in a single module. Moreover, Nvidia’s strategy of tightly coupling hardware with its software stack, combined with the broad adoption of CUDA Toolkit, creates a high barrier for competitors. The Jetson Thor comes ready to leverage Nvidia’s complete Isaac robotics software platform, AI model libraries, and simulation tools, all optimized for its architecture. This end-to-end integration gives Nvidia a significant moat against would-be competitors in robotics AI.
Early adoption by industry leaders reinforces that advantage – companies like Caterpillar (heavy equipment), Medtronic (surgical robots), and Amazon Robotics have signed on to use Thor, indicating confidence that Nvidia’s solution is the one to beat. At the same time, some large tech players are exploring their own paths. For instance, Tesla is developing custom AI chips for its humanoid robot project. Still, Nvidia is positioning Jetson Thor as a universal platform to power “the entire robotics revolution” across sectors.
For most enterprises and startups, building in-house silicon or piecing together less-proven alternatives is impractical, making Thor the default choice if top-tier performance at the edge is needed. There are, however, practical considerations tempering the hype. The Jetson AGX Thor developer kit is priced at $3,499, reflecting its enterprise focus. The higher power requirements also mean Thor is best suited to robots with sufficient battery or electrical supply (think autonomous vehicles, factory robots, delivery bots) rather than tiny drones or IoT sensors. Nvidia and its partners will need to demonstrate that the real-world benefits justify these costs and trade-offs. If they do, Thor stands to become the de facto brain behind the smartest robots of this coming decade.
Nvidia’s aggressive push with Thor also means the robotics landscape could evolve quickly. Companies that embrace these more capable “physical AI” systems early may gain a competitive edge in efficiency, safety or customer experience, while laggards risk being disrupted by nimble adopters who do more with intelligent automation.
As autonomous machines move from controlled environments to broader deployment, Jetson Thor is positioned to be a foundational platform powering this shift. For the industry at large, it marks a step closer to the long-promised era of robots that are as adept and responsive as the scenarios we’ve imagined – an era where the limiting factor will be less about technology capabilities and more about how creatively businesses can apply them.
"
29,https://www.forbes.com/sites/adamsarhan/2025/08/26/nvidia-earnings-preview-will-ai-momentum-power-another-blowout-report/,Nvidia Earnings Preview: Will AI Momentum Power Another Blowout Report?,"Aug 26, 2025, 06:23am EDT",Adam Sarhan,"Nvidia Corp. is scheduled to report earnings after Wednesday’s close. The stock just hit a record high of $184.48/share and is currently trading near $180. The stock is prone to big moves after reporting earnings and can easily gap up if the numbers are strong. Conversely, if the numbers disappoint, the stock can easily gap down. To help you prepare, here is what the Street is expecting:
The company is expected to report a gain of $0.98/share on $45.91 billion in revenue. Meanwhile, the so-called Whisper number is a gain of $1.06/share. The Whisper number is the Street's unofficial view on earnings.
The company’s earnings have grown nicely over the last few years. In 2020, the company earned $0.15/share. In 2021, earnings jumped to $0.25. Then, in 2022 earnings grew to $0.44. In 2023, earnings slid to $0.33 before exploding higher. In 2024, earnings grew to $1.30. In 2025, earnings are expected to grow to $2.99. In 2026, earnings are expected to grow to $4.42 and grow again to $6.03 in 2027! That growth is largely due to the very strong demand for its A.I. and crypto related chips.
Technically, the stock is acting very well and just hit a fresh record high. The stock is trading above its 50 day moving average line (DMA) and above its longer-term 200 day moving average line which are both healthy signs. Furthermore, the stock is “only” 3% below its record high and can easily gap up to new highs if earnings are strong. Conversely, the bears want to see it gap down and fall if earnings fail to impress.
NVIDIA’s earnings are getting a lot of attention because big trends like artificial intelligence, new rules from governments, and global demand could impact the bottom line. Investors want to know if the strong need for AI chips will continue, how export limits and global trading rules will affect sales—especially to places like China—and whether other new technologies might start to matter more for NVIDIA’s business. When results are reported, people will be watching closely to see how these things impact NVIDIA’s future. Here are some questions investors will be asking: Will AI stay a major driver going forward? How will regulations and international markets affect business? Could changes in technology or global demand shift the company’s growth path? How does the crypto market impact earnings?
The stock has has established itself as a true market leader, demonstrating exceptional performance and influence across the tech sector and broader market. NVIDIA has significantly outpaced the overall market, driven by its pivotal role in the artificial intelligence revolution, crypto, and other important areas in our economy.
Market strategists consider NVIDIA ""the tell to the market,"" emphasizing its influence on overall market trends. The company's consistent outperformance, beating revenue estimates in the last several quarters, coupled with analysts' positive long-term outlook, underscores its position as a bellwether for the tech sector and a key driver for the broader market.
Here’s the company profile, according to Yahoo! Finance:
NVIDIA Corporation, a computing infrastructure company, provides graphics and compute and networking solutions in the United States, Singapore, Taiwan, China, Hong Kong, and internationally.
The Compute & Networking segment includes its Data Centre accelerated computing platforms and artificial intelligence solutions and software; networking; automotive platforms and autonomous and electric vehicle solutions; Jetson for robotics and other embedded platforms; and DGX Cloud computing services.
The Graphics segment offers GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions for gaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU or vGPU software for cloud-based visual and virtual computing; automotive platforms for infotainment systems; and Omniverse software for building and operating industrial AI and digital twin applications.
It also offers customized agentic solutions designed in collaboration with NVIDIA to accelerate enterprise AI adoption.
The company's products are used in gaming, professional visualization, data center, and automotive markets. It sells its products to original equipment manufacturers, original device manufacturers, system integrators and distributors, independent software vendors, cloud service providers, consumer internet companies, add-in board manufacturers, distributors, automotive manufacturers and tier-1 automotive suppliers, and other ecosystem participants.
NVIDIA Corporation was incorporated in 1993 and is headquartered in Santa Clara, California.

From where I sit, the most important trait I look for during earnings season is how the market and a specific company reacts to the news. Remember, always keep your losses small and never argue with the tape.






Disclosure: The stock has been featured on FindLeadingStocks.com."
30,https://www.forbes.com/sites/karlfreund/2025/08/22/hotchips-preview-nvidia-scales-ai-beyond-the-data-center/,HotChips Preview: Nvidia Scales AI Beyond The Data Center,"Aug 22, 2025, 11:00am EDT",Karl Freund,"The annual HotChips conference starts this Sunday, Aug. 24, in San Francisco. Nvidia is scheduled to present six sessions covering topics of interest to AI data center users and operators and will make several key announcements I’ll cover in this article. (Like most AI semiconductor-related companies, Nvidia is a client of Cambrian-AI Research.)
NVLink Fusion is perhaps the most fascinating topic, enabling the entire industry of CPUs and GPUs to create chips to access NVLink, the company’s secret sauce for interconnecting up to 72 accelerators and 36 CPUs in a rack. While I’m working on another article that specifically covers how Qualcomm is using NVLink Fusion to enter the data center with its super-fast Arm-based Oryon CPUs, I’ll focus here on how Nvidia is enabling AI to expand beyond a single data center, and a new 4-bit format that could significantly improve the efficiency of training AI models by as much as four-fold.
As older data centers struggle to grow AI due to power constraints, many are seeking a method to break through the walls and distances to connect their network of data centers, delivering on the promise of AI and growing their business. Nvidia has launched a new Ethernet card called Spectrum-XGS to enable these data centers to enter the world of giga-scale AI.  This scale is needed for training large AI models but increasingly is also used for agentic AI and reasoning models. Nvidia claims this network can nearly double the performance of multi-site AI workloads.
Nvidia is somewhat unique in the industry in having a large in-house research organization under Bill Dally, the company’s chief scientist and senior vice president. Dr. Dally’s team has developed many of the breakthroughs that have kept Nvidia in the lead and caused its competitors to rush to catch up with its multi-year head start.
Last year at HotChips '24, Dr. Dally said that he thought there was more gold to mine in the realm of “quantization”; the ever-shrinking data formats that double or even quadruple the performance efficiency by using smaller and smaller data formats. While we may be nearing the end of that road, the new 4-bit floating point NVDP4 is pretty remarkable way to finish the story. NVDP4 will be available on all Blackwell and future Nvidia GPUs.
In another of Nvidia’s research results, the company discussed the use of speculative decoding, where the GPU creates drafts of the next token and then uses AI (duh!) to verify that that draft token is valid or not. Speculative execution has been used for decades in CPUs, and now is increasing being considered for deploying more efficient AI. Note that Cerebras has disputed the representation of their numbers on the graph below.
I hope you can attend the many fine sessions being offered next week at HotChips. I will, at least on-line! This is the hottest conference every year for the geekiest of the industry, those presenting and in attendance. It is this sort of sharing of ideas and research results that feeds our industry and enables the USA’s leadership in semiconductors.
Disclosures: This article expresses the opinions of the author and is not to be taken as advice to purchase from or invest in the companies mentioned. My firm, Cambrian-AI Research, is fortunate to have many semiconductor companies as our clients, including Baya Systems BrainChip, Cadence, Cerebras Systems, D-Matrix, Esperanto, Flex, Groq, IBM, Intel, Micron, NVIDIA, Qualcomm, Graphcore, SImA.ai, Synopsys, Tenstorrent, Ventana Microsystems, and scores of investors. I have no investment positions in any of the companies mentioned in this article. For more information, please visit our website at https://cambrian-AI.com."
31,https://www.forbes.com/sites/petercohan/2025/08/22/why-nvidia-matters-more-to-markets-than-the-fed/,Why Nvidia Earnings Matter More To Markets Than What The Fed Chair Said,"Aug 22, 2025, 12:49pm EDT",Peter Cohan,"Nvidia's upcoming Q2 earnings report is expected to move markets, even more than Fed Chair Jerome Powell's speech, given the chipmaker's S&P 500 weighting and options market expectations.Growing investor skepticism about the low returns from AI investments puts pressure on Nvidia to beat expectations and raise guidance.Failure to meet high expectations could lead to a significant Nvidia stock plunge, potentially triggering a broader tech selloff.
Markets move on gaps between reality and expectations. I expect the gap that could emerge after Nvidia reports second-quarter earnings next Wednesday will move markets more than what Federal Reserve Chair Jerome Powell said on Friday in Jackson Hole, Wyoming.
How so? Investors are growing worried about how much companies are investing in artificial intelligence and how few of them are getting a payoff. Unless Nvidia beats investor expectations and raises guidance, its stock could plunge. This drop would be exacerbated by investors who would view a bad report as a sign the AI bubble is bursting.
In Jackson Hole on Friday, Powell shifted his primary concern from rising inflation to a weakening job market. “The balance of risks appears to be shifting,” Powell said, according to the Wall Street Journal.
While labor markets appear to be stable, the Fed perceives the risks of worse-than-expected labor market outcomes to be rising. “If those risks materialize, they can do so quickly in the form of sharply higher layoffs and rising unemployment,” Powell said.
Read on for a closer look at why Nvidia earnings are so market-moving and how investors will know whether the AI chip designer will beat and raise.
Nvidia’s quarterly earnings reports have been market moving since May 2023, when the chip designer issued the revenue growth forecast heard around the world – prompting me to write my latest book, Brain Rush.
Since then, Nvidia’s market capitalization has grown so much that changes in the company’s stock price drive the S&P 500. Indeed, Nvidia’s $4 trillion stock market capitalization accounts for 8% of the value of the index, according to the The Times.
Moreover, the options market is expecting Nvidia’s earnings report to move markets more than Powell’s remarks. That’s because S&P 500 options are pricing a 0.8 percentage point move up or down after the Fed Chair speaks, whereas prices for Aug. 28 — the first chance for investors to trade on Nvidia’s report – imply a move of 0.9 percentage points up or down, according to The Times.
In the last few weeks, investors have grown more skeptical of investing in AI companies. For example, Palantir, whose shares peaked a few weeks ago at $190, has recently lost 20% of its value despite an excellent second-quarter financial report, as I wrote in an Aug. 20 Forbes post.
One reason for the drop, which was primarily due to a short seller’s estimate that the stock is 74% overvalued, is the abysmal payoff from companies’ investments in AI.
Last September, generative AI was looking to me like a big dud. While people were using ChatGPT to help them draft emails and reports, there was no killer app – akin to what the iTunes store did for the iPod or the electronic spreadsheet did for personal computers, I wrote in the Boston Globe.
This week, MIT reinforced this point with hard numbers.
""Despite $30B-$40B in enterprise investment into generative AI, this report uncovers a surprising result in that 95% of organizations are getting zero return,"" according to a study from MIT’s NANDA Institute featured by SeekingAlpha. The study was based on 150 interviews with professionals, a survey of 350 employees, and an analysis of 300 public AI deployments.
Will investors be worried this minimal payoff from AI will cause Nvidia to offer a less bullish growth forecast? If they do, Nvidia’s stock could fall and drive a significant tech selloff.
This brings us to the question of what Nvidia is likely to say next Wednesday, when it provides investors with its second-quarter report. In May, the company forecast $45 billion in revenue for the second quarter – $920 million below investor expectations, but 50% higher than last year, according to Investor’s Business Daily.
A week before the latest report, investors are expecting more from Nvidia. Specifically, analysts expect Nvidia to post revenue of $45.65 billion – a 52.4% increase from last year and a 47% boost in earnings per share to $1.00, according to MarketBeat.
Investors are expecting Nvidia to forecast Q3 2026 revenue of $52.5 billion, according to The Motley Fool.
If Nvidia exceeds these expectations and raises revenue guidance, its shares will likely rise. Otherwise, the drop in Nvidia could cascade to other AI stocks. Meanwhile, I anticipate the market’s reaction to Nvidia’s report could be more pronounced than its response to what Powell said Friday."
32,https://www.forbes.com/sites/greatspeculations/2025/08/18/buy-nvidia-stock-ahead-of-earnings/,Buy Nvidia Stock Ahead of Earnings?,"Aug 18, 2025, 05:00am EDT",Trefis Team,"Nvidia (NASDAQ:NVDA) is expected to announce its Q2 2026 earnings at the end of August (January fiscal year). Analysts anticipate earnings of $1 per share, an increase from $0.68 in the same quarter last year, and revenues are projected to grow by over 50% year-over-year to $45.60 billion. This growth is likely to be fueled by the sustained strong demand for the company’s GPU chips utilized in generative AI applications. Nvidia has been increasing large-scale production of its latest Blackwell AI supercomputers. These new chips, with their cutting-edge AI functionalities and premium pricing, may assist in driving revenue growth during Q2 FY’26, while also enhancing margins. Historically, NVDA stock has tended to outperform following earnings releases, having risen 60% of the time with a median one-day increase of 4.5% and a maximum observed increase of 24%.
The company holds a current market capitalization of $4.4 trillion. Over the past twelve months, revenue totaled $149 billion, and it was operationally profitable, achieving $86 billion in operating profits and a net income of $77 billion. Although much will depend on how results compare to consensus expectations, recognizing historical trends may improve your chances if you are an event-driven trader. Should you Buy Or Fear Nvidia stock?
For event-driven traders, historical trends might provide an advantage, whether by positioning prior to earnings or responding to post-release movements. That said, if you are looking for upside with less volatility than that of individual stocks, the Trefis High Quality portfolio offers an alternative, having outperformed the S&P 500 and generated returns exceeding 91% since its inception.
See earnings reaction history of all stocks
Some insights on one-day (1D) post-earnings returns:
Additional data for observed 5-Day (5D) and 21-Day (21D) returns post earnings are summarized along with the statistics in the table below.
A relatively less risky strategy (although not useful if the correlation is low) is to assess the relationship between short-term and medium-term returns post earnings, identify a pair with the highest correlation, and execute the suitable trade. For instance, if 1D and 5D show the strongest correlation, a trader could position themselves “long” for the upcoming 5 days if the 1D post-earnings return is positive. Here is some correlation data based on a 5-year and a 3-year (more recent) history. Note that the correlation 1D_5D refers to the association between 1D post-earnings returns and subsequent 5D returns.
Occasionally, peer performance can impact post-earnings stock reactions. Indeed, the pricing-in may start before the earnings announcements are made. Here is some historical data illustrating the past post-earnings performance of NVIDIA stock compared with the stock performance of peers that reported earnings immediately before NVIDIA. For an equitable comparison, peer stock returns also reflect post-earnings one-day (1D) returns.
Learn more about Trefis RV strategy that has outperformed its all-cap stocks benchmark (the combination of all three, the S&P 500, S&P mid-cap, and Russell 2000), delivering strong returns for investors. Additionally, if you prefer upside with a smoother ride compared to an individual stock like NVIDIA, consider the High Quality portfolio, which has outperformed the S&P and recorded >91% returns since its inception."
33,https://www.forbes.com/sites/howardgleckman/2025/08/15/the-trouble-with-trumps-deal-with-nvidia-and-amd-its-an-export-tax/,The Trouble With Trump’s Deal With Nvidia And AMD: It’s An Export Tax,"Aug 15, 2025, 09:39am EDT",Howard Gleckman,"
Two of the nation’s most prominent chipmakers have agreed to pay the US government 15 percent of their revenues from the sale of artificial intelligence chips to China. In effect, the firms will be paying an export tax, though neither they nor the administration describe it this way. One reason why: Such a levy appears to be unconstitutional.
Whatever it is called, this arrangement may prove bad for the economy, for national security, and even for many of the president’s own priorities.
In April, the Administration banned sales to China of Nvidia’s H20 chip and Advanced Micro Devices’ (AMD) MI301 chip. While these are not the most sophisticated AI chips the companies make, they are advanced enough that national security experts raised concerns about US firms selling them to China.
In July, the Administration reversed course and announced it would approve the sales but delayed issuing the necessary export licenses. On August 6, Nvidia’s CEO and President Trump agreed to the 15 percent payment. Two days later, the Commerce Department approved the licenses.
At an August 11 news conference, Trump recounted his talks with the corporate leader: “I said, “If I’m going to do that, I want you to pay us something. So I said, “Listen, I want 20 percent if I'm going to approve this for you, for the country.'” Trump eventually agreed to a 15 percent payment.
As with Trump’s import taxes, these export levies come as he tries to achieve multiple, conflicting goals. He wants:
Like Trump’s recent tariff agreements with Japan and European Union, few details accompanied this arrangement. How exactly will it be structured? What guardrails will be put in place to limit avoidance and evasion?
Neither the companies nor the Administration is calling this a tax, for at least two possible reasons.
First, raising taxes on US businesses is a bad look for an Administration that wants to be seen as cutting corporate taxes.
Second, and perhaps most important, export taxes are explicitly prohibited by the US Constitution. Article I says, “No Tax or Duty shall be laid on Articles exported from any State.”
Trump and the firms may be trying to avoid that ban by implying the payments are voluntary. But if the firms could not get export licenses without agreeing to the payments, their optionality is questionable.
Developing countries often impose export taxes on low-value goods, such as agricultural products.  But they create well-known problems.
Export taxes are economically inefficient since they encourage firms to produce and sell goods not subject to the export levy. Understanding the impact of these levies is challenging because Trump is imposing them in a complex policy environment for chipmakers, filled with carrots and sticks.
President Biden curbed semiconductor sales to China. During his Administration, Congress also passed the CHIPS Act that provided $52 billion in subsidies and tax credits for chips made in the US.
Trump has vowed to repeal that law. However, the just-passed 2025 budget bill that he backed includes new and expanded subsidies for US chipmakers.
Trump also threatened to withhold CHIPS Act subsidies and impose 100 percent tariffs on imports of certain chips, then backed off when manufacturers promised to produce more semiconductors in the US.
Trump has made no secret of his desire to greatly expand government’s ability and his personal authority to manage investment decisions normally made by business. This goes well beyond broad policy initiatives to encourage domestic manufacturing. For example, the Administration reportedly is considering acquiring a stake in another chipmaker, Intel and Trump says he’ll personally control a share of foreign investment in the US.
The export tax will be difficult to collect. Chip smuggling already is a serious problem. This tax is likely to increase the practice.
And without guardrails, what will prevent US producers from selling to a middleman who can resell to China? For example, firms avoided paying 2018 tariffs on goods from China by first shipping them to Vietnam and then to the US. The same can happen in reverse with export taxes.
The levy will create additional incentives for China to find other markets or, more likely, produce domestic versions of AI chips.
The Administration seems inclined to frame its arrangement with Nvidia and AMD as the US becoming a sort-of business partner with private companies. But that won’t avoid other problems.
The government would share upside profit with no downside risk, an unusual business relationship at best. It could make policy decisions that benefit its partner, perhaps to the detriment of competitors. And it could attempt to influence business decisions, a practice unlikely to result in the most efficient use of a firm’s capital.
Finally, levies such as this create serious potential for corruption. If government can pick and choose what products and what producers are subject to an export tax, there is nothing to prevent a firm from using political influence to gain an exception that is unavailable to its competitors.
This export tax by any name is a poor idea. It will generate relatively little revenue, fail to protect US security interests, and ultimately make US chipmakers less competitive in world markets.
"
34,https://www.forbes.com/sites/karlfreund/2025/08/11/skipping-nvidia-left-amazon-apple-and-tesla--behind-in-ai/,"Skipping Nvidia Left Amazon, Apple And Tesla  Behind In AI","Aug 11, 2025, 06:28pm EDT",Karl Freund,"Everyone thinks they are a comic. And everyone in big cap high tech thinks they can design better and/or cheaper AI chip alternatives to the industry-leader,  Nvidia. Turns out, it’s simply not that easy.
Apple and AWS have recently run aground in AI growth, and Tesla has just abandoned their  own Dojo Supercomputer chip development, saying they are switching to Nvidia and AMD for training AI models. (Like many semiconductor developers, Nvidia is a client of Cambrian-AI Research). Oh, and today, The Information reported that “Microsoft’s AI Chip Effort Falls Behind.”  There is definitely an important trend here.
A few companies have eschewed getting locked in to Nvidia,  paying the high prices state-of-the-art AI technology commands. This penny-smart but pound-foolish approach left the world’s largest consumer electronics company (Apple) and the undisputed cloud leader (AWS) far behind, just when generative AI created massive end user opportunities they could not adequately address.
Nvidia’s CUDA platform is the de-facto standard for training and deploying large language models for generative AI. CUDA offers unmatched performance, developer tooling and ecosystem support. Companies that build on CUDA — like Microsoft (with OpenAI) and Meta (with LLaMA) — have been able to scale quickly and deliver cutting-edge AI products.By contrast, Amazon and Apple chose to go their own way, and Tesla took the Nvidia off-ramp in 2019. Let’s take a look, as each took a different approach, and mostly failed.
Apple’s generative AI journey has been even more problematic. After unveiling “Apple Intelligence” in 2024, the company’s most anticipated upgrade — a fully LLM-powered Siri — has been delayed until 2026.
Apple has some serious semiconductor bona fides, with its M-class Arm-based chips for desktops and the A-class for mobile. The company is justifiably proud of these efforts. But Apple tried its hand at AI acceleration early on using its own chips and then shifted to Google TPU-based AI development.  Not a bad choice, mind you, but the TPU does not have the performance nor the AI development tool-set that Nvidia Blackwell enjoys. The result? Well, how’s that AI-enhanced Siri and Apple Intelligence working out for you?  Yeah, not at all.
To be sure, Apple has significant technical challenges that come with having an installed base and a focus on privacy above all, but not using Nvidia from the start probably cost it more in extra work and time to market than the “expensive” Nvidia infrastructure would have cost.
Siri’s architecture, built over a decade ago, wasn’t designed for generative AI, and retrofitting it has proven more difficult and taking longer than Apple expected. To make matters worse, Apple’s AI teams have faced internal fragmentation, with some pushing for in-house developed AI models and others advocating partnerships with with OpenAI, Perplexity or Google.
The company also lost key talent to competitors. Ruoming Pang, who led Apple’s foundation models team, left for Meta in 2023. Other researchers followed, citing slow progress and lack of clarity in Apple’s AI strategy.
AWS recently paid the price of its slow generative AI sales on Wall Street caused by Amazon’s hubris and NIH (not invented here). The market share of new generative AI use cases landing on AWS is reportedly lower than its overall cloud share, with Microsoft taking over the lead.
According to IOT-Analytics, Microsoft has about 16% share of new genAI case studies, as does AWS, well below AWS leadership share in 2023 of  37%. AWS is not losing its first-place share in the overall cloud market, at least not yet, but for genAI-specific apps and new enterprise AI workloads, Azure and Google are increasingly competitive, and in some cases are outpacing AWS in genAI-related tools and adoption.
Reducing reliance on Nvidia and lowering costs sounded like a good strategy. So, Amazon’s AWS division, like Google and Microsoft, invested heavily in custom silicon for training and inference, named, of course, Trainium and Inferentia. The latest release, Trainium2, was launched in 2024 and appears to offer impressive specs: up to 83.2 petaflops of FP8 compute and 6 TB of HBM3 memory bandwidth. Amazon even created a 40,000-chip Trainium UltraCluster to support generative AI workloads.But accelerator performance alone doesn’t create AI. You need software, great chip-to-chip networking and a thriving developer ecosystem. AWS developers found Trainium software harder to work with than CUDA, and they reportedly pushed back to management against Trainium’s limitations. Management essentially said shut up and get to work. So, Trainium adoption lagged.
Amazon realized it needed to invest even more to create the developers ecosystem, and it launched the Build on Trainium initiative — a $110 million investment in university research. While appealing, this effort came years after Nvidia had firmly cemented its dominance in AI research and development. That is $110 million that could have been better spent on Nvidia hardware and better AI. And that $110 million is on top of the money that AWS spent developing the Trainium and Inferentia chips, probably well over $1 billion.So, Amazon decided to invest another $4 billion in Anthropic, the company behind Claude. Anthropic agreed to use Trainium chips for training its models in return. But behind the scenes, tensions emerged. Engineers at Anthropic reportedly also pushed back against Trainium. Many preferred Nvidia’s stack for its maturity and tooling. Anthropic teams had to rework their CUDA-based pipelines to work on Trainium, leading to delays and performance issues. While Amazon touted the partnership as a breakthrough, it was a compromise — Anthropic needed funding, and Amazon needed a flagship AI partner.
Amazon appears of late to be changing course, deepening its partnership with Anthropic and expanding support for Nvidia GPUs. AWS is building a massive Nvidia cloud infrastructure, Project Ceiba, with over 20,000 Nvidia GPUs. But it is only available to Nvidia engineers for use in developing AI and chips, not for public cloud access.
In 2019, Tesla shifted from using Nvidia to its custom FSD Chip for vehicle Autopilot hardware and neural network inference, replacing Nvidia’s Drive PX2 system. And it began a major effort to build its own AI Supercomputer, DOJO, with its in-house chips. Since 2019, Tesla has reportedly spent over $1 billion developing DOJO along with another $500 million developing a DOJO supercomputer in Buffalo, New York.
Last week, Elon Musk announced on X that he was ending this program and would instead deploy on Nvidia and AMD GPUs. I suspect Tesla will mostly deploy Nvidia this year and see how AMD’s MI400 looks in 2026.
Well, first, let’s look at a company that did not. OpenAI has recently reached $12 billion in annualized revenue  and broke the $700 million ChatGPT weekly active user barrier. And guess what it uses?  Yep, Nvidia. Sam Altman does have the gleam of OpenAI chips in his eye, to be sure, but he also realizes that speed, ease of use and development time matters more to OpenAI than the savings that proprietary chips could provide.  At least for now.
Meta has its own MTIA chip, but it is used for internal workloads, like recommendation engines for its Facebook and other properties. Microsoft has its own Maia chips starting with the Maia 100, announced in 2023, Used primarily for internal testing and select workloads. The planned successor, Maia 200, is now expected in 2026 due to delays. Maia 200 is designed for data center AI acceleration and inference workloads. We will see if Microsoft learns from Tesla and Apple’s mistakes.
I suspect Google is perhaps alone in realizing a decent return on its TPU investments, but it has generally failed to attract large outside customers, aside from Apple.  But it gets a lot of bang for the buck for internal workloads and training.
My advice to CSPs is this: if you can get Nvidia GPUs, use them.  If you have a workload for which you believe they are not ideal and can model a decent ROI, then go for it. Otherwise, save your capital.
A year in the world of generative AI can mean the difference between heaven and hell, or at least multi-billion-dollar successes or failure. The hubris of some high tech companies have cost them billions of dollars, spent needlessly.
Amazon ceded early leadership in cloud AI to Microsoft Azure, which now hosts many of the world’s top models. Apple missed the “AI supercycle” for iPhone upgrades, as consumers saw little reason to buy new devices without meaningful Siri improvements. Tesla has seen the light and is moving fast. All three of these companies now face pressure to catch up — not just in model performance, but in developer mindshare and ecosystem momentum.
Yeah,  you can build your own AI chip.  But you might regret it.
﻿﻿Disclosures: This article expresses the opinions of the author and is not to be taken as advice to purchase from or invest in the companies mentioned. My firm, Cambrian-AI Research, is fortunate to have many semiconductor firms as our clients, including Baya Systems BrainChip, Cadence, Cerebras Systems, D-Matrix, Esperanto, Flex, Groq, IBM, Intel, Micron, NVIDIA, Qualcomm, Graphcore, SImA.ai, Synopsys, Tenstorrent, Ventana Microsystems, and scores of investors. I have no investment positions in any of the companies mentioned in this article. For more information, please visit our website at https://cambrian-AI.com."
35,https://www.forbes.com/sites/siladityaray/2025/08/11/nvidia-and-amd-slip-in-premarket-as-reports-say-us-will-take-15-cut-of-china-ai-chip-sales/,Nvidia And AMD Slip In Premarket As Reports Say U.S. Will Take 15% Cut Of China AI Chip Sales,"Aug 11, 2025, 05:18am EDT",Siladitya Ray,"Shares of Nvidia and AMD fell in premarket trading early on Monday, after reports said both companies have agreed to pay 15% of the revenue generated on AI chip sales to China to the U.S. government, as part of an unconventional deal with the Trump administration to secure export licenses.
According to the Financial Times, which first reported on the matter, Nvidia will pay the 15% cut on the revenue it generates from the sale of its H20 AI chips, a cut-down version of its H100 chip designed to comply with export rules on China.
AMD has also agreed to pay a similar cut on the sales of its MI308 chip designed for China, the report added.
Citing an unnamed U.S. official and others familiar with discussions, the FT report said the deal was a necessary condition for the China export licenses granted to the companies last week.
It is unclear if these 15% export levies will raise the price of these chips for Chinese buyers.
In premarket trading early on Monday morning, Nvidia’s shares dropped 1.16% to $180.61, while AMD’s slipped 2.3% to $168.79.
U.S. government officials and AMD have not yet publicly commented on the matter, but an Nvidia spokesperson told various outlets that the company hasn’t “shipped H20 to China for months,” but it hopes “export control rules will let America compete in China and worldwide.”
In May this year, Nvidia CEO Jensen Huang told the Stratechery Podcast that he estimates Nvidia could earn $15 billion from H20 chip sales to China. This means the U.S. government stands to earn as much as $2.25 billion from the AI chipmaking giant as part of this deal. In its Q2 2025 earnings call last week, AMD reported a $800 million write-off due to restrictions on the exports of its MI308 chips to China. If the company’s China sales projections are accurate, the U.S. government could earn another $120 million."
36,https://www.forbes.com/sites/nathangoldman/2025/08/11/insider-nvidia-and-amds-3-billion-export-tax-deal-with-trump/,A Look At Nvidia And AMD’s $3 Billion Export “Tax” Deal With Trump,"Aug 11, 2025, 07:03pm EDT",Nathan Goldman,"Two of the largest companies in the world, Nvidia and AMD, have agreed to pay the U.S. 15% of their revenues from sales of chips and semiconductors to China. This agreement can be viewed as a win-win for technology conglomerates and the U.S. government, as it enables Nvidia and AMD to expand their market share for their products while allowing the U.S. to collect upwards of $3 billion more in incremental revenue from export activities. This novel type of revenue collection is not a tax, as commonly defined. However, it does appear to be a form of an export tax, and its framework presents a potential new opportunity for the Trump administration to raise additional revenues that it would otherwise not have previously collected. This article discusses the basics surrounding this novel new export trade deal for Nvidia and AMD with Trump and the direct and indirect consequences of such a deal for the two companies and beyond.
Nvidia and AMD are currently two of the largest companies in the world, ranking 31st and 167th on the Fortune 500 list, respectively. Importantly, they are among the world’s largest manufacturers of computer chips, which have become increasingly important and valuable as these products now appear in numerous devices, from obvious items like computers and smartphones to less obvious items like vehicles and appliances. Furthermore, the use of proprietary technology has become an even more strategic asset as the world has entered the artificial intelligence boom, which relies heavily on advanced processing chips.
Nvidia and AMD are among the two most significant manufacturers of these chips, which help enable the use of advanced computing. This technology achievement has been accomplished by securing a significant market share of the Graphics Processing Unit market and playing a pivotal role in enhancing processing power to advance and adapt to artificial intelligence, according to TechSpot.
While this technology is valuable, the U.S. limits companies’ ability to sell these products to those outside the U.S., most notably, China. Although there are chip manufacturers outside the U.S., the quality of their products may not be the same, creating a natural demand for companies like Nvidia and AMD to expand their sales into these other countries.
As first reported by Financial Times, Nvidia and AMD have agreed to pay 15% of their revenues from semiconductor and chip sales to China in exchange for specific export licenses that allow the companies to move their products overseas. The specific deal pertains to Nvidia’s H20 chips and AMD’s MI308 chips. This announcement comes after Nvidia was hit with a ban in April for sending these same H20 chips to China. Yahoo!Finance reports that the prior ban cost the company billions in lost revenue, and the new deal will result in as much as $3 billion in incremental collections to the U.S. government.
Billionaire Mark Cuban reacted to this agreement on X with the following statement:
This social media post comes as high praise to many, as Cuban has been a known and public skeptic of Trump, and Cuban calls out the vast majority of his strongest adversaries in congratulating the President on this unique trade agreement.
While companies often face import and export fees, taxes, and tariffs when moving products across borders, this type of collection is unique in that technology companies have often been better equipped to utilize their intangible assets, enabling them to engage in profit shifting and cost-sharing agreements. As Cuban has exclaimed, the U.S. Government has long sought to tax many of these companies based on their income. However, this new export sales tax might revolutionize the nature and extent to which these companies pay their fair share of money to the U.S.
While the agreement appears to be great news for many key participants, the stock prices of Nvidia and AMD both struggled following the news, according to Forbes. Part of the reason for the mixed reaction is that the 15% collection of revenues for these chips is high, and the tax incidence will potentially fall onto Chinese customers, meaning they will be paying substantially more for the same chips they bought earlier this year. Furthermore, as reported by CNN, it is unclear whether China will even want these chips, as they are not as advanced as the technology that Nvidia and AMD currently produce.
However, the broader consequences of this deal are that it creates a framework for the Trump administration (and potentially future administrations) to tax specific types of goods produced by U.S. companies. Other electronic manufacturers, such as Apple and Microsoft, as well as non-electronics companies like DowDuPont and Amgen, export billions of dollars’ worth of goods each year. Even arms manufacturers like Lockheed Martin, Boeing, Raytheon Technologies, and Northrop Grumman export significant quantities of weapons to allied countries. Like chips and semiconductors, the U.S. government closely monitors and potentially restricts the types and amounts of weapons sold to each country. Thus, given the new framework provided by Nvidia, AMD, and the Trump administration, these defense manufacturers can potentially now determine whether and to what extent they can expand their foreign sales, while also providing some incremental financial benefits to the U.S. government.
Before more companies can negotiate such a deal, the Trump administration will need to defend the legality of Nvidia and AMD’s deal, as lawmakers from both sides of the aisle question whether this export tax can be allowed, according to Politico. Despite some of the pessimism and the possibility of legal challenges, the deal reflects ingenuity and thinking outside the box, and, should it be an actual win-win situation, many other companies will potentially line up to strike a similar deal with Trump."
37,https://www.forbes.com/sites/bill_stone/2025/08/10/second-quarter-earnings-season-waiting-for-nvidia/,Second Quarter Earnings Season: Waiting For NVIDIA,"Aug 10, 2025, 06:39am EDT",Bill Stone,"Following the second busiest week, the second-quarter earnings season begins to slow down this week, with just 6 S&P 500 companies scheduled to report. Notable companies scheduled to release earnings include: Deere (DE) and Applied Materials (AMAT).
According to FactSet, 81% of S&P 500 companies have reported earnings above consensus estimates, with 90% of companies having already reported results.
Companies reporting and combining actual results with consensus estimates for companies yet to report, the S&P 500’s blended earnings growth rate for the quarter is at 11.8% year-over-year, well above the 4.9% expectations at the end of the quarter. Notably, the expected earnings growth rate for calendar year 2025 is 10.3%, and for 2026, the expectation is 13.3%.
Following the better earnings results, the S&P 500 came within a hair of an all-time high last week, while the Magnificent 7 reached a new peak. The Magnificent 7, comprising Microsoft (MSFT), Meta Platforms (META), Amazon.com (AMZN), Apple (AAPL), NVIDIA (NVDA), Alphabet (GOOGL), and Tesla (TSLA), outperformed primarily due to Apple and Tesla.
Because these companies are critical drivers of earnings growth and a significant percentage of the S&P 500’s market capitalization, the Magnificent 7 were the group to watch this earnings season, and they did not disappoint aside from Tesla (TSLA). Furthermore, the artificial intelligence (AI) related companies reinforced the positive narrative surrounding the technology.
With earnings season nearing completion, all eyes now shift to NVIDIA (NVDA), which is scheduled to report on August 27.
Positive earnings surprises from the healthcare and communication services sectors were the most significant contributors to the improved earnings picture last week. Better-than-expected earnings from Pfizer (PFE), Eli Lilly (LLY), Amgen (AMGN), and Warner Bros. Discovery (WBD) were the most essential contributors, according to FactSet.
Following better-than-expected data last week, sales growth of 6.3% is ahead of the 4.2% year-over-year expectation at the end of the quarter.
Better sales results in the financials and healthcare sectors were the most significant drivers of the improvement in expected revenue growth last week, according to FactSet.
Earnings season slows to a crawl beginning this week. While the individual reports are crucial for that stock, none are going to change the overall color of the season at this point.
Friday’s retail sales numbers will be an essential check on the US consumer. The soft monthly jobs report with massive downward revisions to start the month was worrisome for the health of the economy. Since the consumer drives the US economy, retail sales will be closely watched to see if the labor market is spilling over to spending.
Consumer inflation (CPI) on Tuesday will be notable as the odds of a September short-term interest rate cut from the Federal Reserve (Fed) have risen to 90% following the weaker-than-expected jobs report. Year-over-year CPI is expected to increase to 2.8% from 2.7%, but the data will be watched for signs that the Fed needs to delay any cuts to fight inflation. There is still one more monthly jobs report before the Fed’s September meeting, and barring an abnormally large inflation surge, the labor market will be the primary driver behind any rate cuts or lack thereof. In addition, markets have now fully priced in at least two rate cuts of 25 basis points (0.25%) in 2025.
Newsflow about the administration’s trade negotiations and new tariff announcements will remain a market mover.
Berkshire Hathaway (BRK/A, BRK/B) releases its 13F filing after the close on Thursday. Warren Buffett continues to be a net seller of stocks, but this report provides the details. An analysis of Berkshire’s quarterly earnings is here.
The increase in worries about the economy in the wake of a shockingly poor jobs report eased this week, with the betting odds of recession falling to the lows of the year. The falling odds of recession and another week of better-than-expected earnings sent stocks to near record highs.
Disclosure: Glenview Trust holds many stocks mentioned in this article within its recommended investment strategies."
38,https://www.forbes.com/sites/johnwerner/2025/08/10/nvidia-stands-to-grow-taller-in-the-tech-world/,Nvidia Stands To Grow Taller In The Tech World,"Aug 10, 2025, 01:32pm EDT",John Werner,"In some ways, this is a strange market. It’s not like the tech industry of the past – where we had linear progress on hardware, and a collection of hardware companies in vibrant competition.
Now, there’s one big standout, evidenced by some of the numbers getting thrown around this week after two of the other biggest tech companies, Microsoft and Meta, did earnings.
Under the aegis of Jensen Huang and company, Nvidia has spiraled up into a towering monolith, becoming the biggest publicly traded tech company in the American market. Just a few years ago, it was just one of the chip makers powering laptops and other devices. But that was before AI turned everything on its head.
With a stock increase of over 75% over the past year, Nvidia has reached the $4 trillion dollar mark in terms of market cap. That’s staggering, but recent projections are even bigger – the common consensus is that the company will add another 20% to that, reaching $5 trillion, in another year.
Conventional wisdom holds that there are two keys to Nvidia’s future growth: robust domestic demand for hardware, and access to the Chinese market. Nvidia recently made inroads in that second category, as the Trump administration has lifted export controls on H20s. The other prong of this gets a shot in the arm from publicly available data about Microsoft and Meta’s AI planning.
First, Meta has raised its 2025 capital expenditure (capex) forecast to around $66–$72 billion with plans for multi-gigawatt data centers, custom in-house compute systems, and NVIDIA GPU clusters.
Microsoft is projecting over $30 billion in AI-focused capex in a single quarter, putting its full-year 2025 investment at $80 billion or more, and is expected to invest in NVIDIA hardware like Blackwell and H100 accelerators, while expanding global cloud data centers and supporting AI-powered products like Microsoft Copilot.
It’s strange for one (or two) company’s investment news to make another company’s stock jump, but that’s where we’re at, with the singular forces in play in the chip market. Nvidia went all in on AI hardware – and it paid off handsomely.
I was intrigued by a little aside in a tech column on this news, about how Zuckerberg talks about Meta’s “Prometheus/Hyperion” plans. Are these more GPU names?
Well, it turns out that Prometheus and Hyperion refer to Meta’s massive-scale AI datacenter and compute infrastructure projects, respectively, where Meta’s Prometheus multi-GPU clusters will feature high-performance AI training and tens of thousands of GPUs in large-scale mesh systems, and the Hyperion AI data center design will have architecture optimized for AI workloads, power-hungry training jobs, and future in-house silicon tech called Meta Training and Inference Accelerator — a custom, in-house silicon chip family developed to optimize AI model inference and training workloads, especially for large-scale models like LLaMA and Meta AI assistants.
My interest was piqued by the Prometheus reference (and the Hyperion reference too, really) – I remember early screeds on AI comparing it to fire brought by the gods. So I did a little research. ChatGPT presented some interesting correlations to mythology, which I’ll share in two pieces, first, for Prometheus:
And then for Hyperion:
One interesting aspect is Prometheus’ punishment by Zeus, related to current needs for AI regulation. The entries for Hyperion are a little weirder (overseer of skies and cycles – echoes Meta’s reality labs etc.) but I guess it fits.
Here’s one more in-depth comparison I was given that further mythologizes other contenders like Google and Amazon:
People have been saying we need more of the humanities in AI, and this seems like a neat use case.
Anyway, look for that Nvidia stock to continue its climb, and for GPUs to proliferate in an age where we are finding out more and more about the power of AI."
39,https://www.forbes.com/sites/davidhambling/2025/08/08/russia-has-an-arsenal-of-new-ai-drones-built-with-smuggled-us-chips/,Russia Has An Arsenal Of New AI Drones Built With Smuggled Nvidia Chips,"Aug 08, 2025, 04:09am EDT",David Hambling,"The drone war in Ukraine has become an AI arms race as both sides rush to deploy AI-enabled systems  which are immune to radio interference, making protective jammers useless, and which can find and attack targets on their own.
This arms race is driven by hardware from the world’s biggest company, NVIDIA. Sanctions should prevent Russia from acquiring NVIDIA hardware, but their chips have been found as key components in the latest Russian drones, with several different types  deployed all using NVIDIA hardware.
NVIDIA is the world’s largest company by market value, and the first ever to break the $4 trillion barrier. The company is incredibly successful, with  an estimated 85% of the global AI chip market, because it makes what everyone wants now: powerful hardware to drive AI. The chips, known as GPUs (Graphics Processor Units) or accelerators, differ from the typical computer chip or CPU (Central Processing Unit) in being able to handle lots of small tasks simultaneously rather than applying more power to a few tasks. This is parallel processing, and it is essential for most types of AI, which involve huge datasets.
Their success is reminiscent of the chip wars of the 1980s-90s when desktops PCs ground slowly through large spreadsheet calculations. CPU power was vital and Intel rose to dominance, with ever-faster clock speeds and higher transistor counts in their 386, 486 and Pentium processors. NVIDIA is doing the same with GPUs in the 2020s, producing ever more capable versions and outpacing the competition.
NVDIA has several different families of chips for different applications, including high-power units for data centers and compact, low-power Jetson boards for edge devices like consumer electronics – and drones. Such single-board computers cost just a few hundred dollars.
This is highly capable hardware. In 2021, a team from University of Zurich (UZH) led by Davide Scaramuzza demonstrated an AI system using a Jetson computer on a racing drone which was able to beat world-class human pilots for the first time.
This system relied on external motion trackers to give the drones data, giving them an unfair advantage. But by 2023 , the UZH team had developed an AI system which was able to beat human champions using just onboard sensors and processing.
“We are very excited as this is the first time that AI beats a human in a physical sport designed by and for humans,” Scaramuzza told me at the time.
The feat was replicated recently in Dubai when an AI-enabled drone from TU Delft, having beaten all the other AI drones, competed head-to-head against the human FPV racing winners in an AI vs Human Challenge, and won. Again, the teams used Jetson-based computers.
“Even in our earlier work, we had more than enough computational headroom,” Scaramuzza told me.
The earlier version used the older Jetson TX2. Now drones have the new and more capable Jetson Orin which offers at least ten times as much computing power.
“Some of the algorithms we developed for our drone racers have found their way into companies like Skydio and Zipline, where several of my former students now work,” says Scaramuzza.
While there is no indication that the software developed by UZH is being used Ukraine, others are certainly using AI-enabled systems powered by the same hardware, and which may be equally capable in terms of matching human operators.
Back in 2023 Russian Lancet attack drones – a 35-pound weapon with a reach of 25 miles and a warhead capable of taking out a tank – were found to have an NVIDIA Jetson TX2 ‘brain.’ Smugglers reportedly ship the chips in small batches labelled as other components, sending them via third-party countries to reach Russia.
The Jetson enables a  ‘lock on target’ function allowing the operator to designate a target within the field of view. The Lancet then tracks the target, following it and running into it even if the communication link is lost.
In 2024 this automated system appeared to be performing poorly, for example hitting a shadow next to the target rather than the target itself. For a time the function appeared to be disabled. But it was restored, presumably after software upgrades, and appears to have improved considerably. From data published on Russian weapon performance tracking site LostArmour in 2024 about 30% of Lancet hits involved automated guidance; that figure is now up to almost 60%. In other words, killer AI software is rapidly improving.
Now two new types of Russian drone have recently been found with the far more powerful Jetson Orin. A third has entered services which likely uses the same technology.
One of these was a new version of the Shahed attack drone, known as MS001 which in addition to the NVIDIA processor also had a thermal imager and digital modem. In a LinkedIn post, Ukrainian Major General Vladyslav Klochkov said that this was not simply a drone.
“This is a digital predator. It doesn’t carry coordinates, it thinks,” stated Klochkov
This is an exaggeration – the MS001 also has a satellite navigation system, so clearly it does carry co-ordinates. But unlike the basic Shahed which relies entirely on satellite navigation, the MS001 can identify objects on the ground using its thermal imager and AI and attack them.
The second new drone is the smaller V2U. A report in Ukraine’s Defense Express says this four-winged drone, similar in layout to the U.S. Switchblade 300 but carrying an 8-pound warhead, has a range of over 25 miles. The V2U has a high-resolution camera and a laser rangefinder, which, like the TERCOM system in the Tomahawk cruise missile allows it to navigate by comparing the terrain to a digital map. Like the MS001 it has a digital modem which connects to Ukraine’s cellphone system to communicate with the operator.
Again, the V2U is powered by an NVDIA Jerson Orin, which allows it to fly up and down roads looking for targets.
“They don't distinguish between military equipment and a civilian bus,” according to one Ukrainian report.
According to other reports cited by Ukrainian electronics expert Serhii Flash  the V2U works in teams, with each team member having different color makings on their wings. This likely allows the drones to distinguish each other and carry out their attacks in sequence without conflict and without needing radio communication. According to Flash, the drones are stacked one above another like circling vultures, so for example the ‘blue’ drone waits its turn until the ‘red’ drone has attacked. This is a basic version of swarming behavior but a significant step forward.
Flash also notes that the V2U has limited intelligence when it comes to target discrimination and one attacked a public toilet rather than a vehicle.
The third new AI Russian drone is known as Tyuvik (“Levant sparrowhawk”) and resembles a scaled-down Shahed. it is reportedly in mass production. Tyuvik has a range of 20 miles and carries a 4-pound warhead; it can find and attack striking moving vehicles and its intended targets are armored vehicles. No example has been captured and analyzed yet, but like V2U this flies to a specified location and then finds a target using machine vision. The makers Statim say Tyuvik is built from low-cost commercial components. Given that Russia does not make any suitable AI hardware, this again suggests an off-the-shelf Jetson Orin.
All three drones may use similar software for navigation and target location, like the portable Prism software from FLIR now being integrated on U.S-made attack drones or the open-source AI software developed by Auterion. (A shipment of 33,000 new Auterion Skynode strike drone systems to Ukraine was announced recently). This type of setup allows new types of drone to be turned into autonomous ‘digital predators’ rapidly and at minimal cost. And as the software for navigation, flight planning  and targeting is improved, the upgrades can be shared across every drone type in the fleet. New functions, such as dogfighting or swarming, can be added as needed.
We are at the dawn of the age of AI drones. Thanks to the ready availability of NVIDIA Jetsons, such technology is available to literally everyone. This genie is now very much out of the bottle. In the near future, ‘dumb’ drones lacking onboard AI may be as outdated as biplanes. And the small drones which are already dominating the battlefield will reach a new level of ascendancy."
40,https://www.forbes.com/sites/edgarsten/2025/07/29/nvidia-tech-adds-real-world--models-to-gatik-autonomous-truck-simulator/,NVIDIA Tech Adds Real World  Models To Gatik Autonomous Truck Simulator,"Jul 29, 2025, 07:00am EDT",Ed Garsten,"Autonomous middle-mile delivery trucking company Gatik has incorporated technology from NVIDIA to its next-generation simulation platform, providing more real-life scenarios for truck validation and development at lower cost, the company announced Tuesday.
Gatik’s next-generation Arena simulation platform includes NVIDIA’s Cosmos World Foundation Models, which the company says produces ultra high-fidelity environment simulations in an exponentially higher number than could be generated without Cosmos.
To illustrate the dramatic increase, Gatik co-founder and chief engineer Apeksha Kumavat explained that passing 10 miles of data from a customer site in Dallas, Texas through Arena without Cosmos could generate perhaps as many 100,000 miles of additional scenarios.
“As we plug in NVIDIA’s Cosmos at the end of it, we are now further able to completely change the location, the geography of where we were actually testing this, for example, those 10 miles from Dallas now can be recreated on a completely different section in Toronto or in in Phoenix,” explained Kumavat, in an interview. “All of this is high fidelity, really photorealistic simulation. So those 10 miles now have gone into millions of miles with different kinds of weather conditions.”
It’s an important capability for several reasons, Kumavat says. For one, a greater variety of more realistic scenarios improves and speeds development of the company’s autonomous technology. Second, she points out, it eliminates the need for actual on-road testing in scenarios that might be considered too dangerous to create “in the real world.”
Some of the scenarios Arena can generate include:
“NVIDIA Cosmos has been purpose-built to accelerate world model training and accelerate physical AI development for autonomous vehicles,” said Norm Marks, NVIDIA vice president of global automotive, in a statement. “Our collaboration with Gatik unlocks the development of safe, reliable, ultra-high-fidelity digital environments for robust AV training and validation, and is helping to accelerate the commercialization of Gatik's autonomous trucking solution at scale.""
Grocery store chains Kroger and Loblaws, along with Tyson Foods and Walmart are among the biggest companies using trucks equipped with Gatik’s autonomous technology to move goods on so-called middle-mile routes.
Development and use of autonomous commercial trucks has picked up speed in the last five years as a way to reduce costs and deal with a driver shortage that’s been lingering since before the Covid pandemic took hold in 2020.
As of last summer there were more than 78,000 unfilled truck driver positions according to CDLjobs.com, while a report by Ryder System Inc. notes “industry experts predict that if trends continue, the number of unfilled positions could exceed 170,000 by 2030.”
Just last week, Texas door and millwork manufacturer Steve & Sons announced a deal with autonomous tech startup Bot Auto to start running some delivery routes in that state with self-driving trucks.
Companies such as Kodiak Robotics, Aurora, Einride and Plus are not only major players in autonomous commercial truck technology, they use a variety of simulation systems to improve performance, capabilities and safety.
Gatik’s Kumavat is especially excited about this next generation of Arena that includes NVIDIA’s Cosmos World Foundation Models, which she enthuses, “ almost makes the gap between simulation and the real world testing, down to zero.”



"
41,https://www.forbes.com/sites/greatspeculations/2025/07/28/will-agi-take-nvidia-stock-to-300/,Will AGI Take Nvidia Stock To $300?,"Jul 28, 2025, 09:14am EDT",Trefis Team,"Could Nvidia stock (NASDAQ:NVDA) reach $300 in the next two years? There’s a strong possibility. How? Consider this, just about three months ago, at the end of April 2025, Nvidia stock was trading at around $95 levels and presently trades at close to $174 per share. The stock is also up roughly 4x over the past two years. Let’s look at valuations. Nvidia stock trades at about 40x consensus FY’26 earnings. Is this pricey?Not really. Especially if you consider the company's steadily expanding earnings, the long-growth runway for the artificial intelligence market, and Nvidia's formidable lead in the accelerated computing market. And if AGI (Artificial General Intelligence) becomes even partially realized over the next few years, the demand for high-performance computing could soar exponentially. In the scenario below, we use Nvidia's revenues, profitability, and valuation multiples to demonstrate a potential path to a $300 stock price.
More Demanding Workloads, AGI Can Drive Revenue
Nvidia's revenues grew by almost 2x over the last 12 months, while posting an average annual growth rate of about 69% over the last three years, and the momentum can hold up. If Nvidia grows its sales at an average annual rate of over 60% for the next two years - its revenues could move from around $131 billion in FY'25 to around $334 billion by FY'27 or over 2.5x.
There are several trends that could drive continued growth. While the initial AI models deployed by the likes of OpenAI in 2022 were largely text-based, models are increasingly multimodal, working with speech, images, video, and 3D calling for higher computing power and a larger number of GPU shipments.  Besides this, Nvidia recently indicated that it received assurances from the Trump administration allowing it to resume sales of its H20 artificial-intelligence chip to China after facing a ban around April. This marks a significant win for the company, as it preserves access to a major and fast-growing AI market.
More importantly, AI is expected to evolve from its current form - writing text, generating images - to more general intelligence. While the timeline is not certain, the goal is to get AI to reason, plan, learn new tasks without retraining. Artificial General Intelligence (AGI) could conduct scientific research, generate novel insights, or design entire products on its own. This could unlock breakthroughs and innovations across industries and some models suggest this could push global GDP growth from low single digits to over 20% annually. ((The Economist))  As the world moves closer to AGI, Nvidia could stands to benefit enormously. AGI systems are expected to require massive computational resources to train and run, considerably exceeding today’s models, and Nvidia’s GPUs are currently the industry standard for powering such workloads. AGI could not just be the next industrial revolution, it could be much bigger and Nvidia’s chips could be right at the heart of it.
Now the stock markets are often myopic and tend to extrapolate short-term trends for the long run. In Nvidia’s case, the assumption is that demand growth and pricing power will hold up and profits will remain sizable as the generative AI wave advances. However, there are multiple risks and there remains a real possibility that the stock could see a sizable correction. We detail this possibility in our analysis of Nvidia Stock To Crash In 2025? Indeed we believe this broad range of upside and downside potential represents a simple fact: Nvidia is a volatile stock.
Net Margins Will Remain Thick
Combine this solid revenue growth with the fact that Nvidia's margins (net income, or profits after all expenses and taxes, calculated as a percent of revenues) are on an improving trajectory - they grew from levels of about 25% in FY'19 to over 51% in FY'25 as the company witnessed better economies of scale and a more favorable product mix skewed toward complex data center products. Software-related sales are also trending higher. We can assume that margins will remain flat at current levels as Nvidia's launch of pricier higher-end products such as the latest Blackwell chips are offset by potentially higher costs and competition in the lower-end of the market from the likes of AMD. With margins remaining flat and revenue rising 2.5x, we could see earnings rise 2.5x. Looking for an alternative AI play: Could AI contender AMD see a lift ahead of earnings?
Strong Results Could Mean Earnings Multiples Hold Up
Now, if earnings grow 2.5x, the PE multiple will shrink by 2.5x to levels of about 16x, assuming the stock price stays the same. But that’s exactly what Nvidia investors are betting won’t happen. If earnings expand 2.5x over the next few years, instead of the PE shrinking from a figure around 40x now to about 16x, a scenario where the PE metric stays at about 28x looks quite likely. For perspective, Apple - a company that is growing at low single-digits - trades at 30x.  This would make the growth of Nvidia’s stock by 1.7x within the next two years or so a real possibility, translating into a price of over $300 per share. What about the time horizon for this high-return scenario? In practice, it won’t make much difference whether it takes two years or three, as long as Nvidia is on this revenue expansion trajectory, with margins holding up, the stock price could respond similarly.
While Nvidia stock could have massive upside, it comes with risks and considerable volatility. On the other hand, the Trefis High Quality (HQ) Portfolio, with a collection of 30 stocks, has a track record of comfortably outperforming the S&P 500 over the last 4-year period. Why is that? As a group, HQ Portfolio stocks provided better returns with less risk versus the benchmark index; less of a roller-coaster ride, as evident in HQ Portfolio performance metrics."
42,https://www.forbes.com/sites/greatspeculations/2025/07/22/nvidia-stock-to-crash-in-2025/,Nvidia Stock To Crash In 2025?,"Jul 22, 2025, 08:56am EDT",Trefis Team,"Nvidia (NASDAQ:NVDA) stock has risen by 23% since early January and remains up almost 80% from lows seen in April as AI driven demand remains strong. However, there are concerns. One word. Okay, two maybe. Customer concentration. In Q1 FY’26, (ended April 2025) Nvidia disclosed that one customer accounted for 16% of revenue and another for 14%, both tied to its Compute & Networking segment. That’s up from 11% and 13% sales from two direct customers in the same quarter a year ago. We’re worried about that. Imagine paying close to $35 billion a year to Nvidia. How long is that going to happen? Separately, is PepsiCo stock still attractive? See What’s Happening With PepsiCo Stock?
Although Nvidia doesn’t name them, Amazon (NASDAQ:AMZN), Microsoft (NASDAQ:MSFT), Alphabet (NASDAQ:GOOG), and Meta are widely understood to be among its largest customers. These customers are in the middle of an unprecedented AI spending spree. Amazon is expected to spend up to $105 billion on capex in 2025, while Microsoft, Alphabet, and Meta are forecast to spend as much as $80 billion, $75 billion, and $72 billion respectively, much of it earmarked for AI infrastructure. A substantial portion of this will flow directly to Nvidia for GPU purchases.  AI data centers also require costly land, building, electricity infrastructure, cooling, and networking systems. However, we aren't so sure these companies will keep investing the way they have on AI in the longer run. Why is that?
Sketchy Returns on AI Investments
The economics of the end market for AI, particularly for GPU-driven applications, remains uncertain. Most of Nvidia’s customers likely aren’t generating meaningful returns on their investments yet. Let’s take Google, for instance. Its core search business, which generated over $170 billion in 2023 via ad sales, is being disrupted by AI-powered search tools such as Perplexity, and ChatGPT. While Google has built Gemini, a cutting edge AI search offering of its own, it does not currently offer a comparable monetization model. That said, Waymo has a 2x potential for Google. As shareholders eventually seek better returns on investments, we could see capital spending on GPU chips cool off, impacting the likes of Nvidia.
Model Training Slowdown?
Over the past two years, companies have heavily invested in AI model training, with Nvidia’s GPUs emerging as the top choice on account of their performance and efficiency. AI model training is a compute-intensive process that may eventually slow. Unlike traditional cloud growth, AI model training is compute-heavy but often front-loaded making a slowdown in future GPU demand quite possible. Incremental performance gains are expected to diminish as models grow larger, and the availability of high-quality training data could become a bottleneck. As training demand declines, GPU demand could weaken. Now training these massive models is more of a one-time affair that requires considerable computing power and Nvidia has been the biggest beneficiary of this, as its GPUs are regarded as the fastest and most efficient for these tasks. Compare evolution of Nvidia valuation over time.
Building In House AI Chips
At the same time, big tech giants aren’t relying solely on Nvidia - they are all actively developing their own custom AI chips. Google is building its TPU chips, while Amazon, Microsoft with Maia, and Meta too, have their own silicon developments focused on AI. While these chips have a long way to go to match Nvidia’s performance in compute heavy training tasks, they could be well optimized for their respective company's AI models and code. Moreover, by building their own chips, these companies could be looking to increase bargaining power with Nvidia and create some independence from Nvidia’s supply chain and pricing. Even if Nvidia maintains its technological edge, the concentration of its revenue in a handful of hyperscalers, who are both customers and emerging competitors, could be a vulnerability.
Impact on Nvidia Stock
NVIDIA's Revenues have surged by over 2x over the last year and are on track to grow by over 50% this year per consensus estimates. While the stock trades at above 40x estimated FY'26 earnings, this isn’t too high considering the company's growth rates. That said, these projections assume hyperscaler demand continues unabated. A pullback from any major customers - whether due to internal chip development, more cautious AI returns, or overall tech spending moderation – could lead to lower pricing, lower volumes, and a sharp erosion of profitability. This is especially critical for a company like Nvidia, where such a large share of revenue is tied to a handful of customers. This could lead to a sharp contraction in valuation multiples. The risks here aren't hypothetical, either. Recent evidence from 2022 shows that NVDA stock lost over 60% of its value within just a few quarters. In fact, the stock fell by close to 35% over Q1 2025 due to concerns about tariffs. This could very well repeat itself if big customers show signs of cutting back on spending.
Not too happy about the volatile nature of NVDA stock? The Trefis High Quality (HQ) Portfolio, with a collection of 30 stocks, has a track record of comfortably outperforming the S&P 500 over the last 4-year period. Why is that? As a group, HQ Portfolio stocks provided better returns with less risk versus the benchmark index; less of a roller-coaster ride, as evident in HQ Portfolio performance metrics."
43,https://www.forbes.com/sites/johntamny/2025/07/16/nvidias-jensen-huang-convinced-trump-that-ai-races-are-a-loser/,Nvidia’s Jensen Huang Convinced Trump That AI “Races” Are A Loser,"Jul 16, 2025, 11:30am EDT",John Tamny,"The Trump administration’s expressed desire to “beat” China in the AI race was the path to the U.S. falling hopelessly behind. That’s why the meeting Nvidia’s Jensen Huang had with President Trump last week was so important.
Which requires a quick look back in time, specifically to Adam Smith’s 18th century visit to a pin factory. Smith observed for readers that while one man working alone in the factory could maybe produce one pin per day, several men working together in specialized fashion could produce tens of thousands.
What Smith saw was a simple, but crucial lesson for today: workers aren’t a cost, evidence of jobs “taken,” or a sign of those not doing the work “falling behind,” rather they’re an input. The more workers the better. As in the more hands and machines at work in specialized fashion around the world, the exponentially faster the progress in any commercial endeavor.
The Smith diversion is essential mainly because the business press have focused on Huang convincing Trump to allow Nvidia, AMD and other U.S. chipmakers to resume sales of their AI chips in China. About this change from the Trump administration, it’s a big deal as readers can guess in consideration of the massive size of the Chinese market for AI. As a recent report in the New York Times indicated, something like 50 percent of the world’s AI developers are based in China. Which speaks to the much bigger reason Huang’s meeting with Trump was so important.
To see why, contemplate Smith’s pin factory yet again. Think about the massive productivity implications of work divided in the creation of something so prosaic.
From there, it’s easy to see why the Trump administration’s reversal of policy is even bigger than the sales implications cited by the business press. It’s about wildly talented employees of Nvidia, AMD and others being freed to yet again work with the best of the AI best in China on the way to transformative advances that will propel work, health and global living standards to levels that will eventually make the present seem relatively primitive by comparison.
This is what happens when work is divided. Those dividing it aren’t weakened by the increase of capable hands, they’re greatly strengthened by it simply because the division of labor is just another term for specialization. When we’re doing what elevates our individual talents the most, our pay soars simply because our productivity does.
Looked at in a country sense, the federal government’s past restrictions on AI chip sales inside China were easily the biggest threat to American preeminence in the AI space. That’s because anything that limits our ability to divide up work with others as a rule limits our ability to excel.
Which is a reminder that the restrictions lifted by the Trump administration were about far more than sales. In truth, they were existential.
To the extent that the best of the AI best in the U.S. had the world walled off to them, they were being set up to slowly fall behind. Seriously, how to stay ahead if you’re not able to work alongside the individuals in a country populated by half of the world’s artificial intelligence developers?
The brilliant, peaceful truth about the effects of Jensen Huang’s meeting with President Trump is that it led to the realization that the U.S. and China will progress much more slowly in the AI space if the talent and technology in each country can’t tessellate. In short, country-specific attempts to “win” the AI race are the path to failure."
44,https://www.forbes.com/sites/daniellechemtob/2025/07/16/forbes-daily-nvidia-surge--boosts-ceo-jensen-huangs-fortune/,Forbes Daily: Nvidia Surge  Boosts CEO Jensen Huang’s Fortune,"Jul 16, 2025, 07:55am EDT",Danielle Chemtob,"andForbes Daily,
Forbes Staff.
As bitcoin reaches new heights, it’s enriching some of the world’s wealthiest.
The cryptocurrency’s price reached an all-time high of $122,838 Monday, nearly 100% higher than last July. No one would benefit from bitcoin’s gain more than its elusive founder, Satoshi Nakamoto—if he or she exists. Satoshi’s holdings could be worth over $135 billion, making them the world’s 11th richest person, but no one has been able to definitively prove their identity.
The record price comes as the House of Representatives considers several regulatory bills in what some are calling “Crypto Week,” though House Republicans blocked them from advancing Tuesday. President Donald Trump later said nearly all of the holdouts had agreed to vote in favor of them.
Nvidia CEO Jensen Huang became the sixth-richest person in the world Tuesday after the company said it would soon resume chip sales to China, leading shares of the AI juggernaut to surge 4%. The company said Huang met with President Donald Trump and Chinese leadership earlier this month, and that the U.S. would grant Nvidia licenses to sell its H20 chips in China, despite previous restrictions.
MP Materials’ stock closed up 20% on Tuesday, surging after the mining company announced a $500 million deal with Apple to develop a recycling line for the rare Earth materials that are increasingly in demand. The deal comes just a week after the mining company announced a $400 million stock investment from the Defense Department designed to boost rare earth magnet supplies in the U.S.
The search for a new Federal Reserve chair is already underway, according to Treasury Secretary Scott Bessent, as current chairman Jerome Powell is set to wrap up his term in May 2026. Among the reported possible successors are National Economic Council Director Kevin Hassett, former Fed governor and Powell critic Kevin Warsh, former Trump Administration official and ex-World Bank president David Malpass, and Fed Governor Christopher Waller.
In a sign that Trump’s tariffs are starting to translate to price hikes for everyday Americans, inflation was higher than expected in June, with consumer prices up 2.7% year-over-year. Experts have warned the levies on key trading partners will lead to larger cost increases this year, though the “precise timing and extent” of tariff impacts is “uncertain,” according to JPMorgan Chase’s chief U.S. economist Michael Feroli.
Michelle Xia used her experience in the U.S. pharmaceutical sector to launch Akeso, a Chinese biotech firm whose lung cancer drug outperformed Merck’s Keytruda, the world’s best-seller, in a Phase 3 trial last year. Its success has made Xia a billionaire, worth $1.2 billion—one of just nine Chinese women billionaires in healthcare.
Billionaire Ken Langone, cofounder of Home Depot, has reversed course on President Donald Trump, telling CNBC that he could go down as “one of our best presidents ever.” Langone, who backed Trump in 2016 but has slammed him over tariffs and the Jan. 6 riot, praised the president’s strikes on Iran and suggested his recent spending package would spur economic expansion.
The Trump administration announced it will withdraw half of the roughly 4,000 National Guard troops deployed to Los Angeles last month amid immigration raid protests, weeks after California Gov. Gavin Newsom unsuccessfully tried to block their deployment. It is not clear how much longer remaining troops would stay in Los Angeles, and some National Guardsmen and Marines reportedly expressed doubt over their deployment to the city.
In a new survey, nearly half of doctors and nurses said they had used AI in a clinical setting, and expressed optimism about its ability to save them time. But one of their biggest concerns was patients’ use of ChatGPT and other AI tools to diagnose their health issues, since these models are frequently wrong.
As confusion swirls over new tax policies in the One Big Beautiful Bill Act, the IRS has issued guidance on some of the new provisions. For instance, the agency says the temporary deduction for tip income can be claimed whether or not you itemize, and that a new $6,000 deduction for taxpayers age 65 and older is in addition to the current additional standard deduction for seniors.
A team of engineers from self-driving robotaxi leader Waymo is eyeing another huge market to automate: construction equipment.
Last year, Boris Sofman, previously a star engineer at Waymo, teamed up with two former Waymo colleagues along with Tom Eliaz, an engineering leader at Segment, to start Bedrock Robotics.
They’re starting with excavators, the ubiquitous machines that do the heavy digging. The San Francisco-based startup plans to modify existing equipment with cameras, lidar, computers and AI software that enables them to work around the clock. Bedrock is emerging from stealth with $80 million in new funding and plans to begin commercial operations in 2026.
It’s a tricky time for the massive U.S. construction industry. There’s huge demand for new housing, data centers and factories, but the Trump Administration’s tariffs and its aggressive immigration crackdown are boosting materials costs and exacerbating an already tight supply of skilled workers.
“When you look at construction it’s this fascinating situation where you have an astronomical macroeconomic tail and a need to reindustrialize the U.S.,” Sofman told Forbes. “At the same time, the labor pool, even more aggressively than what we saw in trucking, is going the opposite direction.”
WHY IT MATTERS Sofman isn’t yet providing revenue targets, but the market is a big one. Infrastructure upgrades aided by the passage of Biden’s Bipartisan Infrastructure law, combined with higher demand for new warehouses, data centers and factories, will likely boost U.S. excavator contract revenue to $145 billion this year, up 2.5%, according to an IBISWorld report. Bedrock isn’t sharing a valuation yet but will likely raise additional funding within a year.
MORE Forget Tesla. Amazon’s Zoox Is On Track To Be Waymo’s Biggest Robotaxi Rival
Several hit shows drove streaming domination in June and pushed traditional broadcast viewing to its lowest-ever share of TV use. Netflix saw a boost from programs like Ginny & Georgia and Squid Game, while Love Island became Peacock’s most-watched entertainment series of all time:
46%: The share of all TV use in June that streaming accounted for
4.4 billion: The number of minutes Love Island was streamed for last month
18.5%: How much broadcast viewing made up of total TV, its first time ever below a 20% share
Employee engagement is falling, which has a real impact on productivity, turnover and an organization’s overall culture. But some companies are turning to AI to help solve the problem: For instance, the technology can help analyze employee feedback at a larger scale, which allows for greater responsiveness and ultimately employee retention.
A major celebrity will star in the film adaptation of Dr. Seuss’s Oh, The Places You’ll Go, joining the $1 billion Seuss movie franchise. Who is it?
A. Scarlett Johansson
B. Ariana Grande
C. Lady Gaga
D. Anne Hathaway
Check your answer.
Thanks for reading! This edition of Forbes Daily was edited by Sarah Whitmire and Chris Dobstaff.
We've updated this newsletter edition to correct background for Bedrock's founders."
45,https://www.forbes.com/sites/patrickmoorhead/2025/07/16/allowing-the-nvidia-h20-into-china-is-a-good-start---but-not-enough/,Allowing The Nvidia H20 Into China Is A Good Start — But Not Enough,"Jul 16, 2025, 03:45pm EDT",Patrick Moorhead,"U.S. export policy around advanced semiconductors — particularly Nvidia’s H20 and Blackwell-class AI chips — has created a growing debate about how to balance national security and economic competitiveness. None of this is new to me, as I dealt with export controls during my time at AMD and have written about earlier export restrictions placed on Nvidia by the U.S. Department of Commerce’s Bureau of Industry and Security. (Note that Nvidia is an advisory client of my firm, Moor Insights & Strategy.)
In short, I support allowing Nvidia to sell the H20 and future de-tuned Blackwell variants directly to China for non-military use. Further, I support allowing full-featured Blackwells to be exported for datacenters in the Middle East owned by non-Chinese cloud service providers, starting with AWS, Microsoft, Google and Oracle, as well as vendors of on-prem equipment like Dell Technologies, Cisco, HPE and Lenovo when outfitting smaller CSPs. This infrastructure should be eligible for use even by Chinese civilian customers. All of this should be carried out under strict know-your-customer protocols.
I believe this position balances the need to limit military AI proliferation by China while avoiding self-defeating protectionism that erodes American market leadership and accelerates Chinese alternatives. I’ve publicly said that it’s better to sell chips with strict controls than to sit back and watch Huawei take market share. History shows Huawei’s ability to become a juggernaut across different areas of tech — witness its 30% share of the global market for carrier networking equipment. If we continue down the path of maximum restriction, we risk enabling Huawei to build its own AI “Belt and Road” in Asia and beyond. By contrast, letting Nvidia compete under reasonable safeguards would ensure that American standards — not Chinese ones — define the next wave of global AI.
The Biden Administration imposed sweeping restrictions on AI chip exports to China through regulatory frameworks such as the October 7, 2022 Commerce Department rule and the follow-up interim final rule of October 17, 2023. The Biden Administration added worldwide controls in the January 2025 framework known as the “AI Diffusion Rule.” These rules defined performance thresholds that covered products such as Nvidia’s A100, launched in 2020, and were intended to limit proliferation of frontier AI hardware. Nvidia’s H100, H200 and other advanced GPUs were blocked, but the H20 was designed to comply with the new rules with plenty of room to spare — falling well within the thresholds of the BIS “green zone” and thus not requiring a license or notice before shipping. While critics argue that this is a loophole, the rules were designed to encourage sales of green-zone products like the H20. In December 2023, Biden’s Commerce Secretary, Gina Raimondo, stated that Nvidia “can, will and should sell AI chips” such as the H20 — while reserving the highest-performing products for the U.S. and its allies.
After a slow start, H20 sales gradually picked up, because the product is a workhorse for workloads such as recommender systems, consumer internet and chatbots. Earlier this year, however, the Trump Administration froze further sales of the H20 through a process called an “is informed” letter — not a regulation — instructing Nvidia to halt until further notice.
In the wake of the U.S.–China trade and export negotiations of the past few months, the Trump Administration has indicated that H20 sales can resume, subject to notice and license requirements. The new policy is still much more strict than the Biden Administration’s — which allowed sales without any notice — but it will allow Nvidia to compete on the ground with China’s national champion, Huawei.
Some China trade hawks argue against allowing H20 sales to China or enabling CSPs or other vendors to serve Chinese clients with cutting-edge AI compute in the cloud. The claim is that China may obtain enough H20s to support the development of multiple AI superclusters. Other arguments hold that the current export framework is too flimsy, making enforcement difficult, or suggesting that it’s bad (i.e., low-margin) business for Nvidia while simultaneously strengthening Huawei’s domestic alternatives.
But the concern that U.S.-designed chips may end up supporting Chinese military ambitions overlooks China’s own immense stockpile of AI chips (such as the Huawei Ascend) and the control mechanisms already available through cloud service providers or even the vendors for non-cloud superclusters required to create a leading-edge AGI frontier model. Because they are low-powered enough to operate in the BIS “green zone,” H20s are already available to CSPs in China — as they should be to keep those Chinese companies happily using U.S. technology. Meanwhile, civilian use of more advanced GPUs is both separable and traceable when routed through CSPs or vendors for on-premises or private-cloud infrastructure or smaller CSPs — based in the U.S. or allied nations — that operate under strict KYC requirements. For example, these protocols already enable U.S. cloud infrastructure providers to serve Chinese customers from U.S.-based datacenters, making the same approach feasible for deployments in the Middle East or elsewhere. Other mechanisms, such as U.S.-provided and -approved operating system software, or U.S.-provided service and support, can provide reasonable assurances as well.
Further, claims about weak enforcement fail to acknowledge the physical and operational security present in the Gulf region. Data campuses in places like the UAE — such as the planned 5-gigawatt AI campus — are hardened, auditable, and have well-developed telemetry systems that enable real-time visibility into GPU workloads. This makes diversion significantly harder than assumed. Five years ago, the fungible matter in question was a card; now it is a complete rack in Nvidia’s new B300 world.
As for the “bad business for Nvidia” part, it’s important to recognize that even low-margin volume helps sustain Nvidia’s manufacturing scale, and finances the R&D pipeline that drives U.S. leadership in AI. I also believe the newest Blackwell derivative for the China market will have a better cost basis due to its memory architecture. Starving U.S. companies — and not just Nvidia — of addressable markets only weakens the broader national-security and economic foundations they support. And every yuan of AI spending that goes to Nvidia is a yuan that doesn’t go to Huawei to reinvest into R&D, create better AGI sooner and spread it around the world — as it successfully did with its equipment for the telecom market.
The UAE and Saudi Arabia, which are key players in proposed GPU deployments, have no incentive to smuggle chips — they are resource-rich nations focused on energy exports, not illicit trade in AI hardware. Moreover, U.S.-based operators such as Microsoft and Amazon, even when deploying infrastructure abroad, still fall under U.S. export enforcement jurisdictions. The logistical difficulty of smuggling datacenter-grade hardware is often underestimated; these GPUs arrive preassembled into racks that can weigh over 3,600 pounds, making physical diversion highly impractical.
David Sacks, the Trump Administration’s AI and crypto czar, made some good points when he said on his All-In podcast, “Just look at market share. If we have like 80 to 90% market share, that’s winning. . . . If (China) has 80% market share, then we’re in big trouble.”
This framework can ensure that Chinese customers that access compute through Middle Eastern CSPs do so for civilian purposes only: e-commerce, LLM inference, logistics and so on. With military or frontier model training barred effectively, such usage does not create a meaningful national security threat, in my opinion.
One major consideration is the potential for Huawei to dominate AI infrastructure across the Belt and Road Initiative countries if the U.S. restricts Nvidia’s ability to compete. The Ascend AI stack — Huawei’s response to Nvidia’s CUDA stack — is maturing rapidly. Blocking U.S. exports will unintentionally provide Huawei with captive market share in the Gulf region and South America, thereby accelerating its learning curve and scale.
Second, diffusion of American AI hardware, software, and frameworks is how the U.S. has historically maintained technological supremacy. Broad adoption of U.S. technology such as CUDA makes it difficult for alternative ecosystems to take hold. In that sense, the CUDA software stack is not just a performance accelerator — it’s a strategic lever. Widespread diffusion of Nvidia’s technology means technology lock-in that benefits U.S. interests.
Another underappreciated factor is mutual dependency. While the U.S. continues to rely on Chinese rare earths and materials, China remains reliant on Nvidia’s hardware and software stack. Chinese enterprises overwhelmingly prefer U.S. chips over domestic alternatives, when they can get them, due to performance and software maturity. Accelerating the decoupling of supply chains by denying access to Nvidia exports may ultimately backfire by pushing China toward full self-sufficiency.
Rather than blanket bans, U.S. policy makers should implement targeted measures that mitigate risk without ceding strategic advantage. H20 sales to China should resume, subject to the licensing regime the Trump Administration has adopted. Blackwell GPU exports should be permitted to CSPs from the U.S. or allied nations operating facilities in the Middle East that serve Chinese civilian clients under rigorous KYC mechanisms — or with other safeguards acceptable to BIS, which can enable safe and secure on-prem and private cloud deployments as well. Large systems of concern can maintain detailed telemetry, usage logs and auditable records accessible to U.S. regulators.
Export licenses should clearly define prohibited end uses, including military and sensitive non-military applications, along with intelligence-related activities. More generally, national security should be redefined to include market share as a metric — because if the U.S. controls the global AI stack, it inherently retains strategic leverage.
Finally, the United States should collaborate with allies to formalize a global KYC enforcement regime for AI, harmonizing standards to prevent backdoor access and creating collective strength in export compliance.
U.S. leadership in semiconductors and AI has always been driven by innovation and market scale. Choking off the global market — particularly in regions aligned with U.S. interests — only serves to isolate Nvidia (and other American companies) and subsidize Huawei. China’s AI ambitions will continue regardless of U.S. policy. The question is: will they use American technology, or their own? By selling Nvidia solutions to China, can the U.S. get to AGI first? It seems to me that the U.S. government would want that.
With CSP-mediated controls, telemetry audits, and export compliance frameworks, I believe we can thread the needle — preventing diversion of cutting-edge AI technology while retaining global relevance. Blocking H20 from China or blocking Blackwell from even CSP-mediated delivery into China-aligned regions would be a strategic mistake.
We should lead by selling, not retreating.
Moor Insights & Strategy provides or has provided paid services to technology companies, like all tech industry research and analyst firms. These services include research, analysis, advising, consulting, benchmarking, acquisition matchmaking and video and speaking sponsorships. Of the companies mentioned in this article, Moor Insights & Strategy currently has (or has had) a paid business relationship with AMD, AWS, Cisco, Dell Technologies, Google, HPE, Microsoft and Nvidia."
46,https://www.forbes.com/sites/sasirekhasubramanian/2025/07/15/nvidia-the-chips-are-falling-into-place/,Nvidia: Raising Price Target To At Least $190 By Year-End,"Jul 15, 2025, 03:43pm EDT",Sasirekha Subramanian,"Nvidia’s China Comeback: H20 Export Policy Reversal Strengthens the $200 By Year-end Bull Case
Just a week after I flagged the possibility of a U.S. policy reversal on Nvidia’s China-bound AI chips, that very pivot has materialized.
On July 15th, the Trump administration authorized Nvidia to resume exports of its H20 AI chips to China, reversing an April ban that had effectively shut Nvidia out of one of its most important markets. Nvidia’s CEO Jensen Huang has spent months pushing for resumption of H20 chip exports.
On July 7th I wrote here
The U.S. decision to allow Nvidia to resume H20 chip exports to China is a significant turning point, as China accounts for approximately 13% of Nvidia’s total revenue.
Reopening this channel could help Nvidia:
Notably, the Blackwell-based China chip rumored to be in development was expected to be priced lower than the H20. So even if it were approved for export, Nvidia’s revenue from it would likely have been significantly lower than what it stood to gain from H20 sales.
More importantly, The U.S. reversal on Nvidia’s H20 exports marks more than a tech trade shift, it’s a strategic recalibration in U.S.–China relations, and suggests that the U.S. is now weighing economic competitiveness alongside national security risks, especially as Huawei gains ground.
Other than recovering lost reveue and making new sales, if export frameworks evolve further to allow next-gen Blackwell chips, Nvidia’s China story could reaccelerate beyond H20. Nvidia became the first $4 trillion company last week, and with such a huge positive development, the Nvidia stock could be well on its way to $200 by year-end. I am thinking more like $190.  Nvidia stock has a forward PEG (Price-to-earnings-growth) of 1.37 vs. its historical 5-year average of 1.76. On a back-of-the-envelope calculation, a PEG rerating to 1.5 (conservatively) will see the stock at $190.
Looks like, the chips are falling into place…
Please note that I am not a registered investment advisor and readers should do their own due diligence before investing in this or any other stock. I am not responsible for the investment decisions made by individuals after reading this article. Readers are asked not to rely on the opinions and analysis expressed in the article and encouraged to do their own research before investing.
"
47,https://www.forbes.com/sites/tylerroush/2025/07/15/nvidia-shares-hit-all-time-high-as-trump-administration-backs-china-chip-sales/,Jensen Huang Overtakes Bernard Arnault To Become World's 6th Richest—As Nvidia Hits Record Share Price,"Jul 15, 2025, 09:58am EDT",Ty Roush,"Nvidia CEO Jensen Huang moved ahead of LVMH’s Bernard Arnault to become the world’s sixth-richest person as shares of the chipmaker rallied to an all-time high Tuesday, after the company said sales of its H20 AI chips would resume “soon” in China, ending months of restrictions imposed by the Trump administration.
Shares of Nvidia increased by more than 4% to just over $171 as of around 11:10 a.m. EDT, paring back earlier gains after reaching an all-time high of $172.40 and opening to a record $171.19.
Huang met with President Donald Trump and Chinese leadership in Beijing earlier this month, Nvidia announced Monday, adding the U.S. government “assured” Nvidia it would grant the company licenses to sell its H20 chips in China, with deliveries expected to start “soon.”
In April, the company forecast a $5.5 billion reduction in sales after the U.S. imposed restrictions on Nvidia’s chip sales to China, as Huang claimed a $50 billion market in China for AI chips was “effectively closed to U.S. industry.”
Huang, who holds a roughly 3% stake in Nvidia, is the sixth-wealthiest person in the world with a fortune valued at $148.1 billion, according to Forbes’ estimates. The value of Huang’s stake increased by more than $5.3 billion as shares rose, ranking him just ahead of Arnault ($147.9 billion) and behind Google’s Larry Page ($150.6 billion).
$145 billion. That’s how much was added to Nvidia’s market capitalization as shares surged Tuesday morning. Nvidia, with a valuation of nearly $4.15 trillion, ranks ahead of Microsoft ($3.7 trillion), Apple ($3.1 trillion) and Amazon ($2.3 trillion) as the world’s most-valuable firm.
Analysts have been optimistic about Nvidia’s role in the AI market in recent months despite export controls under the Trump administration, as shares are up 24% on the year. Huang reportedly increased his lobbying efforts against export controls after the U.S. blocked deliveries of its H20 chips, arguing the regulations undermined tech leadership in the U.S. Ananda Baruah, an analyst for Loop Capital, said last month that Nvidia was at the “front-end” of the next “Golden Wave” for generative AI and more companies adopt the technology. Baruah raised his price target for Nvidia’s stock at the time to $250 from $175, surpassing a then-average $173 forecast among Wall Street analysts, and claimed the company’s valuation could peak near $6 trillion."
48,https://www.forbes.com/sites/ywang/2025/07/15/chinese-data-center-billionaire-zhou-chaonans-wealth-jumps-on-nvidia-ai-chip-sales-resumption/,Chinese Data Center Billionaire Zhou Chaonan’s Wealth Jumps On Nvidia AI Chip Sales Resumption,"Jul 15, 2025, 02:53am EDT",Yue Wang,"Zhou Chaonan, the billionaire chairman of Chinese data center company Range Intelligent Computing Technology Group, saw her company’s shares rally as much as 9.8% on Tuesday on the back of Nvidia's announcement that it plans to resume sales of its H20 artificial intelligence chip to China—a surprise reversal that also boosted Chinese data center stocks broadly.
Zhou’s Shenzhen-listed Range Intelligent Computing now trades at 51.6 yuan ($7.2) apiece, giving the 64-year-old a fortune of $5.8 billion based on her stake in the company, according to Forbes estimates. Shares in other data center service providers listed in Shenzhen and Hong Kong jumped as well: Beijing Sinnet Technology (6.6%), Kehua Data Co. (7.5%) and GDS Holdings (10.2%).
“Less restrictions on Nvidia sales to China will speed up the country’s AI development,” says Shen Meng, Beijing-based managing director at boutique investment bank Chanson & Co. “This will in turn lead to more demand for data centers.”
The data center operators, which use Nvidia’s H20 chips to crunch and process data for various AI services, have been struggling to find a local alternative that is as good as Nvidia’s, according to Kenny Ng, a Hong Kong-based securities strategist at Everbright Securities International. The surprise announcement from the American technology giant has boosted investor confidence in their growth outlook, Ng says.
In a Monday blog post, Nvidia’s billionaire CEO Jensen Huang, who is now the world’s eighth richest person with a net worth of $143 billion, announced that the company is expected to receive U.S. government licenses to sell H20 chips to China again. The product, which has already been rendered less powerful so that it can comply with U.S. export restrictions on semiconductor sales to China, once faced even tighter controls back in April.
At the time, Nvidia took a $5.5 billion write-off as the Trump administration ratcheted up restrictions on advanced AI chip sales to China. The dramatic reversal comes after Huang met with U.S. President Donald Trump last week. The billionaire is now in Beijing to attend a trade expo, according to the official Xinhua News Agency.
“The U.S. government has assured Nvidia that licenses will be granted, and Nvidia hopes to start deliveries soon,” the company wrote in a blog post.
Chanson & Co.’s Shen says the U.S. about-face comes as the Trump administration might seek to use the H20 sales resumption as a negotiation tactic with China. Although the world’s two largest economies have settled on a trade truce earlier this year, many sticking points remain, including Beijing’s slow-walking of rare earth sales approval to overseas markets including the U.S.
The rare earth magnets are used in industries ranging from automobiles to robotics, and restrictions from China—which effectively controls the world’s production of such metals—have caused disruption in factory productions around the world. Billionaire Elon Musk said in April that China’s export curbs on rare earths had affected the production of Tesla’s Optimus humanoid robot.
“The U.S. can’t stop China’s development of chip technology whether it restricts Nvidia sales or not,” says Shen. “The change in its policies can help to reduce trade frictions, and alleviate restrictions on export of rare earth metals.”"
49,https://www.forbes.com/sites/johntamny/2025/07/14/traveling-to-china-nvidias-jensen-huang-embraces-economic-reality/,"Traveling To China, Nvidia’s Jensen Huang Embraces Economic Reality","Jul 14, 2025, 02:00pm EDT",John Tamny,"Nvidia’s Jensen Huang should be praised for traveling to China this week in pursuit of expansion in what is a crucial market for all manner of blue-chip U.S. companies. A world that is economically interconnected is a much more peaceful one.
It’s also a much more prosperous world, as is always the case when talented minds divide up work around the world rather than erecting walls to limit labor division within it. Which means that instead of attacking the CEO, Huang should be garnering abundant praise for recognizing truths about the global economy plainly lost on his critics.
Just consider the patronizing tone in a recent letter to Huang from Senators Elizabeth Warren (D-MA) and Jim Banks (R-IN). They write of a “a new bipartisan consensus that the hardware powering advanced AI, which includes NVIDIA graphics processing units (GPUs), is of immense strategic importance.” The Senators are missing the big picture.
As longtime Nvidia employee Dwight Diercks could have told them, “Everyone takes a look at their competitors’ hardware and how it works.” Which is just a comment that short of Nvidia sitting on its world-leading technology, what it produces is available to the world for inspection and copying from the time of its creation.
The Senators tell Huang that “The Department of Commerce restricts the ability of your company, NVIDIA, to sell its most advanced chips to the PRC.” Their words are music to the ears of the Chinese Communist Party (CCP).
That’s because the CCP very much wants to remove U.S. technology from Chinese production in the AI space. Which is just a hint that Commerce and Senators like Warren and Banks are doing for the CCP precisely what free markets would not.
The reality is that Chinese technologists revere and desire American ingenuity. Too bad the U.S. political class is using force to ensure that the best of the best U.S. companies (including Nvidia) miss out on remarkable growth opportunities within China, growth that will happen with or without U.S. know-how.
To which some will respond that the technology produced stateside is yet again of strategic importance such that it must be kept in the U.S. and other friendly nations. Please re-read paragraph three. There’s quite simply no way, short of Nvidia retreating from the marketplace altogether, that the world won’t see its technology.
Which means that progress in the AI space will once again take place with or without corporations headquartered in the U.S. Yes, the void will be filled, and it will in many cases be filled by Chinese technology companies.
If readers are still confused, they need only Google Huawei’s Mate Pro 60 to see how the globally innovative can and do work around protectionism. When U.S. politicians made it illegal for U.S. technology corporations to sell crucial smartphone inputs to Huawei, the Shenzhen giant innovated on its own. Put another way, protectionism stateside didn’t restrain Huawei, rather it positioned the corporation to thrive not just in smartphones without U.S. input, but in AI itself. Call it blowback, protectionism edition.
Which brings us back to Huang’s trip to China. Quite unlike his critics, Huang recognizes that in the “closed economy” that is the world economy, the absence of Nvidia from the Chinese market isn’t just harmful from a profit-making perspective, it’s existential.
If Nvidia isn’t working with Chinese talent already free to divide up work with the rest of the non-U.S. world, it will fall behind the corporations that can and will do what U.S. politicians won’t let Nvidia do. Huang should once again be praised for seeing what his critics plainly do not."
50,https://www.forbes.com/sites/johntamny/2025/07/12/nvidias-4-trillion-valuation-tells-a-beautiful-retirement-story/,Nvidia’s $4 Trillion Valuation Tells A Beautiful Retirement Story,"Jul 12, 2025, 10:00am EDT",John Tamny,"Nvidia’s valuation reached the $4 trillion mark this week, while co-founder Jensen Huang’s shares are now worth something like $143 billion. Present and future retirees should rejoice the rapidly increasing wealth inequality that Nvidia and Huang represent.
That’s because inequality is the greatest gift to retirement of all. To see why, contemplate the “Magnificent Seven” stocks known to be the biggest drivers of the bull market of the present. Doing so requires a brief look backwards.
If you purchased Apple, Amazon, Alphabet, Meta, Microsoft, Tesla and Nvidia early, you’re presently not worried about having enough money for retirement. Huang was of course at Nvidia in the beginning, and his enormous wealth exists as evidence of what can happen when early investors or employees stay the proverbial course amid immense share-price volatility.
Twice in Nvidia’s public life its shares have dropped 90 percent on their way to gains greater than 300,000% to the present. That has been the norm among corporations that make up the Magnificent 7. Lots of ups and downs that the typical retail investor saving for retirement can’t endure. Which is the point.
Thanks to index funds and ETFs that track the S&P 500, Nasdaq 100, or for that matter global stock market indices, retirement savers don’t have to be early to transformative business concepts. Much more importantly, they don’t have to pick the shares that will eventually soar.
Instead, they merely need to be invested. From there the stock market will not only do their worrying for them, their investment in broad indices ensures that they’ll have growing exposure to the very few corporations that unexpectedly emerge as the bluest of blue chips.
That’s why present and future retirees should cheer rising inequality. Exactly because index funds and ETFs are frequently market-cap weighted, ownership of stock indexes evolves to reflect the most highly valued stocks most prominently.
That’s where wealth inequality comes in. It’s an effect of entrepreneurs discovering a future of commerce that most never imagined. Since entrepreneurs are much earlier to world-changing and improving ideas than the markets themselves, they capture the majority of market gains from their discovery of what was only obvious after the fact.
Still, would-be retirees once again don’t need to be early. They just need to be positioned for exposure to the high-flyers of today and tomorrow through broad market exposure. If so, as in if and when the next commercial visionary happens on a new idea that investors gradually fall in love with, the typical retirement investor stands to prosper substantially thanks to the genius of compounding.
The good news is that the potential to compound one’s wealth grows by the day as the world shrinks by the day through commercial advances that reach greater and greater numbers of people. This is wealth inequality, and it’s transforming the quality of retirement for the much better."
51,https://www.forbes.com/sites/daniellechemtob/2025/07/10/forbes-daily-nvidia-reaches-a-4-trillion-milestone/,Forbes Daily: Nvidia Reaches A $4 Trillion Milestone,"Jul 10, 2025, 07:59am EDT",Danielle Chemtob,"andForbes Daily,
Forbes Staff.
AI juggernaut Nvidia just crossed another milestone: a $4 trillion valuation, the largest in history.
The firm, which was worth just over $10 billion a decade ago, now outranks household names like Microsoft, Apple and Amazon, as its graphics processing units (GPUs) are the gold standard in training advanced artificial intelligence programs.
It’s hard to grasp just how much money $4 trillion is, but for some context: The company of less than 40,000 people is now worth more than 97% of the world’s economies, the entire New York area real estate market and the combined net worth of the 30 richest people in the world.
X CEO Linda Yaccarino announced her resignation Wednesday, just one day after Elon Musk’s AI chatbot Grok made a series of antisemitic comments on the social media platform—though a source told NBC News that the exit has been in the works for over a week. On X, Musk wrote: “Thank you for your contributions,” in response to the former advertising executive’s departure after a two-year stint leading the company. It’s unclear who will succeed her.
MORE: Later Wednesday night, xAI launched the latest and most advanced version of its AI model: Grok 4. In a nearly hour-long livestream, Musk and xAI staffers demoed the new model, which Musk said was “smarter than almost all graduate students in all disciplines, simultaneously.” The billionaire did not address Grok’s recent controversial posts.
Good news for investors eagerly awaiting lower borrowing costs: Most Federal Reserve officials still expect interest rate cuts this year, according to minutes released from its recent meeting, but tariffs could complicate the Fed’s efforts to fight price increases. Still, many members of the central bank’s rate-setting panel noted that the inflationary impacts of tariffs could be more limited if trade deals are reached soon.
As President Donald Trump suggested he’d implement a 50% tariff on copper imports, the price of the metal skyrocketed in its largest intraday surge since the 1980s, before paring back some of its gains Wednesday. The levies would likely increase prices for copper in the U.S., analysts predicted, as just over half of the total refined copper consumed in the U.S. is imported.
MORE: Higher copper costs could also lead to an increase in crime, as the metal would become a valuable target for thieves. It’s the third-most consumed metal in the U.S., with uses ranging from electrical wiring in buildings to computers and cars, and is often stolen from construction sites.
Critics of President Donald Trump’s “Big Beautiful Bill” argue that it takes from the poor to benefit the wealthy, but not all billionaires are celebrating its passage. Some of the bill’s measures are more likely to help multi-millionaires than billionaires, and a number of the world’s richest people are concerned about the expected $3.4 trillion that will be added to the deficit by 2034 as a result of the legislation. One estate planning attorney told Forbes some of his billionaire clients are moving more of their wealth offshore to “more stable economies.”
Adam Valkin, who helped grow General Catalyst into a global venture capital powerhouse, wants to help Israeli founders break into the U.S. market in his new role at Vine Ventures. Valkin will help the firm invest in the country known as “Startup Nation,” as—despite experiencing a drop in investments after the October 7 attack—Israeli startups have raised $3.1 billion so far in 2025.
After Shaun Maguire of Sequoia Capital received swift backlash for his comments about New York City mayoral candidate Zohran Mamdani, a letter supporting the venture capital firm partner garnered hundreds of signatures. The letter defending Maguire claims his remarks, including accusing Mamdani of being an “Islamist,” did not constitute hate speech, even as nearly 1,000 tech founders demanded Sequoia take disciplinary action.
Former Real Madrid coach Carlo Ancelotti has been sentenced to one year in prison and will pay a hefty fine after being found guilty of tax fraud. He had been accused of failing to pay over $1.1 million in Spanish taxes related to image rights. Managing Real Madrid is a big deal in the world of professional sports—the team topped Forbes’ list of The World’s Most Valuable Soccer Teams in 2025, the fourth year in a row it ranked No. 1.
Billionaire Bill Ackman’s wildcard entry into the Hall of Fame Open tennis tournament ended in swift defeat Wednesday, drawing criticism from fans and officials who questioned whether the hedge fund manager had earned his spot in the professional event. Participants in the tournament are typically selected based on ATP rankings, wildcard entries and qualifiers. Randy Walker, the director for the Vero Beach Futures tennis tournament, posted on X after the match: “I just watched the absolute worst professional tennis match I have ever seen.”
Even as child care remains a major hurdle for many working parents, few companies are taking advantage of a federal tax break given to firms that provide child care for their employees, according to IRS data. But the just-signed tax and budget bill includes a little-known provision that more than triples the Employer-Provided Childcare Tax Credit, and data shows that child care benefits are helping companies’ bottom lines.
Five years ago, Ohio’s most revered entrepreneur Les Wexner stepped down as chairman of L Brands and soon began to sell off his stake in the Victoria’s Secret parent company amid controversy over his close relationship with convicted sex offender Jeffrey Epstein.
Many believed that Wexner, then age 82, would quietly retire. Instead, the fashion mogul has more than doubled his fortune thanks in large part to some savvy bets in a different industry: technology. Over just the past three months, his net worth—which includes assets held in his children’s and wife Abigail’s names—has soared to $10.1 billion from $7.9 billion, according to Forbes’ estimates.
The biggest driver of this increase: a 4% stake in CoreWeave, one of the buzziest artificial intelligence companies. On Monday, CoreWeave announced it is buying crypto miner Core Scientific in a roughly $9 billion all-stock deal. CoreWeave’s market capitalization has almost tripled since its March IPO to nearly $73 billion today, and Wexner’s stake is now worth $2.8 billion.
The eight-year-old CoreWeave helps companies build data centers and loans out cloud access to much-sought-after graphics processing units, or GPUs, which firms like Microsoft, IBM and Meta pay to use to build AI models. With no shortage of customers, the company recorded $982 million in revenue during the first quarter of this year, a 420% increase versus the same quarter last year. However, it’s yet to turn a profit and reported a net loss of $315 million during the same period.
WHY IT MATTERS “After originally making a fortune in fashion, Wexner has become one of the biggest (and most unlikely) winners of the AI gold rush,” says Forbes staff writer Jemima McEvoy. “In addition to becoming an early backer of the $73 billion (market cap) CoreWeave, one of the buzziest AI companies around, Wexner's real estate development firm has played a crucial role in bringing big tech players like Meta, Google, Microsoft, Amazon and Intel to Central Ohio.”
MORE How Victoria’s Secret Billionaire Owner Accidentally Scored A $800 Million Stake In An AI Unicorn
Millions of borrowers will soon be on the hook for thousands of extra dollars as the Trump Administration is slated to start charging interest next month on student loans under the Biden-era SAVE plan. Borrowers enrolled in the plan have had their loans in forbearance since last summer while it’s been held up in court:
About 7.84 million: The number of borrowers with loans under the SAVE plan that will be affected by the change
$3,500: The amount in additional interest per year that the average borrower enrolled in the SAVE plan will be charged, which works out to about $300 per month
1.5 million: How many applicants for income-driven repayment plans were yet to be processed, the Education Department said in a June court filing
Many employees have experienced “quiet firing,” where a manager makes a work environment so unpleasant that it pushes people to quit. Sometimes quiet firing occurs because a boss isn’t doing their job, or it could just be part of the company culture—it’s an easier way for firms to reduce headcount without very public layoffs. If you’ve been silently pushed out, try not to take it personally: You deserve a workplace that supports your growth.
A 40-year-old prototype of a popular designer handbag is expected to fetch hundreds of thousands of dollars at an auction Thursday. Which brand is it?
A. Gucci
B. Louis Vuitton
C. Hermès
D. Burberry
Check your answer.
Thanks for reading! This edition of Forbes Daily was edited by Chris Dobstaff and Caroline Howard."
52,https://www.forbes.com/sites/dereksaul/2025/07/09/ai-titan-nvidia-becomes-first-4-trillion-company-ever/,AI Titan Nvidia Becomes First $4 Trillion Company Ever,"Jul 09, 2025, 10:07am EDT",Derek Saul,"Artificial intelligence chip architect Nvidia is the first company in history valued at $4 trillion, the latest milestone achieved by the undisputed leader of the generative AI gold rush.
Nvidia stock climbed as much as 2.8% Wednesday morning to a new all-time high of $164.42.
The Silicon Valley giant’s market capitalization climbed to $4.01 trillion, becoming the first ever publicly traded firm to score a $4 trillion valuation.
Nvidia outranks the perhaps more household name tech giants Microsoft ($3.7 trillion market cap), Apple ($3.1 trillion), Amazon ($2.4 trillion) and Google parent Alphabet ($2.2 trillion) on the list of the world’s five largest companies.
Nearly 35,000%. That’s how much Nvidia’s share price has risen over the last decade, trouncing the S&P 500’s roughly 260% advance, including reinvested dividends. That means a $1,000 investment in Nvidia in July 2015 would now be worth $350,000.
Nvidia is worth more than the United Kingdom’s $3.9 trillion gross domestic product last year.
Worth just more than $10 billion a decade ago, Nvidia’s rise has been nothing short of meteoric, first eclipsing a $1 trillion valuation in 2023 and crossing the $2 trillion and $3 trillion marks in 2024. Wall Street’s infatuation with the firm based in Santa Clara, California closely followed the rise of generative AI services such as OpenAI’s ChatGPT chatbot, as Nvidia holds a dominant share in designing the hardware and software stack needed to power the pricey programs. Nvidia’s core products are its graphics processing units (GPUs), the gold standard to train advanced AI programs. Among Nvidia’s big-ticket customers are OpenAI, Elon Musk’s Tesla and xAI, Meta and Amazon.
The $4 trillion Nvidia has a humble origin story, tracing back to a 1993 meeting at a booth of a Denny’s diner between cofounders Jensen Huang, Chris Malachowsky, and Curtis Priem.
Huang, whose first job was a busboy at Denny’s, is now the world’s ninth-richest person with a $142 billion net worth, according to our latest calculations. Nearly all of his fortune stems from his 3.5% stake in Nvidia."
53,https://www.forbes.com/sites/dereksaul/2025/07/09/whats-nvidia-more-valuable-than-the-uk-economy-worlds-30-richest-and-total-nyc-real-estate-make-the-list/,How Valuable Is Nvidia? More Than 97% Of Countries' Economies—And Much More,"Jul 09, 2025, 01:07pm EDT",Derek Saul,"After becoming the world’s first-ever $4 trillion company Wednesday, the scope of artificial intelligence colossus Nvidia’s size is hard to grasp, as the less than 40,000-person company is now worth more than 97% of the world’s economies and all of the world’s military spending.
All but five global economies: Only the U.S., China, Germany, India and Japan had larger 2024 gross domestic products than Nvidia’s market value, according to the International Monetary Fund, meaning Nvidia is worth more than the total economies of powerhouses such as the United Kingdom, Canada and Russia.
The world’s 30 richest people: With a combined net worth of $3.76 trillion, the 30 wealthiest people are worth less than Nvidia, according to Forbes’ real-time estimates.
Social Security, Medicaid and Medicare spending: The federal government spent $2.98 trillion on the three primary components of the U.S. safety net during the government’s 2024 fiscal year, according to the Congressional Budget Office.
Global defense spending: Countries spent $2.46 trillion on military items in 2024, according to the International Institute for Strategic Studies.
The New York area housing market: Nvidia is far less valuable than the $49.7 trillion U.S. residential real estate market, according to Redfin’s end of 2024 estimates, but it’s still more valuable than the housing market in all of the country’s metropolitan areas, including the priciest markets of New York ($2.43 trillion) and Los Angeles ($2.18 trillion).
The U.S.’ most valuable car, entertainment, financial services, healthcare and energy companies – combined: Tesla, Netflix, JPMorgan Chase, Eli Lilly and ExxonMobil have an aggregate market cap of just under $3.5 trillion.
The global cryptocurrency market: All cryptocurrencies combined are worth $3.45 trillion, according to CoinGecko, led by leading digital asset bitcoin’s $2.17 trillion market cap.
The S&P 500’s 200 smallest companies: Nvidia is more valuable than the $3.52 trillion combined market value of the S&P 500 benchmark’s 200 smallest companies, a group of blue chip American companies including Campbell’s, HP Inc., Tyson Foods and United Airlines, according to FactSet data.
Nvidia’s $4 trillion milestone is a testament to the soaring valuations of the U.S.’ largest public companies. General Electric was the biggest American company in 1995 with a $92 billion market cap and Exxon was the largest in 2010 with a $314 billion market cap, less than a tenth of Nvidia’s 2025 market value, according to the American Business History Center’s data, which does not adjust for inflation. The stock market has also turned to American exceptionalism in recent years, as the U.S.’ proportion of global equity value shot up from 41% to more than 60% from 2008 to 2024, according to LSEG research. Tech companies make up nearly 30% of the global equity market, according to LSEG, making it by far the most valuable sector. Nvidia is the poster child of the U.S.-driven tech boom, as it is the unquestioned leader in AI. The Silicon Valley firm enjoys a 75% share in the market for AI accelerators, the highly expensive systems used to train advanced machine learning models, according to Bank of America estimates."
54,https://www.forbes.com/sites/stevenwolfepereira/2025/07/09/nvidia-just-hit-4-trillion-but-the-real-ai-boom-hasnt-started-yet/,"Nvidia Just Hit $4 Trillion, But The Real AI Boom Hasn’t Started Yet","Jul 09, 2025, 01:16pm EDT",Steven Wolfe Pereira,"Nvidia became the first company ever to reach $4 trillion in market value Wednesday, a milestone that took just over two years, from $1 trillion.  This unprecedented rise has been driven by insatiable demand for AI infrastructure and its new Blackwell graphics processing units (GPUs) architecture.  But the real story isn’t the valuation.
This is just another signal that three massive economic shifts are accelerating, creating both existential threats and extraordinary opportunities for businesses across every industry.  Let’s put Nvidia in perspective.
The company generated $130.5 billion in fiscal 2025 revenue, up 114% year-over-year. Its data center segment alone reached $115.2 billion, growing 142%.  This isn’t a chip company anymore; it’s the primary infrastructure provider for the AI revolution.
The company’s dominance in AI computing (holding 92% of the data center GPU market) has positioned it at the epicenter of the multi-trillion dollar infrastructure buildout that will reshape the global economy through 2030. This achievement represents more than financial success; it signals the transformation of computing from a support function to the primary engine of economic value creation, with Nvidia's AI factories producing the computational power that drives everything from ChatGPT to autonomous vehicles.
The journey from $1 trillion to $4 trillion took just over two years, a velocity that reflects both the explosive growth of AI adoption and Nvidia’s strategic positioning.  At the heart of it all is CUDA.
Nvidia’s true competitive moat lies not in silicon but in software. In 2006, the company introduced CUDA (Compute Unified Device Architecture) as a proprietary parallel computing platform and application programming interface. CUDA lets software tap into graphics cards to speed up computing tasks beyond just graphics.  Think of it like this: Graphics cards are really good at doing lots of calculations at once. CUDA figured out how to use that horsepower for regular computing work, not just making video games look pretty.
Since its launch, the CUDA ecosystem has created an impenetrable fortress of developer lock-in that competitors have failed to breach despite years of effort.  Today, the success of the CUDA model has led to the creation of a thriving ecosystem that creates network effects that strengthen with each passing year.  The ecosystem now includes over 5 million CUDA developers, 40,000 companies and thousands of generative AI companies that are all building on the Nvidia platform.
This software dominance translates directly to pricing power. Nvidia maintains 78% gross margins offering software-like returns on hardware sales, while competitors like AMD and Intel have gross margins at 47% and 41%, respectively. The company's ability to charge $30,000+ per high-end GPU reflects not just scarcity but the irreplaceable value of the CUDA ecosystem in accelerating AI development timelines.
While everyone debates whether AI is overhyped, companies are committing $6.7 trillion to AI infrastructure by 2030, according to McKinsey. To put that in perspective, that’s about 23% of the $30 trillion U.S. GDP.
Microsoft is spending $80 billion on AI infrastructure this year alone. Google allocated $75 billion. Amazon topped $100 billion. These aren’t R&D experiments.  Big Tech is building the foundation for an entirely new AI-driven economy.  Access to AI capabilities will soon determine competitive advantage as much as access to electricity did in the 1920s.
The companies securing priority access to these tools today will have insurmountable advantages over those waiting for prices to come down.  With so much demand, Nvidia customers currently face up to four- to eight-month lead times for their advanced AI chips.
Forget chatbots and content generators. The real transformation is autonomous AI systems that work like digital employees, handling entire workflows from start to finish.
Here’s the difference: ChatGPT responds to prompts. An AI agent identifies problems, researches solutions, executes multistep plans and adapts its approach based on results, all while you focus on other priorities. It's the difference between a helpful assistant and a capable colleague.
The agentic AI market is set to explode. Market.us projects it will grow from $5.2 billion in 2024 to $196.6 billion by 2034 — a nearly 40-fold increase in just one decade. Salesforce CEO Marc Benioff even went so far as to say at this year’s Davos conference that today’s executives will be the last generation to manage entirely human workforces. AI agents aren’t coming to the workplace, they’re already here, quietly handling an expanding range of business tasks.
Companies are already deploying AI agents that handle customer service, manage inventory, process insurance claims and conduct market research. Early adopters report significant productivity gains and millions in cost savings.
So where can one start?  Identify your most repetitive, rule-based processes. These are prime candidates for AI automation within the next 18 months.
While everyone focuses on software, the biggest opportunity may be in physical AI — embodied intelligence that thinks and acts, where robots and automated systems understand and manipulate the real world.
Nvidia CEO Jensen Huang made the bold prediction at the Computex tech conference in Taiwan that ""all factories will be robotic."" The factories will orchestrate robots, and those robots will be building products that are robotic. This isn’t science fiction.  He’s describing the multi-trillion industrial transformation already underway.
According to Grandview Research, the global AI in robotics market was valued at approximately $12.77 billion in 2023 and is projected to grow at a compound annual growth rate of 38.5%, reaching around $124.77 billion by 2030. Tesla’s factory already uses autonomous robots. BMW has deployed thousands of AI-powered robots globally. Even Amazon’s warehouses run on AI-directed automation.
Manufacturing costs are about to plummet for companies that adopt AI-powered automation, while traditional manufacturers get left behind. But the opportunity extends far beyond factories.  This will extend to retail, logistics, agriculture, construction, and every aspect of industry will be transformed.
Maybe now is a good time to audit your physical operations. Where do you still rely on manual processes that robots could handle more efficiently and reliably?
The infrastructure buildout creates a narrow window for businesses to establish their AI foundation. I estimate that companies have roughly 18 to 24 months before competitive advantages solidify around access to AI capabilities.  Once AI adoption reaches critical mass in your industry, the advantages compound. Early movers get better data, more experience and superior customer relationships. Late adopters face the impossible task of catching up while the leaders accelerate further.  Build AI capabilities that can work across different platforms rather than locking into any single provider's technology.
IDC projects AI will drive a $20 trillion economic transformation by 2030. Said another way, every dollar spent on AI will generate $4.60 into the global economy.
The question for every business leader is simple: Will your company participate as a beneficiary or become a casualty of someone else's AI-driven disruption?
Nvidia’s $4 trillion milestone isn’t the peak — it’s the beginning of the next wave. The real AI boom hasn’t started yet because most businesses are still treating AI as a tool rather than a transformation. The companies that recognize AI as a new way of operating, not just a new technology to implement, will define the next decade of business success."
55,https://www.forbes.com/sites/greatspeculations/2025/07/08/buy-this-alternative-to-nvidia-stock-for-2x-gains/,Buy This Alternative To Nvidia Stock For 2x Gains?,"Jul 08, 2025, 05:00am EDT",Trefis Team,"Is it possible for Applied Materials stock (NASDAQ:AMAT) to reach close to $380 in the coming years? There is a strong likelihood, as the company is poised to benefit from the rising capital expenditures driven by the generative artificial intelligence boom. While AI leader Nvidia (NASDAQ: NVDA) captures headlines with its stock having increased over 3x in the past two years and its valuation nearing $4 trillion, lesser-known players like Applied are essential for producing the very AI chips that Nvidia markets. These stocks could present more value and significant upside potential.
The data presents a compelling case. According to SEMI, capital spending on advanced chip manufacturing equipment is expected to nearly double between 2023 and 2028, with global capex expenditures anticipated to exceed $100 billion in 2025 alone. Applied, which specializes in advanced equipment and software utilized to manufacture semiconductor chips—ranging from etchers and deposition systems to process control software—may be well situated to capture a substantial portion of this investment. The company’s clientele includes industry giants such as TSMC, Samsung, and Intel, positioning it as a central player in both the logic and memory sectors of the chip market.
Applied Materials has seen its revenues increase at a robust pace, with an annual growth rate of 13% over the last five years. Although the company is expected to experience a slowdown in sales growth to about 6% for FY'25, reaching $29 billion, growth could accelerate. If Applied manages to grow its sales at an average annual rate of nearly 22% over the next three years—driven by increased demand for advanced tools for producing memory and logic chips for AI—its revenues could rise from approximately $29 billion in FY'25 to about $53 billion by FY'28, representing an increase of roughly 81%.
Several trends may facilitate revenue growth in the upcoming years. The generative artificial intelligence (AI) surge is fueling a sharp increase in semiconductor demand. AI workloads necessitate substantial computational power, greater memory capacity, and more intricate chips, all of which require advanced manufacturing processes. Additionally, AI demands high-bandwidth memory and sophisticated chip packaging—areas where Applied Materials excels. For instance, producing HBM chips is three times as wafer-intensive as standard DRAM, owing to lower bit density and the necessity for 3D stacking. This directly results in higher demand for products manufactured by companies like Applied.
While Applied’s considerable exposure to China, which made up over a third of its revenue in FY'24, has raised investor concerns due to U.S. export restrictions, signs of easing tensions are emerging. Recently, the U.S. and China reached a trade agreement that includes rare earth exports. As part of this accord, the Trump administration relaxed recent export license requirements for certain chip design software sold to China. If this thawing of tensions continues and extends to semiconductor equipment exports, firms like Applied could gain improved access to an essential growth market.
Applied Materials has performed well in recent years, climbing from approximately $85 in early 2021 to around $190 currently. However, the increase in AMAT stock over the past four years has not been consistent, as annual returns have exhibited much greater volatility compared to the S&P 500. The stock's returns were 84% in 2021, -38% in 2022, and 68% in 2023, followed by 9% in 2024 and 17% year-to-date so far in 2025. In contrast, the Trefis High Quality (HQ) Portfolio, comprising 30 stocks, has proven to be significantly less volatile. Moreover, it has outperformed the S&P 500 every year in that timeframe. What accounts for this? As a collective, HQ Portfolio stocks have yielded better returns with diminished risk compared to the benchmark index; offering fewer wild fluctuations as shown in HQ Portfolio performance metrics.
When we combine this substantial revenue growth with the fact that Applied's adjusted net margins (net income, or profits remaining after all expenses and taxes, represented as a percentage of revenues) are on an upward trajectory—growing from 19.6% in FY'19 to 26.5% in FY'24—we see that the company has benefitted from improved economies of scale and a more premium product mix. We anticipate that margins could continue to rise to approximately 31% by FY'28 as Applied concentrates on new technologies such as Gate-All-Around (GAA) semiconductor equipment, while also effectively managing its costs. Applied is experiencing faster growth in its services sales compared to products, which could furthermore enhance margins since services contracts typically yield recurring revenues and are increasingly focused on more lucrative software. Additionally, Applied has demonstrated considerable discipline in its capital expenditures as compared to other competitors in the chip sector, which could also contribute to higher margins. The combination of approximately 81% revenue growth with about a 20% increase in margins suggests an estimated 2.2x growth in earnings over the subsequent three years.
If earnings do in fact grow by 2.2x, the P/E multiple would contract by 2.2x, assuming the stock price remains constant. However, this is precisely what investors in Applied Materials are wagering will not be the case. If earnings increase by 2.2x in the next few years, instead of the price to earnings multiple compressing from around 20x now to approximately 10x, a scenario where the P/E ratio stabilizes around 18x appears quite plausible, as the robust growth and expanding margins instill greater confidence in the future of Applied Materials.This would result in a potential doubling of Applied stock price from $190 to roughly $380 within the next few years, making it a realistic scenario.What is the time frame for this high-return possibility? While our example above illustrates a timeframe of roughly three years, in practice, whether it takes three years or four may not significantly differ; as long as Applied is on this revenue expansion path, with margins remaining stable, the stock price could respond accordingly.
While there may be upside potential for AMAT stock, there are inherent risks associated with investing in individual stocks. Conversely, the Trefis Reinforced Value (RV) Portfolio has exceeded its all-cap stocks benchmark (which combines the S&P 500, S&P mid-cap, and Russell 2000 benchmark indices) to generate strong returns for investors. What is the reason for this? The quarterly rebalanced mixture of large-, mid-, and small-cap RV Portfolio stocks offered a responsive approach, allowing optimal capitalizing on thriving market conditions while minimizing losses during downturns, as elaborated in RV Portfolio performance metrics."
56,https://www.forbes.com/sites/investor-hub/article/can-nvidia-nvda-stock-hit-200-by-end-2025/,Can Nvidia Stock Hit $200 By The End Of 2025?,"Jul 07, 2025, 03:38pm EDT",Sasirekha Subramanian,"This article isn’t about Nvidia’s parabolic rise amid the AI boom. It won’t dwell on the well-known fact that Nvidia’s GPUs power roughly 90% of AI data centers worldwide or that tech giants like Microsoft, Amazon, Google and Meta remain deeply invested in its silicon innovations. It’s not even about CUDA, Nvidia’s giant competitive moat that is still relevant today.
Nor will we focus on how the stock struggled earlier this year, as investors kept raising the bar, until even spectacular results weren’t good enough. And yet, Nvidia has not only reclaimed its top spot, it is trading at record highs.
The trillion-dollar question now is: Can Nvidia stock reach $200 by year-end? Or is that a stretch even for a strong compounder like the AI bellwether?
On June 25, 2025, Nvidia surged over 4%, closing at a record $154.31, reclaiming its spot as the most valuable publicly traded company (~$3.77 trillion market cap), surpassing Microsoft and Apple. The stock is even higher now, and the last closing price is $159.34, marking a stellar comeback, from Nvidia’s April 4th closing low of $94.30. Nvidia stock erased more than 40% of its value between February and April of this year, not because of any failing on its part. Concerns over moderating AI spending, and unpredictable tariff policies triggered the decline.
This is not Nvidia’s first rodeo either. Prior to that, DeepSeek revelations in late January created uncertainty about America’s AI dominance, wiping $600 billion off Nvidia’s market value in a single day. The market was simply recalibrating its perception of perfection. While DeepSeek showed promise, it is nowhere in the leagues of an established AI leader like Nvidia. So, the sell-off was unjustified in my opinion. The Nvidia stock bounced back within a month.
Last year too, Nvidia shares fell nearly 23% from mid-July through early August, closing August 7th trade at $98.88. The sell-off was triggered by fears that Nvidia’s big tech customers may trim their AI spending budgets. However, Nvidia stock staged a strong recovery of more than 30% from these low levels, closing August 19 at nearly $130.
Every time a quality stock drops, the internet floods with ""Should you buy the dip?"" takes. My view is simple: Study the cause. If the sell-off happened for reasons that can be fixed – go ahead and buy. But if it stems from deeper, structural weaknesses, then walk away. It’s not like there’s only one good stock out there.
Nvidia’s deeply integrated AI ecosystem gives it a wide berth over competition, and its innovation remains unsurpassed. Nvidia does not think out of the box, it thinks like there is no box. That said, investor perception is everything when it comes to stock pricing. The recent rally has given reason to hope that the market may be finally waking up to the true value of this stock following a period of inflated expectations and sharp corrections.The fear of missing out (FOMO) is likely to keep the buying pressure active and drive NVDA to at least $180 by the end of this year. Of course, I could be wrong.
China is a key market for Nvidia; it represented $17 billion or 13% of Nvidia’s sales for the fiscal year that ended in January. But Nvidia’s market share in the region has weakened dramatically from 95% before 2022, to 50% currently, after U.S. regulators tightened restrictions on selling high-end AI chips to China, citing national security concerns. To comply with the U.S. export curbs, Nvidia has been selling H20 to China. H20 is  a watered-down version of its H100 chips with significantly lower computing power.
However, in April, the U.S. government tightened restrictions again, reclassifying even the scaled-down H20 as requiring an export license. The decision effectively halted shipments and dealt Nvidia a serious financial blow.
CEO Jensen Huang put it bluntly: “The H20 export ban ended our Hopper data center business in China.” Calling the $50 billion Chinese AI chip market “effectively closed to the U.S. industry,” Huang also warned that if U.S. export restrictions continue, more Chinese customers will turn to Huawei's chips.
I highlighted this risk in a previous article that flagged both the threat of an H20 ban and Huawei’s preparation for competitive positioning, citing Jefferies and WSJ reports.
Nvidia recognized $4.6 billion in H20 revenue prior to April 9, before the new export restrictions kicked in. The financial fallout from the ban was significant. Nvidia wrote down $4.5 billion in unsold H20 inventory for the first quarter, and was unable to ship an additional $2.5 billion of what would have been H20-generated revenue. Gross margin fell to 61%, versus the 71.3% it would have reported without the China-related charges. In an interview with the Stratechery podcast in May, Huang also revealed the company had to walk away from an additional $15 billion of sales. The topline guidance of $45 billion for the July quarter, would have been higher by $8 billion, but for the new export restrictions.
So, why is there still hope for a China rebound for Nvidia?
Nvidia is pivoting swiftly. A Reuters report citing sources familiar to the matter said Nvidia is planning to build a GPU or graphics processing unit for China based on its latest generation Blackwell-architecture AI processors that will be priced at $6,500-8,000, well below the H20’s price range of $10,000-$12,000. The lower price could  reflect the new export limits on GPU memory bandwidth – a metric crucial  for AI workloads that require extensive data processing.
Also, recent developments suggest a thaw in the U.S.’s formerly hardline stance against China, signaling a broader shift in tone
This gives rise to a key question: In this more conciliatory phase, is there any possibility that the U.S. could revisit its chip export restrictions to China as well?
The New York Times reported on June 27th that China confirmed details of a trade framework with the Trump administration. “China will review and approve applications for the export of controlled items,” China’s Ministry of Commerce said in a statement, and “the United States will correspondingly cancel a series of restrictive measures it has taken against China.”
While specifics remain vague, the framework hints at a meaningful de-escalation. The most compelling driver behind this shift? Not a change of heart, just China’s dominance in rare earth elements – critical for semiconductors, robotics and aerospace applications.
This accord between the U.S. and China, includes the latter’s commitment to deliver rare earth, in exchange for the U.S. lifting its export curbs on ethane, chip software and jet engines. As trade talks evolve, Beijing is not above deploying its rare earth “Trump card” to reopen access to restricted U.S. AI chip technologies.
If the U.S. should revise or relax chip export policies or greenlight a new line of compliant processors, Nvidia could regain access to the highly lucrative market. Such a development would materially bolster the case for Nvidia hitting $200 by year’s end.
However, even without China, Nvidia remains on solid footing, thanks to its ability to innovate relentlessly.
Nvidia’s growth story is deeply structural, spanning beyond GPUs to hyperscaler ramp-ups, AI factories, sovereign AI, Networking, gaming, cybersecurity and self-driving cars. Nvidia isn’t just firing on all cylinders – it is  building entirely new growth engines.
Blackwell’s Blockbuster Ramp: In the first quarter of fiscal 2026, Nvidia’s data center revenue surged 73% year-over-year (y-o-y) to $39 billion, with compute revenue accounting for $34.2 billion, up 76% y-o-y. At the heart of this explosive growth lies Blackwell, the fastest-ramping architecture in Nvidia’s history.
AI Factories and Sovereign AI: As nations seek to build in-country AI infrastructure using Nvidia’s full-stack,  Sovereign AI is a solid growth engine for the AI bellwether. Nvidia has sovereign AI deals in place with Saudi Arabia, Taiwan and the UAE.
AI factory build-outs are driving significant revenue for Nvidia with deployments at AT&T, BYD, Capital One, Foxconn, MediaTek and Telenor. The count of Nvidia-powered AI factories doubled y-o-y to roughly 100 in the first quarter, with the average number of GPUs powering each factory also doubling in the same period.
Nvidia’s new Spectrum-X and Quantum-X silicon photonics switches will enable next-level AI factory scaling to millions of GPUs. These switches offer 3.5x greater power efficiency, 10x higher network resiliency and accelerate customer time-to-market by 1.3x.
Other revenue drivers: This brings us to networking, which is shaping up as a critical contributor for Nvidia with first-quarter networking revenue reaching nearly $5 billion, up 56% y-o-y and 64% sequentially. Networking growth was driven by NVLink, with Q1 shipments exceeding $1 billion. Hyperscale customers can now build semi-custom CCUs and accelerators that connect directly to the Nvidia platform with NVLink.
Also helping networking growth is the continued adoption of Ethernet for AI solutions. Spectrum X – which is the  world’s first Ethernet networking platform for AI – is now annualizing over $8 billion in revenue with deployments across Microsoft Azure, Oracle Cloud, CoreWeave, Meta, Google Cloud and xAI.
For AI-powered cybersecurity – Checkpoint, CrowdStrike and Palo Alto Networks are using NVIDIA’s AI security and software stack to build, optimize and secure agentic workflows. CrowdStrike, for instance, has achieved 2x faster detection triage with 50% fewer compute resources.
Gaming revenue for the first quarter was a record $3.8 billion, up 42% from a year ago and up 48% sequentially, driven by sales of the Blackwell architecture.
Professional Visualization revenue, which includes Nvidia Omniverse, rose 19% from a year ago to $509 million, thanks to broader adoption of Ada RTX workstation GPUs, addressing workflows in AI acceleration, real-time graphics rendering and data simulation.
Automotive and Robotics revenue rose 72% to $567 million from a year ago,  driven by sales of Nvidia’s self-driving platforms. Most recently, Nvidia CEO Jensen Huang identified robotics as the company’s next biggest growth opportunity after AI, noting that self-driving cars are likely to be the first major commercial application of this technology.
Even at record highs, Nvidia’s stock may not be as expensive as it is touted to be. Some valuation metrics suggest room for upside:
Interestingly, those discount levels align closely with the roughly 25-30% upside Nvidia would need to reach $200. That leaves the question less about if Nvidia can reach $200 and more about when investors will be ready to price that in.
So, what are analysts saying about the Nvidia stock?
Nvidia currently has a “Strong Buy” rating with a 12-month price target of $174 per share.
Loop Capital lifted its price target for Nvidia to $250 from $175, while maintaining its ""buy"" rating. The rating firm’s client note said, ""Our work suggests we are entering the next ‘Golden Wave’ of Gen AI adoption and NVDA is at the front-end of another material leg of stronger-than-anticipated demand.”
Cantor Fitzgerald maintained a Buy rating with a $200 target, citing robust fundamentals and strong growth expectations through 2025 and especially in 2026.
A few Bumps on Nvidia's road to $200 include:
Bottom line
In my opinion, Nvidia stock is likely to reach at least $180  by year-end, supported by a re-rating in its forward PEG to around 1.47, if market sentiment continues to remain constructive. Key catalysts will be earnings announcements for the rest of the year, but it all boils down to market perception. Nvidia can continue to generate stellar results, and if that fails to align with already-high investor expectations, Nvidia stock could pause or pull back.  That said, for the long-term, Nvidia is a structural compounder with deeply embedded growth levers and robust AI tailwinds. Any dip in the stock is most likely a buying opportunity. If there is any change in China export policy, the Nvidia stock could achieve its $180 target even sooner and be well on its way to $200 by year-end.
Please note that I am not a registered investment advisor and readers should do their own due diligence before investing in this or any other stock. I am not responsible for the investment decisions made by individuals after reading this article. Readers are asked not to rely on the opinions and analysis expressed in the article and encouraged to do their own research before investing."
57,https://www.forbes.com/sites/johnwerner/2025/07/06/the-blackwell-successor-what-nvidias-rubin-means-for-your-phone/,The Blackwell Successor: What Nvidia’s Rubin Means For Your Phone,"Jul 06, 2025, 07:53pm EDT",John Werner,"It doesn’t take too much digging to see that the hardware world is heating up.
One company remains dominant in some key ways. Nvidia, led by Jensen Huang, is a big new star in the tech world, and recently became the largest company by market cap on the U.S. stock market, eclipsing both Apple and Microsoft.
How did this happen?
Nvidia has become adept at setting the trends when it comes to hardware. Its GPUs are in the biggest data centers, like XAI’s Colossus and other projects operating at enormous scales, as when Musk kept capriciously adding 100,000 more GPUs to the aforementioned project.
Nvidia has also garnered a large share of the foundry operations of TSMC, the Taiwanese company that primarily manufacturers chips, providing a wide range of top clients with processing power.
Along with its track record in production and market dominance, Nvidia has also been announcing a lot of new models for its processors.
It wasn’t too long ago we were talking about Nvidia Blackwell, named after David Blackwell, and the Grace Blackwell superchip, that had groundbreaking capacity and a customer base around the world.
At a recent tech event in 2024, though, Huang announced the company’s intention to start offering a new kind of microarchitecture called “Rubin,” named after physicist Vera Rubin. It would have a GPU called Rubin and a CPU called Vera, for a powerful one-two punch in terms of clock speed.
In addition, there will be an alternative called Rubin Ultra developed after that, and a hardware model called Feynman (after physicist Richard Feynman) in 2028.
As for memory bandwidth, Rubin is expected to have around 900 GB per second, with Rubin Ultra operating around 1200 GB per second. Feynman is expected to operate around 1000 GB per second, being optimized for certain kinds of data handling and precise mathematical operations.
There’s also a Rubin AI platform, the company’s ecosystem, set up with Rubin chips. This will have ultra fast HBM4 memory, with NV Link 6 switches, to deliver up to 3600 GB per second in bandwidth.
It’s all made for data centers; it’s made for AI.
Now, let’s talk about the market effect of Nvidia’s Rubin.
Experts point out that it’s not necessarily that you’ll get an entire model working entirely off-line – but the architectures themselves will drive more edge computing models for robust systems. That means the AI entities that you do have on your local devices will become more competent, more capable, and able to reason at a more profound depth.
So we are likely to soon see the effect of these hardware revolutions in the consumer world with our wearables and mobile device devices.
Everything from fitness and education to retail, medicine and law is getting re-Imagined with powerful LLMs that are portable and decentralized from big data centers.
“These devices encompass a wide range, from powerful edge servers to resource-constrained IoT sensors, and include familiar examples like smartphones, smart home appliances, autonomous vehicles, and even industrial robots,” writes Bhavishya Pandit at Datacamp, explaining some of the values of edge computing and its potential effects on our tech world.
Presumably, there’s still important work going on in those big data centers, but increasingly, it will be relegated to research and esoteric IT, while we’ll continue to see more implementation at the user device level.
We’ll probably see more impact on the average user, too. That may come in the form of higher user adoption for applications, or more respect for the powers that AI has, or both.
How do you see the future of AI when everyone’s carrying powerful artificial intelligence engines in their pockets?
Let’s keep an eye on this throughout the rest of 2025 and in 2026 as Rubin first comes to the hardware market."
58,https://www.forbes.com/sites/investor-hub/article/beyond-nvidia-5-lesser-known-ai-stocks-poised-growth-buy-now/,Beyond Nvidia: 5 Lesser-Known AI Stocks Poised For Growth To Buy Now,"Jul 02, 2025, 04:22pm EDT",Peter Cohan,"Nvidia is the best-known company capitalizing on the generative AI boom that began in 2023, when ChatGPT quickly scaled to over 100 million users. In fact, it was Nvidia’s first-quarter 2023 report, which featured a decline in revenue and forecast huge growth, that made me realize a new technology wave was building akin to the internet boom of the 1990s.
Nvidia’s prospects gave me the kick to write a book, Brain Rush, published last year. Since then I have continued to track publicly-traded companies that benefit from growth in demand for generative AI.
In my book I mapped out the generative AI value network, including AI chip producers, data center technology makers, cloud services providers, large language model builders, generative AI application developers and AI business consultants.
From this network, I created an index of publicly-traded companies in the generative AI ecosystem. Based on the performance of this index, I have identified five lesser-known AI stocks poised for growth.
CoreWeave is a New Jersey-based provider of cloud computing services for AI developers and enterprises. Its stock market value has soared 308% since it went public in March. CoreWeave’s stock rise can be attributed to the company’s torrid revenue growth – up 420% in the quarter ending in March, according to a CoreWeave investor letter.
Retail investor interest in the company and partnerships with leading AI companies have also contributed to the stock’s performance. Specifically, CoreWeave partners with Nvidia,  which Fortune reported invested in CoreWeave before it went public,  as well as OpenAI, which makes ChatGPT.
CoreWeave is on my list because its exceptional growth exceeded my expectations and its stock-price increase leads the pack of Nvidia peers. Prior to the company’s initial public offering, I highlighted key risks – CoreWeave’s dependence on a few large customers, the company’s heavy debt load and the CEO’s lack of experience running a public company.
But if the company can keep exceeding high growth expectations, its shares could rise more.
Denver-based Palantir Technologies provides software to help public and private-sector clients identify trends, detect fraud and optimize operations through big data analytics. It has enjoyed an 81.3% increase in its stock price in the first half of this year.
Palantir achieved rapid growth and high profitability. For example, the company’s revenue increased 39.3% in the first quarter while generating an impressive 24.2% net profit margin, according to a Palantir investor letter.
Why is Palantir on a list of AI stocks? Palantir is one of the few companies that have been able to generate substantial revenue growth from the application of generative AI, as I wrote in  February. Indeed with the exception of Nvidia, no other company has achieved such significant growth from generative AI-powered products.
What’s more, Palantir recently raised its revenue growth forecast for the year from 31% to 36%. In addition to benefiting from government contracts provided by the Trump administration, Palantir stock could be propelled by potential contracts for projects such as the Golden Dome – a U.S. missile shield akin to Israel’s Iron Dome.
Citi Research analyst Tyler Radke is not all smiles. Investor's Business Daily reported recently that after meeting with Palantir management, Radke wrote, ""We continue to have concerns on how Palantir stock can grow into its valuation, especially if magnitude of positive revisions slow or large contracts (Golden Dome) don't materialize as expected.""
Snowflake, a Bozeman, Montana-based provider of data analysis services, has enjoyed a 42.1% increase in its share price during the first half of the year. While Snowflake is unprofitable, the company’s revenue grew nearly 26% in the quarter ending in April, CNBC reported.
Behind the increase in Snowflake’s stock price are successful initiatives to integrate generative AI into the company’s products, as well as partnerships with OpenAI and Anthropic, according to CNBC. In addition, Snowflake exceeded investor expectations and raised its growth guidance.
Snowflake is growing thanks to its new CEO, Sridhar Ramaswamy, whom I interviewed in May 2024. When Ramaswamy took over as chief executive in February 2024, I was unsure whether he would be as successful as his predecessor, Frank Slootman, who handed over the top job because his successor had a deeper understanding of AI.
Since then, Ramaswamy has given Snowflake’s generative AI strategy new life. Indeed, I am including Snowflake on my list because a Snowflake manager told me at a conference in San Francisco in May 2025 that Ramaswamy is doing an excellent job of leading the company’s AI product development.
Shares of Meta Platforms, the social media parent company of Facebook, Instagram and WhatsApp, rose 23% in the first half of the year. In the first quarter of 2025, Meta reported 16.1% revenue growth and earned a whopping 39.3% net profit margin, according to Meta’s investor letter.
The company has distinguished itself from rivals by using AI to help digital ad buyers sell more. In addition, Meta’s investments in AI-powered products such as Meta AI could become a new growth curve, as I wrote last October.
Meta is on my list because the company is betting heavily on AI. Its new “Superintelligence Labs” unit, led by former Scale AI CEO Alexandr Wang and former GitHub CEO Nat Friedman, could create new growth curves for the company, Barron’s reported.
However, a note of skepticism is called for because Meta CEO Mark Zuckerberg has bet big in the past – remember the metaverse, which drove the company’s name change from Facebook? That bet has produced billions of dollars worth of losses and does not appear poised to pay off.
Nevertheless, Meta could surprise me if the company can attract and motivate the talent needed to surpass more focused rivals like OpenAI and Perplexity.
Shares of Taiwan Semiconductor, which makes the chips Nvidia and others design, have risen 12.3%, increasing steeply, since late April. TSMC grew revenue 41.6% in the first quarter and earned a 43% net profit margin, according to the company’s investor letter.
While tariff uncertainty and geopolitical instability could threaten TSMC, a long-standing partnership with Nvidia could propel TSMC stock higher. Analysts project growth in TSMC’s AI-related revenue as high as 45% compounded annually through 2030, according to AInvest.
If this prediction is realized, investors could benefit from owning TSMC stock.
Bottom Line
Nvidia is not the sole beneficiary of the generative AI boom. Other companies who play in the AI ecosystem – CoreWeave, Palantir, Snowflake, Meta Platforms and TSMC – may also enjoy big increases in their stock prices. Investors should consider whether to add them to their portfolios."
59,https://www.forbes.com/sites/gilpress/2025/06/30/the-ai-network-is-the-computer-says-nvidia/,"The AI Network Is The Computer, Says Nvidia","Jun 30, 2025, 09:00am EDT",Gil Press,"Nvidia just reclaimed its title as the world’s most valuable company. Whether it retains this top position and for how long depends on its success in defining and developing a worldwide network of AI processing units.
Nvidia is pursuing a vision of a future where “part of the application runs in the data center, another part in a data center at the edge, and another part in an autonomous machine roaming around the world.” This is how Jensen Huang, Nvidia’s co-founder and CEO, described the future of computer applications in a conversation with Bob Metcalfe, inventor of the Ethernet. Huang and Metcalfe are prime examples of the remarkable marriage of engineering ingenuity and marketing creativity that has made many American entrepreneurs successful.
Already five years ago, Huang saw the data center as “a composable disaggregated infrastructure,” where the critical path is the interaction of one “computing node” with another “computing node” over the Ethernet network. In response, Metcalfe asked, “Is this why you bought Mellanox?” and Huang answered, “It is exactly the reason why I bought Mellanox,” adding a great insight: “Understanding the direction of software inspires you about what’s the best way to design and evolve hardware.” In other words, anticipating how applications will be developed and run in the future, Nvidia has added to its portfolio (developing in-house or acquiring) new hardware elements so that it can offer its customers faster, more efficient, more resilient, and less expensive shuttling of data inside and outside the data center.
Founded in Israel in 1999, Mellanox initially focused on developing computer networking products based on the then-new InfiniBand standard. These products featured high throughput and low latency, ensuring fast data movement between one “computing node” and another. Mellanox later added networking products based on the Ethernet standard and was acquired by Nvidia for $6.9 billion in 2019.
Kevin Deierling, the first Mellanox employee in the U.S., is now Nvidia’s senior vice president of networking. Nvidia’s networking division develops and sells the Spectrum-X networking platform, which the company calls “the world’s first Ethernet networking platform for AI.”
Deierling explains that the unique nature of data processing for AI makes the capabilities of the network critical. Cloud computing serves millions of users, each transferring a small amount of data, and that data is completely unsynchronized. In contrast, AI—and Nvidia’s processing units, or GPUs—do things in parallel. “With AI workloads,” says Deierling, “we have enormous, what we call elephant [data] flows, that are synchronized.” Each of the vast number of AI computing nodes operates on its part of the data and then shares all the data it's processed with the other nodes. “That ends up being extremely bursty traffic,” observes Deierling.
The second trend driving the need for Spectrum-X's capabilities is the shift in the focus of AI projects. Until recently, AI work mainly involved “training,” feeding an AI model vast amounts of data to learn patterns and relationships. Enterprises are now moving to “inference,” or using the trained model to process new data, make predictions, or take action.
With inferencing, many customers share the same network infrastructure, increasing performance expectations and requirements. The Spectrum-X platform answers these, bringing InfiniBand's high-performance bandwidth and latency specifications to Ethernet. The significant benefit of using Ethernet for connecting all the components of the AI infrastructure—the data storage unit, the network moving the data, and the data processing units or GPUs—is that it is a widely deployed standard familiar to the many customers now investing in AI. Spectrum-X “uses standard Ethernet protocols,” says Deierling, “but it does things under the hood that make it extremely high performance. The largest AI supercomputer in the world today is based on our Spectrum-X platform.”
The faster and more efficient data movement in the data center implies increased profits for the service provider. “If you're offering an AI service, you're extremely interested in the performance per dollar and the performance per watt of the data center,” says Deierling. In addition, Spectrum-X allows the data center to offer a customized service, adjusting the network's performance based on the varying needs of different end-users and, of course, on what they pay.
Deierling reports that enterprises are rapidly adopting AI agents, adapting them by adding their proprietary data to a model trained on what’s found on the internet. Especially in the context of AI research agents, that’s a sure way to reduce AI “hallucinations” and comply with regulations. “The next wave we're starting to see is physical AI, edge applications, and robotics,” says Deierling, with the Ethernet connecting everything from the cloud to enterprise data centers to mobile and stationary sensors.
“The Network is the Computer” was the 1984 tag line for Sun Microsystems, a maker of “workstations,” or networked desktop computers. Nvidia’s founders played together flight simulator and “theorized that the killer app would be virtual reality, video games, and 3D games,” Huang told Metcalfe, and that “everybody would want to be a gamer.”
Four decades later, with AI constituting “a new way of writing software,” everybody would want to be a coder, writing applications for the composable disaggregated infrastructure developed and maintained by Nvidia and its partners. “We found ourselves at the right place at the right time. Part insight, part strategy, part serendipity,” said Huang.
"
60,https://www.forbes.com/sites/tylerroush/2025/06/27/jensen-huang-now-worlds-9th-richest-as-nvidia-win-streak-continues/,Jensen Huang Now World’s 9th-Richest As Nvidia Win Streak Continues,"Jun 27, 2025, 01:20pm EDT",Ty Roush,"Nvidia shares rose again on Friday, adding to CEO Jensen Huang’s net worth—now the world’s ninth-largest—while pacing what would be the stock’s fifth consecutive day in the green, as one analyst claimed the chipmaker’s market cap could reach $4 trillion this summer.
Nvidia’s shares rose by more than 1.6% to over $157.60 as of around 1:10 p.m. EDT Friday, an increase of more than 10% since the stock opened trading on Monday at $142.50.
Huang’s 3% stake in Nvidia, totaling more than 859 million shares, has increased by about $11.6 billion over the last five trading sessions, including Friday, which added about $3 billion.
Wedbush Securities analyst Dan Ives suggested in a note Friday that Nvidia and Microsoft will each hit a $4 trillion market cap this summer and $5 trillion over the next 18 months, as he referred to the companies as the “poster [children]” of AI.
Nvidia ranks ahead of Microsoft ($3.7 trillion) as the world’s largest firm with a $3.83 trillion valuation as of Friday.
On Wednesday, Loop Capital analyst Ananda Baruah wrote that Nvidia would be at the “front-end” of the next “Golden Wave” for generative AI that would propel Nvidia’s market cap up to $6 trillion.
Huang’s fortune is valued at $137.4 billion, making him the world’s ninth-wealthiest person ahead of Alphabet cofounder Sergey Brin, according to our latest estimates.
81.7%. That’s how much Nvidia’s stock has increased in value since hitting a 52-week low of $86.62 on April 7, though shares are up just under 14% on the year.
How long Nvidia’s win streak continues. Baruah raised his price target for Nvidia’s stock to $250 from $175, above the average $173 forecast among Wall Street analysts, according to FactSet, as Baruah said he believed Nvidia would benefit from “stronger-than-anticipated demand” for its products.
The latest winning streak for Nvidia follows concerns the company would be hit by President Donald Trump’s tariffs and export controls on China. The U.S. imposed restrictions on Nvidia’s H20 AI chips to China earlier this year, which Nvidia expected to total a $5.5 billion hit to sales through its current fiscal year. Huang, 62, said these restrictions would lock Nvidia out of a $50 billion China market and argued the export changes cut sales without a “grace period.” Nvidia reported higher-than-expected earnings and revenue through its first quarter in May, as Huang told investors the “global demand” for Nvidia’s AI infrastructure was “incredibly strong.”"
61,https://www.forbes.com/sites/tylerroush/2025/06/25/nvidias-jensen-huang-becomes-5-billion-richer-as-shares-hit-record-high/,Nvidia’s Jensen Huang Becomes $5 Billion Richer As Shares Hit Record High,"Jun 25, 2025, 03:27pm EDT",Ty Roush,"Nvidia CEO Jensen Huang’s net worth swelled by about $5 billion Wednesday as Nvidia shares jumped by more than 4% to a new intraday high, after some analysts projected the success of Nvidia’s AI products could help propel the chipmaker’s market valuation to $6 trillion.
Nvidia shares increased by 4.3% to $154.31 as trading closed Wednesday, breaking the stock’s closing record of $149.53 after setting an intraday record of $154.43 earlier in the day.
The value of Huang’s roughly 3% stake in Nvidia, totaling more than 859 million shares, increased by more than $5 billion as the stock rose.
Nvidia added about $150 billion to its market cap Wednesday, as the company surpassed Microsoft—valued at more than $3.6 trillion—as the world’s largest firm, with a $3.75 trillion valuation.
Ananda Baruah, an analyst for Loop Capital, said in a note Wednesday that Nvidia would be at the “front-end” of the next “Golden Wave” for generative AI as more companies adopt the technology, which Baruah said would feature “stronger-than-anticipated demand.”
Baruah raised his price target for Nvidia’s stock to $250 from $175, surpassing the average $173 forecast among Wall Street analysts, according to FactSet, while claiming Nvidia’s market valuation could peak near $6 trillion.
Huang’s fortune is valued at $134 billion, ranking him as the 10th-wealthiest person in the world, according to Forbes’ estimates.
Huang sold 100,000 shares of Nvidia over the last week as part of his plan to sell up to 6 million through the end of the year, according to a Securities and Exchange Commission filing. He sold about $700 million worth of Nvidia shares in a similar plan last year.
Nvidia’s shares, up 11% on the year, have risen in recent weeks as fears the company would suffer from export controls appear to have dispelled among investors. Nvidia said in April the company expected to take a $5.5 billion hit to sales after the U.S. imposed restrictions on Nvidia’s H20 AI chips to China. Huang, 62, told investors in May a $50 billion market in China for AI chips was “effectively closed to U.S. industry,” though he noted the “global demand” for Nvidia’s AI infrastructure was “incredibly strong.”"
62,https://www.forbes.com/sites/marcochiappetta/2025/06/24/nvidia-expands-its-gpu-line-up-with-low-power-geforce-rtx-5050-for-laptops-and-desktops/,Nvidia Expands Its GPU Line-Up With Low-Power GeForce RTX 5050 For Laptops And Desktops,"Jun 24, 2025, 01:14pm EDT",Marco Chiappetta,"Nvidia has been steadily fleshing out its Blackwell-based GeForce RTX 50 series of GPUs for a few months now. Before Tuesday, the line-up spanned all the way from the $299 GeForce RTX 5060, on up to the powerful, flagship $1,999 GeForce RTX 5090. On June 24th though, the company unveiled the GeForce RTX 5050, a new, low-power GPU that’s coming to both laptops and desktops.
The GeForce RTX 5050 arrives in laptops first, in systems that’ll start at $999. In fact, there are already some GeForce RTX 5050-powered laptops for sale, including this MSI Katana 15 HX, which just hit Walmart’s website as I was writing up this coverage. The GeForce RTX 5050 Laptop GPU features 2,560 CUDA Cores, with Nvidia’s 5th Generation AI Tensor Cores, 4th Generation Ray Tracing Cores, a 9th Generation Nvidia Encoder (NVENC), and a 6th Generation Nvidia Decoder (NVDEC).  As you’ll see, those core counts are similar to the  upcoming desktop variant, but the laptop RTX 5050 does have one significant difference – other than its form factor, of course.  The GeForce RTX 5050 laptop GPU will be paired to 8GB of GDDR7 video memory, running at 24 Gbps. The desktop version of the GeForce RTX 5050 will use GDDR6 memory.  The reason for using GDDR7 memory in laptops is power efficiency, which directly translates to better battery life and more manageable thermals – two things that are paramount for laptops. GDDR7 is up to twice as efficient as GDDR6; that’s a huge benefit for laptop OEMs and ODMs.
The first wave of GeForce RTX 5050-powered laptops will be as thin as 15 millimeters, and as light as 1.3 kilograms, or 2.9 lbs. For students who want the benefits of a discrete GPU for STEM applications, or mobile gamers who are happy with gaming at 1080p resolutions, the GeForce RTX 5050 laptop GPU should be a good option. It’ll most definitely outperform integrated graphics solutions and offer all of the benefits of Nvidia's Blackwell architecture, like support for DLSS 4 and Blackwell’s more advanced media engine.
GeForce RTX 5050 graphics cards for desktop systems will arrive in pre-built systems and on store shelves in mid-July, with a base MSRP of $249.  According to Nvidia, cards will be coming from numerous add-in card partners, including Asus, Colorful, Gainward, Galaxy, Gigabyte, Inno3D, MSI, Palit, PNY, and Zotac, in a variety of configurations, including factory-overclocked models.
Like the laptop variant, the GeForce RTX 5050 desktop GPU will feature 2,560 CUDA Cores, with 5th Generation AI Tensor Cores, 4th Generation Ray Tracing Cores, a 9th Generation Encoder (NVENC), and a 6th Generation Decoder (NVDEC). As I’ve already mentioned, however, the cards will pack 8GB of GDDR6 video memory on a 128-bit memory bus. Base GPU clocks should hover around 2.31GHz, but if the GeForce RTX 5050 is as overclocking-friendly as some of its higher-end counterparts, expect them to hit much higher clocks with a little tweaking.
GeForce RTX 5050 series cards for desktops will have TDPs in the 130-watt range and be powered by a single PCIe 8-pin cable. This is an important consideration for anyone considering upgrading a desktop system that may be currently using integrated graphics or a previous-gen GPU. The GeForce RTX 5050 cards have relatively modest power requirements and should be suitable for systems with basic power supplies of around 550 watts.
The GeForce RTX 5050 will be Nvidia’s lowest-power RTX 50 series GPU, and as such, it won’t be for everyone. The GPU targets 1080p gamers looking for a modern GPU, with the latest feature support. As you can see in the charts posted earlier in this article, the GeForce RTX 5050 should be a big upgrade over older GeForce RTX 3050 series cards or laptop GPUs, even without factoring in the latest technologies offered by DLSS 4, like multi-frame gen. There were no GeForce RTX 4050 series cards for desktops (at least not officially). And with DLSS 4 and multi-frame generation, it’s really no contest – the GeForce RTX 5050 easily surpasses the previous gen GPUs.
Consumers looking to upgrade from an older RTX xx50 GPU or legacy, mid-range GTX-class GPU, to a low-power modern graphics card may want to check out the GeForce RTX 5050. I haven’t gotten to test one just yet, but based on Nvidia’s data it looks to be a solid upgrade over legacy GPUs in its class, while also being relatively power friendly and affordable. Driver support is coming in early July for anyone that scores a GeForce RTX 5050 laptop or desktop GPU early, but systems that hit store shelves will obviously ship with a pre-installed driver as well."
63,https://www.forbes.com/sites/karlfreund/2025/06/17/is-nvidia-competing-with-its-gpu-cloud-partners/,Is Nvidia Competing With Its GPU Cloud Partners?,"Jun 17, 2025, 06:16pm EDT",Karl Freund,"Nvidia recently announced two new cloud initiatives. First, the company announced DGX Cloud Lepton, designed to connect artificial intelligence developers with Nvidia’s wide network of cloud providers. Second, Nvidia announced a new cloud service, the Industrial AI Cloud, intended to provide AI services to manufacturing companies in Europe. While these moves pit Nvidia against its cloud partners, the larger cloud service providers (CSPs) chose to compete with Nvidia using their in-house developed GPU alternatives. Google has the TPU, Amazon has Trainium, Microsoft has Maia, etc. (Nvidia is a client of Cambrian-AI Research.)
Turn about is fair play, and Nvidia is helping its cloud partners sell AI services that keep their GPUs running at high utilization, maximizing profit, while also helping developers access a broader inventory of rare and expensive GPUs.
Much to the consternation of its cloud partners, Nvidia launched the new DGX Cloud Lepton service at Computex this year, and has already garnered a healthy suite of CSPs to agree to join the service.  While Oracle and Google have yet to sign up publicly for Lepton, Amazon AWS and Microsoft Azure have done so. They see the benefits of having their clouds accessible and promoted by Nvidia.
The smaller GPU cloud players have also joined the party, including CoreWeave, Crusoe, Firmus, Foxconn, GMI Cloud, Lambda, Yotta Data Services, Nebius, Nscale, Firebird, Fluidstack, Hydra Host, Scaleway, Together AI, Mistral AI, SoftBank Corp. These providers offer both on-demand and long-term GPU access, supporting a wide range of AI development and deployment needs. Other CSPs won’t want to miss the train, and will likely join soon.
At the Paris GTC, Nvidia CEO Jensen Huang announced that Nvidia and Deutsche Telekom were building an AI Cloud for European manufacturing companies. The Industrial Cloud will provide access to state-of-the-art AI infrastructure and Nvidia’s rich portfolio of software.  Support will be available for CAD, CAE, Omniverse, Robotics, and Autonomous Vehicles. The cloud is fully configured to support Nvidia’s optimized enterprise AI software portfolio, and should be open for business in early 2026.
Nvidia's Industrial Cloud for Europe represents a major step in building sovereign, AI-powered infrastructure for the continent's industrial sector. By providing secure, high-performance compute resources and a robust AI software ecosystem, the initiative aims to propel European manufacturing into the next era of digital innovation
The Industrial Cloud will be powered by 10,000 Nvidia GPUs, including the latest DGX B200 systems and RTX PRO servers, making it one of the largest industrial AI deployments in Germany. Think of this as a manufacturing-focussed sovereign data center managed and operated by Deutsche Telekom, ensuring data sovereignty and compliance with European regulations, addressing concerns about dependency on non-European cloud providers.  The lack of NVL72 racks tells us that Nvidia expects customers to fine-tune and serve AI inferencing, not create new foundation models.
Users will have access to Nvidia’s CUDA-X libraries and workloads accelerated by Nvidia GPUs and Omniverse, supporting a wide range of industrial applications such as simulation, digital twins, robotics, design, engineering, and factory planning.
The cloud will also support applications from leading industrial software providers including Siemens, Ansys, Cadence, and Rescale, enabling advanced manufacturing workflows for companies such as BMW, Maserati, Mercedes-Benz, and Schaeffle.
First, it says that Nvidia isn’t afraid to compete with its cloud partners in its quest to provide access to state-of-the-art AI infrastructure to its end users. As we noted, the larger CSPs chose to develop competing AI accelerators, so they should not be surprised.
Second,  in reality Lepton doesn’t compete with CSPs; it provides aggregated access to their massive arrays of Nvidia GPUs, not a cloud that is owned and operated by Nvidia. And the Industrial Cloud is filling a gap left by the CSPs to provide focussed and sovereign resources for the European manufacturing base.
Customers will love it, and so will the ISVs whose software has been optimized to run on Nvidia GPUs.
Disclosures: This article expresses the opinions of the author and is not to be taken as advice to purchase from or invest in the companies mentioned. My firm, Cambrian-AI Research, is fortunate to have many semiconductor firms as our clients, including Baya Systems BrainChip, Cadence, Cerebras Systems, D-Matrix, Esperanto, Flex, Groq, IBM, Intel, Micron, NVIDIA, Qualcomm, Graphcore, SImA.ai, Synopsys, Tenstorrent, Ventana Microsystems, and scores of investors. I have no investment positions in any of the companies mentioned in this article. For more information, please visit our website at https://cambrian-AI.com."
64,https://www.forbes.com/sites/lutzfinger/2025/06/16/nvidia-wants-to-power-europes-ai-dreams-the-eu-isnt-ready/,Nvidia Wants To Power Europe’s AI Dreams. The EU Isn’t Ready,"Jun 16, 2025, 11:48am EDT",Lutz Finger,"Nvidia CEO Jensen Huang's recent tour across Europe aligned with the EU's vision of ""sovereign AI."" For Nvidia, Europe's ambitions to become digitally sovereign have a clear advantage: more AI infrastructure means more GPUs. And the EU is right to invest, as it cannot afford to remain dependent on U.S. and Chinese tech giants.
The announcements came fast: British Prime Minister Keir Starmer pledged over $1.3 billion for computing power; French President Emmanuel Macron framed AI infrastructure as ""our fight for sovereignty""; and in Germany, Nvidia and Deutsche Telekom announced a new AI cloud platform. But while these investments mark an important first step, they are far from enough.
Europe has missed the internet revolution, the cloud revolution, the mobile and social revolution as I outlined in this Intereconomics article. Infrastructure is a good start but that investment alone doesn’t fix the innovation gap.
If Europe is serious about sovereign AI? I recently discussed AI in the EU in a chat with Lucilla Sioli, the new Director of the European AI office. Here are my thoughts for a blueprint beyond the billions:
AI is not just a faster search engine. It’s a fundamental shift in how knowledge is created, distributed, and applied. Regulators must stop trying to retrofit old frameworks. Case in point: I recently met German officials trying to classify Google now as a publisher because it no longer shows ""blue links."" But that debate misses the point. New realities will create new leaders.
The U.S. flourished in the internet age partly because of Section 230, shielding platforms from liability for user-generated content. Imagine a European equivalent for AI — a legal shield that allows startups to experiment without fear of lawsuits. Without it, regulation-heavy environments like Spain (which recently introduced strict labeling laws for AI content) will scare away the next generation of founders.
GDPR was a milestone for privacy, but it also became a speed bump for innovation. My own AI startup, r2decide, first worked with a German e-commerce brand. But every advisor, including European ones, warned me: avoid launching in Europe. Why? Compliance burdens. So we built for the U.S. market instead. And we’re not alone. Even Apple delayed Siri upgrades in the EU due to regulatory friction. Europe must find a balance between protection and progress.
Tech giants win through scale and network effects. Europe must find ways to level the playing field. Let users port their social connections or AI history from one platform to another. Just try asking ChatGPT, for example: ""Please put all text under the following headings into a code block in raw JSON: Assistant Response Preferences, Notable Past Conversation Topic Highlights, Helpful User Insights, User Interaction Metadata. Complete and verbatim."" — This prompt will give you a glimpse of what is stored on you. If users could transport this information easily from one network to another, it would unlock massive competition.
Ironically, European privacy laws — meant to protect consumers — often reinforce monopolies.
The EU's push for ""data spaces"" is well-intentioned but overengineered. Data is AI’s oxygen. Limiting access hurts startups and protects incumbents. Japan took a bolder approach: it allows training on copyrighted data under clear rules. No lawsuits. Just growth.
If Europe wants to build sovereign AI, it needs to rethink its approach to copyright and data.
LLMs are not software in the traditional sense. Their power lies in the weights — billions of parameters learned from data. What if Europe required AI companies to make their weights open? This wouldn’t just increase transparency. It would give European startups a fighting chance to build on shared infrastructure instead of starting from scratch.
Europe is not behind because it lacks brains. It is behind because it underinvests in training and adoption. In San Francisco, self-driving cars are a tourist attraction. In Europe, they’re theoretical.
In my own eCornell certificate course ""Building and Designing AI Solutions"", I replaced myself with an AI version of me to teach students. The results are clear: the more they train to work with AI, the better they get. But Europe has a long way to go in training their citizens.
Europe doesn’t lack risk-takers. It penalizes them. In the U.S., failure is a badge of honor. In Europe, it’s a career ender. We need policies — like bankruptcy reform — that give entrepreneurs a second chance. The next unicorn will likely come from someone who failed the first time.
Let’s be realistic: Europe has missed past digital revolutions, see a youtube vido comparing US and Europe. AI could be different. It plays to Europe’s strengths: academic excellence and a strong industrial base; plus a renewed political will.
Nvidia’s tour shows they are willing to support. Infrastructure is just the first step. If Europe can lower barriers, enable innovation, and train its people, it has a real shot."
65,https://www.forbes.com/sites/janakirammsv/2025/06/13/nvidia-wants-to-build-a-planet-scale-ai-factory-with-dgx-cloud-lepton/,Nvidia Wants To Build A Planet-Scale AI Factory With DGX Cloud Lepton,"Jun 13, 2025, 03:58am EDT",Janakiram MSV,"In April 2025, Nvidia quietly acquired Lepton AI, a Chinese startup specializing in GPU cloud services. Founded in 2023, Lepton AI focused on renting out GPU compute that’s aggregated from diverse infrastructure and cloud providers. While the deal value is unknown, the founders of Lepton AI, Yangqing Jia (former VP of Technology at Alibaba) and Junjie Bai, joined Nvidia to continue building the product. Lepton AI had previously raised $11 million in seed funding from investors such as CRV and Fusion Fund.
Nvidia has rebranded Lepton AI as DGX Cloud Lepton and relaunched it in June 2025. According to Nvidia, the service delivers a unified AI platform and compute marketplace that connects developers to tens of thousands of GPUs from a global network of cloud providers.
How Does DGX Cloud Lepton Work
DGX Cloud Lepton serves as a unified AI platform and marketplace, bringing the global network of GPU resources closer to developers. It aggregates the GPU capacity offered by cloud providers, such as AWS, CoreWeave and Lambda, through a consistent software interface. This enables developers to access GPU compute through a centralized interface, regardless of the cluster’s location.
While leveraging the underlying GPU compute, Nvidia is exposing a consistent software platform powered by NIM, Nemo, Blueprints and Cloud Functions. Irrespective of the cloud infrastructure, developers can expect the same software stack to run their AI workflows.
DGX Cloud Lepton supports three primary workflows:
Dev Pods: Interactive development environments (e.g., Jupyter notebooks, SSH, VS Code) for prototyping and experimentation.
Batch Jobs: Large-scale, non-interactive workloads (e.g., model training, data preprocessing) that can be distributed across multiple nodes, with real-time monitoring and detailed metrics.
Inference Endpoints: Deploy and manage models (base, fine-tuned, or custom) as scalable, high-availability endpoints, with support for both NVIDIA NIM and custom containers
Apart from this, DGX Cloud Lepton delivers operational features such as real-time monitoring and observability, on-demand auto-scaling, custom workspaces, security and compliance. Developers can choose the region of their preference to maintain data locality and comply with data sovereignty requirements.
DGX Lepton’s Growing Network
Nvidia has partnered with major cloud providers and infrastructure providers worldwide. Andromeda, AWS, CoreWeave, Foxconn, Hugging Face, Lambda, Microsoft Azure,  Mistral AI, Together AI and Yotta are some of the listed partners for DGX Cloud Lepton.
At the recently held GTC event in Paris, Nvidia announced that it is working with some of the leading European cloud providers to enable local developers to meet the data sovereignty needs. It also announced a partnership with Hugging Face to deliver training clusters as a service.
Nvidia collaborates with European venture capital firms, Accel, Elaia, Partech, and Sofinnova Partners, to provide up to $100,000 in GPU capacity credits and assistance from NVIDIA specialists for eligible portfolio firms via DGX Cloud Lepton.
While the pricing varies based on the cloud provider, the service is currently in preview. Developers can sign up at https://developer.nvidia.com/dgx-cloud/get-lepton to apply for early access to Lepton.
With DGX Cloud Lepton, Nvidia aims to make GPU computing accessible to global developers. Instead of launching its own cloud platform that competes with the hyperscalers, Nvidia has chosen to partner with them to deliver aggregated compute resources to developers."
66,https://www.forbes.com/sites/rashishrivastava/2025/06/12/the-worlds-largest-technology-companies-2025-nvidia-continues-to-soar-amid-ai-boom/,The World’s Largest Technology Companies 2025: Nvidia Continues To Soar Amid AI Boom,"Jun 12, 2025, 06:00am EDT",Rashi Shrivastava,"The whiplash growth of artificial intelligence has minted a gaggle of billionaires and unicorns, but chip behemoth Nvidia continues to remain among the largest beneficiaries of the AI boom against all odds.
The company took a hit on the $50 billion Chinese AI chip market after the Trump administration placed export controls on its H20 chips, which “effectively closed” the market to the U.S. industry, CEO Jensen Huang said in a recent earnings call. Earlier this year its stock briefly nosedived after Chinese darkhorse DeepSeek’s popular AI model launch triggered a panic sell-off.
Despite challenges—external and internal—the $3.5 trillion (market cap) juggernaut has continued to capitalize on the global demand for AI infrastructure. In its latest quarterly earnings in May, Nvidia beat Wall Street analysts’ expectations and reported $44.06 billion in revenue, up 69% from a year ago.
A hefty portion of its business growth, $39.1 billion of its revenue, comes from its data center operations as the company plans to build “AI factories” in the US and internationally that will help billions of people across the world run AI tools like ChatGPT. In late May, Nvidia said it will supply its powerful silicon chips to Stargate UAE, a joint project with OpenAI, Oracle and others to build a 1 gigawatt “compute cluster” in Abu Dhabi's AI campus.
Such projects have helped push the company to the top 100 on Forbes’ Global 2000 ranking the world’s largest public companies. It’s now at position No. 47, 63 spots higher than last year, thanks to a period of strong business growth spurred by widespread use of generative AI. The company is now among the likes of massive tech conglomerates like Tencent (No. 37) and Taiwan Semiconductor (No. 38). The Global 2000 measures market value, revenue, profit and assets equally using the last 12 months of data as of April 25, 2025.
Even as AI companies have shifted from training large language models to building applications on top of them, they require a steady supply of Nvidia's powerful silicon chips called graphics processing units (GPUs) to do what’s called inference— where a model produces an answer based on new information fed to it in real time. To top that, new so-called “reasoning AI models” that take a step-by-step approach to answer complex problems consume 100 times more computational resources, creating more demand for Nvidia’s hardware.
While Nvidia is the market leader, other companies are growing as well. Semiconductor company Micron, which builds memory and data storage products, has hopped more than 400 spots on the list and is now at No. 228. And South Korean memory chip provider SK Hynix Inc is up 419 spots to No. 155.
But not all semiconductor companies have benefited from the AI frenzy. Silicon Valley stalwart Intel, now at No. 488, dropped 381 spots on the Forbes’ Global 2000 this year. The company has struggled after a string of woes including declining revenue, leadership challenges and losing its market share to rivals like Advanced Micro Devices (AMD) and Arm Holdings. In December 2024, Intel’s CEO Pat Gelsinger stepped down, and was replaced by Lip-Bu Tan in March 2025. In April, the company said its plans to lay off 20% of its staff, Bloomberg reported.
The largest tech companies like Microsoft, Meta, Amazon and Alphabet have all made gigantic bets on artificial intelligence, throwing billions of dollars and the best engineers to develop AI models and build products for their users. Those four firms are all among the top 20 of the Global 2000. South Korean conglomerate Samsung is the only tech giant based overseas in the top 20, moving up to 19th this year.
One new AI beneficiary to watch is CoreWeave, which rents cloud computing power to customers and went public in March. It’s making its first appearance on the list at 1,799th this year and is likely to move up fast. It’s not profitable yet, but after the data was compiled for this year’s Global 2000 on April 25, it reported $982 million in first quarter revenue, a 420% increase compared with the first quarter last year, and its stock price has surged 250%."
67,https://www.forbes.com/sites/petercohan/2025/06/12/rigetti-computing-stock-down-37-as-nvidia-ceo-sees-inflection-point/,Rigetti Computing Stock Down 37% As Nvidia CEO Sees ‘Inflection Point’,"Jun 12, 2025, 11:39am EDT",Peter Cohan,"Nvidia CEO says quantum computing is reaching “an inflection point” and Rigetti Computing stock is up slightly — but down 37% in 2025. Should you buy?
Nvidia CEO Jensen Huang is now a quantum computing booster after a March conversion.
Actively traded Rigetti Computing enjoyed a slight boost to its stock price which is down 37% in 2025 — after a 970% increase since June 2024.
The company’s recent earnings report does not support a bull case for Rigetti stock.
Yet the longer-term opportunity for quantum computing could be significant, according to my June 10 interview with MIT expert Jonathan Ruane.
Quantum computing — which uses super-cooled metal to power more calculations per circuit than classical computing can perform — is capturing the attention of investors.
How so? Publicly traded quantum computing stocks soared Wednesday before reversing direction later in the day. For example, according to Yahoo! Finance:
The reason? “Quantum computing is reaching an inflection point,” Nvidia CEO Jensen Huang told a crowd at Nvidia GTC Paris on Wednesday, reported Yahoo! Finance. “We are within reach” of using quantum computers for “areas that can solve some interesting problems in the coming years,” Huang added.
This brings up an important observation about the industry: When Jensen Huang talks about quantum computing, people listen. That’s my modern-day twist on the old catchphrase — “When E.F. Hutton talks, people listen” — from a TV commercial for the late brokerage firm.
Rigetti is among the most heavily traded public companies on the stock exchange — but should it be? The arguments opposing investor interest seem compelling: The company’s stock has fallen 37% so far in 2025 and its first quarter 2025 financial results fell short of expectations.
Yet if prognosticators such as MIT Lecturer John Ruane are correct, the future of quantum computing could be very bright if certain problems are solved in the next five to 10 years, he told me in a June 10 interview.
Meanwhile Rigetti sees a bright future by around 2030. “That’s where the market is really supposed to grow and be large enough where things like sales and EPS start becoming much more critical,“ Rigetti CEO Subodh Kulkarni told Yahoo! Finance’s Asking For A Trend.
“At this point, it’s all about technology development and how we are getting the milestones done so that we enable this large $100 plus billion market in the future,” he added.
Huang’s views on QC have evolved significantly this year. He went from saying in January that QC could be 15 to 30 years away from offering practical solutions to business problems, noted Investopedia, to seeing QC at an inflection point on June 11.
During the interim, in March, Huang announced a bet on a Boston-area research center to help Nvidia overcome the impediments to realizing the technology’s potential, I wrote in my March Forbes post.
At the moment, QC is based on fragile hardware and error rates that are unacceptably high for most practical applications. Classical computers rely on linear algebra, noted Yahoo! Finance.
QCs, however, have far more processing capability because they apply quantum mechanics and advanced mathematics — making them amenable to solving problems in “cybersecurity, cryptography, and chemistry,” reported Yahoo! Finance.
Perhaps Huang was motivated to make his Wednesday remarks by IBM — which on June 10 announced plans to launch in 2029 a large-scale QC capable of “operating without errors,” according to IEEE Spectrum.
Google released a quantum computing chip called Willow in December, saying the technology ""paves the way to a useful, large-scale quantum computer,"" noted Yahoo! Finance. Meanwhile Amazon and Microsoft announced two quantum chips in February.
Compared to these recent announcements, Huang’s remarks were relatively content free.
Meanwhile, Rigetti seems to have little capital compared to these tech giants.
What’s more, Kulkarni said the company is years away from being little more than a technology researcher hoping for a breakthrough from which the company can generate revenues. How so? “We are four to five years from real commercial value of quantum computing,” Kulkarni told Yahoo! Finance’s Asking For A Trend.
Rigetti’s first quarter 2025 results were disappointing. The company’s $1.5 million in revenue for the quarter fell more than 42% short of expectations while earnings per share of 13 cents missed by 89%, according to Google Finance.
Rigetti’s net profit was less than met the eye in the first quarter. The company reported a $42.6 million profit, “driven mainly by non-cash gains tied to financial instruments,” noted Quantum Insider. As Rigetti ramped up utility-scale quantum systems, the company revealed a $21.6 million operating loss.
One thing helping Rigetti is the 970% increase in the company’s stock price in the last year. The company has wisely capitalized on that increase to sell $350 million in common stock — boosting its cash balance to about $570 million, according to Investing.com.
Perhaps this cash will shore up the company’s finances over the next four or five years as a hoped-for increase in revenue ultimately emerges.
Ruane, who co-authored MIT’s Quantum Index Report 2025,  sees a path for QC to create value for business leaders. “Quantum computing breakthroughs in science and engineering will make the tool more valuable for business,” he told me in a June 10 interview.
“Better error correction algorithms will make QC less expensive and more consistent. Better performance will increase QC’s commercial viability. Google’s Willow chip showed big improvements in error correction,” he added.
Different technical experts must collaborate for QC to solve real world business problems well. “Engineering larger systems — including the chip and the surrounding infrastructure — that work together can improve QC’s value to business,” he explained.
“Google and IBM will operate across a range of innovations to build powerful hardware platforms. Software startups will develop technology which venture capitalists will find amenable.”
Ecosystems will be essential if the U.S. wants to lead the world in QC. “Universities like MIT and Harvard will collaborate with large companies and startups to build ecosystems,” Ruane told me.
“Nvidia’s Boston-based Accelerated Quantum Research Center is a good example. In May 2025, a new act was proposed — the quantum sandbox — which aims to help create QC ecosystems,” he added.
Problems with hardware and algorithms will need to be solved before achieving quantum advantage — where quantum surpasses classical computing. “QC has the potential to make pharmaceutical development more efficient. This is not certain. There is a need to focus on hardware and algorithms. MIT and Harvard have algorithm developers like Aram Harrow,” he explained.
It remains to be seen which problems will be more amenable to QC than classical computing. “It is unclear whether QC will help make large language models easier to train and operate. Who could know what AI would end up doing?”
He does not envision a handheld QC. “There will never be a day when every person can have their own quantum computer — due to the physics. The use cases of QC are also unclear. QC may be operating more like supercomputers in hybrid clouds with classical computers. Everybody could end up using QC through apps powered by the hybrid cloud.”
Business leaders will need new skills to take advantage of QC. Executives must “learn how to reimagine solutions to previously unsolvable business problems using QC. This could be in areas such as logistics where a company is trying to optimize its supply chain.”
If quantum advantage is near, will Rigetti be able to capitalize on it — or will IBM, Nvidia, Amazon, Google, and Microsoft get the lion’s share?"
68,https://www.forbes.com/sites/saibala/2025/06/11/nvidia-announces-massive-new-initiatives-in-pharma-and-clinical-research/,Nvidia Announces Massive New Initiatives in Pharma And Clinical Research,"Jun 11, 2025, 07:00am EDT","Dr. Sai Balasubramanian, M.D., J.D.","Founder and CEO of Nvidia, Jensen Huang, announced today in his GTC Paris keynote that the company has inked two new large partnerships to advance the company’s work in healthcare and life-sciences.
The first is with European based global pharmaceutical giant, Novo Nordisk, to advance drug discovery and development efforts by leveraging an existing partnership with the Danish Centre for AI Innovation’s (DCAI) Gefion AI supercomputer. Novo Nordisk will utilize Gefion and a variety of Nvidia platforms such as BioNeMo, Nim, and Omniverse to build and develop customized AI models, foster agentic AI workflows and even create simulation and digital twin environments to advance physical AI applications. The primary goal will be to use these tools to better understand potential drug candidates and structures in order to build molecular models that can further the drug discovery and development pipeline.Rory Kelleher, senior director of business development for life sciences at Nvidia, explains that drug discovery can claim massive benefits from generative AI, especially in the R&D space. Mishal Patel, senior vice president of AI and digital innovation at Novo Nordisk, comments that the combination of Gefion and Nvidia’s computing platforms is an unprecedented approach and will enable the building of custom models that can truly empower better efficiency and efficacy. More generally, Gefion is a computational behemoth and has been used by numerous enterprises to advance their computing capabilities; for example, Danish startup Teton has been working with Nvidia and Gefion to build out an AI care companion for clinical settings.
The second partnership that Nvidia announced today is with IQVIA to advance the use of AI agents in the clinical research and commercialization spaces. The companies will collaborate to launch multiple AI powered agents to accelerate pharmaceutical development workflows for biotech and medical device customers globally. Importantly, the new agents will be “orchestrator agents,” meaning that they will act as supervisors for groups of “sub-agents” that each have their own specialties; the supervising agent will route a received task to the appropriate sub-agent, enabling an efficient and automated workflow. Using this technology, IQVIA is hoping to tackle some of the hardest problems in the drug development and pharmaceutical workflows. For example, clinical trials often require a significant amount of work to launch and execute. Agents can help identify targets, develop a knowledge base using existing research databases and even review clinical data to better understand insights and automate the review process. Avinob Roy, vice president and general manager at IQVIA, explains that AI agents will transform the entire “molecule to market” lifecycle.
Overall, the news comes at a time when Nvidia’s reach into the AI ecosystem has been incredibly impactful. The company indicated in its latest quarterly report a continuing surge in demand for its GPUs and hardware ecosystem. Though most traditionally a hardware giant, it has also increasingly diversified its work into the cloud and software ecosystems, furthering its moat in the AI space.
Undoubtedly, there is stiff competition in the AI race. With regard to hardware alone, technology giants such as Google and Amazon depend heavily on Nvidia for its GPUs; however, the companies are also rapidly developing their own silicon products, such as Google’s work with tensor processing units (TPUs) and Amazon’s customized silicon products (i.e., Trainium, Graviton and Inferentia).
With regard to healthcare and life-sciences more broadly, all of the large technology hyperscalers are investing billions of dollars in these fields. For example, one of the most prominent success stories is Alphabet’s Isomorphic Labs and its work with drug development models. Companies like Microsoft are also rolling out new enterprise grade tools and infrastructure capabilities to empower traditional life-science companies. Indeed, the innovation is rapid and unprecedented.
Despite the perception of a “competition” however, there is no need for a clear cut winner. The reality of this progress across the entire spectrum of technology companies is that ultimately, both the healthcare and life-sciences industries stand to gain immense benefits."
69,https://www.forbes.com/sites/karlfreund/2025/06/04/latest-mlperf-shows-how-amd-compares-with-nvidia-sort-of/,MLPerf Shows AMD Catching Up With Nvidia’s Older H200 GPU,"Jun 04, 2025, 11:00am EDT",Karl Freund,"As you AI pros know, the 125-member MLCommons organization alternates training and inference benchmarks every three months. This time around, its all about training, which remains the largest AI hardware market, although not by much as inference drives more growth as the industry shift from research (building) to production (using).  As usual, Nvidia took home all the top honors.
For the first time, AMD joined the training party (they had previously submitted inference benchmarks), while Nvidia trotted out their first GB200 NVL72 runs to demonstrate industry leadership. Each company focussed on their best features. For AMD it is larger HBM memory, while Nvidia exploited its Arm/GPU GB200 superchip and NVLink scaling.
The bottom line is that AMD can now compete head to head with H200 for smaller models that fit into MI325’s memory. That means AMD cannot compete with Blackwell today, and certainly cannot compete with NVLink-enabled configurations like NVL72.
Let’s take a look. (Note that Nvidia is a client of Cambrian-AI Research, and I am a former employee of AMD.)
AMD has more HBM memory on their MI325 platform than any Nvidia’s GPU, and can therefore contain an entire medium-sized model on a single chip. So, they ran the training benchmark that fits, the Llama 2-70B LORA model. The results are reasonably impressive, besting the Nvidia H200 by an average of 8%. While a good result, I doubt many would choose AMD for 8% better performance, even at a somewhat lower price. The real question, of course, is how much better the MI350 will be when it launches next week, likely with higher performance and even more memory.
One thing AMD will not offer soon is better networking for scale-up; the UA-Link needed to compete with NVLink is still many months away (possibly in the MI400 timeframe in 2026).  So, if you only need a 70B model, AMD may be a better deal than Nvidia H200; but not by much.
AMD is also showing traction with partners, and better performance from its ROCm software, which took quite a beating from SemiAnalysis last December. With better ease-of-use from ROCm, partners can benefit from offering customers a choice; many enterprises do not need the power of an NVL72 or NVLink, especially if they are focussed on simple inference processing. And of course, AMD can offer better availability, as NVIDIA GB200 is much harder to obtain due to overwhelming demand and pre-sold capacity. The rumor mill says GB200 still takes over a full year delivery time if you order today.
So, if you net it out, the MI325 result foreshadows a decent position for the MI350, but support for only up to 8 GPUs per cluster limits their use for large-scale training deployments.
Nvidia says the GB200 NVL72 has now arrived, if you were smart enough to put in an early order. With over fifty benchmark submissions using up to nearly 2500 GPUs, Nvidia and their partners ran every MLPerf benchmark on the ~3000 pound rack, winning each one.  CoreWeave submitted the largest configuration, with nearly 2500 GPUs.
While the GB200 NVL72 can outperform Hopper by some 30X for inference processing, its advantage for training is “only” about 2.5X; thats still a lot of savings in time and money. The reason is that inference processing benefits greatly from the lower 4- and 8-bit precision math available in Blackwell, and the new Dynamo ""AI Factory OS”  optimizes inference processing and reuses previously calculated tokens in KV-Cache.
While AMD does not yet have the scale-up networking required to train larger models at Nvidia’s level of performance, this benchmark shows that they are getting close enough to be a contender once that networking is ready next year. And AMD can already out-perform the Nvidia H200, once you clear the ROCm development hurdle.
It could take a year or more for AMD to be able to scale efficiently, and by then Nvidia will have moved on to the Kyber-based NVL576 with the new NVLink7, Vera CPU and upgraded Rubin GPU.
If you start late; you stay behind."
70,https://www.forbes.com/sites/johntamny/2025/06/02/nvidia-in-china-is-much-more-peaceful-than-nvidia-outside-of-it/,Nvidia In China Is Much More Peaceful Than Nvidia Outside Of It,"Jun 02, 2025, 02:30pm EDT",John Tamny,"Nvidia is planning a new office and facility in Shanghai. Which makes sense. Already a huge market, China continues to grow.
That’s why it’s puzzling that Sen. Jim Banks (R-IN) and Sen. Elizabeth Warren (D-MA) are protesting Nvidia’s mainland expansion. In a letter written to Nvidia CEO Jensen Huang, Warren and Banks claim a facility in Shanghai “raises significant national security and economic security issues that warrant serious review.” The senators are incorrect three times, and surely more.
It's best to start with simple commercial and economic realities. China is once again a huge, and growing market. For Nvidia to not prioritize the sale of its world-leading products in China would be like Gucci turning up its nose to Beverly Hills. And that’s an understatement.
Banks and Warren would perhaps pivot to the importance of keeping innovative technology out of the hands of an alleged enemy, except that there’s no way to do that. With market goods, there’s no accounting for their final destination. While Nvidia can choose its customers, it can’t choose its customers’ customers. Translated, so long as Nvidia is producing chips, its chips will find their way to China.
Which means there’s no way to keep mimicking foreign eyes off American production, nor American eyes off of foreign production. As longtime Nvidia employee Dwight Diercks has said, “Everyone takes a look at their competitors’ hardware and how it works.” Well, yes.
Luckily the present of chipmaking is invariably the past. As Huang himself put it to Stephen Witt (author of a history of Nvidia, The Thinking Machine), “if we don’t reinvent ourselves, and we don’t open the canvas for the things we can do,” we “will be commoditized out of business.” Assuming the competition can steal Nvidia’s brilliant present, it can’t steal Nvidia’s brilliant minds. That’s very important when it’s remembered that Nvidia’s geniuses never sit still.
Which leads to the “national security” angle. Here it should simply be said that Banks and Warren get things backwards. That’s usually the case when politicians insert themselves into commerce.
How peaceful that Nvidia is trying to expand in China. And that’s because commerce between producers in nations potentially at odds makes war between those nations frightfully expensive. Which means the more Nvidia is selling into China, the more expensive it will be for the U.S. to aim guns and bombs at China. Really, who would want to war with such a great customer?
What’s true for the U.S. is true for China. Again, Warren and Banks plainly fear that with unfettered access to Nvidia products, Chinese producers will make great leaps in the AI space that Nvidia is in many ways the face of. Can the senators promise such a beautiful outcome?
It’s a question worth asking in consideration of the certainty that there are more than a few Steve Jobs types in China just dying to make their global mark on the commercial stage. If so, think of all the amazing, life and commerce-enhancing leaps that Americans will get to achieve thanks to Chinese genius in the AI space. And if Americans are big buyers of Chinese production necessary to take major leaps, it will similarly be expensive for China to direct guns and bombs at the U.S. Trade is not war, it’s war’s opposite.
It’s a reminder that trade among people is easily the greatest foreign policy humankind has ever known. By expanding its presence in China, Nvidia is paving an expanded path for peace between the world’s foremost powers. Banks and Warren should recognize this happy development, and step aside."
71,https://www.forbes.com/sites/johntamny/2025/06/01/book-review-stephen-witts-take-on-nvidia-the-thinking-machine/,"Book Review: Stephen Witt’s Take On Nvidia, ‘The Thinking Machine’","Jun 01, 2025, 02:00pm EDT",John Tamny,"The world becomes more amazing by the day. The surest evidence of the previous assertion is that wealth inequality continues to soar.
Yes, you read that right. Inequality is a wonderful thing opposite the apologetic tone about it taken by left and right. Members of the left plainly disdain it, while members of the right claim it doesn’t exist in the way the left imagines, that thanks to transfer payments (yes, wealth redistribution by government) inequality isn’t that “bad.”
Except that it’s not bad. It’s great. How could remarkable achievements that power wealth creation be bad? If someone comes up with a cure for paralysis, will readers demand that the cure never reach the marketplace lest the creator grow rich?
The beautiful, freedom-infused meaning of wealth inequality came to mind while reading Stephen Witt’s new book, The Thinking Machine: Jensen Huang, Nvidia, and the World’s Most Coveted Microchip. Nvidia co-founder Huang was already billionaire bracket rich when Nvidia was largely a company popular with gamers, but it’s been Nvidia’s central role in the rise of AI that will increasingly do and think for us in ways that will propel billions into amazing work they can’t not do, that has resulted in Huang achieving centi-billionaire status.
Thank goodness for inequality. It signals stupendous, life-enhancing improvement. Huang was already a relentless worker, but it was in 2014 when employee Bryan Catanzaro truly opened Huang’s eyes to the possibilities of AI, and Nvidia’s potential to lead it. After that, and in the words of Witt, “there was only work” for Huang. “There was only AI.”
As Huang saw it, AI was “O.I.A.L.O.,” a once in a lifetime opportunity. Huang had the best talent, and he would use it to usher in a future where machines yet again think in addition to doing. Per Witt, the new software “can speak like a human, write a college essay, solve a tricky math problem, provide an expert medical diagnosis, and cohost a podcast.” And that’s just the early stuff.
To read Witt is to find out that the known with AI has pedestrian qualities relative to what remains unknown, and even better, what’s not yet been discovered. Markets are a look into the future, equity prices in particular, and Witt quotes Nvidia employee Bas Aarts as saying about what’s known only to those in the proverbial arena, that it’s of the “Wow, I cannot believe this is possible in this age” variety, that “people are oblivious about what is already going on. People have no clue” about the amazing leaps on the way.
There’s your inequality. It’s once again born of making the world so much better. The bet here is that AI will unearth human achievement that will render the poorest parts of Africa prosperous, all the while unearthing human flourishing globally that will make the abundant, incredibly prosperous, and – yes – beautifully unequal present appear Bangladesh poor and relatively wealth equal by comparison.
Which requires a pause. Just as everyone reads a different book, everyone reviews the same book differently. The review you’re about to read is being written by someone who hasn’t a faint clue about how to “back up” a computer, “upload” a medical form, or even connect a laptop to a printer. This is a way of saying in advance that while Witt’s book is heavy on technical jargon of the CPU, GPU, and CUDA variety, this review will largely focus on the presumed meaning of Nvidia’s ascent to one of the world’s most valuable and most important corporations.
Some will ask what’s the point of reading a review of a book about the man and corporation behind a remarkable technological leap since the writer is a technical dolt. It’s a fair point, but one that arguably misses the point. That’s because it doesn’t take someone technologically proficient to grasp the brilliance of work divided across as many hands, machines, and mechanical minds as possible. Even better, it’s not unreasonable to suggest that the mechanized technological genius that’s here and that’s on the way will render the technological illiterates of today the technologists of tomorrow exactly because so much formerly done by humans will be handled by machines. Time will tell.
For now, it’s best to start with gratitude for the opposite thinkers. Huang is clearly one of them. Witt writes in the book’s introduction that he’s setting out to tell “the story of a stubborn entrepreneur who pushed his radical vision for computing for thirty years.” As is frequently stated here, there’s no “majoring” in entrepreneurialism simply because no one chooses to be an entrepreneur. Entrepreneurialism is a state of mind, it’s a path certain individuals can’t not take.
It’s not planned because it’s not obvious. Furthermore, if it were obvious it wouldn’t be entrepreneurial, and it wouldn’t be entrepreneurial simply because what’s obvious is already being done. To show how outlandish Nvidia’s path was, just look at its valuation along with Huang’s net worth. Oh wow did they ever discover something amazing. Wealth inequality is the measure of this happy truth.
Huang as most know is of Taiwanese descent, though his parents (his father was a chemical engineer) sent him and a brother to a boarding school in Kentucky until they arrived. Not a fancy school (though it’s fancier now thanks to Huang), it’s believed Huang’s parents didn’t really know. They perhaps just guessed it was a school full of somebodys instead of one full of misfits. Which on its own is a story. What’s important is that where Huang went to school wasn’t going to matter, nor did it.
To his parents, and ultimately him, the U.S. was the goal. Good athletes in the capable sense just need to get here. That's the hard part, while the rest is elevation.
When he went off to college, it was Oregon State, not Stanford. In Huang’s words, “I just followed my best friend.” Really, what could school teach him? Genius can’t be taught, which is no insight. Genius was Huang’s parents getting his talents to the United States. The rest was his own invention. How to major in AI since AI wasn’t a thing, or at the very least wasn’t an operable thing when Huang was making his way up. Gaming was primitive. No need. Entrepreneurs create. They invent. Huang and the remarkable talent he cultivated would invent the future, not be told how to do it by some academic.
The individual in Bryan Catanzaro whose passion helped spark Huang’s own passion for AI was a humanities major. Hopefully conservatives convinced colleges and universities should pay the federal government back when their graduates don’t thrive read Witt’s book. One’s major is immaterial, particularly in an economy as dynamic as ours. Work that can be taught is, by definition, old news.
Out of Oregon State in 1984 Huang secured a job with Advanced Micro Devices. The pay was $28,700/year.
Following AMD, Huang went to work for LSI Logic. He rose quickly, and was running a $250 million division when he, along with Sun Microsystems employees Curtis Priem and Chris Malachowsky, decided to pursue what became Nvidia.
Even though they were told that the market for PC video gaming hardware was “crowded,” entrepreneurs have evangelical qualities about what they’re doing. Huang clearly did. The bullet-riddled Denny’s where they started making plans for Nvidia is open to this day, and is located at 2484 Berryessa Road in San Jose. Apparently Huang still goes there, and tips in the thousands after ordering all manner of menu items (including chicken fried steak – good taste!) that he eats part of.
Huang had even once worked at a Denny’s. Anecdote sucks, but Huang plainly had the immigrant desire to assimilate himself. Of course he did. Conservatives who should know better claim “we” can’t properly assimilate immigrants, but there would never be a need even if there were a real “we” beyond the rhetorical one. Implied in immigration is a desire to assimilate simply because immigration is an expression of a rich-person desire within some of the world’s poorest to increase the value of their labor.
Sequoia Ventures, arguably the world’s greatest VC, was along with Sutter Hill the early source of Nvidia’s funds. No less than Don Valentine (Cisco Systems, among others) told Huang after his subpar pitch that “Wilf Corrigan says I have to fund you, so you’re in business.” Corrigan was the head of LSI. He saw up close that Huang was going places.
Notable about Nvidia is that it started strong. It’s NV1 chip for video gamers sold in the 100,000 range, and as sales grew so did Nvidia. The problem was that the chips soon enough stopped selling, and their prices were lowered. In Huang’s words about Nvidia’s beginnings, “Every single decision we made was wrong.” Which is an economics lesson on its own.
Nvidia’s “recession” was the recovery, or would-be recovery from an NV1 error seen as “catastrophic.” When governments fight recessions they dampen the recovery. That’s the truth about the Great Depression that still eludes 99.99999% of economists.
So bad were things from 1993-96 at Nvidia that Valley graphics expert David Kirk made his contract employment there contingent on receiving a paper check every week. Stock options were not an option. Amid all this, including another failed chip, the sign went up at headquarters about how “our company is thirty days from going out of business.” Recessions signal recovery since recessions are when errors are fixed. Huang and Nvidia’s troubled times of old are useful as a way of exposing some of the many fallacies that inform “economics.”
For one, there’s a view that won’t die among economics types that the Fed funds rate is the cost of credit. Oh please. There wasn’t a number high enough in the mid-1990s to properly compensate a lender to Nvidia. Except that it wasn’t just that the loan market was shut to Nvidia. Equity finance was a non-starter too as evidenced by Kirk’s demand for actual paper checks over stock options.
It all asks readers to contemplate again why Huang is so rich today. It’s so easy to say that he’s that way because he’s a co-founder of Nvidia, but that’s the point. It’s too easy. The better explanation is that few initially wanted Nvidia’s shares. There’s quite simply no such thing as “easy money,” but since there isn’t, Huang almost certainly has more shares than he otherwise would. But that only tells part of the story.
In January of 1999 Nvidia went public, but consider the year: 1999. When did you, the reader, first hear of Nvidia? If you own shares, when did you buy them?
It’s worth asking mainly because of those who owned Nvidia shares in 1999, it’s no wild speculation that most don’t own those same shares today. It would have been too gut-wrenching for the average bear. Witt reports between the summer of 2001 and the fall of 2002, Nvidia shares lost 90 percent of their value. While Nvidia became part of the S&P 500 after replacing Enron in 2001, and while Huang was briefly a billionaire that same year, Witt notes that Nvidia’s shares didn’t just decline 90%, but that “It would fourteen years before he [Huang] saw that much money again.”
To have kept faith in Nvidia on the way to enjoying enormous upside, the typical shareholder would have held on through some rough times, including 2006 when Nvidia’s shares dropped 90 percent once again. Witt quotes one tech pundit as writing amid the 2006 collapse that “For a long time, we have wondered when Nvidia’s abject stupidity would have a price.” Stop and think about that. Or re-read it while thinking more about why Huang is so rich. He believed deeply when few did.
It's a reminder that with extraordinary wealth that spurs so much inequality handwringing, it’s perhaps better to think about the unseen. This is particularly useful with Nvidia since Huang, though known to have a “terrifying” temper, was also loathe to fire employees. He yelled them into line with his “cathartic” rants that he was very public about. Ok, but how many left after a tirade, or another dip in Nvidia shares, or because a “better” company like Intel offered them a job. How many individuals walked away from billion-dollar fortunes?
One long-time Nvidia employee, Dwight Dierks, sold some shares right after the float in 1999 to buy his father a car. When he describes it as a “billion-dollar-Cadillac” in joking fashion, there’s serious reality underlying the joke. Since going public in 1999, Nvidia shares are up 300,000%. Which is the point, but perhaps not the expected point.
It’s so easy to say how foolish it was for early Nvidia employees to leave, or for co-founder Curtis Priem to sell all of his founders shares for hundreds of millions from 2004-2006 when they would be worth $100 billion+ today, but as evidenced by how much departed employees and founders like Priem could have made, Huang pulled off the miracle of miracles. There’s no other way to explain the would have, could have been fortunes. Naturally individuals sold, quit, left, came back, left again, and all sorts of actions unrelated to amassing shares that, by virtue of their miraculous climb, were never supposed to climb this high.
In Huang’s case, and amid the occasional costly stumble rooted in his relentless investment “in speculative technologies that would either revolutionize computing or flop,” the future was far from certain. Exactly because any successful chip was immediately a target for imitation, Huang had to keep seeing ahead. In his own words, “If we don’t reinvent computer graphics, if we don’t reinvent ourselves, and we don’t open the canvas for the things we can do,” we “will be commoditized out of business.” Imagine the pressure of having to constantly see around the proverbial corner, while knowing full well the brutal market reaction that awaits after the inevitable mistake.
It all speaks to the entrepreneurial mind. And this includes people like Elizabeth Holmes who tragically sit in jail for believing so deeply in her vision. Never asked by Holmes’s myriad critics is why, assuming she knew she was committing fraud, she never sold any of her shares. There was a huge and very liquid market for private shares, yet Holmes held on to hers. It’s that state of mind thing again. It’s why the unequal are unequal. Huang held on, and did so through incredibly difficult stretches that included him fighting to keep his job. The unequal believe in themselves, and they don’t lose faith as others are expressing their lack of faith through share sales. Again, much more interesting than the billionaires minted by Nvidia would be those that weren’t because they sold, exited, or both too soon.
Which was once again logical. Witt is clear that earnings represented to Huang the chance to try something new. Rather than pivot toward the obvious, Huang was routinely searching for the “zero-billion-dollar market,” as in the market that’s not yet there. Is it any wonder that truly entrepreneurial visions are rejected by nearly everyone, and most notably the existing commercial successes? Writing about Huang, Witt indicates that he seemed to find comfort in markets “that only he would participate in – one that only he would even see.”
Huang indicates to this day that Intel isn’t nor was it ever a competitor. Nvidia was yet again in search of the ""zero-billion-dollar” markets that Intel wasn’t interested in, but that Huang was. And is. He saw things and sees things.
Very interesting in consideration of Nvidia’s wild ride was the talent that Huang lured into the fold. Keep in mind that this is Silicon Valley, which means the most ferocious battle of all is the ongoing one for talent. Yet despite Nvidia’s scary ups and downs on the way to arguably the world’s most important company, “Nvidia employees were unabashedly elitist.” It was a known that Nvidia was full of talent, which says so much about Huang. With sharks all around him trying to poach the most crucial capital of all, Huang managed to maintain a very deep bench. Terrifying as he could be, Witt notes that almost all of existing and former Nvidia employees interviewed by him “had a tender story about Huang to relate.” What fun it would be to follow him around for a day, or days. And not just Huang.
A company this dynamic is logically dense with characters. There’s Bill Dally, who dropped out of high school given an aversion to sitting through history class, became an auto mechanic, but ultimately got a BA at Virginia Tech, a Masters from Stanford, and a PhD from MIT. Dally had options, including Intel, and was told he was “crazy” for not taking an Intel job that surely more than a few Nvidians (?) did.
There was John Nickolls who always yelled instead of talking. Witt notes that Nickolls “had no interest in video games at all,” but was passionate about proving Moore’s Law wrong. He did. Nvidia did. Cancer ultimately took Nickolls way too early, but to read about him was to want to ask him (or others in the Nvidia orbit) if he felt the mechanization of knowledge and thought might eventually save him from the cancer that existing medicine couldn’t.
Alex Krizhevsky didn’t work for Nvidia, but he trained neural nets to see and think. By his actions, this soft-spoken Ukrainian immigrant conveyed to all manner of academics that (per Witt) “they had so far wasted their careers” on beliefs that Krizhevsky discredited with a week’s worth of “training,” and enormous amounts of electricity used in the training. Krizshevsky oversaw a giant leap, one indicating that “if you could teach computers to see, you could teach them everything.”
Of course, in writing what was just written it’s almost certain that all sorts of things were mangled in the telling. But that’s surely the point. And it’s why the AI advances elicit so much optimism in your reviewer.
AI isn’t a threat to humanity, rather it will elevate humankind in ways too grand to describe. Is there a way to say how it will reveal its genius? Certainly not. If there was a way to know what’s ahead, then those who know would be billionaires for knowing.
All we know, or should know, is that it will be brilliant. Think of the pin factory Adam Smith entered in the 18th century. One man working alone could maybe produce one pin per day, but several men working together could produce tens of thousands. Multiply the latter hundreds of thousands of times over with AI as technology thinks and does as though thousands and millions of brilliantly skilled individuals are at work. Oh, to be born today. Or fifty years from now.
Surprisingly, Witt expresses fear throughout the book that AI could have evil qualities, but in the words of Huang, “In order to be a creature, you have to be conscious.” AI will lift humans to the best versions of themselves precisely because it will enable specialization that will have more and more of us working all the time not because we have to, but because we want to.
In a very real sense, the AI that has been authored by the remarkable chips created by Nvidia will turn the world’s workers into Nvidia’s workers. Which will be a beautiful sight to see. Impossibly rich as Nvidia employees are, they’re still coming to work. They can’t not. They work all the time because they love what they’re doing, and they’re in love with discovery.
It cannot be stressed enough that progress in life is defined not by what we’re doing and learning, but by what we no longer need to do and learn. That’s why Huang is so right that AI “can only lead to good.” It’s so true.
About halfway through The Thinking Machine, Witt quotes AI visionary (and oddly enough, skeptic) Geoffrey Hinton talking about “the uncontainable thrill of sneak previewing embargoed AI technology that would shock the world once unveiled.” Yes! Once again, to be young. There’s no limit to human advance, particularly as production is spread across a soaring number of hands, machines, and thought that’s mechanized. Read Stephen Witt’s book to get a sense of what’s ahead as wealth inequality soars thanks to human flourishing going skyward, but almost as fun, sit back and watch. We haven’t seen this movie before, but it will make all that we’ve seen before seem incredibly bland by comparison."
72,https://www.forbes.com/sites/julianhayesii/2025/05/31/ceos-jensen-huang-and-dario-amodei-on-ai-adapt-or-be-replaced/,Nvidia’s Jensen Huang And Anthropic’s Dario Amodei Warn Leaders On AI,"May 31, 2025, 01:30pm EDT",Julian Hayes II,"One predicts AI will put 40 million people back to work. The other warns it could drive mass unemployment. Nvidia's Jensen Huang and Anthropic's Dario Amodei, despite their contrasting tones, are both broadcasting the same underlying truth: evolve or risk becoming obsolete.
Whether it’s reshaping industries, redefining roles, or redrawing the line between relevance and redundancy, AI is here in full force. For leaders, the question isn’t whether AI will change everything. It's whether you're willing to change yourself fast enough to stay afloat. This realization of AI is what makes Huang's and Amodei's recent comments so critical. Not because they agree on the outcomes but because they're waving the same flag: radical adaptability isn't optional anymore.
At the recent Milken Conference, Nvidia’s Jensen Huang didn’t sugarcoat the potential impact of AI. ""Every job will be affected, and immediately,"" he warned. But Huang wasn’t forecasting dystopia; he was emphasizing opportunity. In his view, AI can close global talent gaps, increase the GDP, and level the playing field. However, this only happens if people commit to learning and fully embracing artificial intelligence. ""You're not going to lose your job to AI,"" he said. ""You're going to lose your job to someone who uses AI.""
Anthropic CEO Dario Amodei, by contrast, is raising the alarm. In recent interviews, he has been straightforward: AI is improving at nearly all intellectual tasks, even those typically performed by CEOs. In a discussion with Axios, he predicted that up to 50% of entry-level white-collar jobs could vanish within five years. He put it this way: “Cancer is cured, the economy grows at 10% a year, the budget is balanced—and 20% of people don't have jobs.”
These aren't opposing views. They're parallel warnings. Both point to the same truth: the cost of ignoring AI is compounding, particularly for those in leadership positions.
No matter which prediction you believe, the path forward is the same. Radical adaptability is no longer a nice to have. It's the currency of continued relevance that gives you the best chance of staying ahead. It's about rewiring how you lead, think, and operate at the speed of disruption. In this new reality, clinging to stability is akin to a slow-motion decline. Here are two principles to begin practicing radical adaptability:
Intel founder Andy Grove once wrote, ""Only the paranoid survive."" That mindset isn’t about fear—it’s about foresightedness. Huang echoes this sentiment: no role is immune from AI’s reach, and machines won’t overtake those who delay adapting—they'll be outpaced by those who have already learned to use them. Amodei also points to this blind spot, warning that most people—and many leaders—still don't realize how fast things are moving. That kind of lag in awareness is where disruption thrives.
Practicing healthy paranoia means scanning for cracks before they break open. It's asking difficult questions before the market forces you to. It's not just designing a strategy for what could succeed—but instead for what could make today's model obsolete. Build before it's obvious. In the age of AI, staying a little paranoid might be your strongest competitive edge.
According to the 2025 World Economic Forum’s ""Future of Jobs"" Report, 41% of companies plan to downsize due to the impact of AI; however, 77% plan to reskill or upskill their employees. It’s never been more obvious: what got you here won't keep you here. Amodei's warning about AI's acceleration on intellectual tasks shows that static expertise now has a short shelf life. Leaders who thrive will treat reinvention not as a pivot—but as a permanent operating system.
That means evolving your digital fluency and skills, rethinking communication styles, and being willing to redefine what leadership looks like in a more machine-augmented world.
Whether AI becomes a net benefit or net threat will be debated for years. But as both Huang and Amodei stress, this isn't something you can ignore. And for those in leadership, AI won't just test your strategy. It will test your identity. This scenario is where your ego comes into play. Leaders who are rigid in how they see themselves, their roles, or their industry will fall behind. The future won't belong to the biggest or the smartest. It'll belong to the most adaptable."
73,https://www.forbes.com/sites/greatspeculations/2025/05/30/nvidia-stocks-1-big-risk/,Nvidia Stock’s 1 Big Risk,"May 30, 2025, 07:00am EDT",Trefis Team,"Nvidia’s (NASDAQ:NVDA) primary customers — Microsoft, Google, Meta, and Amazon — each spent tens of billions on their chips last year. Can these tech giants realistically double their spending on Nvidia chips again next year, and the year after, when their own revenues are only growing by about 15% annually? Absolutely not.
The music of Nvidia's 80-100% growth has to stop. When this high-pitched growth finally slows, Nvidia's valuation will drop significantly. The point is, even slowing growth can be devastating.
In fact, Nvidia has experienced strong declines time and again. Earlier this year, the stock saw a large correction amid the rise of DeepSeek. It was again hit over tariff concerns and issues with H20 chips. Looking back slightly further, the impact on Nvidia stock has been worse than the benchmark S&P 500 index during some recent downturns. For instance, Nvidia stock fell 66% during the 2022 inflation shock, versus a peak-to-trough decline of 25.4% for the S&P 500.
Now, Nvidia’s phenomenal growth is a recent phenomenon and might not last. The entire AI boom could fizzle, or perhaps large AI models could be deemed unnecessary. DeepSeek's demonstration that smaller models can perform just as well without Nvidia's latest chips raises serious questions.
What are the odds that demand for Nvidia’s chips will suddenly vanish? That’s where the speculation truly begins. Nvidia's revenues have surged by more than 80% annually for the past three years, exceeding 100% growth last year. But can this growth decelerate to 60% or even 40%? It almost certainly has to — and soon!
As mentioned, Nvidia's largest customers cannot sustain 2x year-over-year increases in spending on Nvidia chips when their own revenues are growing in the mid-teens. When this rapid expansion concludes, Nvidia’s valuation will likely drop significantly.
However, growth could then stabilize at a more sustainable pace of 20-30%, which, while not exceptionally high, is still respectable. That’s why you build a portfolio — a resilient one — to balance risk and reward. We did exactly that with the Trefis High Quality (HQ) portfolio. Balancing risk-reward is precisely how HQ outperformed the S&P 500, the Nasdaq, and the Russell 2000, clocking over 91% returns since inception!"
74,https://www.forbes.com/sites/saibala/2025/05/29/nvidia-is-investing-heavily-in-healthcare-especially-with-robotics-and-hardware/,Nvidia Is Investing Billions In Healthcare—Especially With Robotics And Hardware,"May 29, 2025, 08:00am EDT","Dr. Sai Balasubramanian, M.D., J.D.","Nvidia, which has gained immense popularity and fame in recent years thanks to its industry leading hardware and GPUs fueling the AI boom, has been investing billions of dollars to make in-roads into the healthcare industry.
The company has always been interested in healthcare. In fact, it reports partnering with nearly 200 organizations and 3,500 startups as a part of its efforts in this space. More definitively, the company has made significant progress across the realms of healthcare delivery and life sciences through use-cases such as advancing protein structure prediction models, optimizing medical imaging LLMs and even progressing with VR technology.
Earlier this month, the company announced a landmark partnership with Foxconn, which is one of the world’s largest electronics and hardware manufacturers, to help develop a robust robotics infrastructure. Specifically, Foxconn will leverage Nvidia’s technology to advance Nurabot, its collaborative nursing robot which is currently being tested in hospital settings to help alleviate time-consuming and physically tiring manual labor tasks. The companies will partner to build out the ecosystem for Nurabot as a part Foxconn’s larger “smart hospital” system, using Nvidia’s experiences in supercomputing, digital twin technology and edge computing.
The massive AI boom has brought with it numerous non-traditional players; in the case of Nvidia, it has been intriguing to see how a company that has traditionally been slotted as purely a hardware chip giant is transitioning to much more than that—partnering to actually help collaborators build out large language models, software solutions and entire ecosystems for the future of healthcare. One example is Philips, which announced last week that it would be working with Nvidia to use its 3D medical imaging models to create an MRI-specific solution.
But this non-traditional approach is likely a positive step forward for the technology industry as a whole, which for decades, has been incredibly siloed into niche sub-sectors. In fact, to parallel this phenomenon, many software giants are also entering new territories which were perhaps previously considered not their forte. For example, Google has made immense investments in the hardware space, specifically to build out its Cloud Tensor Processing Units (TPUs). The company recently announced its latest TPU last month, designed to run significantly complex workloads and AI models at scale.
Another example is Amazon, perhaps one of the best examples of a company that has largely diversified from its original core thesis. Although it started as an online retail giant, the company has made significant inroads into the world of healthcare AI as well. In fact, AWS is a leader in the healthcare and life sciences spaces, helping both customers and partners by powering their workloads and innovation using AWS’ proprietary models and platform. Additionally, Amazon is also investing heavily in its own silicon as well; its Trainium chips are being developed to handle heavy machine learning capabilities for the next generation of AI development.
Non-traditional approaches by these companies have created an incredibly competitive, yet innovative, environment. Hyperscalers, along with hundreds of smaller startups in this space, simply do not have time to make slow progress; rather, the heavy competition is forcing rapid innovation, progress and optimization, acting as the tide that is ultimately raising all the ships."
75,https://www.forbes.com/sites/investor-hub/article/is-nvidia-stock-a-buy-ahead-of-earnings/,Is Nvidia Stock A Buy Ahead Of Earnings?,"May 27, 2025, 09:58am EDT",Jason Kirsch,"Nvidia Corporation (NVDA) stands at a pivotal moment as it prepares to release its first-quarter fiscal 2026 earnings on May 28, 2025. The semiconductor giant has transformed from a gaming graphics company into the backbone of AI infrastructure worldwide. However, recent stock performance and mounting geopolitical challenges have investors questioning whether Nvidia remains a compelling investment opportunity.
This analysis examines Nvidia's year-to-date performance, key earnings metrics, macroeconomic headwinds and expert sentiment. With Wall Street projecting record revenue of $43.3 billion alongside a significant $5.5 billion writeoff related to China restrictions, the earnings report promises to be closely watched. Understanding these dynamics is crucial for investors considering their position before results are unveiled.
Nvidia's 2025 journey has been marked by volatility, with shares trading approximately 5% lower year-to-date as of late May. The year began with turbulence when Chinese startup DeepSeek released a cost-effective AI model that challenged Big Tech's AI infrastructure spending assumptions. This development and signals from Microsoft regarding potential slowdowns in AI data center investments created uncertainty about Nvidia's growth sustainability.
Geopolitical tensions have further complicated the stock performance under the Trump administration. Trade restrictions resulted in export bans on Nvidia's specialized chips to China, with CEO Jensen Huang acknowledging these policies cost the company approximately $15 billion in sales. The subsequent $5.5 billion write-off of H20 inventory designed for the Chinese market represents a tangible impact on Nvidia's financials.
Despite challenges, Nvidia maintains exceptional financial strength, with $38.5 billion in cash versus $8.5 billion in debt. Recent developments, including expansion into Saudi Arabia and the UAE and U.S. President Donald Trump's repeal of certain chip trade restrictions, have provided positive momentum. Options traders are pricing in potential movements of up to 7.4% following earnings, reflecting continued volatility expectations.
Nvidia's earnings will be scrutinized across multiple dimensions, with investors seeking insights into near-term performance and long-term positioning. The company has historically beaten quarterly guidance while providing forward estimates exceeding consensus. However, these ""beat-and-raise"" scenarios have become less dramatic as Wall Street caught up with Nvidia's trajectory.
The earnings call will be crucial for understanding how to navigate geopolitical challenges while capitalizing on AI demand. Supply constraints remain the primary U.S. growth limitation, while international expansion strategies are critical for offsetting substantial China revenue loss. CEO Jensen Huang's commentary on competitive positioning and customer diversification will provide vital future insights.
Wall Street projects record quarterly revenue of $43.3 billion for Q1 fiscal 2026, representing 66% growth versus $26 billion last year. While impressive, this means decelerating from the prior year's 262% revenue growth. Earnings per share are expected at $0.73, up from $0.61 year-over-year, with some analysts projecting adjusted EPS of $0.88, a 44% increase.
These projections must consider the $5.5 billion H20 inventory write-off impacting reported results despite underlying business strength. Historical data shows that Nvidia shareholders who purchased before earnings and held for 12 months achieved median returns of 120% over the past decade. However, as always, given evolving competitive and regulatory landscapes, past performance doesn't guarantee future results.
Data center revenue represents Nvidia's crown jewel, growing from $3 billion in fiscal 2020 to $115 billion in fiscal 2025. This segment exited recent quarters with $35.6 billion revenue and $142 billion annualized run rate. Morningstar models $40 billion for April 2025, but reduced July estimates from $44 billion to $37.6 billion due to China restrictions.
Supply constraints continue limiting Nvidia's demand fulfillment ability. Analysts expect incremental quarterly growth of approximately $4 billion through fiscal 2026, driven by additional supply and continued AI adoption. Growth sustainability depends on broader AI infrastructure investment trends and customer base diversification beyond hyperscale cloud providers.
AI demand remains Nvidia's fundamental growth driver, dominating GPU provision for AI training and inference workloads. The CUDA software platform creates significant switching costs, establishing competitive advantages beyond hardware. However, recent developments, including DeepSeek's model and scaling law debates, have introduced questions about AI investment sustainability.
Nvidia's unique position provides superior AI trend visibility across industries. The earnings call will reveal whether AI investment slowdown concerns are materializing or demand remains robust across customer segments and regions. Expansion beyond cloud providers into government and sovereign initiatives, particularly in the Middle East, could provide significant diversification benefits.
Nvidia's next-generation Blackwell architecture represents a critical future growth strategy, offering significant performance improvements over Hopper chips. Launch timing and execution face competitive pressure from semiconductor rivals and custom cloud provider chips. Early adoption, production schedules, and pricing will indicate Nvidia's ability to maintain technological leadership and premium positioning.
Blackwell's strategic importance extends beyond immediate revenue, demonstrating continued innovation and commitment to evolving AI workload requirements. Supply chain execution is critical, given that historical demand exceeds supply for new generations. Success could offset China revenue headwinds while reinforcing Nvidia's preferred AI infrastructure provider position.
The AI chip competitive landscape evolves rapidly, with major cloud providers investing in custom silicon while traditional semiconductor companies expand AI portfolios. Amazon's Trainium and Inferentia, Google's TPUs, and Microsoft/Meta design plans represent direct threats. AMD aggressively expands GPU lineups for cloud customers, while Intel develops AI accelerator products.
Nvidia's competitive strategy extends beyond hardware to software, networking and services, creating comprehensive ecosystems that are difficult to replicate. However, major customers' desire to diversify suppliers and reduce single-vendor dependence represents structural challenges. Maintaining the pace of innovation while preserving switching costs through software advantages will determine sustained market position and premium valuations.
U.S.-China geopolitical tensions represent the most significant macroeconomic headwind, with trade restrictions fundamentally altering Nvidia's addressable market. The Trump administration's export controls effectively blocked access to one of the world's largest AI markets. The $15 billion in lost sales represents just the beginning, as China's domestic semiconductor industry develops competitive alternatives like Huawei.
Broader trade tension implications extend beyond direct restrictions to supply chain vulnerabilities and technology transfer concerns. Given Taiwan tensions, Nvidia's TSMC reliance on advanced production creates additional geopolitical risk. Middle East expansion attempts to diversify geographically, but newer markets may not offset the Chinese business scale. Reciprocal tariff threats could impact broader technology sector sentiment.
Global economic conditions influence Nvidia through enterprise IT spending and AI startup venture capital impacts. Higher interest rates can reduce speculative AI investment appetite while slowing industry adoption. However, Nvidia shows remarkable economic cycle resilience, with AI infrastructure investment appearing relatively insensitive to traditional indicators. Well-capitalized customer bases provide some economic downturn insulation.
Morningstar assigns Nvidia a 3-star ""fairly valued"" rating based on a $125 fair value estimate, suggesting the current $132-135 trading represents a slight overvaluation. The analysis acknowledges the potential for a higher fundamental value if geopolitical concerns diminish or AI demand proves more durable. The wide economic moat rating reflects confidence in competitive advantages, while the ""Very High"" uncertainty rating acknowledges AI market nascency and geopolitical complications.
Wall Street sentiment remains generally positive despite challenges, with $43.3 billion consensus revenue projections showing continued growth confidence. However, decelerating growth rates and a focus on execution challenges suggest the ""easy money"" AI expansion phase may be ending. Analysts increasingly focus on navigating supply constraints, competitive pressures and geopolitical headwinds while maintaining premium pricing.
Market indicators suggest continued volatility, with options pricing implying significant post-earnings movement potential. Historically strong 12-month pre-earnings buyer returns provide long-term investor comfort, but changing competitive and regulatory environments may limit historical trend applicability. Given concentrated Nvidia holdings among major index funds, institutional investor behavior will be closely watched.
Bottom Line
Nvidia looks like a clear buy heading into earnings, with strong fundamentals, dominant AI leadership and the potential to outperform if growth holds steady. Record revenue expectations underscore continued demand, and despite its $5.5 billion China write-off, the company remains at the forefront of next-gen computing.
Still, risks remain. Geopolitical pressures, export restrictions and growing competition could affect future performance. While the stock appears reasonably valued, any upside will depend on Nvidia’s ability to maintain its technological edge and weather a more complex global environment."
76,https://www.forbes.com/sites/johnkoetsier/2025/05/28/worlds-largest-chip-sets-ai-speed-record-beating-nvidia/,"World’s Largest Chip Sets AI Speed Record, Beating Nvidia","May 28, 2025, 06:06pm EDT",John Koetsier,"Updated May 29: number of transistors on the WSE chip
Today I held the world’s largest computer chip in my hands. And while its size is impressive, its speed is much more impressive, and of course much more important. Most computer chips are tiny, the size of a postage stamp or smaller. By comparison the Cerebras WSE (Wafer Scale Engine) is a massive square 8.5 inches or 22 centimeters on each side, and the latest model boasts a staggering 4 trillion transistors on a single chip. All those trillions of transistors let the WSE set a world speed record for AI inference operations: about 2.5 times faster than a roughly equivalent Nvidia cluster.
“It’s the fastest inference in the world,” Cerebras chief information security officer Naor Penso told me today at Web Summit in Vancouver. “Last week Nvidia announced hitting 1,000 tokens per second on Llama 4, which is impressive. We just released a benchmark today of 2,500 tokens per second.”
In case all this is Greek to you, think of “inference” as thinking or acting: building sentences, images, or videos in response to your inputs, or prompts. Think of “tokens” as basic units of thought: a word, character, or symbol.
The more tokens an AI engine can process per second, the faster it can get you results. And speed matters. Maybe not so much for you, but when enterprise clients want to add an AI engine to a grocery shopping cart so they can tell you that just one more ingredient will give you everything you need for Korean-Style BBQ Beef Tacos, they want to be able to do so instantly for potentially thousands of people.
Interestingly, speed is about to get even more critical.
We’re entering an agentic age, where we have AIs that can perform complex multi-step projects for us, like planning and booking a weekend trip to Austin for a Formula 1 race. Agents aren’t magic: they eat an elephant the exact same way you would … one bite at a time. That means exploding a big overall task into 40, 50, or a 100 sub-tasks. Which means much more work.
“AI agents require way more jobs, and the various jobs need to communicate with each other,"" Penso told me. “You can’t have slow inference.”
The WSE’s four trillion transistors are a part of what enables that speed. For comparison, the Intel Core i9 has just 33.5 billion transistors, and an Apple M2 Max chip offers just 67 billion transistors. But it’s more than sheer number that builds a compute speed demon. It’s also co-location: putting everything together on one chip, along with 44 gigabytes of the fastest RAM (memory) available.
“AI compute likes a lot of memory,” Penso says. ""Nvidia needs to go off-chip but withCerebras, you don’t need to go off-chip.""
Independent agency Artificial Analysis corroborates the speed claims, saying it tested the chip on Llama 4 and achieved 2,522 tokens per second, compared to Nvidia Blackwell’s 1,038 tokens per second.
“We’ve tested dozens of vendors, and Cerebras is the only inference solution that outperforms Blackwell for Meta’s flagship model,” says Artificial Analysis CEO Micah Hill-Smith.
The WSE chip is an interesting evolution in computer chip design.
While we’ve been making integrated circuits since the 1950s and microprocessors since the 1960s, the CPU was the dominant force in computing for decades. Relatively recently, the GPU or graphical processing unit shifted from being an aide for graphics and games to being the critical processing component of choice for AI development. The WSE is not an x86 or ARM architecture but something entirely new that accelerates GPUs, Cerebras chief marketing officers Julie Shin told me.
“This is not an incremental technology,” she added. “This is another leapfrog moment for chips.”"
77,https://www.forbes.com/sites/dereksaul/2025/05/28/nvidia-earnings-ai-giant-delivers-another-quarterly-sales-record-despite-china-speed-bump/,Nvidia Earnings: AI Giant Delivers Another Quarterly Sales Record Despite China Speed Bump,"May 28, 2025, 04:27pm EDT",Derek Saul,"Artificial intelligence titan Nvidia took center stage Wednesday afternoon when the company shared financial results from its quarter ending last month, and though it was another stretch of record-setting sales amid the AI boom, Nvidia reported unusually weak bottom line growth as Washington regulations on the company’s China dealings took hold.
Shares of Nvidia jumped 3% to $138 following the better-than-anticipated quarterly results.
Nvidia reported $44.1 billion in revenue for the three-month period ending April 27, besting Wall Street forecasts of $43.3 billion in revenue, according to consensus analyst estimates compiled by FactSet.
That equates to year-over-year top line growth of a robust 69%, smashing the $39.3 billion sales record set the prior quarter.
Nvidia beat bottom line growth expectations as well, though by a more modest margin, as it brought in $0.83 adjusted diluted earnings per share and $19.9 billion net income, compared to respective projections of $0.73 diluted earnings and $18.3 billion net income, equating to an annual EPS increase of 33%.
That’s a far cry from the more than 70% year-over-year earnings expansion Nvidia reported in each of the prior seven quarters.
So why the ebbing growth as the global generative AI push remains in full force? It largely traces to the $4.5 billion charge the company took this quarter after the Trump administration curbed the company’s ability to export its H20 AI chips to China, which has a $50 billion market for Nvidia’s chips, according to Rosenblatt analysts.
$8 billion. That’s how much revenue Nvidia expects to lose during the summer quarter due to new H20 export controls, the company said Wednesday.
The 61% gross margin Nvidia raked in last quarter was its weakest gross margin since 2022. Bank of America expects margins to recover to nearly 75% by year’s end.
Nvidia is the unquestioned market leader in designing the semiconductor technology needed to train and power generative AI applications ranging from OpenAI’s ChatGPT to Tesla full self-driving. The pervasiveness of Nvidia’s graphics processing units, or GPUs, in the AI gold rush – as Nvidia enjoys a whopping 75% share in this burgeoning AI accelerator market – led Nvidia to become one of the world’s most valuable companies, Nvidia’s $3.3 trillion market capitalization trails only Microsoft’s $3.4 trillion market cap. Since the November 2022 release of ChatGPT sparked a spike in interest in AI’s ability to drive corporate growth, Nvidia stock has blossomed. Shares of the Silicon Valley firm are up 700% during that timeframe, outperforming the S&P benchmark by a magnitude of 14.
Now worth more than Berkshire Hathaway, Walmart, JPMorgan Chase and Coca-Cola, combined, Nvidia has quite the humble origins, tracing its founding to a 1993 meeting between Huang and his two cofounders at a Denny’s diner.
Huang, the company’s only-ever CEO, has come a long way since his first job as a Denny’s busboy, as he is the 11th-richest person in the world thanks to his $119 billion fortune, according to Forbes’ latest calculations.
Shares of Nvidia pulled back 0.5% to $134.81 during Wednesday’s session. Major indexes were down slightly ahead of Nvidia earnings, an event which often catalyzes broader moves. There needs to be a “positive earnings surprise” from Nvidia for the market to “squeeze higher,” according to Fabio Bassi, JPMorgan’s head of global cross asset strategy. Nvidia’s earnings come as the broader equity market stages a massive comeback from its tariff-driven slide this spring, as the S&P 500 is up 6% in May, heading toward its best monthly gain since 2023.
"
78,https://www.forbes.com/sites/geruiwang/2025/05/27/nvidia-partners-with-swedish-consortium-ai-factory-redefines-economy/,Nvidia Partners With Swedish Consortium As AI Factories Redefine Economy,"May 27, 2025, 05:05pm EDT",Gerui Wang,"AI is touching every corner of enterprise activity, from autonomous vehicles and robotics industries to drug discovery and finance. This makes the compute infrastructure essential for any economy, and company competitiveness is increasingly shaped by AI technologies. Countries are also racing to make plans to build AI infrastructure to sustain business growth and innovation.
In Sweden, a consortium of Ericsson, AstraZeneca, Saab and SEB just announced collaborations with Nvidia to build an AI factory powered by the latest DGX GB300 systems. In Europe, the EU’s AI Factory initiative, backed by the EuroHPC Joint Undertaking, is creating a network of at least 15 AI-optimized supercomputing hubs and several AI gigafactories to support startups, SMEs and research institutions across sectors like healthcare, climate and manufacturing throughout Europe. In the U.S., Nvidia has announced it will manufacture AI supercomputers domestically for the first time, tapping partners like TSMC, Wistron, Amkor and SPIL to produce up to $500 billion in AI infrastructure over the next four years. Likewise, in Asia, private and public sectors alike are investing in building AI factories with an emphasis on business innovation and training AI talents.
The era of AI factory has arrived, bringing with it economic opportunities and enhanced productivity but also safety risks and energy challenges, and the dwindling of tangible human connections.
Enterprises across sectors are integrating AI into their research and development. At Pfizer, AI and supercomputing were instrumental in its development of Paxlovid, the company’s COVID-19 oral treatment. AI accelerated molecule discovery, reduced clinical data analysis time by 50% and cut production cycle time by 67%.
In collaboration with Nvidia and Classiq, BMW is exploring how quantum-AI synergies can enhance vehicle engineering and manufacturing logistics. Similarly, Dell’s enterprise clients are leveraging agentic AI to deploy digital workers across cloud and edge environments.
Goldman Sachs has rolled out its GS AI Assistant to over 10,000 employees, including bankers, traders and asset managers. Designed to evolve from simple task automation (proofreading, code translation) to agentic behavior, the system is trained to act like a seasoned Goldman employee, consulting the right data, using appropriate algorithms and reflecting the firm’s values and decision-making protocols.
As computing becomes embedded into daily workflows — drafting documents, generating code, performing statistical modeling, even designing new products — enterprise demand for compute is exploding. McKinsey has reported that over the next three years, 92% of companies plan to increase their AI investments. To keep up, organizations need not just more GPUs, but entire supply chains that deliver compute at scale, speed and security.
This is where AI factories come in. Unlike traditional data centers or chip fabs, AI factories fuse three critical layers: chip production and packaging, supercomputer system assembly and data center-scale deployment. Dell Technologies, for example, in collaboration with Nvidia, has launched Dell AI Factory 2.0, an end-to-end enterprise platform tailored for agentic AI, enabling federated data access, semantic search and feedback loops optimized for large-scale digital workforces.
By co-locating these functions, AI factories eliminate latency between design, build and deployment. They give companies more direct control over how their models are trained, where their data resides and how resilient their infrastructure is to global shocks.
Nvidia now offers a fully integrated AI factory platform designed to accelerate this buildout. It includes not only hardware components, but a full software and orchestration stack. The TensorRT ecosystem supports high-performance AI inference, while Nvidia Dynamo and NIM microservices streamline deployment and model optimization. Data Flywheel enables continuous customization based on real-world feedback, ensuring that AI systems improve over time.
Nvidia’s full-stack platform allows AI factories to operate like industrial assembly lines for intelligence. Raw materials include foundation models, customer data and toolkits. These inputs are shaped by high-performance training, fine-tuning and inference, while continuous feedback loops allow systems to adapt and improve in real time.
The Nvidia Blackwell Ultra-based GB300 NVL72 solution provides 50x higher output for AI reasoning than previous generations. Nvidia’s DGX SuperPOD integrates this compute power into a turnkey system, while DGX Cloud delivers the same AI infrastructure virtually through major cloud providers. Both enable enterprises to scale rapidly without compromising performance.
To coordinate all these elements, Nvidia’s Omniverse Blueprint provides a method to simulate entire factories as digital twins, optimizing design, layout and workflows in a virtual environment.
Unlike traditional manufacturing plants, AI factories don’t promise mass employment on the factory floor. Instead, they signal a shift toward high-skill, cross-disciplinary roles that include:
1. Digital twin architects who model industrial workflows in simulation;
2. AI-operations engineers who monitor and optimize factory performance;
3. Specialists who program and maintain automated systems.
These jobs are fewer in number but higher in complexity and wage potential. In many ways, the AI factory reflects a broader shift from routine cognitive tasks to advanced cognitive skills. Such a shift demands the integration of expertise across computer science, statistics, manufacturing and process management to solve problems holistically.
At GTC 2025 earlier this year, Jensen Huang unveiled “Blue,” a robot capable of learning tasks by modeling physical environments, a breakthrough underpinned by the same infrastructure AI factories now support. With mass production capabilities, AI factories will not only power digital models but fabricate the next generation of physical robots for logistics, manufacturing, healthcare and domestic use.
AI factories thus serve a dual role: they support the software revolution while simultaneously enabling a AI hardware transformation.
But these benefits don’t come without cost. AI factories are energy-intensive and, if unmanaged, carbon-heavy. With Oracle planning a $40 billion investment in Nvidia chips for its Abilene, Texas-based Stargate facility, expected to deliver 1.2 gigawatts of compute, sustainability is a critical concern.
To mitigate this, Nvidia and its partners are implementing cutting-edge technologies. Liquid immersion and direct-to-chip cooling systems are being deployed to dramatically reduce energy spent on thermal management. Nvidia’s Blackwell GPUs, used in the GB200 NVL72 systems, deliver up to 25× more energy-efficient inference compared to previous generations, improving compute output per watt.
Meanwhile, some AI factories are co-locating with renewable energy sources to reduce fossil fuel dependence. Sweden, for instance, is supporting AI factories through green energy partnerships leveraging its abundant hydroelectric power. Microsoft’s datacenters in Finland have similarly emphasized low-carbon electricity sourcing.
Sustainability is now a design requirement, not a footnote. The future of AI factories may hinge not only on how much compute they deliver, but on how responsibly they deliver it, balancing energy efficiency with AI capabilities. As AI moves from the cloud into an ecosystem of factories, the organizations and countries that take a lead in the long-term development of this terrain will shape the contours of global economy."
79,https://www.forbes.com/sites/davealtavilla/2025/05/20/nvidias-strategic-nvlink-fusion-play-could-be-a-major-market-mover/,Nvidia’s Strategic NVLink Fusion Play Could Be A Major Market Mover,"May 20, 2025, 03:35pm EDT",Dave Altavilla,"At Computex 25 this week in Taiwan, Nvidia made a strategic move that underscores its broader long-term effort in shaping AI infrastructure: the unveiling of NVLink Fusion. More than just a new interconnect technology, NVLink Fusion marks a strategic shift in Nvidia’s approach to the modern AI data center, positioning the company not only as a supplier of high-performance accelerator and infrastructure solutions, but as a foundational platform for a modular, highly optimized AI-driven computing future.
NVLink Fusion represents a new branch of how Nvidia approaches the cloud data center and hyperscaler markets, empowering partners like Qualcomm, Marvell, Fujitsu and MediaTek to design purpose-built silicon solutions that can interface directly to Nvidia’s Blackwell GPU architecture via the company’s now open and extensible NVLink architecture.
As Jensen Huang, CEO of Nvidia, put it during the reveal: “a tectonic shift is underway.” And indeed this is not an overstatement. For decades, data centers have been standardized around general-purpose CPUs and predictable compute models. However, the rise of trillion-parameter AI models and real-time agentic inference has pushed traditional architectures to the brink. That said, high performance GPUs like Nvidia’s Blackwell architecture are only part of the solution. High speed interconnect that allows GPUs, CPUs and data plane processors to communicate in unison is essential, and that’s where Nvidia’s brilliant move to acquire high-speed networking and interconnect innovator Mellanox years ago, has paid off in spades.
A derivative of Nvidia’s proprietary high bandwidth NVLink interconnect technology, NVLink Fusion addresses this head-on by allowing third-party silicon, like custom CPUs and accelerators, to be tightly coupled with Nvidia GPUs at the board and rack level. NVLink excels in high-speed, intra-node GPU comms, while Nvidia’s Mellanox InfiniBand and Ethernet technologies enable efficient inter-node communication across cabinets, clusters and data centers.
Notably, Nvidia’s NVLink platform enables up to 1.8 TB/s per GPU in bandwidth—over 14x the bandwidth of PCIe Gen5—and that’s what makes the technology so interesting in partnership with other silicon solution providers.
Qualcomm is perhaps the most significant surprise in the mix, in my view. Best known for its dominance in mobile and recent entrance into client computing, Qualcomm will now extend its reach into the data center by developing custom CPUs designed to link directly into Nvidia’s AI stack via NVLink Fusion. According to Qualcomm CEO Cristiano Amon, this alignment enables “high-performance, energy-efficient computing” tailored for data center AI deployments. With a track record of power-optimized silicon and a growing stake in AI inference and revenue diversification, Qualcomm’s entry signals how NVLink Fusion is opening the door for new players to compete in data center infrastructure.
MediaTek, already in close collaboration with Nvidia in automotive platforms, is expanding its reach into hyperscale AI infrastructure. With advanced ASIC design services and proven SoC architecture and interconnect experience, MediaTek will strive bring new, efficient compute options to AI factories with the help of NVLink.
Synopsys and Cadence, meanwhile, are moving to enable NVLink Fusion support deep in the chip design process. Their interface IP, chiplet and silicon design tools make it easier for partners to build compliant, production-ready chips that will integrate easily into an Nvidia NVLink-enabled system.
Together, these players, as well as other big name chip companies like Marvell and Fujitsu which also announced support, represent a new wave of modular, bespoke AI infrastructure—configurable, scalable, and optimized for task-specific performance.
From a business perspective, NVLink Fusion does two big things for Nvidia. It helps solidify its AI ecosystem dominance while simultaneously embracing a more open model. This is a calculated play. Rather than walling off its chip and platform architecture, Nvidia is inviting the industry in—with the caveat that the core still runs through its GPU platform and interconnect fabric.
This “open but owned” model has proven successful many times in the semiconductor industry over the years; maintain control of a standard, but let others innovate on top of it. With NVLink Fusion, Nvidia is doing just that in the data center, but with AI at the center of it all. The timing of this announcement is also strategic. As hyperscalers like AWS, Microsoft, and Google weigh their own in-house silicon strategies, Nvidia is offering a middle path: bring your chips, plug into our infrastructure, and still ride the wave of Nvidia GPU-driven performance.
On the ground, the potential market impact is significant, and it could accelerate adoption of Nvidia platforms across industries that have struggled with the cost and complexity of scaling.
NVLink Fusion may not be flashy on the surface; it’s just silicon plumbing after all. But underneath, it signals a fundamental rearchitecting of how AI infrastructure could be built. Over the years, Nvidia has evolved from a chip company into the backbone of AI-scale computing. And with NVLink Fusion, it’s inviting the rest of the industry to build alongside it.
If you’re watching where the next few years of AI infrastructure is headed, look closely at this Nvidia announcement. It’s less about speeds and feeds, and more about who might build what, and on whose terms.
Dave co-founded and is principal analyst at HotTech Vision And Analysis, a tech industry analyst firm specializing in consulting, test validation and go-to-market strategies for major chip and system OEMs. Like all analyst firms, HTVA provides paid services, research and consulting to many chip manufacturers and system OEMs, including companies mentioned in this article. However, this does not influence his objective coverage."
80,https://www.forbes.com/sites/janakirammsv/2025/04/25/nvidia-releases-nemo-microservices-to-streamline-ai-agent-development/,Nvidia Releases NeMo Microservices To Streamline AI Agent Development,"Apr 25, 2025, 01:56am EDT",Janakiram MSV,"Nvidia unveiled the general availability of its NeMo microservices, equipping enterprises with tools to build AI agents that integrate with business systems and continually improve through data interactions. These microservices are being launched at a time when organizations are seeking concrete AI implementation strategies that can provide measurable returns on significant technology investments.
Enterprise AI adoption faces a critical challenge: building systems that remain accurate and useful by continuously learning from business data. The NeMo microservices address this by creating what Nvidia describes as a “data flywheel,” allowing AI systems to maintain relevance through ongoing exposure to enterprise information and user interactions.
The newly available toolkit includes five key microservices:
These components work together to build AI agents that function as digital teammates, capable of performing tasks with minimal human supervision. Unlike standard chatbots, these agents can take autonomous actions and make decisions based on enterprise data. They connect to existing systems to access current information stored within organizational boundaries.
The distinction between NeMo and Nvidia’s Inference Microservices, branded as NIMs, lies in their complementary functions. According to Joey Conway, Nvidia’s senior director of generative AI software for enterprise, “NIMs is used for inference deployments – running the model, questions-in, responses-out. NeMo is focused on how to improve that model: Data preparation, training techniques, evaluation.” When NeMo finishes optimizing a model, it can be deployed through a NIM for production use.
Early implementations demonstrate practical business impacts. Telecommunications software provider Amdocs developed three specialized agents using NeMo microservices.  AT&T collaborated with Arize and Quantiphi to build an agent that processes nearly 10,000 documents updated weekly. Cisco’s Outshift unit partnered with Galileo to create a coding assistant that delivers faster responses  than comparable tools.
The microservices run as Docker containers orchestrated through Kubernetes, allowing deployment across various computing environments. They support multiple AI models including Meta’s Llama, Microsoft’s Phi family, Google’s Gemma and Mistral. Nvidia’s own Llama Nemotron Ultra, which focuses on reasoning capabilities, is also compatible with the system.
This release enters a competitive landscape where enterprises have numerous AI development options. Alternatives include Amazon’s Bedrock, Microsoft’s Azure AI Foundry, Google’s Vertex AI, Mistral AI, Cohere and Meta’s Llama stack. Nvidia differentiates its offering through integration with its hardware ecosystem and enterprise-grade support through the AI Enterprise software platform.
Nvidia Nemo and Enterprise AI Adoption
For technical teams, the microservices provide infrastructure that reduces implementation complexity. The containerized approach allows deployment on premises or in cloud environments with enterprise security and stability features. This flexibility addresses data sovereignty and regulatory compliance concerns that often accompany AI implementations.
Organizations evaluating these tools should consider their existing GPU infrastructure investments, data governance requirements and integration needs with current systems. The need for AI agents that maintain accuracy with changing business data will drive adoption of platforms that support continuous learning cycles.
The microservices approach reflects a broader industry shift toward modular AI systems that can be customized for specific business domains without rebuilding fundamental components. For technology decision makers, the release represents another step in the maturation of enterprise AI tooling that narrows the gap between research capabilities and practical business implementations.
As enterprises move beyond experimentation toward production AI systems, tools that simplify the creation of continually improving models become increasingly valuable. The data flywheel concept represents an architecture pattern where AI systems remain aligned with business needs through ongoing exposure to organizational information."
81,https://www.forbes.com/sites/moorinsights/2025/04/21/nvidias-ai-omniverse-expands-at-gtc-2025/,Nvidia’s AI Omniverse Expands At GTC 2025,"Apr 21, 2025, 05:21pm EDT",Anshel Sag,"I have had the opportunity to attend every GTC since Nvidia launched the annual conference in 2009. This year’s show built on GTC 2024’s absurd scale and hype with a full year of Nvidia Blackwell product releases and customer wins. CEO Jensen Huang once again delivered his keynote address to a packed SAP Center in San Jose, California and continued many of the themes we’ve seen from the company in the last year, holding steady rather than introducing many new concepts. If anything, this event was a clear demonstration of Nvidia’s confidence in its own vision and how it plans to bring the world along with it.
(Note: Nvidia is an advisory client of my firm, Moor Insights & Strategy.)
Part of Nvidia’s role as a leading provider of chips for AI infrastructure is to become more predictable for its own customers. This means not only delivering products that perform up to expectations, but also delivering them on a dependable timeframe with good reliability. That’s why it came as no surprise when, just like clockwork, Nvidia announced the Blackwell Ultra chip, also known as the B300. Nvidia also announced Dynamo, the company’s replacement for its Triton open-source libraries that helps accelerate inference and improve scalability while lowering deployment costs.
Looking beyond Blackwell, Nvidia also gave more details about the Rubin GPU and Vera CPU that are expected in 2026 and 2027. This was part of a broader disclosure of its compute, NVLink, networking and system roadmap. The base-model Rubin is slated for early 2026 release, complete with HBM4 high-bandwidth memory, while the Rubin Ultra is set to follow later in 2026. Nvidia’s CPUs will also get an upgrade to the Vera architecture, which will enable the Vera Rubin series of servers based on those CPUs and GPUs. Nvidia also plans to upgrade its NVLink to sixth- and seventh-generation NVSwitches with 3600 GB/s of bandwidth paired with Spectrum6 and CX9 networking chips. The Spectrum6 is also expected to be Nvidia’s first silicon photonics product.
Moving into 2028, Nvidia plans to introduce the Feynman GPU, which will pair again with Vera CPUs but leverage “next-gen HBM” — without further specification since there hasn’t been much public disclosure of HBM standards beyond HBM4. Feynman will also pair with Nvidia’s eighth-generation NVSwitch and Spectrum7 (including its second generation of silicon photonics) for scale-out networking.
Nvidia has been teasing DGX Spark, formerly known as Project Digits, since the CES show at the start of 2025, but it has finally gotten to the point of launching this product, which it calls a “desktop supercomputer.” It is not only Nvidia’s smallest-scale Blackwell product, but it is also its most accessible platform for developing AI. The GB10 Superchip that powers it pairs a Blackwell GPU with a 20-core Arm-based CPU from MediaTek to create a very small-scale system that basically fits in the palm of your hand but is scalable and powerful.
I believe that this is Nvidia’s way of becoming more accessible for inference and enabling developers to tap into the Nvidia ecosystem at any scale while still offering scalability, given that multiple DGX Spark machines can be connected to improve performance. I also believe that it can and will be deployed as a sort of AI appliance for running Nvidia AI workloads at the edge when a user needs compute, but not something as big as a GB300. Nvidia was taking pre-orders during GTC, but it was also clear that ASUS, Dell and HP will be coming to market with their own GB10-based solutions, which I believe will enable much better reach for Nvidia.
Speaking of the GB300, both Dell and HP also showed off workstations based on the DGX Workstation that Nvidia announced at GTC. DGX Workstations are basically GB300 nodes, with a single Grace CPU and dual Blackwell GPUs running on a single liquid-cooled desktop. This puts maximum power in the hands of the data scientist or AI developer without requiring constant access to the cloud or an investment in a complete rack solution. I happened to witness Huang and Michael Dell sign the Dell GB300 DGX Workstation prototype machine on the GTC 2025 expo floor.
One of Nvidia’s other big announcements from the show was the last of the Blackwell line, the RTX Pro series of GPUs. The RTX Pro series helps to clarify Nvidia’s product categories, with the Pro series specifically representing professional graphics and aimed at empowering the laptop, desktop and server categories. On top of its dominant position in AI, Nvidia remains the world leader in professional graphics and is the provider of the graphics that power The Sphere in Las Vegas, among many other venues and specialized graphics implementations. The RTX Pro series is based on the GB200 series of chips and comes in five performance levels, with the top-end RTX Pro 6000 featuring a 600-watt, 300 Max-Q configuration. I got to see Dell’s Pro workstation running the RTX Pro 6000 with a 600-watt card on the show floor, which indicates to me that Nvidia is keeping Dell very close on AI — and for more than just datacenter use cases. HP and Lenovo are also partners with Nvidia for the RTX Pro series, and HP announced Nvidia-based systems the same week as GTC at its own Amplify conference in Nashville.
The RTX Pro 6000 also comes with the full 20,064 CUDA cores that the Blackwell architecture supports (the RTX 5090 ships with a mere 20,000 cores) and comes with a whopping 96GB of GDDR7 with ECC error correction. It also offers 4,000 TOPS of AI performance (FP4) and 380 TFLOPS of ray tracing performance. Additionally, because this is a PCIe Gen5 GPU, Nvidia has enabled VR-SLI for high-end XR solutions, enabling dual-GPU configurations that drive one GPU per eye for maximum photorealistic experiences. The RTX Pro series will extend into laptops but will start with the RTX Pro 5000 and work all the way down to the RTX Pro 500. The RTX Pro 5000 for laptops will cap out at 24GB of VRAM, while the desktop and server versions of the RTX Pro 6000 will reach up to 96GB. One interesting detail is that these laptops will be available this summer from Dell, HP and Lenovo — as one would expect — but also from Razer, which sells gaming laptops and may be looking to move into the workstation market.
I believe that Nvidia made a mistake by announcing these RTX Pro GPUs at GTC 2025, because there is already so much going on at the show and it felt like this announcement didn’t get the attention it deserved. In fact, since these GPUs won’t be available until the summer anyway, I think it would have made much more sense to wait until August to make the announcement at the Siggraph 2025 conference. This is where Nvidia has traditionally announced its professional graphics products. As it stands, there are now so many details that the industry wants to know about, but I guess Siggraph will be the place to ask those questions.
During the GTC 2025 Keynote, Huang rapid-fired his appreciation to many of the company’s partners in making Blackwell a success. One key partner cited was Cadence, of which Huang said last year: “Blackwell would not have been possible without Palladium.” Palladium is Cadence’s high-powered emulation platform. (For more, see this analysis of Cadence that I wrote in mid-2024.)
Nvidia is not only a customer of Cadence, but also a partner in accelerating Cadence’s own design products, including the Voltus, Celsius, Clarity, Fidelity and Reality tools. Cadence has said that its Fidelity CFD (computational fluid dynamics) tool can perform multi-billion cell simulations in under 24 hours using B200 GPUs, versus a Top 500 Supercomputer cluster with hundreds of thousands of CPU cores requiring several days. Fidelity LES was also optimized for Blackwell — specifically for digital twins — and the comparative performance numbers running Nvidia are similarly stark. Using the highly complex mathematics of large eddy simulations, Fidelity LES can perform a sophisticated automotive simulation in 11 minutes using eight B200s, as compared to 16 hours on 1,000 CPUs, or a turbofan simulation in 50 minutes on eight B200s, versus 69 hours on 1,000 CPUs. Blackwell also accelerated 3-D-IC multiphysics design, seeing improvements on a single Blackwell ranging from 7.7x to 8.2x compared to 32 CPUs.
Nvidia and Cadence are also expanding their partnerships on digital twinning of datacenters with the Cadence Reality platform. Part of that means integrating Cadence Allegro and Cadence Reality into Nvidia Omniverse; it also includes Cadence embracing OpenUSD and joining the AOUSD promoter group. This will bring SoCs, PCBs, blades, racks, infrastructure and environments all into OpenUSD, which should make it easier than ever to build a digital twin of a datacenter for anyone. Nvidia is also releasing an Omniverse Blueprint for datacenter digital twins to make it easier for anyone to use when building their own Blackwell-based datacenter.
If GTC 2025 reinforced any message, it’s that Nvidia is far from done. For years now, the company has been building many partnerships and growing its influence in the enterprise with its many AI tools — and there’s no end in sight. Nvidia understands its role within the industry, especially now that we are seeing the company become more prominent in 5G networks with partners like Ericsson and carriers like T-Mobile and Verizon. While I do believe that AI RAN will become more prominent, I wouldn’t write Nvidia off, especially when leaders like Cisco are now joining forces with Nvidia for 6G.
Nvidia’s GPU products are industry-leading across the board, and GTC is clearly a key venue for the company to show off its importance — but also to connect some of its closest partners to potential new customers. I was really impressed with what I saw from the RTX Pro series, and even though I think the timing was off, these are some incredibly powerful GPUs that are driving a lot of excitement within the industry, especially for XR applications. I believe that AMD will have to do a lot more work to compete with Nvidia’s latest products, and that GTC was a tour de force even if there weren’t many surprises.
Moor Insights & Strategy provides or has provided paid services to technology companies, like all tech industry research and analyst firms. These services include research, analysis, advising, consulting, benchmarking, acquisition matchmaking and video and speaking sponsorships. Of the companies mentioned in this article, Moor Insights & Strategy currently has (or has had) a paid business relationship with Arm, Cadence, Cisco, Dell, Ericsson, HP, Lenovo, MediaTek, Nvidia, T-Mobile and Verizon."
82,https://www.forbes.com/sites/dereksaul/2025/04/16/heres-why-theres-a-silver-lining-for-nvidia-after-trumps-china-ai-chip-curb/,Here's Why There's A 'Silver Lining' For Nvidia After Trump’s China AI Chip Curb,"Apr 16, 2025, 11:24am EDT",Derek Saul,"Artificial intelligence heavyweight Nvidia became the latest company entangled in President Donald Trump’s global trade war this week, though Wall Street analysts say it’s far from a disaster scenario for the chipmaker.
Nvidia reported late Tuesday it expects to take a $5.5 billion hit this quarter due to the U.S.’ new severe restrictions on Nvidia exporting its H20 AI chips to China, curbs which pose an up to 10% earnings headwind for Nvidia this year, according to Bank of America analyst Vivek Arya.
The H20 restrictions are an “unwelcome but somewhat expected, manageable risk” for Nvidia, explained Arya, leading a chorus of analysts largely interpreting the update as far from a worst-case scenario despite the brutal headline hit.
It’s “not enormous in the grand scheme of things” as China accounted for the lowest proportion of Nvidia revenue in more than a decade last year at 13%, wrote Bernstein analyst Stacy Rasgon, while Rosenblatt analysts Kevin Cassidy and Kevin Garrigan predicted in a Wednesday note to clients.
Nvidia can “make up most of this revenue” hit from China with sales of its latest graphics processing units (GPUs) outside of the Asian power.There’s a key “silver lining” to what is “effectively a ban” on Nvidia exporting its coveted AI technology to China, according to UBS analyst Timothy Arcuri: This may be a “concession” from the Silicon Valley firm to get the White House to “to effectively kill” the Biden administration’s most severe AI regulatory frameworks.
Shares of Nvidia declined 7% Wednesday, wiping out nearly $200 billion in market capitalization. The losses for Nvidia, the most valuable company in the world other than Apple or Microsoft, dragged down the broader stock market, as the S&P 500 and Nasdaq dipped 1.2% and 1.9%, respectively. Hard hit were fellow semiconductor chip makers, as shares of Intel and TSMC both declined 3% and shares of Advanced Micro Devices tanked 6% after the company made a similar disclosure on losses tied to Chinese chip export curbs.
Rasgon slammed the White House ban on Nvidia’s chips as making “little sense” as it “simply hands the Chinese AI market over to Huawei.”
This is the latest evidence of a choppy operating environment for American technology companies. Deutsche Bank analyst Ross Seymore remarked “the ongoing volatility and resulting uncertainty surrounding the trade war between the US & China” has caused investors to be “whipsawed between hopes for exemptions/reconciliation and fears of further escalation.” The H20 chip curb comes less than two weeks after Nvidia CEO Jensen Huang reportedly dined with Trump at the president’s Mar-a-Lago resort White House. Trump has spoken publicly about the influence of his discussions with business leaders on policy, saying Monday he “helped” Apple CEO Tim Cook by exempting smartphones from much of the damage of the Chinese tariffs. Nvidia dominates the market in designing the semiconductor technology necessary to power generative AI applications like OpenAI’s ChatGPT and Tesla full self-driving.
25%. That’s how much Nvidia shares are down since Election Day, far worse than the S&P’s 8% decline, not including dividends. Nvidia was the S&P’s top performer of both 2023 and 2024 as its profits soared amid the generative AI revolution."
83,https://www.forbes.com/sites/benjaminlaker/2025/04/16/nvidia-building-ai-supercomputers-in-the-us-but-who-will-operate-them/,Nvidia Building AI Supercomputers In The U.S. But Who Will Operate Them?,"Apr 16, 2025, 09:00am EDT",Benjamin Laker,"Nvidia's reshoring of AI supercomputer production to the US faces a critical skills gap. While factories are being built, a lack of trained AI professionals threatens to hinder operations. This talent shortage necessitates a national strategy for workforce development, or the US risks undermining its own reshoring efforts.
Nvidia’s decision to bring AI supercomputer production to the U.S. was headline gold earlier this week: 1 million square feet of manufacturing space, factories in Arizona and Texas, partnerships with TSMC, Foxconn and Wistron—all geared toward reshoring the future of high-performance computing. It felt like a triumph, both economic and symbolic, as U.S. tech reasserts control over the infrastructure of the AI era.
But while the buildings will rise and the chips will flow, a fundamental question lingers in the shadow of this industrial resurgence: who’s going to run it all?
That’s not a throwaway line. According to Autodesk’s 2025 State of Design & Make report, based on surveys and interviews with 5,594 industry leaders, futurists, and experts, the U.S. is facing a sharp and accelerating shortage of the very skills these new facilities will require. AI talent isn’t just scarce—it’s becoming a serious bottleneck. Nearly half of global industry leaders now rank AI capabilities as a top hiring priority, and 58% say a lack of skilled talent is already limiting their company’s growth.
So while Nvidia’s factories will be American-made, the expertise needed to optimize and operate them may still be imported—or worse, left unfulfilled. The risk is clear: a domestic supply chain is only as resilient as the workforce behind it.
If 2024 was the year of AI hyperbole, 2025 is shaping up as the year of introspection. The Autodesk report shows a cooling of expectations around artificial intelligence, not because the technology failed, but because the workforce wasn’t prepared for its success.
Confidence has slipped. Only 65% of professionals say they trust AI in their sector, down from 76% last year. Even fewer—just 69%—feel confident in their own company’s ability to make sound decisions around AI strategy. Meanwhile, 48% fear that AI could destabilize their industries entirely—a seven-point jump from 2024.
This erosion of trust is not irrational. It stems from firsthand experience: early adopters who rushed to deploy AI now find themselves grappling with its complexity, its unintended consequences and the reality that most workers don’t feel ready to use it. As tools become more advanced, the skills gap widens, and with it comes hesitation.
The result? A two-speed economy. Tech-forward companies are reaping early benefits—higher productivity, smarter operations, stronger margins—while slower adopters struggle to convert ambition into outcomes. Without the right people, even the best technology underperforms.
The talent gap in high-tech manufacturing and AI implementation isn’t a new problem—but it’s a worsening one. For years, companies have leaned on global labor pools to fill gaps in technical roles, outsourcing everything from chip design to software development. But reshoring means those capabilities need to exist locally—and increasingly, they don’t.
Autodesk’s data paints a troubling picture: 61% of leaders say it’s harder than ever to find new hires with the right technical skills. Nearly half say their internal training systems are inadequate for keeping up with technological change. Even among companies that identify as “tech-advanced,” 77% are investing in digital upskilling—but whether that training is sufficient or scalable remains an open question.
What’s emerging is a structural problem, not just a tactical one. Companies need employees who can work alongside machines, interpret AI-generated insights, solve novel problems and collaborate across disciplines. These are hybrid skills—part technical, part human—that our current education system isn’t producing at scale.
The challenge isn’t only in training engineers or coders. It’s in building a workforce culture that embraces constant reinvention. Workers at every level need to think critically, learn continuously and adapt quickly. That mindset can’t be assumed. It must be taught.
Reshoring isn’t just about factories—it’s about ecosystems. And ecosystems need education. Yet while federal investments are flowing into physical infrastructure, far less attention is being paid to the talent infrastructure required to sustain it.
Autodesk’s report offers a warning shot. The companies investing most heavily in AI and automation—the same ones poised to benefit most from reshoring—are also the most vocal about their workforce challenges. Without a national strategy that integrates education reform, industry-academic partnerships and accessible, continuous upskilling, the U.S. risks building capacity without competence.
This is especially true for underserved communities. If reshoring is to be a genuine national opportunity, it must include deliberate strategies to democratize access to the jobs being created. That means rethinking vocational education, making room for non-traditional learners and embedding real-world AI projects into curricula at every level—from high schools to technical colleges to advanced research institutions.
The private sector can’t fix this alone. Nor should it have to. But right now, it’s acting faster than the education system can follow. And unless there’s alignment between workforce development and industrial ambition, America risks recreating the same offshoring vulnerabilities it’s trying to solve—only this time, at home.
There’s no question that the return of advanced manufacturing is a milestone. Nvidia’s announcement is a landmark moment in the reshoring movement and a signal that the U.S. is taking its AI future seriously—not just as a matter of innovation, but as a matter of national strategy.
But in the rush to reclaim technological leadership, we cannot forget the single most important factor: people.
The Autodesk report closes with a simple but urgent recommendation: design learning environments that mirror the workplace, bridge the gap between education and industry and create pathways into the most in-demand roles of the future. These are not radical ideas—they’re the basics of what a 21st-century industrial policy should look like.
And time is not on our side. Nvidia and its partners expect full-scale production within 12 to 15 months. That’s just enough time to build a factory—probably not a workforce.
Whether America can build half a trillion dollars’ worth of AI infrastructure in the next four years is no longer in question. The equipment will be there. The buildings will be there.
The bigger question is who will be there to turn the lights on."
84,https://www.forbes.com/sites/antonyleather/2025/04/15/nvidia-announces-rtx-5060-graphics-cards-priced-from-299/,Nvidia Announces RTX 5060 Graphics Cards To Be Priced From $299,"Apr 15, 2025, 09:02am EDT",Antony Leather,"Nvidia has announced its highly anticipated GeForce RTX 5060-series graphics cards including pricing, availability and specifications.
Three separate models will be available, with the RTX 5060 Ti in both 16GB and 8GB flavors arriving April 16. The cheaper RTX 5060 non-Ti version will be available next month in May and will also have 8GB of video memory.
Nvidia’s pricing is rumored to sit at just $299 for the RTX 5060, which is the same launch price as the RTX 4060, although it remains to be seen if supply can match demand on launch and if pricing will rise higher than this given the far higher premiums we’ve seen for other RTX 50 series models such as the RTX 5080 and RTX 5090.
The RTX 5060 Ti 16GB and 8GB models are rumored to have launch prices of $429 and $379 respectively. For longevity given many games make use of more than 8GB of video memory already, the 16GB model is likely to be the most sought-after.
Unsurprisingly, the benchmarks Nvidia has released mostly include the use of frame generation, which has divided options with consumers and reviewers. Without it in Delta Force, the RTX 5060 Ti sees a frame rate rise from around 100fps with the RTX 4060 Ti to around 130fps. Elsewhere, the fact that the RTX 50-series cards support 4x frame generation, while older cards cap out at 2x isn’t particularly clear in the graph above (which uses the maximum frame generation mode for each card), and explains why the RTX 5060 Ti is further ahead.
We’ll have to wait for reviews to land for a more comprehensive view on how the RTX 5060 and 5060 Ti perform and these are expected to go live no later than April 16. These will include benchmarks against various other models of graphics cards as well as at different resolutions.
AMD currently doesn’t have any of its new Radeon RX 9000-series models that sit at this price range, with the RX 9070 launching at $549. The Radeon RX 9060-series is expected in the near future, though, with the rumor mill already churning away on reports of RX 9060 XT 8GB and 16GB models. For now, Nvidia’s new cards will have to compete with previous generation models from both AMD’s Radeon RX 7000-series and Nvidia’s own RTX 4000-series.
AMD’s Radeon RX 7600 can be had for around $300 while the 7600 XT will set you back upwards of $400. Currently Nvidia’s RTX 4060 8GB costs around $330, with the RTX 4060 Ti available for less than $600 while the 16GB model rarely available for much less than $700. The release of the RTX 5060-series is likely to see prices of these older cards fall, assuming there is enough stock to prevent those annoying premiums, which is perhaps unrealistic given what’s happened following every RTX 50-series launch so far.
I’ll be covering news and benchmarks on the day so follow me here on Forbes using the blue button below, Facebook or YouTube to get the latest news and reviews."
85,https://www.forbes.com/sites/chrisdobstaff/2025/04/15/forbes-daily-nvidia-chooses-texas-as-its-us-supercomputer-base/,Forbes Daily: Nvidia Chooses Texas As Its U.S. Supercomputer Base,"Apr 15, 2025, 07:59am EDT",Chris Dobstaff,"andForbes Daily,
Forbes Staff.
With a record 3,028 people around the globe on Forbes’ most recent World’s Billionaires List—with a collective worth of $16.1 trillion—it’s clear that the richest people in the world are wealthier and more influential than ever.
This week, Forbes will host a members-exclusive conversation, “Behind The Billions: Inside The Forbes Billionaires List,” diving into what we can learn from the super-rich and why tracking how the world’s wealthiest amass their fortunes is more important than ever.
Join executive editor Luisa Kroll, assistant managing editor Kerry A. Dolan and senior editor Chase Peterson-Withorn for a member-exclusive discussion Wednesday at 1 p.m. ET. Don’t forget to sign up here.
Nvidia is working with Taiwanese electronics manufacturers Foxconn and Wistron to build two supercomputer factories in Houston and Dallas, the company announced Monday. Nvidia expects the investment to result in “hundreds of thousands of jobs,” and said that “mass production at both plants is expected to ramp up in the next 12-15 months.”
U.S. automakers saw stock pops Monday after President Donald Trump hinted at short-term tariff relief, saying that car companies “need a little bit of time” to source parts domestically. Imported cars have had a 25% tariff since early April, and auto parts are set to face similar 25% tariffs no later than May 3. Shares of Ford, General Motors and Stellantis were up about 4%, 3.5% and 5.6%, respectively.
Boeing’s shares slid sharply in premarket trading early Tuesday, after Bloomberg reported that the Chinese government had ordered the country’s airlines to stop taking deliveries of new aircraft from the American plane maker. The move, which also orders Chinese airline companies to stop buying aircraft parts from Boeing, is reportedly part of China’s retaliation against President Donald Trump’s 145% tariff rate on nearly all Chinese-made goods.
Trump appeared to take a victory lap Monday, celebrating what he called “the largest gain in the stock market in history on every single category last week,” though last Wednesday’s gain was not the biggest jump ever on a percentage basis, a more commonly-cited measure of stock movements. All three major indexes remain down at least 4% since Election Day, and 7% since Inauguration Day.
After the massive crypto washout of 2022 and 2023, most industry-leading crypto lenders like BlockFi and Celsius imploded, but a new report suggests a resurrection is underway: Tether, Galaxy and Ledn have emerged as the dominant players in centralized finance lending, holding a combined $9.9 billion in outstanding loans at the end of 2024. That’s nearly 90% of the CeFi market, according to Galaxy Digital.
Shares of Apple jumped Monday as the world’s largest company received a bump from the White House’s tariff exemptions on smartphone imports. The tech giant’s stock price finished the day at about $202, roughly a 17% jump from its $172 close a week ago—its lowest level since last May. The tariff exemption also signaled good news for prospective iPhone buyers, as prices are less likely to see an immediate hike.
In an Oval Office meeting between President Donald Trump and El Salvadoran President Nayib Bukele on Monday, White House advisor Stephen Miller said that bringing a Maryland father who was mistakenly deported back to the U.S. would amount to “kidnapping,” because he is now in the custody of El Salvador. The Trump Administration admitted that Kilmar Abrego Garcia was deported due to a “clerical error,” but has refused to comply with a district court order to “facilitate” his return, which was partially upheld by the Supreme Court last week.
Amid a recent wave of private equity firms purchasing stakes in professional sports teams, New York-based Velocity Capital Management will go a different route and announce today that it has agreed to make a strategic investment in Unique Sports Group, a soccer talent agency headquartered in London that represents more than 350 soccer players. The agency will use the capital to fuel its growth strategy and open new lines of business, such as entering women’s sports.
Residential solar developers already tend to have complicated financial statements, but a recently revealed hidden spreadsheet from troubled firm Sunnova Energy suggests the company was manipulating its public disclosures, according to short sellers. As vultures circle Sunnova, allegations that it had been overstating its solar system statistics “to fabricate larger amounts of federal investment tax credits” aren’t likely to help.
The honorees on the 2025 Forbes 30 Under 30 Europe list are building virtual worlds, revolutionizing fertility, reinventing retail and dominating the entertainment game—all while navigating a global trade war, a live war in Ukraine and the whiplash that comes with the constant evolution of generative AI.
A number of listmakers are harnessing AI’s power to tackle important problems. Felicia von Reden, 29, is the cofounder of Ovom Care, a fertility clinic using AI to take the guesswork out of the IVF process. The company uses machine learning to identify the most viable eggs and sperm for a patient, and since opening its first clinic in 2024, the startup has helped 350 women on their IVF journeys—while raising some $8 million in funding at a $22 million valuation.
Others are betting artificial technology will transform how we advertise and shop. Flore Lestrade, 26, Tristan François, 26, and Christian Kotait, 25, are building Veeton, a tool that allows clothing sellers to show their items using hyperrealistic AI fashion models. Their tech lets clothing companies show off a wide variety of sizes and styles—and for a whole lot cheaper.
And then there’s Frederic Boesel and Jonas Muller, who are building text-to-image AI technology with their startup Black Forest Labs. The pair has raised $31 million in seed money from Andreessen Horowitz and is already partnering with corporations like German telecom titan Deutsche Telekom to help them generate all their marketing images with Forest Labs’ model, Flux. All in all, the 2025 Under 30 Europe companies have raised more than $800 million in funding.
WHY IT MATTERS To identify this standout group, Forbes editors worked with expert judges to review more than 10,000 candidates, evaluating them on impact, financials, and creativity. The result: 300 young leaders steering the future of Europe and harnessing a moment of global uncertainty to build the future’s next great companies.
MORE How We Make The Forbes 30 Under 30 List
Harvard University said Monday it will not accept an agreement with the Trump Administration, which asked the institution to make major changes to its governance, admissions and hiring processes:
$9 billion: The amount of Harvard’s government contracts that are under review by various agencies, including the Education Department
$2.2 billion: How much of Harvard’s federal funding that the Trump Administration froze on Monday, following the university’s refusal
“No government … should dictate what private universities can teach.” Harvard President Alan Garber, responding to the administration’s recent list of demands
If you’re reading this, we hope you’ve already filed your federal taxes for 2024, in which case you’re probably deserving of a Tax Day treat! Here’s a list of a few national retailers and chains with special promotions happening today. (If you haven’t filed yet, procrastinators are certainly also eligible for a Tax Day treat, but you might want to file for an extension.)

Competition between big box retailers is as fierce as ever, and the CEO of a major company said last week that he intends to open 15 new locations each year “for the foreseeable future.” Which retailer is it?
A. Sam’s Club
B. Costco
C. Target
D. IKEA
Check your answer.
Thanks for reading! This edition of Forbes Daily was edited by Sarah Whitmire and Caroline Howard."
86,https://www.forbes.com/sites/gennacontino/2025/04/14/nvidia-to-make-ai-supercomputers-entirely-in-us/,Nvidia To Make AI Supercomputers Entirely In U.S.,"Apr 14, 2025, 01:08pm EDT",Genna Contino,"Chipmaker Nvidia said it is building new factories that can produce its AI supercomputers entirely on U.S. soil as trade war tensions build with top chipmaking countries such as China and Taiwan.
Nvidia is working with Taiwanese electronics manufacturers Foxconn and Wistron to build two supercomputer factories in Houston and Dallas, the company announced Monday, and “Mass production at both plants is expected to ramp up in the next 12-15 months.”
The U.S. chipmaker said it is already producing its Blackwell chips, which provide capacity for generative AI and accelerated computing, at the Taiwan Semiconductor Manufacturing Company (TSMC) factory in Phoenix.
Nvidia expects the investment to result in “hundreds of thousands of jobs and drive trillions of dollars in economic security over the coming decades,” according to a news release.
The Trump administration is touting the news of building U.S. supercomputer factories as a win in its efforts to boost domestic manufacturing.
White House officials, who previously issued an exemption on semiconductors from tariffs, said over the weekend the exemption is temporary, with President Donald Trump posting to Truth Social that he is “taking a look at Semiconductors and the WHOLE ELECTRONICS SUPPLY CHAIN in the upcoming National Security Tariff Investigations.”
Shares of NVIDIA, one of the Magnificent 7 tech stocks, ticked down slightly to $110.83 during afternoon trading, even though major markets were making gains.
Hewlett Packard Enterprise describes AI supercomputing as “when organizations use ultrafast processors made up of hundreds of thousands of powerful machines to manage and interpret vast quantities of data using artificial intelligence (AI) models.”
Since Trump announced his “Liberation Day” tariffs on April 2, the president has issued a 90-day pause on levying them–except for the 145% rate for China–and future trade relations with big semiconductor manufacturers in Asia remain unclear. While the U.S. claims nearly half the global market share of semiconductors, according to the Semiconductor Industry Association, Taiwan, Japan, China and other Asian countries are also big chip exporters. The original executive order imposing the tariffs exempted semiconductors, among other goods. However, Commerce Secretary Howard Lutnick told ABC’s “This Week” on Sunday that the exemption is temporary.
The White House’s 8 Big Tariff Flip-Flops Since ‘Liberation Day’ (Forbes)
Apple Soars After Tariff Exemptions—And iPhones May Not Get More Expensive After All (Forbes)"
87,https://www.forbes.com/sites/petercohan/2025/04/10/nvidia-stock-could-fall-if-125-china-tariff-drives-price-increases/,Nvidia Stock Could Fall If 145% China Tariff Drives Price Increases,"Apr 10, 2025, 01:18pm EDT",Peter Cohan,"Updated, April 10, 2025: This post has been adjusted to reflect the change in U.S. tariffs on China to 145%.
Nvidia stock has fallen 30% from its January 2025 peak of $153, evaporating $1.03 trillion from the company’s market capitalization, according to Google Finance.
One thing contributing to the drop is the results of the 2024 presidential election which brought Donald Trump back to the Oval Office. Since April 2, his tariff policy has thrown markets into turmoil.
And despite Trump’s trumpeting of a 90-day pause on most tariffs, Nvidia could be in worse shape. That’s because tariffs are high on countries where Nvidia products are built. More specifically, Taiwan faced a 34% tariff  – reduced Wednesday to 10% – while China’s rose to 145% on April 10, according to the Wall Street Journal.
Is the drop in Nvidia’s stock a buying opportunity? I see no reason to buy now because although orders may have soared in the current quarter in anticipation of tariffs, I expect the company’s May 2025 earnings report to feature a disappointing growth forecast as a result of tariff uncertainty.
More specifically, the uncertainty is due to the difficulty of answering precisely the following questions:
Nvidia remains bullish. In March, CEO Jensen Huang said tariffs would not cause meaningful short-term harm to the company due to the company’s agile supply chain; demand for AI would drive demand, and TSMC was building a plant in Arizona, according to Barron’s.
Nvidia declined to comment. A spokesperson referred me to a March 2025 company statement from Huang, which said. “Tariffs will have a little impact for us short term. Long term, we’re going to have manufacturing onshore.”
Read on for why Nvidia’s costs could rise significantly, whether Nvidia will pass along those costs to customers, and how the uncertainty around future moves in the global tariff wars may crimp consumer spending and company’s investments in generative artificial intelligence.
Nvidia will report quarterly financial results next month featuring slower top-line growth and thinner margins based on the company’s forecasts when it last reported in February.
Nvidia expressed enthusiasm about its prospects. “We’ve successfully ramped up the massive-scale production of Blackwell AI supercomputers, achieving billions of dollars in sales in its first quarter,"" said Huang in a statement. ""AI is advancing at light speed as agentic AI and physical AI set the stage for the next wave of AI to revolutionize the largest industries.”
Analysts were underwhelmed by Nvidia’s results – which lacked “the positive shock value of some of last year’s reports,” Sungarden YARP Portfolio Investing Group Leader Rob Isbitts told SeekingAlpha. Nvidia’s stock is likely to trade “more on raw demand and supply, rather than on some sort of surprise element,” Isbitts added.
“Guidance was slightly underwhelming,” Edward Jones analyst Logan Purk said in a report featured by Bloomberg. ""We think it will be challenging for management to continue to significantly beat expectations for future growth,” Purk added.
Tariffs could increase significantly the prices of some Nvidia products. However, it is unclear whether Nvidia will pass all of its increased costs on to customers and whether the higher prices would lower demand.
A more significant question for Nvidia’s future is whether companies will view the high investment in generative AI as being worth the $1 trillion Goldman Sachs estimated. If the technology enables companies to do better work with fewer people, demand for generative AI may remain strong in the event of a recession.
Not all of Nvidia’s products are vulnerable to being harmed by Trump’s tariffs.
For example, fears of a Trump administration crackdown on Nvidia’s H20 chip – which the company has been exporting to China – may not be realized. The reason? “Nvidia promised the Trump administration new U.S. investments in AI data centers,” according to a report from NPR.
However, tariffs could raise consumer personal computer prices by as much as 50%, according to WCCFTech. These estimates were made April 3 when the tariff on Chinese goods was a mere 54% – a far cry from the 145% tariff imposed on April 10.
Tariffs could raise manufacturer’s suggested retail price  prices on the following Nvidia gaming machines by at least 50% – based on the latest triple-digit tariff:
It remains unclear whether Nvidia will be able to avoid tariffs on GPUs for now. The tariffs could be avoided if actual products are assembled in the U.S., noted Tom’s Hardware.
For now, Nvidia may be able to ship “actual GPUs and processors/chipsets to the U.S. without paying import duties along with half-finished logic boards – but paying an import tariff -- then assemble actual products in the U.S.,” reported Tom’s Hardware.
Generative AI has been in search of a killer app, as I wrote in the Boston Globe. There has been no obvious progress on this goal and the investment required for the technology is enormous – potentially totaling $1 trillion.
Prior to the tariff announcement, the largest hyperscalers – Amazon, Google, Meta Platforms, and Microsoft – announced plans to spend $270 billion on generative AI data centers in 2025, according to the Wall Street Journal.
However, that spending could take a hit – especially for companies like Meta and Google which are dependent on advertising for much of their revenue. If tariffs and the resulting uncertainty cause a recession – due to rising prices and fear of declining demand and higher unemployment – companies could decide to cut back on their generative AI spending.
The reason for cutting back would be a simple realization that the return on investment in generative AI is not yet compelling. Amazon’s cloud-computing arm historically has generated $4 of incremental revenue for every $1 of capital spending, TD Cowen John Blackledge tech analyst told the Journal. Generative AI’s ratio “is currently something like 20 cents for every dollar,” Blackledge added.
Microsoft, which plans to spend $80 billion on infrastructure in the current fiscal year, is slowing construction – including a $1 billion project in Ohio. Microsoft canceled leases in the U.S. and Europe -- partly due to an oversupply of data-center space relative to the company’s demand forecast, noted TD Cowen analysts. However, “the moves signal a more cautious stance in the longer term,” the Journal reported.
All that could be painful for Nvidia investors – unless surviving with fewer people during a long period of stagflation becomes the killer app for generative AI."
88,https://www.forbes.com/sites/dariashunina/2025/04/07/nvidia-backed-rescale-builds-the-ai-eras-digital-engineering-backbone/,NVIDIA-Backed Rescale Builds The AI Era’s Digital Engineering Backbone,"Apr 07, 2025, 10:00am EDT",Dasha Shunina,"Rescale, the digital engineering platform powering the future of innovation and scientific discovery, today announced a $115 million Series D with participation from Applied Ventures, Atika Capital, Foxconn, Hanwha, Hitachi Ventures, Prosperity7, NEC, NVIDIA, Translink Capital, the University of Michigan, and SineWave Ventures—bringing Rescale’s total funding to more than $260 million. Early investors include Sam Altman, Jeff Bezos, Paul Graham, and Peter Thiel. Rescale is on a mission to empower the world’s leading engineers and R&D teams with advanced computing, intelligent data, and applied AI.
""Accelerated computing is the engine of the AI industrial revolution,"" said Jensen Huang, NVIDIA founder and CEO. “With Rescale’s full-stack NVIDIA software and infrastructure, industries can push the boundaries of AI-driven modeling and simulation—advancing discovery, design, and engineering at an unprecedented pace.”
“Today’s most important breakthroughs depend on how fast engineering and R&D teams can move from idea to insight,” said Joris Poort, founder and CEO of Rescale. “Unfortunately, most teams are using computing infrastructure that wasn’t built for the scale, complexity, or speed to match the rapid pace of technological advancement. Rescale gives every engineer and scientist access to the compute, data, and AI tools they need to accelerate innovation.”
In response to greater demand for new innovative products, enterprises are increasing their investments in technologies that expedite time to market. High-performance computing is now a $50 billion market, simulation software sits at $20 billion, and product lifecycle data management is valued at $30 billion. Yet integrating this increasingly complex tech stack—software, hardware, and data—remains a challenge. Rescale offers a comprehensive platform for integrating compute, data, and AI capabilities into a seamless experience, enabling organizations to focus on innovation and discovery.
Rescale is trusted by industry leaders advancing the frontier in aerospace, automotive, life sciences, energy, manufacturing, semiconductors, and defense. The company’s hundreds of enterprise customers — which include Arm, General Motors, Samsung, SLB, and the U.S. Department of Defense — spend over $1B in infrastructure to power their virtual product development and scientific discovery environments. From accelerating drug development through molecular analysis to optimizing aircraft aerodynamics and enhancing automotive safety through crash simulations, Rescale supports a diverse array of mission-critical use cases. Rescale enables innovators and scientists to bring more advanced products to market with greater efficiency.
“Rescale’s high-performance compute software platform is helping industry-leading companies, including Applied Materials, accelerate innovation through the power of advanced computing and intelligent data,” said Dr. Om Nalamasu, CTO of Applied Materials and President of Applied Ventures. “We’ve experienced firsthand how Rescale’s software product suite can speed our AI physics simulation capabilities and catalyze adoption of our digital twin initiatives. We look forward to building on our successful collaboration with Rescale to empower our engineers and scientists.”
Looking back on 14 years of building Rescale, founder and CEO Joris Poort reflects on the key lessons that have shaped the company’s journey.
Lead with Mission“The most important thing at a high level is having an exciting mission you’re pursuing as a company,” says Joris Poort. “Having a compelling mission helps some of the brightest engineers and scientists do all this innovative stuff—and it keeps you going when things get hard.”
Invest in PeopleAnother important factor is building the best team possible. Joris believes that with a great team, like the one at Rescale, you can tackle any challenge. Investing in people matters.
Long-term commitmentUltimately, being long-term committed leads to much greater success. “Missionaries outperform mercenaries.""  Joris adds, “If you are focused on a mission, you know you will outlast somebody who is just doing it because they want to make a bunch of money or start a company for the sake of it. We have always stayed focused on that.
Recognize the MomentPoort also highlights the importance of timing. “If you’re in the market long enough, things will eventually swing in your favor—or against you”, he says. “Right now, there’s a really strong tailwind with AI, and we have an entirely new category with AI physics that we can develop. But it’s been quite a journey to get here.”
When it comes to raising capital, Poort says there was never a template. “It wasn’t necessarily easy, but we did it. The reason behind every investment decision was unique,” he explains. Convincing investors wasn’t about following a playbook.
Rescale’s new funding will expedite growth of its comprehensive digital engineering platform for advanced computing, intelligent data, and applied AI. To support the increasing demand for its platform, Rescale is accelerating the delivery of its roadmap including:
Rapid advancements in AI technologies are changing how every organization seeks to gain a competitive edge. Rescale’s digital engineering platform is reimagining modeling and simulation for the AI era. Rescale’s cloud-native technologies provide customers with agility and automation that fundamentally advance product development and scientific discovery."
89,https://www.forbes.com/sites/investor-hub/article/is-nvidia-nvda-stock-worth-buying-the-dip/,Is Nvidia Stock Worth Buying The Dip?,"Apr 07, 2025, 04:29pm EDT",Catherine Brock,"You can buy Nvidia stock on the cheap right now, but should you? That’s the question many investors have been asking since the once-unstoppable NVDA stock fell to its lowest P/E ratio since early 2019.
Get your answer now with this look at what happened to Nvidia, factors that could spark a rebound and risks ahead for the company.
Nvidia stock gained 168% in the first 10 months of 2024 as investors bought up shares to get their piece of the AI boom. The stock price rally peaked in early November and then turned south when investors got nervous about lingering high interest rates.
The negative trend for Nvidia deepened in 2025 as a series of headlines prompted many to question the chip designer’s growth outlook:
January through early April, Nvidia stock is down almost 30%.
The AI buildout has driven Nvidia's growth in recent years. Nvidia's AI data center sales are still increasing, but the pace has slowed and competition could become a factor soon. A full Nvidia rebound will likely require a different kind of breakthrough. Three opportunity areas to watch are robotics, PC gaming and autonomous driving.
Huang recently valued the opportunity in AI-powered robotics and automation at $50 trillion. The CEO says these technologies will ""transform manufacturing, logistics, healthcare and other industries."" Nvidia has two platforms, Isaac and Cosmos, that support robotics and other types of physical AI.
Nvidia’s GeForce Now service uses the cloud to upgrade the gaming experience on any PC. Gamers connect their PC gaming accounts at Steam, Epic Games Store or Ubisoft Connect to GeForce Now for added hardware support—essentially turning any device into a gaming PC.
Nvidia announced expanded GeForce Now device support and capabilities at the 2025 CES trade show.
Nvidia’s automotive line-up includes a platform for in-vehicle computing plus solutions for training AI driving models and simulating environments for testing. In 2025, the company announced automotive AI partnerships with Toyota, Aurora, Continental, GM, Gatik and Torc.
Nvidia expects its automotive business to deliver fiscal 2026 revenue of $5 billion, up from $1.7 billion in fiscal 2025.
Alongside these opportunities, Nvidia faces some challenges. Regulatory changes, cheaper AI solutions and in-house chip development top the list.
Changing Trump administration policies could pressure Nvidia’s stock price. Reciprocal tariffs, even with semiconductors excluded, could raise input costs and reduce margins.
While Huang seems unbothered by the trade war developments, he also said he could move manufacturing to the U.S. if needed. That is a long-term solution. Unfortunately, growth investors are not known for their patience. A transition to domestic manufacturing would impact the NVDA stock price.
More concerning than tariffs is the potential for export bans to China and import bans by China. Reports indicate Trump has considered tighter limits on chip sales to China. Meanwhile, Chinese regulators are discouraging sales of Nvidia's H20 chip—a product designed for export to China—because it doesn't meet efficiency standards, according to the Financial Times.
The rollout of DeepSeek caused angst for Nvidia shareholders. At issue was the $5.6 million total investment quoted by the AI app’s creator. The price tag seems tiny next to, say, Microsoft's $80 billion AI budget for fiscal 2025. The discrepancy prompted questions about Nvidia's growth outlook. If DeepSeek can do it, why can't big tech spend far less on Nvidia's AI hardware for the same results?
Some experts have said the $5.6 million number from DeepSeek is misleading. Still, there will be a time when cost-efficiency enters the AI fray. That could create space for competitors to chip away at Nvidia's AI dominance.
Big tech companies don’t want Nvidia to be the only AI chip game in town. It's too risky. They are mitigating that risk by developing in-house solutions. Alphabet (GOOG) recently introduced a collection of lightweight open-source AI models, Meta (META) is testing an AI chip and OpenAI has nearly finalized the design of its AI chip.
These solutions will reduce Nvidia's business with existing customers over time. Nvidia will have to improve on performance and price as a result.
Nvidia is an attractive, long-term buy right now despite regulatory and competitive challenges. The stock may not return to its pre-Trump growth rates, but it is an innovative, healthy company with a forward-thinking leader—and those qualities usually create shareholder value over time.
Like many growth stocks, NVDA is volatile. Its per-share price can fall as quickly as it rises. You can see that dynamic in play for NVDA stock since it went public in 1999. So this stock is best suited for hardy, patient investors and those who already own NVDA and want to reduce their average cost basis. If you don't feel quite ready for NVDA, see this list of best stocks for 2025 for more investing ideas.
Bottom Line
Nvidia is down nearly 30% this year as investors worry about tariffs, trade bans and lower-cost AI models. But the chip designer is not down for the count. Nvidia has continued AI growth, plus opportunities in robotics, PC gaming and autonomous driving that make it an attractive buy at the current valuation.
Is Nvidia stock overvalued?
After the recent downturn, Nvidia’s P/E ratio is about 32. That compares favorably to other semiconductor companies, including Advanced Micro Devices (AMD), Broadcom (AVGO) and Arm Holdings (ARM).
Should I buy Nvidia stock now or wait?
Nvidia is a well-run company that can navigate financial downturns and industry shifts. Relative to its history, the company's current valuation is appealing. If you don't mind volatility and believe in the AI promise as well as Nvidia CEO Jensen Huang, now is a good time to buy Nvidia stock.
What are the biggest risks for Nvidia?
The biggest risks for Nvidia are margin degradation from tariffs, the global impact of tariffs,  import and export bans and waning demand if others can replicate DeepSeek's low-power AI model."
90,https://www.forbes.com/sites/dereksaul/2025/04/16/heres-why-theres-a-silver-lining-for-nvidia-after-trumps-china-ai-chip-curb/,Here's Why There's A 'Silver Lining' For Nvidia After Trump’s China AI Chip Curb,"Apr 16, 2025, 11:24am EDT",Derek Saul,"Artificial intelligence heavyweight Nvidia became the latest company entangled in President Donald Trump’s global trade war this week, though Wall Street analysts say it’s far from a disaster scenario for the chipmaker.
Nvidia reported late Tuesday it expects to take a $5.5 billion hit this quarter due to the U.S.’ new severe restrictions on Nvidia exporting its H20 AI chips to China, curbs which pose an up to 10% earnings headwind for Nvidia this year, according to Bank of America analyst Vivek Arya.
The H20 restrictions are an “unwelcome but somewhat expected, manageable risk” for Nvidia, explained Arya, leading a chorus of analysts largely interpreting the update as far from a worst-case scenario despite the brutal headline hit.
It’s “not enormous in the grand scheme of things” as China accounted for the lowest proportion of Nvidia revenue in more than a decade last year at 13%, wrote Bernstein analyst Stacy Rasgon, while Rosenblatt analysts Kevin Cassidy and Kevin Garrigan predicted in a Wednesday note to clients.
Nvidia can “make up most of this revenue” hit from China with sales of its latest graphics processing units (GPUs) outside of the Asian power.There’s a key “silver lining” to what is “effectively a ban” on Nvidia exporting its coveted AI technology to China, according to UBS analyst Timothy Arcuri: This may be a “concession” from the Silicon Valley firm to get the White House to “to effectively kill” the Biden administration’s most severe AI regulatory frameworks.
Shares of Nvidia declined 7% Wednesday, wiping out nearly $200 billion in market capitalization. The losses for Nvidia, the most valuable company in the world other than Apple or Microsoft, dragged down the broader stock market, as the S&P 500 and Nasdaq dipped 1.2% and 1.9%, respectively. Hard hit were fellow semiconductor chip makers, as shares of Intel and TSMC both declined 3% and shares of Advanced Micro Devices tanked 6% after the company made a similar disclosure on losses tied to Chinese chip export curbs.
Rasgon slammed the White House ban on Nvidia’s chips as making “little sense” as it “simply hands the Chinese AI market over to Huawei.”
This is the latest evidence of a choppy operating environment for American technology companies. Deutsche Bank analyst Ross Seymore remarked “the ongoing volatility and resulting uncertainty surrounding the trade war between the US & China” has caused investors to be “whipsawed between hopes for exemptions/reconciliation and fears of further escalation.” The H20 chip curb comes less than two weeks after Nvidia CEO Jensen Huang reportedly dined with Trump at the president’s Mar-a-Lago resort White House. Trump has spoken publicly about the influence of his discussions with business leaders on policy, saying Monday he “helped” Apple CEO Tim Cook by exempting smartphones from much of the damage of the Chinese tariffs. Nvidia dominates the market in designing the semiconductor technology necessary to power generative AI applications like OpenAI’s ChatGPT and Tesla full self-driving.
25%. That’s how much Nvidia shares are down since Election Day, far worse than the S&P’s 8% decline, not including dividends. Nvidia was the S&P’s top performer of both 2023 and 2024 as its profits soared amid the generative AI revolution."
91,https://www.forbes.com/sites/antonyleather/2025/04/15/nvidia-announces-rtx-5060-graphics-cards-priced-from-299/,Nvidia Announces RTX 5060 Graphics Cards To Be Priced From $299,"Apr 15, 2025, 09:02am EDT",Antony Leather,"Nvidia has announced its highly anticipated GeForce RTX 5060-series graphics cards including pricing, availability and specifications.
Three separate models will be available, with the RTX 5060 Ti in both 16GB and 8GB flavors arriving April 16. The cheaper RTX 5060 non-Ti version will be available next month in May and will also have 8GB of video memory.
Nvidia’s pricing is rumored to sit at just $299 for the RTX 5060, which is the same launch price as the RTX 4060, although it remains to be seen if supply can match demand on launch and if pricing will rise higher than this given the far higher premiums we’ve seen for other RTX 50 series models such as the RTX 5080 and RTX 5090.
The RTX 5060 Ti 16GB and 8GB models are rumored to have launch prices of $429 and $379 respectively. For longevity given many games make use of more than 8GB of video memory already, the 16GB model is likely to be the most sought-after.
Unsurprisingly, the benchmarks Nvidia has released mostly include the use of frame generation, which has divided options with consumers and reviewers. Without it in Delta Force, the RTX 5060 Ti sees a frame rate rise from around 100fps with the RTX 4060 Ti to around 130fps. Elsewhere, the fact that the RTX 50-series cards support 4x frame generation, while older cards cap out at 2x isn’t particularly clear in the graph above (which uses the maximum frame generation mode for each card), and explains why the RTX 5060 Ti is further ahead.
We’ll have to wait for reviews to land for a more comprehensive view on how the RTX 5060 and 5060 Ti perform and these are expected to go live no later than April 16. These will include benchmarks against various other models of graphics cards as well as at different resolutions.
AMD currently doesn’t have any of its new Radeon RX 9000-series models that sit at this price range, with the RX 9070 launching at $549. The Radeon RX 9060-series is expected in the near future, though, with the rumor mill already churning away on reports of RX 9060 XT 8GB and 16GB models. For now, Nvidia’s new cards will have to compete with previous generation models from both AMD’s Radeon RX 7000-series and Nvidia’s own RTX 4000-series.
AMD’s Radeon RX 7600 can be had for around $300 while the 7600 XT will set you back upwards of $400. Currently Nvidia’s RTX 4060 8GB costs around $330, with the RTX 4060 Ti available for less than $600 while the 16GB model rarely available for much less than $700. The release of the RTX 5060-series is likely to see prices of these older cards fall, assuming there is enough stock to prevent those annoying premiums, which is perhaps unrealistic given what’s happened following every RTX 50-series launch so far.
I’ll be covering news and benchmarks on the day so follow me here on Forbes using the blue button below, Facebook or YouTube to get the latest news and reviews."
92,https://www.forbes.com/sites/chrisdobstaff/2025/04/15/forbes-daily-nvidia-chooses-texas-as-its-us-supercomputer-base/,Forbes Daily: Nvidia Chooses Texas As Its U.S. Supercomputer Base,"Apr 15, 2025, 07:59am EDT",Chris Dobstaff,"andForbes Daily,
Forbes Staff.
With a record 3,028 people around the globe on Forbes’ most recent World’s Billionaires List—with a collective worth of $16.1 trillion—it’s clear that the richest people in the world are wealthier and more influential than ever.
This week, Forbes will host a members-exclusive conversation, “Behind The Billions: Inside The Forbes Billionaires List,” diving into what we can learn from the super-rich and why tracking how the world’s wealthiest amass their fortunes is more important than ever.
Join executive editor Luisa Kroll, assistant managing editor Kerry A. Dolan and senior editor Chase Peterson-Withorn for a member-exclusive discussion Wednesday at 1 p.m. ET. Don’t forget to sign up here.
Nvidia is working with Taiwanese electronics manufacturers Foxconn and Wistron to build two supercomputer factories in Houston and Dallas, the company announced Monday. Nvidia expects the investment to result in “hundreds of thousands of jobs,” and said that “mass production at both plants is expected to ramp up in the next 12-15 months.”
U.S. automakers saw stock pops Monday after President Donald Trump hinted at short-term tariff relief, saying that car companies “need a little bit of time” to source parts domestically. Imported cars have had a 25% tariff since early April, and auto parts are set to face similar 25% tariffs no later than May 3. Shares of Ford, General Motors and Stellantis were up about 4%, 3.5% and 5.6%, respectively.
Boeing’s shares slid sharply in premarket trading early Tuesday, after Bloomberg reported that the Chinese government had ordered the country’s airlines to stop taking deliveries of new aircraft from the American plane maker. The move, which also orders Chinese airline companies to stop buying aircraft parts from Boeing, is reportedly part of China’s retaliation against President Donald Trump’s 145% tariff rate on nearly all Chinese-made goods.
Trump appeared to take a victory lap Monday, celebrating what he called “the largest gain in the stock market in history on every single category last week,” though last Wednesday’s gain was not the biggest jump ever on a percentage basis, a more commonly-cited measure of stock movements. All three major indexes remain down at least 4% since Election Day, and 7% since Inauguration Day.
After the massive crypto washout of 2022 and 2023, most industry-leading crypto lenders like BlockFi and Celsius imploded, but a new report suggests a resurrection is underway: Tether, Galaxy and Ledn have emerged as the dominant players in centralized finance lending, holding a combined $9.9 billion in outstanding loans at the end of 2024. That’s nearly 90% of the CeFi market, according to Galaxy Digital.
Shares of Apple jumped Monday as the world’s largest company received a bump from the White House’s tariff exemptions on smartphone imports. The tech giant’s stock price finished the day at about $202, roughly a 17% jump from its $172 close a week ago—its lowest level since last May. The tariff exemption also signaled good news for prospective iPhone buyers, as prices are less likely to see an immediate hike.
In an Oval Office meeting between President Donald Trump and El Salvadoran President Nayib Bukele on Monday, White House advisor Stephen Miller said that bringing a Maryland father who was mistakenly deported back to the U.S. would amount to “kidnapping,” because he is now in the custody of El Salvador. The Trump Administration admitted that Kilmar Abrego Garcia was deported due to a “clerical error,” but has refused to comply with a district court order to “facilitate” his return, which was partially upheld by the Supreme Court last week.
Amid a recent wave of private equity firms purchasing stakes in professional sports teams, New York-based Velocity Capital Management will go a different route and announce today that it has agreed to make a strategic investment in Unique Sports Group, a soccer talent agency headquartered in London that represents more than 350 soccer players. The agency will use the capital to fuel its growth strategy and open new lines of business, such as entering women’s sports.
Residential solar developers already tend to have complicated financial statements, but a recently revealed hidden spreadsheet from troubled firm Sunnova Energy suggests the company was manipulating its public disclosures, according to short sellers. As vultures circle Sunnova, allegations that it had been overstating its solar system statistics “to fabricate larger amounts of federal investment tax credits” aren’t likely to help.
The honorees on the 2025 Forbes 30 Under 30 Europe list are building virtual worlds, revolutionizing fertility, reinventing retail and dominating the entertainment game—all while navigating a global trade war, a live war in Ukraine and the whiplash that comes with the constant evolution of generative AI.
A number of listmakers are harnessing AI’s power to tackle important problems. Felicia von Reden, 29, is the cofounder of Ovom Care, a fertility clinic using AI to take the guesswork out of the IVF process. The company uses machine learning to identify the most viable eggs and sperm for a patient, and since opening its first clinic in 2024, the startup has helped 350 women on their IVF journeys—while raising some $8 million in funding at a $22 million valuation.
Others are betting artificial technology will transform how we advertise and shop. Flore Lestrade, 26, Tristan François, 26, and Christian Kotait, 25, are building Veeton, a tool that allows clothing sellers to show their items using hyperrealistic AI fashion models. Their tech lets clothing companies show off a wide variety of sizes and styles—and for a whole lot cheaper.
And then there’s Frederic Boesel and Jonas Muller, who are building text-to-image AI technology with their startup Black Forest Labs. The pair has raised $31 million in seed money from Andreessen Horowitz and is already partnering with corporations like German telecom titan Deutsche Telekom to help them generate all their marketing images with Forest Labs’ model, Flux. All in all, the 2025 Under 30 Europe companies have raised more than $800 million in funding.
WHY IT MATTERS To identify this standout group, Forbes editors worked with expert judges to review more than 10,000 candidates, evaluating them on impact, financials, and creativity. The result: 300 young leaders steering the future of Europe and harnessing a moment of global uncertainty to build the future’s next great companies.
MORE How We Make The Forbes 30 Under 30 List
Harvard University said Monday it will not accept an agreement with the Trump Administration, which asked the institution to make major changes to its governance, admissions and hiring processes:
$9 billion: The amount of Harvard’s government contracts that are under review by various agencies, including the Education Department
$2.2 billion: How much of Harvard’s federal funding that the Trump Administration froze on Monday, following the university’s refusal
“No government … should dictate what private universities can teach.” Harvard President Alan Garber, responding to the administration’s recent list of demands
If you’re reading this, we hope you’ve already filed your federal taxes for 2024, in which case you’re probably deserving of a Tax Day treat! Here’s a list of a few national retailers and chains with special promotions happening today. (If you haven’t filed yet, procrastinators are certainly also eligible for a Tax Day treat, but you might want to file for an extension.)

Competition between big box retailers is as fierce as ever, and the CEO of a major company said last week that he intends to open 15 new locations each year “for the foreseeable future.” Which retailer is it?
A. Sam’s Club
B. Costco
C. Target
D. IKEA
Check your answer.
Thanks for reading! This edition of Forbes Daily was edited by Sarah Whitmire and Caroline Howard."
93,https://www.forbes.com/sites/gennacontino/2025/04/14/nvidia-to-make-ai-supercomputers-entirely-in-us/,Nvidia To Make AI Supercomputers Entirely In U.S.,"Apr 14, 2025, 01:08pm EDT",Genna Contino,"Chipmaker Nvidia said it is building new factories that can produce its AI supercomputers entirely on U.S. soil as trade war tensions build with top chipmaking countries such as China and Taiwan.
Nvidia is working with Taiwanese electronics manufacturers Foxconn and Wistron to build two supercomputer factories in Houston and Dallas, the company announced Monday, and “Mass production at both plants is expected to ramp up in the next 12-15 months.”
The U.S. chipmaker said it is already producing its Blackwell chips, which provide capacity for generative AI and accelerated computing, at the Taiwan Semiconductor Manufacturing Company (TSMC) factory in Phoenix.
Nvidia expects the investment to result in “hundreds of thousands of jobs and drive trillions of dollars in economic security over the coming decades,” according to a news release.
The Trump administration is touting the news of building U.S. supercomputer factories as a win in its efforts to boost domestic manufacturing.
White House officials, who previously issued an exemption on semiconductors from tariffs, said over the weekend the exemption is temporary, with President Donald Trump posting to Truth Social that he is “taking a look at Semiconductors and the WHOLE ELECTRONICS SUPPLY CHAIN in the upcoming National Security Tariff Investigations.”
Shares of NVIDIA, one of the Magnificent 7 tech stocks, ticked down slightly to $110.83 during afternoon trading, even though major markets were making gains.
Hewlett Packard Enterprise describes AI supercomputing as “when organizations use ultrafast processors made up of hundreds of thousands of powerful machines to manage and interpret vast quantities of data using artificial intelligence (AI) models.”
Since Trump announced his “Liberation Day” tariffs on April 2, the president has issued a 90-day pause on levying them–except for the 145% rate for China–and future trade relations with big semiconductor manufacturers in Asia remain unclear. While the U.S. claims nearly half the global market share of semiconductors, according to the Semiconductor Industry Association, Taiwan, Japan, China and other Asian countries are also big chip exporters. The original executive order imposing the tariffs exempted semiconductors, among other goods. However, Commerce Secretary Howard Lutnick told ABC’s “This Week” on Sunday that the exemption is temporary.
The White House’s 8 Big Tariff Flip-Flops Since ‘Liberation Day’ (Forbes)
Apple Soars After Tariff Exemptions—And iPhones May Not Get More Expensive After All (Forbes)"
94,https://www.forbes.com/sites/petercohan/2025/04/10/nvidia-stock-could-fall-if-125-china-tariff-drives-price-increases/,Nvidia Stock Could Fall If 145% China Tariff Drives Price Increases,"Apr 10, 2025, 01:18pm EDT",Peter Cohan,"Updated, April 10, 2025: This post has been adjusted to reflect the change in U.S. tariffs on China to 145%.
Nvidia stock has fallen 30% from its January 2025 peak of $153, evaporating $1.03 trillion from the company’s market capitalization, according to Google Finance.
One thing contributing to the drop is the results of the 2024 presidential election which brought Donald Trump back to the Oval Office. Since April 2, his tariff policy has thrown markets into turmoil.
And despite Trump’s trumpeting of a 90-day pause on most tariffs, Nvidia could be in worse shape. That’s because tariffs are high on countries where Nvidia products are built. More specifically, Taiwan faced a 34% tariff  – reduced Wednesday to 10% – while China’s rose to 145% on April 10, according to the Wall Street Journal.
Is the drop in Nvidia’s stock a buying opportunity? I see no reason to buy now because although orders may have soared in the current quarter in anticipation of tariffs, I expect the company’s May 2025 earnings report to feature a disappointing growth forecast as a result of tariff uncertainty.
More specifically, the uncertainty is due to the difficulty of answering precisely the following questions:
Nvidia remains bullish. In March, CEO Jensen Huang said tariffs would not cause meaningful short-term harm to the company due to the company’s agile supply chain; demand for AI would drive demand, and TSMC was building a plant in Arizona, according to Barron’s.
Nvidia declined to comment. A spokesperson referred me to a March 2025 company statement from Huang, which said. “Tariffs will have a little impact for us short term. Long term, we’re going to have manufacturing onshore.”
Read on for why Nvidia’s costs could rise significantly, whether Nvidia will pass along those costs to customers, and how the uncertainty around future moves in the global tariff wars may crimp consumer spending and company’s investments in generative artificial intelligence.
Nvidia will report quarterly financial results next month featuring slower top-line growth and thinner margins based on the company’s forecasts when it last reported in February.
Nvidia expressed enthusiasm about its prospects. “We’ve successfully ramped up the massive-scale production of Blackwell AI supercomputers, achieving billions of dollars in sales in its first quarter,"" said Huang in a statement. ""AI is advancing at light speed as agentic AI and physical AI set the stage for the next wave of AI to revolutionize the largest industries.”
Analysts were underwhelmed by Nvidia’s results – which lacked “the positive shock value of some of last year’s reports,” Sungarden YARP Portfolio Investing Group Leader Rob Isbitts told SeekingAlpha. Nvidia’s stock is likely to trade “more on raw demand and supply, rather than on some sort of surprise element,” Isbitts added.
“Guidance was slightly underwhelming,” Edward Jones analyst Logan Purk said in a report featured by Bloomberg. ""We think it will be challenging for management to continue to significantly beat expectations for future growth,” Purk added.
Tariffs could increase significantly the prices of some Nvidia products. However, it is unclear whether Nvidia will pass all of its increased costs on to customers and whether the higher prices would lower demand.
A more significant question for Nvidia’s future is whether companies will view the high investment in generative AI as being worth the $1 trillion Goldman Sachs estimated. If the technology enables companies to do better work with fewer people, demand for generative AI may remain strong in the event of a recession.
Not all of Nvidia’s products are vulnerable to being harmed by Trump’s tariffs.
For example, fears of a Trump administration crackdown on Nvidia’s H20 chip – which the company has been exporting to China – may not be realized. The reason? “Nvidia promised the Trump administration new U.S. investments in AI data centers,” according to a report from NPR.
However, tariffs could raise consumer personal computer prices by as much as 50%, according to WCCFTech. These estimates were made April 3 when the tariff on Chinese goods was a mere 54% – a far cry from the 145% tariff imposed on April 10.
Tariffs could raise manufacturer’s suggested retail price  prices on the following Nvidia gaming machines by at least 50% – based on the latest triple-digit tariff:
It remains unclear whether Nvidia will be able to avoid tariffs on GPUs for now. The tariffs could be avoided if actual products are assembled in the U.S., noted Tom’s Hardware.
For now, Nvidia may be able to ship “actual GPUs and processors/chipsets to the U.S. without paying import duties along with half-finished logic boards – but paying an import tariff -- then assemble actual products in the U.S.,” reported Tom’s Hardware.
Generative AI has been in search of a killer app, as I wrote in the Boston Globe. There has been no obvious progress on this goal and the investment required for the technology is enormous – potentially totaling $1 trillion.
Prior to the tariff announcement, the largest hyperscalers – Amazon, Google, Meta Platforms, and Microsoft – announced plans to spend $270 billion on generative AI data centers in 2025, according to the Wall Street Journal.
However, that spending could take a hit – especially for companies like Meta and Google which are dependent on advertising for much of their revenue. If tariffs and the resulting uncertainty cause a recession – due to rising prices and fear of declining demand and higher unemployment – companies could decide to cut back on their generative AI spending.
The reason for cutting back would be a simple realization that the return on investment in generative AI is not yet compelling. Amazon’s cloud-computing arm historically has generated $4 of incremental revenue for every $1 of capital spending, TD Cowen John Blackledge tech analyst told the Journal. Generative AI’s ratio “is currently something like 20 cents for every dollar,” Blackledge added.
Microsoft, which plans to spend $80 billion on infrastructure in the current fiscal year, is slowing construction – including a $1 billion project in Ohio. Microsoft canceled leases in the U.S. and Europe -- partly due to an oversupply of data-center space relative to the company’s demand forecast, noted TD Cowen analysts. However, “the moves signal a more cautious stance in the longer term,” the Journal reported.
All that could be painful for Nvidia investors – unless surviving with fewer people during a long period of stagflation becomes the killer app for generative AI."
95,https://www.forbes.com/sites/dariashunina/2025/04/07/nvidia-backed-rescale-builds-the-ai-eras-digital-engineering-backbone/,NVIDIA-Backed Rescale Builds The AI Era’s Digital Engineering Backbone,"Apr 07, 2025, 10:00am EDT",Dasha Shunina,"Rescale, the digital engineering platform powering the future of innovation and scientific discovery, today announced a $115 million Series D with participation from Applied Ventures, Atika Capital, Foxconn, Hanwha, Hitachi Ventures, Prosperity7, NEC, NVIDIA, Translink Capital, the University of Michigan, and SineWave Ventures—bringing Rescale’s total funding to more than $260 million. Early investors include Sam Altman, Jeff Bezos, Paul Graham, and Peter Thiel. Rescale is on a mission to empower the world’s leading engineers and R&D teams with advanced computing, intelligent data, and applied AI.
""Accelerated computing is the engine of the AI industrial revolution,"" said Jensen Huang, NVIDIA founder and CEO. “With Rescale’s full-stack NVIDIA software and infrastructure, industries can push the boundaries of AI-driven modeling and simulation—advancing discovery, design, and engineering at an unprecedented pace.”
“Today’s most important breakthroughs depend on how fast engineering and R&D teams can move from idea to insight,” said Joris Poort, founder and CEO of Rescale. “Unfortunately, most teams are using computing infrastructure that wasn’t built for the scale, complexity, or speed to match the rapid pace of technological advancement. Rescale gives every engineer and scientist access to the compute, data, and AI tools they need to accelerate innovation.”
In response to greater demand for new innovative products, enterprises are increasing their investments in technologies that expedite time to market. High-performance computing is now a $50 billion market, simulation software sits at $20 billion, and product lifecycle data management is valued at $30 billion. Yet integrating this increasingly complex tech stack—software, hardware, and data—remains a challenge. Rescale offers a comprehensive platform for integrating compute, data, and AI capabilities into a seamless experience, enabling organizations to focus on innovation and discovery.
Rescale is trusted by industry leaders advancing the frontier in aerospace, automotive, life sciences, energy, manufacturing, semiconductors, and defense. The company’s hundreds of enterprise customers — which include Arm, General Motors, Samsung, SLB, and the U.S. Department of Defense — spend over $1B in infrastructure to power their virtual product development and scientific discovery environments. From accelerating drug development through molecular analysis to optimizing aircraft aerodynamics and enhancing automotive safety through crash simulations, Rescale supports a diverse array of mission-critical use cases. Rescale enables innovators and scientists to bring more advanced products to market with greater efficiency.
“Rescale’s high-performance compute software platform is helping industry-leading companies, including Applied Materials, accelerate innovation through the power of advanced computing and intelligent data,” said Dr. Om Nalamasu, CTO of Applied Materials and President of Applied Ventures. “We’ve experienced firsthand how Rescale’s software product suite can speed our AI physics simulation capabilities and catalyze adoption of our digital twin initiatives. We look forward to building on our successful collaboration with Rescale to empower our engineers and scientists.”
Looking back on 14 years of building Rescale, founder and CEO Joris Poort reflects on the key lessons that have shaped the company’s journey.
Lead with Mission“The most important thing at a high level is having an exciting mission you’re pursuing as a company,” says Joris Poort. “Having a compelling mission helps some of the brightest engineers and scientists do all this innovative stuff—and it keeps you going when things get hard.”
Invest in PeopleAnother important factor is building the best team possible. Joris believes that with a great team, like the one at Rescale, you can tackle any challenge. Investing in people matters.
Long-term commitmentUltimately, being long-term committed leads to much greater success. “Missionaries outperform mercenaries.""  Joris adds, “If you are focused on a mission, you know you will outlast somebody who is just doing it because they want to make a bunch of money or start a company for the sake of it. We have always stayed focused on that.
Recognize the MomentPoort also highlights the importance of timing. “If you’re in the market long enough, things will eventually swing in your favor—or against you”, he says. “Right now, there’s a really strong tailwind with AI, and we have an entirely new category with AI physics that we can develop. But it’s been quite a journey to get here.”
When it comes to raising capital, Poort says there was never a template. “It wasn’t necessarily easy, but we did it. The reason behind every investment decision was unique,” he explains. Convincing investors wasn’t about following a playbook.
Rescale’s new funding will expedite growth of its comprehensive digital engineering platform for advanced computing, intelligent data, and applied AI. To support the increasing demand for its platform, Rescale is accelerating the delivery of its roadmap including:
Rapid advancements in AI technologies are changing how every organization seeks to gain a competitive edge. Rescale’s digital engineering platform is reimagining modeling and simulation for the AI era. Rescale’s cloud-native technologies provide customers with agility and automation that fundamentally advance product development and scientific discovery."
96,https://www.forbes.com/sites/investor-hub/article/is-nvidia-nvda-stock-worth-buying-the-dip/,Is Nvidia Stock Worth Buying The Dip?,"Apr 07, 2025, 04:29pm EDT",Catherine Brock,"You can buy Nvidia stock on the cheap right now, but should you? That’s the question many investors have been asking since the once-unstoppable NVDA stock fell to its lowest P/E ratio since early 2019.
Get your answer now with this look at what happened to Nvidia, factors that could spark a rebound and risks ahead for the company.
Nvidia stock gained 168% in the first 10 months of 2024 as investors bought up shares to get their piece of the AI boom. The stock price rally peaked in early November and then turned south when investors got nervous about lingering high interest rates.
The negative trend for Nvidia deepened in 2025 as a series of headlines prompted many to question the chip designer’s growth outlook:
January through early April, Nvidia stock is down almost 30%.
The AI buildout has driven Nvidia's growth in recent years. Nvidia's AI data center sales are still increasing, but the pace has slowed and competition could become a factor soon. A full Nvidia rebound will likely require a different kind of breakthrough. Three opportunity areas to watch are robotics, PC gaming and autonomous driving.
Huang recently valued the opportunity in AI-powered robotics and automation at $50 trillion. The CEO says these technologies will ""transform manufacturing, logistics, healthcare and other industries."" Nvidia has two platforms, Isaac and Cosmos, that support robotics and other types of physical AI.
Nvidia’s GeForce Now service uses the cloud to upgrade the gaming experience on any PC. Gamers connect their PC gaming accounts at Steam, Epic Games Store or Ubisoft Connect to GeForce Now for added hardware support—essentially turning any device into a gaming PC.
Nvidia announced expanded GeForce Now device support and capabilities at the 2025 CES trade show.
Nvidia’s automotive line-up includes a platform for in-vehicle computing plus solutions for training AI driving models and simulating environments for testing. In 2025, the company announced automotive AI partnerships with Toyota, Aurora, Continental, GM, Gatik and Torc.
Nvidia expects its automotive business to deliver fiscal 2026 revenue of $5 billion, up from $1.7 billion in fiscal 2025.
Alongside these opportunities, Nvidia faces some challenges. Regulatory changes, cheaper AI solutions and in-house chip development top the list.
Changing Trump administration policies could pressure Nvidia’s stock price. Reciprocal tariffs, even with semiconductors excluded, could raise input costs and reduce margins.
While Huang seems unbothered by the trade war developments, he also said he could move manufacturing to the U.S. if needed. That is a long-term solution. Unfortunately, growth investors are not known for their patience. A transition to domestic manufacturing would impact the NVDA stock price.
More concerning than tariffs is the potential for export bans to China and import bans by China. Reports indicate Trump has considered tighter limits on chip sales to China. Meanwhile, Chinese regulators are discouraging sales of Nvidia's H20 chip—a product designed for export to China—because it doesn't meet efficiency standards, according to the Financial Times.
The rollout of DeepSeek caused angst for Nvidia shareholders. At issue was the $5.6 million total investment quoted by the AI app’s creator. The price tag seems tiny next to, say, Microsoft's $80 billion AI budget for fiscal 2025. The discrepancy prompted questions about Nvidia's growth outlook. If DeepSeek can do it, why can't big tech spend far less on Nvidia's AI hardware for the same results?
Some experts have said the $5.6 million number from DeepSeek is misleading. Still, there will be a time when cost-efficiency enters the AI fray. That could create space for competitors to chip away at Nvidia's AI dominance.
Big tech companies don’t want Nvidia to be the only AI chip game in town. It's too risky. They are mitigating that risk by developing in-house solutions. Alphabet (GOOG) recently introduced a collection of lightweight open-source AI models, Meta (META) is testing an AI chip and OpenAI has nearly finalized the design of its AI chip.
These solutions will reduce Nvidia's business with existing customers over time. Nvidia will have to improve on performance and price as a result.
Nvidia is an attractive, long-term buy right now despite regulatory and competitive challenges. The stock may not return to its pre-Trump growth rates, but it is an innovative, healthy company with a forward-thinking leader—and those qualities usually create shareholder value over time.
Like many growth stocks, NVDA is volatile. Its per-share price can fall as quickly as it rises. You can see that dynamic in play for NVDA stock since it went public in 1999. So this stock is best suited for hardy, patient investors and those who already own NVDA and want to reduce their average cost basis. If you don't feel quite ready for NVDA, see this list of best stocks for 2025 for more investing ideas.
Bottom Line
Nvidia is down nearly 30% this year as investors worry about tariffs, trade bans and lower-cost AI models. But the chip designer is not down for the count. Nvidia has continued AI growth, plus opportunities in robotics, PC gaming and autonomous driving that make it an attractive buy at the current valuation.
Is Nvidia stock overvalued?
After the recent downturn, Nvidia’s P/E ratio is about 32. That compares favorably to other semiconductor companies, including Advanced Micro Devices (AMD), Broadcom (AVGO) and Arm Holdings (ARM).
Should I buy Nvidia stock now or wait?
Nvidia is a well-run company that can navigate financial downturns and industry shifts. Relative to its history, the company's current valuation is appealing. If you don't mind volatility and believe in the AI promise as well as Nvidia CEO Jensen Huang, now is a good time to buy Nvidia stock.
What are the biggest risks for Nvidia?
The biggest risks for Nvidia are margin degradation from tariffs, the global impact of tariffs,  import and export bans and waning demand if others can replicate DeepSeek's low-power AI model."
97,https://www.forbes.com/sites/selk/2025/04/01/3-takeaways-on-the-future-of-ai-from-the-nvidia-conference/,3 Takeaways On The Future Of AI From The Nvidia Conference,"Apr 01, 2025, 01:37pm EDT",Sarah Elk,"The Nvidia GPU Technology Conference (GTC) in San Jose this March was electrifying. A few of my colleagues were fortunate enough to attend, and discussing their experiences has been thrilling. Their biggest insight echoed what I’ve been hearing from energized executives across industries: AI has moved past experimental pilots and into the operational core of businesses. This isn’t edge experimentation anymore. It’s a full re-architecture of how companies compete.
Here are three signals from GTC that show just how fast the AI future is arriving.
Massive, general-purpose models are giving way to leaner, fine-tuned ones designed for specific tasks. Techniques like quantization, pruning, and retrieval-augmented generation (RAG) are pushing down costs without compromising quality. More companies are moving toward self-hosting for greater control, privacy, and speed. But this shift adds complexity—and few are fully equipped for the ops required.
What’s next? The real winners will plan for a world where AI inference is significantly cheaper, opening the door to broader adoption and new competitive dynamics. They won’t just unlock productivity—they’ll rethink their entire business models. AI will redefine what they offer, not just how they operate. Forward-looking CEOs will push past automation and into innovation, applying AI to build new products, hyper-personalize experiences, and create entirely new services.
We’re quickly moving from assistants to agentic AI. Though fully autonomous agents remain rare, semiautonomous ones—with human oversight—are gaining ground. Trust in these systems hinges on structured design: transparency, escalation paths, redundancy guardrails, traceability and auditability in production, and predictability. Frameworks like Nvidia’s AgentIQ and emerging “agent orchestration platforms” could help simplify the creation and integration of AI agents into enterprise systems.
What can companies do today to prepare? Start with high-ROI use cases, then test fast and iterate even faster. It’s also key to prepare your data to enable agent success. Leaders should be wary of standalone platforms, interrogating the quality of connectors to other systems. Most importantly, the organizations that win with agentic AI will be ones that prioritize learning. They will foster experimentation and embrace continuous improvement.
Tools like Nvidia Picasso and Adobe Firefly are putting creative firepower in everyone’s hands by generating product visuals, videos, 3D assets, and social content from simple prompts. Creative pipelines from platforms like RunwayML, Canva, and Synthesia are speeding up campaign cycles and unlocking personalization at scale. It’s never been easier to deliver high-quality content—fast.
Most companies won’t build solutions in-house. Instead, smart marketers are piloting vendors to see which ones best fit their needs—and scaling quickly when they find a match. The selection process should move much faster than a typical marketing technology investment.
Early adopters are already reaping the benefits: They’ve reduced campaign time to market by up to 50% and cut content creation time by 30% to 50%. In a recent Bain & Company survey, 27% of executives said generative AI has exceeded or far exceeded their expectations for marketing.
To accelerate the next phase of generative AI maturity, CMOs will need to commit to bold ambitions and results. That means prioritizing big wins rather than letting a thousand flowers bloom. Marketing leaders will define their own workflows and opportunities, then partner with IT to cocreate solutions. For broad adoption, they will tailor training to employees’ day-to-day work, showing where generative AI can complement and enhance their roles.
These are just three of several key themes we observed. From data generation to digital twins, Nvidia GTC underlined that we’ve entered the next stage of enterprise AI maturity—and now it’s time to chase the benefits."
98,https://www.forbes.com/sites/janakirammsv/2025/03/25/nvidia-dynamo---next-gen-ai-inference-server-for-enterprises/,Nvidia Dynamo — Next-Gen AI Inference Server For Enterprises,"Mar 25, 2025, 08:39am EDT",Janakiram MSV,"At the GTC 2025 conference, Nvidia introduced Dynamo, a new open-source AI inference server designed to serve the latest generation of large AI models at scale. Dynamo is the successor to Nvidia’s widely used Triton Inference Server and represents a strategic leap in Nvidia’s AI stack. It is built to orchestrate AI model inference across massive GPU fleets with high efficiency, enabling what Nvidia calls AI factories to generate insights and responses faster and at a lower cost.
This article attempts to provide a technical overview of Dynamo’s architecture, features and the value it offers enterprises.
At its core, Dynamo is a high-throughput, low-latency inference-serving framework for deploying generative AI and reasoning models in distributed environments. It integrates into Nvidia’s full-stack AI platform as the operating system of AI factories, connecting advanced GPUs, networking, and software to enhance inference performance.
Nvidia’s CEO Jensen Huang emphasized Dynamo’s significance by comparing it to the dynamos of the Industrial Revolution—a catalyst that converts one form of energy into another—except here, it converts raw GPU compute into valuable AI model outputs at an unparalleled scale.
Dynamo aligns with Nvidia’s strategy of providing end-to-end AI infrastructure. It has been built to complement Nvidia’s new Blackwell GPU architecture and AI data center solutions. For example, Blackwell Ultra systems provide the immense compute and memory for AI reasoning, while Dynamo provides the intelligence to utilize those resources efficiently.
Dynamo is fully open source, continuing Nvidia’s open approach to AI software. It supports popular AI frameworks and inference engines, including PyTorch, SGLang, Nvidia’s TensorRT-LLM and vLLM. This broad compatibility means enterprises and startups can adopt Dynamo without rebuilding their models from scratch. It seamlessly integrates with existing AI workflows. Major cloud and technology providers like AWS, Google Cloud, Microsoft Azure, Dell, Meta and others are already planning to integrate or support Dynamo, underscoring its strategic importance across the industry.
Dynamo is designed from the ground up to serve the latest reasoning models, such as DeepSeek R1. Serving large LLMs and highly capable reasoning models efficiently requires new approaches beyond what earlier inference servers provided.
Dynamo introduces several key innovations in its architecture to meet these needs:
Dynamic GPU Planner: Dynamically adds or removes GPU workers based on real-time demand, preventing over-provisioning or underutilization of hardware. In practice, this means if user requests spike, Dynamo can temporarily allocate more GPUs to handle the load, then scale back, optimizing utilization and cost.
LLM-Aware Smart Router: Intelligently routes incoming AI requests across a large GPU cluster to avoid redundant computations. It keeps track of what each GPU has in its knowledge cache (the part of memory storing recent model context) and sends each query to the GPU node best primed to handle it. This context-aware routing prevents repeatedly re-thinking the same content and frees up capacity for new requests.
Low-Latency Communication Library (NIXL): Provides state-of-the-art, accelerated GPU-to-GPU data transfer and messaging, abstracting away the complexity of moving data across thousands of nodes. By reducing communication overhead and latency, this layer ensures that splitting work across many GPUs doesn’t become a bottleneck. It works across different interconnects and networking setups, so enterprises can benefit whether they use ultra-fast NVLink, InfiniBand, or Ethernet clusters.
Distributed Memory (KV) Manager: Offloads and reloads inference data (particularly “keys and values” cache data from prior token generation) to lower-cost memory or storage tiers when appropriate. This means less critical data can reside in system memory or even on disk, cutting expensive GPU memory usage, yet be quickly retrieved when needed. The result is higher throughput and lower cost without impacting the user experience.
Disaggregated serving: Traditional LLM serving would perform all inference steps (from processing the prompt to generating the response) on the same GPU or node, which often underutilized resources. Dynamo instead splits these stages into a prefill stage that interprets the input and a decode stage that produces the output tokens, which can run on different sets of GPUs.
As AI reasoning models become mainstream, Dynamo represents a critical infrastructure layer for enterprises looking to deploy these capabilities efficiently. Dynamo revolutionizes inference economics by enhancing speed, scalability and affordability, allowing organizations to provide advanced AI experiences without a proportional rise in infrastructure costs.
For CXOs prioritizing AI initiatives, Dynamo offers a pathway to both immediate operational efficiencies and longer-term strategic advantages in an increasingly AI-driven competitive landscape."
99,https://www.forbes.com/sites/janakirammsv/2025/03/23/what-is-ai-factory-and-why-is-nvidia-betting-on-it/,"What Is AI Factory, And Why Is Nvidia Betting On It?","Mar 23, 2025, 10:40am EDT",Janakiram MSV,"At the recent Nvidia GTC conference, executives and speakers frequently referenced the AI factory. It was one of the buzzwords that got a lot of attention after Jensen Huang, the CEO of Nvidia, emphasized it during his two-hour keynote speech.
Nvidia envisions the paradigm for creating AI systems at scale as the AI factory. This concept draws a parallel analogy between AI development and the industrial process where raw data comes in, is refined through computation, and yields valuable products through insights and intelligent models.
In this article, I attempt to take a closer look at Nvidia’s vision to industrialize the production of intelligence, with a focus on the AI factory.
At its core, an AI factory is a specialized computing infrastructure designed to create value from data by managing the entire AI life cycle – from data ingestion and training to fine-tuning and high-volume inference. In traditional factories, raw materials are transformed into finished goods. In an AI factory, raw data is transformed into intelligence at scale. This means the primary output of an AI factory is insight or decisions, often measured in AI token throughput – essentially the rate at which an AI system produces predictions or responses that drive business actions.
Unlike a generic data center that runs a mix of workloads, an AI factory is purpose-built for AI. It orchestrates the entire AI development pipeline under one roof, enabling dramatically faster time to value. Jensen Huang has emphasized that Nvidia itself has “evolved from selling chips to constructing massive AI factories,” describing Nvidia as an AI infrastructure company building these modern factories.
AI factories do more than store and process data – they generate tokens that manifest as text, images, videos and research outputs. This transformation represents a shift from simply retrieving data based on training datasets to generating tailored content using AI. For AI factories, intelligence isn’t a byproduct but the primary output, measured by AI token throughput – the real-time predictions that drive decisions, automation and entirely new services.
The goal is for companies investing in AI factories to turn AI from a long-term research project into an immediate driver of competitive advantage, much like an industrial factory directly contributes to revenue. In short, the AI factory vision treats AI as a production process that manufactures reliable, efficient and scale intelligence.
Generative AI is constantly evolving. From basic token generation to advanced reasoning, language models have matured significantly within three years. The new breed of AI models demand infrastructure that offer unprecedented scale and capabilities, driven by three key scaling laws:
Traditional data centers cannot efficiently handle these exponential demands. AI factories are specifically designed to optimize and sustain this massive compute requirement, providing the ideal infrastructure for AI inference and deployment.
Building an AI factory requires a robust hardware backbone. Nvidia provides the “factory equipment” through advanced chips and integrated systems. At the heart of every AI factory is high-performance compute – specifically Nvidia’s GPUs, which excel at the parallel processing needed for AI. Since GPUs entered data centers in the 2010s, they have revolutionized throughput, delivering orders of magnitude more performance per watt and per dollar than CPU-only servers.
Today’s flagship data center GPUs, like Nvidia’s Hopper and newer Blackwell architecture, are dubbed the engines of this new industrial revolution. These GPUs are often deployed in Nvidia DGX systems, which are turnkey AI supercomputers. In fact, the Nvidia DGX SuperPOD, a cluster of many DGX servers, is described as “the exemplar of the turnkey AI factory” for enterprises. It packages the best of Nvidia’s accelerated computing into a ready-to-use AI data center akin to a prefabricated factory for AI computation.
In addition to raw compute power, an AI factory’s network fabric is crucial. AI workloads involve moving enormous amounts of data quickly between distributed processors. Nvidia addresses this with technologies like NVLink and NVSwitch – high-speed interconnects that let GPUs within a server share data at extreme bandwidth. For scaling across servers, Nvidia offers ultra-fast networking in InfiniBand and Spectrum-X Ethernet switches, often coupled with BlueField data processing units to offload network and storage tasks. This end-to-end, high-speed connectivity approach removes bottlenecks, allowing thousands  of GPUs to work together as one giant computer. In essence, Nvidia treats the entire data center as the new unit of compute, interconnecting chips, servers and racks so tightly that an AI factory operates as a single colossal supercomputer.
Another hardware innovation in Nvidia’s stack is the Grace Hopper Superchip, which combines an Nvidia Grace CPU with an Nvidia Hopper GPU in one package. This design provides 900 GB/s of chip-to-chip bandwidth via NVLink, creating a unified pool of memory for AI applications. By tightly coupling CPU and GPU, Grace Hopper removes the traditional PCIe bottleneck between processors, enabling faster data feeding and larger models in memory. For example, systems built on Grace Hopper deliver 7× higher throughput between CPU and GPU compared to standard architectures.
This kind of integration is important for AI factories, as it ensures that hungry GPUs are never starved of data. Overall, from GPUs and CPUs to DPUs and networking, Nvidia’s hardware portfolio, often assembled into DGX systems or cloud offerings, constitutes the physical infrastructure of the AI factory.
Hardware alone isn’t enough – Nvidia’s vision of the AI factory includes an end-to-end software stack to leverage this infrastructure. At the foundation is CUDA, Nvidia’s parallel computing platform and programming model that allows developers to tap into GPU acceleration. CUDA and CUDA-X libraries (for deep learning, data analytics, etc.) have become the lingua franca for GPU computing, making it easier to build AI algorithms that run efficiently on Nvidia hardware. Thousands of AI and high-performance computing applications are built on the CUDA platform, which has made it the platform of choice for deep learning research and development. In the context of an AI factory, CUDA provides the low-level tools to maximize performance on the “factory floor” of the new breed of AI factories.
Above this foundation, Nvidia offers Nvidia AI Enterprise, a cloud-native software suite to streamline AI development and deployment for enterprises. Nvidia AI Enterprise integrates over 100 frameworks, pre-trained models and tools – all optimized for Nvidia GPUs – into a cohesive platform with enterprise-grade support. It accelerates each step of the AI pipeline, from data prep and model training to inference serving, while ensuring security and reliability for production use. In effect, AI Enterprise is like the operating system and middleware of the AI factory. It provides ready-to-use components such as the Nvidia Inference Microservices – containerized AI models that can be quickly deployed to serve applications – and the Nvidia NeMo framework for customizing large language models. By offering these building blocks, AI Enterprise helps companies fast-track the development of AI solutions and transition them from prototype to production smoothly.
Nvidia’s software stack includes tools for managing and orchestrating the AI factory’s operations. For example, Nvidia Base Command and tools from partners like Run:AI help schedule jobs across a cluster, manage data and monitor GPU usage in a multi-user environment. Nvidia Mission Control (built on Run:AI technology) provides a single pane of glass to oversee workloads and infrastructure, with intelligence to optimize utilization and ensure reliability. These tools bring cloud-like agility to anyone running an AI factory, so that even smaller IT teams can operate a supercomputer-scale AI cluster efficiently.
Another key element is Nvidia Omniverse, which plays a unique role in the AI factory vision. Omniverse is a simulation and collaboration platform that allows creators and engineers to build digital twins – virtual replicas of real-world systems – with physically accurate simulation. For AI factories, Nvidia has introduced the Omniverse Blueprint for AI Factory Design and Operations, enabling engineers to design and optimize AI data centers in a virtual environment before deploying hardware. In other words, Omniverse lets enterprises and cloud providers simulate an AI factory (from cooling layouts to networking) as a 3D model, test changes and troubleshoot virtually before a single server is installed. This reduces risk and speeds up deployment of new AI infrastructure. Beyond data center design, Omniverse is also used to simulate robots, autonomous vehicles and other AI-powered machines in photorealistic virtual worlds. This is invaluable for developing AI models in industries like robotics and automotive, effectively acting as the simulation workshop of an AI factory. By integrating Omniverse with its AI stack, Nvidia ensures that the AI factory isn’t just about training models faster, but also about bridging the gap to real-world deployment through digital twin simulation.
Jensen Huang has positioned AI as an industrial infrastructure akin to electricity or cloud computing – not merely a product but a core economic driver that will power everything from enterprise IT to autonomous factories. This represents nothing less than a new industrial revolution driven by generative AI.
Nvidia’s software stack for the AI factory ranges from low-level GPU programming (CUDA) to comprehensive enterprise platforms (AI Enterprise) and simulation tools (Omniverse). This end-to-end approach offers organizations adopting the AI factory model a one-stop ecosystem. They can obtain Nvidia hardware and utilize Nvidia’s optimized software to manage data, training, inference and even virtual testing with guaranteed compatibility and support. It indeed resembles an integrated factory floor, where each component is finely tuned to function together. Nvidia and its partners continually enhance this stack with new capabilities.  The outcome is a solid software foundation that allows data scientists and developers to concentrate on creating AI solutions instead of grappling with infrastructure.
"
100,https://www.forbes.com/sites/karlfreund/2025/03/21/is-jensen-huang-nvidias-chief-revenue-destruction-officer/,Is Jensen Huang Nvidia’s Chief Revenue Destruction Officer?,"Mar 21, 2025, 06:34pm EDT",Karl Freund,"At this year's GTC event in San Jose, Nvidia CEO Jensen Huang held over 25,000 people in the palm of his hand, captivated by his vision of AI and how it could transform the world we live in. Some folks in the audience couldn't keep up and started fiddling with their phones. (Among many other semiconductor vendors in the AI space, Nvidia is a client of my firm,  Cambrian-AI Research).
But most were hanging on every word to a) understand the potential of AI going forward, b) get a feel for the sustainability of AI demand, and c) assess whether Nvidia is moving too fast for its customers. Moving to a yearly technology cadence can help one stay ahead of slower competitors. Still, it can also make it difficult for customers to adopt the latest tech after buying a trainload full of the latest ones.
Jensen half-jokingly said nobody should buy a Hopper GPU now that Blackwell is in full production, playing the role of what he called the company's ""Chief Revenue Destruction Officer."" He noted the frustration his sellers would feel upon hearing his advice. Investors did not appreciate his sarcasm either, and the stock is down 2.6% since GTC25 kicked off. However, according to Jensen, AI inference requires 100 times more computing now than a year ago, thanks mainly to the introduction of ""reasoning"" and agentic AI.  In fact, these trends, coupled with incredibly dense infrastructure and software revenue, are creating a new Token Boom that Jensen will harvest as a new revenue boom, in spite of the product churn.
While Blackwell is amazingly fast, with up to 40 times more tokens/second for inference, it also requires significant data center power and water cooling upgrades for the AI Factories Jensen is pushing. However, Hopper may be just fine for many AI developers for now. Nvidia has already shipped nearly three times the number of Blackwell GPUs compared to all Hopper chips in 2024. There is no doubt now that the $11B of Blackwell-based systems Nvidia shipped in Q4 is the start of a new demand cycle. The following system, to be shipped later this year, the Blackwell Ultra, can plug right into the Oberon Blackwell NVL72  chassis. So, the following revenue transition should be much smoother.
Moving to an annual product cycle creates tension, but disclosing the roadmap is essential to prepare the supply chain and ecosystem for the next two years of innovation. No data center can power an 800 Kilowatt rack required for Rubin Ultra today, but they will need to power and cool the beast if they want to remain competitive when Nvidia starts shipping its future racks.
Jensen explained that the Blackwell GPU is 40 times faster than Hopper. He then half-jokingly said nobody should buy a Hopper GPU now that Blackwell is in full production, playing the role of the company's ""Chief Revenue Destruction Officer."" But Jensen told everyone how fast Blackwell would be a year ago, so sellers and buyers should not be surprised. Last year, he said 30X, but the new Dynamo software had a significant impact.
When the rest of the industry is trying to match (unsuccessfully) Nvidia GPU performance, Nvidia is optimizing the entire AI factory. The new Dynamo ""AI Factory OS"" and Co-packaged optical networking are two examples.
Nvidia has wholly rewritten its Triton inference software to manage and optimize the distributed inference processing needed to perform agentic and reasoning AI. Nvidia Dynamo is an open-source modular inference framework for serving generative AI models in distributed environments. Think of it as the OS or the Kubernetes for an AI factory. It enables scaling inference workloads across large (potentially millions) GPU fleets with dynamic resource scheduling, intelligent request routing, optimized memory management, and accelerated data transfer. The memory management concept in Dynamo is Key Value Cache (KV Cache) distribution. It enables the entire AI factory to access previous reasoning results and serve them up instantly, avoiding costly recalculations.
When serving the open-source DeepSeek-R1 671B reasoning model on Nvidia GB200 NVL72, Nvidia Dynamo increased the number of requests by up to 30x,  helping AI factories lower costs to maximize token revenue generation..
My colleague Jim McGregor of Tirias Research has already covered this innovation in Forbes, so I won’t belabor the point. The bottom line is that these co-packaged optics were not expected to be ready by any vendor for another couple years. Now Nvidia will ship CPO networking for scale-out to millions of GPUs later this year.
Here’s the new Nvidia GPU lineup through 2028.  The annual increase in computing power, memory, and networking should be awe-inspiring, but the audience at GTC seemed to have expected nothing less from the company that practically reinvented computing.
First up, Blackwell Ultra due later this year:
After another year, in late 2026, Rubin shows up in the same Oberon Rack. So Nvidia is delivering three generations of GPUs into the same rack infrastructure. At this point, the naming changes to identify the number of GPUs in the NVLink-connected rack (144) instead of the number of dual-CPU packages (72), all in prep for Rubin Ultra. Yeah, its confusing, but it was the right long-term marketing call.
Rubin Ultra will be a huge step up in density to 576 GPUs in a single “Kyber” rack with four full-sized GPU dies in a package, a new Arm Vera CPU, and the new NVlink7 with twelve times the throughput. This rack will deliver some 15 Exaflops of FP4 performance (14 times that of the next Blackwell GB300) and 4.6 Petabytes of HBM4e memory.
I think the chart below says it all.  Rubin will be 900 times faster than last years Hopper, at 3% of the TCO. The industry has never seen anything like this pace of performance increase in two years.
Adding in the new 1 PFlop DGX Spark, 20 PFlop DGX Station, etc, this is this year’s Nvidia Enterprise AI Lineup. Note that all of these products are available from Nvidia partners.
When asked what Nvidia is becoming at a small-group analyst session, Jensen said the company is already transitioning to its future place in the industry as THE foundational AI company. Jensen showed slides from many large enterprise clients that made this point; each had green icons with up to a dozen Nvidia Inference Micro-Services (NIMS) modules and hardware embedded in their AI stacks;  once these green icons are in there, they won’t come out easily.
While Nvidia will certainly face increasing competition from its hyper-scale customers and chip vendors, I reiterate my world view: while the rest of the entire industry is just trying to match their GPU performance, and failing to do so, Nvidia is completely revamping the industry landscape.  I haven’t even touched on all the new software Jensen announced at his keynote, nor the new AI PC and workstations that blow every AI PC out of the water by some 1-2 orders of magnitude.
Disclosures: This article expresses the opinions of the author and is not to be taken as advice to purchase from or invest in the companies mentioned. My firm, Cambrian-AI Research, is fortunate to have many semiconductor firms as our clients, including Baya Systems, BrainChip, Cadence, Cerebras Systems, D-Matrix, Esperanto, Flex, Groq, IBM, Intel, Micron, NVIDIA, Qualcomm, Graphcore, SImA.ai, Synopsys, Tenstorrent, Ventana Microsystems, and scores of investors. I have no investment positions in any of the companies mentioned in this article. For more information, please visit our website at https://cambrian-AI.com."
101,https://www.forbes.com/sites/cathyhackl/2025/03/20/4-key-takeaways-from-nvidias-gtc-2025-that-will-reshape-your-business/,4 Key Takeaways From NVIDIA’s GTC 2025 That Will Reshape Your Business,"Mar 20, 2025, 03:43pm EDT",Cathy Hackl,"NVIDIA’s GTC 2025 wasn’t just another AI conference, it was a glimpse into the future of intelligence and automation, revealing how businesses will have to change, create, rethink, and revamp their approaches. The event, dubbed the ""Super Bowl of AI,"" highlighted a fundamental change: AI is not just a tool that people are using to write emails, it is starting to become an intrinsic part of how the world does business.
Jensen Huang, CEO of NVIDIA, envisioned a world where AI doesn’t just write content or perform routine tasks, but sees, understands, and acts in the physical world. This progression, from generative AI, to agentic AI, to physical AI, has profound implications for every industry.
For businesses that want to stay one step ahead, here are four important takeaways from GTC 2025 to keep in mind as you plan the future of your company and how it veiws and embarces AI and the convergence of different technologies.
For years, AI has been living in software—chatbots, automation tools, and content generators. But GTC 2025 revealed:  AI is entering the physical world.
If your company’s AI plan is limited to digital automation, it’s time to think again. Begin to consider how AI-based robotics, multimodal assistants, and spatial AI can change the way you do business, interact with customers, and build your workforce.
One of the biggest changes that is occurring in the present time is multimodal AI – artificial intelligence that works with many types of data at once, including text, video, speech, 3D, and  real-time sensors.
During GTC 2025, Huang demonstrated how NVIDIA’s newest models will produce more contextual,  intelligent, and predictive AI systems that can:
Organizations that embed modal AI in their decision-making and customer journey will have a competitive edge. In finance, healthcare, retail, or industrial settings, it is the AI that can ‘see, hear, understand and act’  that will lead to better business decisions.
While traditional AI has been deployed on the cloud, GTC 2025 marked the shift to edge  AI, where AI runs on devices and networks directly, not through the cloud.
Why does this matter for businesses? Here are a few reasons:
You might want to review your company’s AI infrastructure and possibly think, as much as that is possible today, how you can future-proof your business by investing in on-device AI, 5G/6G connectivity, and real time data processing to become a leader of the pack rather than a follower.
One of the most worrying and pressing issues that many in the business world will be faced with is the potential of job loss caused by AI. But Huang told a different story during GTC: Humans are not being replaced by AI,  but enhanced by it. Here are three things
The smartest companies will not only implement AI but also incorporate it into the training of the employees and management and even use other technologies like XR to. help workers become more efficient and get upskilled faster. Those who embrace AI augmentation will gain greater performance, decision-making, and flexible employees.
NVIDIA’s developer’s conference helped shed light on the current and future state of AI. The organizations that pay attention and understand the implications of how AI is slowly expanding into the phsycial world and start exploring how to implement multimodal AI, edge computing, and robotcis will be the ones that will determine the future of industries.
One of the most important questions that business leaders can ask themselves today is: Are we prepared? The companies that will thrive in this AI era will be those that develop fluid strategies, deploy solutions, test and iterate frequently, and embrace AI as a strategic tool. The next 12-24 months will determine who could be the leaders of this era and who will be the followers. Which one are you?"
102,https://www.forbes.com/sites/davealtavilla/2025/03/20/nvidia-benchmark-recipes-bring-deep-insights-in-real-world-ai-performance/,Nvidia Benchmark Recipes Bring Deep Insights In Real-World AI Performance,"Mar 20, 2025, 01:46pm EDT",Dave Altavilla,"As AI workloads and accelerated applications grow in sophistication and complexity, businesses and developers need better tools to assess their infrastructure’s ability to handle the demands of both training and inference more efficiently. To that end, Nvidia has been working on a set of performance testing tools, called DGX Cloud Benchmark Recipes, that are designed to help organizations evaluate how their hardware and cloud infrastructure perform when running the most advanced AI models available today. Our team at HotTech had a chance to kick the tires on a few of these recipes recently, and found the data they can capture to be extremely insightful.
Nvidia’s toolkit also offers a database and calculator of performance results for GPU-compute workloads on various configurations, including the number of Nvidia H100 GPUs and cloud service providers, while the recipes allow businesses to run realistic performance evaluations on their own infrastructure. The results can help guide decisions on whether to invest in more powerful hardware, cloud provider service levels, or tweak configurations to better meet machine learning demands. These tools also take a holistic approach that incorporates network technologies for optimal throughput.
Nvidia DGX Cloud Benchmarking Recipes are a set of pre-configured containers and scripts that users can download and run on their own infrastructure. These containers are optimized for testing the performance of various AI models under different configurations, making them very valuable for companies looking to benchmark systems, whether on prem or in the cloud, before committing to larger-scale AI workloads or infrastructure deployments.
In addition to offering static performance data, time to train and efficiency calculated from its database, Nvidia has recipes readily available for download that let businesses run real-world tests on their own hardware or cloud infrastructure, helping them understand the performance impact of different configurations. The recipes include benchmarks for training models like Meta’s Llama 3.1 and Nvidia’s own Llama 3.1 branch, called Nemotron, across several cloud providers (AWS, Google Cloud, and Azure), with options for adjusting factors like model size, GPU usage, and precision. The database is broad enough to cover popular AI models, but it is primarily designed for testing large-scale pre-training tasks, rather than inference on smaller models. The benchmarking process also allows for flexibility. Users can tailor the tests to their specific infrastructure by adjusting parameters such as the number of GPUs and the size of the model being trained.
The default hardware configuration in Nvidia’s database of results uses the company’s high-end H100 80GB GPUs, but it is designed to be adaptable. Although currently, it does not include consumer or prosumer-grade GPUs (e.g., RTX A4000 or RTX 50) or the company’s latest Blackwell GPU family, these options could be added in the future.
Running the DGX Cloud Benchmarking Recipes is straightforward, assuming a few prerequisites are met. The process is well-documented, with clear instructions on setting up, running the benchmarks, and interpreting the results. Once a benchmark is completed, users can review the performance data, which includes key metrics like training time, GPU usage, and throughput. This allows businesses to make data-driven decisions about which configurations deliver the best performance and efficiency for their AI workloads. This could also go a long way in helping companies maintain green initiatives in terms of meeting power consumption and efficiency budgets.
While the DGX Cloud Benchmarking Recipes offer valuable insights, there are a few areas where Nvidia’s tools could be expanded. First, benchmarking recipes are currently focused primarily on pre-training large models, not on real-time inference performance. Inference tasks, such as token generation or running smaller AI models, are equally important in many business applications. Expanding the toolset to include more detailed inference benchmarks would provide a fuller picture of how different hardware configurations handle these real-time demands. Additionally, by expanding the recipe selection to include lower-end or even higher-end GPUs (like Blackwell or even competitive offerings), Nvidia could cater to a broader audience, particularly businesses that don’t require the massive compute power of a Hopper H100 80GB cluster for every workload.
Regardless, Nvidia’s new DGX Cloud Benchmarking Recipes look like a very helpful resource for evaluating the performance of AI compute infrastructure, before making major investment decisions. They offer a practical way to understand how different configurations—whether cloud-based or on-premises—handle complex AI workloads. This is especially valuable for organizations exploring which cloud provider best meets their needs, or if the company is looking for new ways to optimize existing infrastructure.
As AI’s role in business and our everyday lives continues to grow, tools like this will become essential for guiding infrastructure decisions, balancing performance versus cost and power consumption, and optimizing AI applications to meet real-world demands. As Nvidia expands these recipes to include more inference-focused benchmarks and potentially expands its reference data with a wider range of GPU options, these tools could become even more indispensable to businesses and developers of all sizes.
Dave co-founded and is principal analyst at HotTech Vision And Analysis, a tech industry analyst firm specializing in consulting, test validation and go-to-market strategies for major chip and system OEMs. Like all analyst firms, HTVA provides paid services, research and consulting to many chip manufacturers and system OEMs, including companies mentioned in this article. However, this does not influence his objective coverage."
103,https://www.forbes.com/sites/timbajarin/2025/03/20/the-biggest-news-from-nvidias-gtc-keynote/,The Biggest News From Nvidia GTC 2025,"Mar 20, 2025, 11:57am EDT",Tim Bajarin,"Nvidia CEO Jensen Huang wowed the audience during his GTC 2025 keynote Tuesday, underscoring how AI and cutting-edge computing continue to reshape industries. Marking the 25th anniversary of GeForce, Nvidia introduced the GeForce 5090 — a GPU that’s 30% smaller and more efficient than its predecessor — setting a new benchmark for gaming and AI workloads. But the real story was about AI's evolution and the hardware enabling it.
Huang emphasized how AI has moved beyond simple perception tasks into agentic systems that reason, plan and take action. These advancements demand even greater computational power, driving Nvidia's innovations in chip architecture and enterprise AI solutions.
This advancement is vital, as we are moving quickly toward an era when agents will dominate the AI landscape. Very soon, agents will work behind the scenes to manage just about everything digital in business and personal lives. These are power-hungry AI functions, and Nvidia is driving the backend architectures that make them happen.
The key to this push is Blackwell, Nvidia’s latest architecture designed to power next-gen AI. Coupled with the Dynamo operating system, Blackwell aims to optimize AI performance across industries, from cloud computing to robotics.
Nvidia is setting the stage for the next generation of AI computing, unveiling:
Nvidia's emphasis on energy efficiency and an open-source AI management approach signals a commitment to responsible AI development. To wrap up the event, Huang showcased AI-generated music, offering a glimpse into how AI is being integrated into creative industries.
As Nvidia continues to push the boundaries of AI and computing, it’s clear that advances in GPUs, robotics and intelligent automation will shape the next era of technology."
104,https://www.forbes.com/sites/tomcoughlin/2025/03/19/storage-and-memory-enable-next-generation-ai-at-the-2025-gtc/,Storage And Memory Enable Next Generation AI At The 2025 Nvidia GTC,"Mar 19, 2025, 03:10pm EDT",Thomas Coughlin,"Jensen Huang, CEO of Nvidia, gave one of this announcement-filled presentations at the 2025 GTC in San Jose.  Among announcements on GPU roadmaps, low power photonics networking, compact liquid cooled GPU systems, new robotics initiatives and the wide variety of CUDA libraries available to users, he also gave some interesting insights on digital storage and memory requirements for next generation GPUs and changes in AI storage platforms.
There is also a Nvidia driven effort underway, Storage-Next that aims to improve memory integration to GPUs versus CPUs. Several storage and memory companies were also at the GTC and we also discuss announcements from Micron, Phison, Vast Data and Vdura.
Jensen presented details on the announcement of the Blackwell GPU series, the follow on the Grace Hopper products, but he also talked about a couple of generations in the second half of 2026 and in 2027 that will be called the Vera Rubin GPU system. Vera Florence Cooper Rubin was an American astronomer who did pioneering research on galaxy rotation rates.  I believe that he also indicated that the GPU generation after Vera Rubin would be named after physicist Richard Feynman.
The later 2027 Vera Rubin system, the Rubin Ultra NVL576 would have HBM4e data rates of 4.5PB/s with 365TB of fast memory and 1.5PBs NVLink7 connectivity as shown below.
The Rubin system package will also be considerably larger than prior generation Blackwell packages, as shown by comparing the size of components in the two packages below.  Nvidia is taking 2D chiplet technologies to new levels with these GPUs.
Jensen, towards the end of his 2+ hour talk, announced a new class of storage infrastructure using AI to enhance data access, see image below.  This Nvidia AI data platform is a reference design that digital storage partners will implement that uses AI query agents to generate insights from data in near real time using Nvidia AI enterprise software, including Nvidia NIM microservices for the company’s Nemotron models and its AI-Q Blueprint for building AI query agents.
Among the companies implementing versions of this reference design are DDN, Dell Technologies, Hewlett Packard Enterprise, Hitachi Vantara, IBM, NetApp, Nutanix, Pure Storage, VAST Data and WEKA.
Storage providers can implement these agents using Nvidia Blackwell GPUs, Bluefield DPUs, Spectrum-X networking and Dynamo open-source inference library.  Using this collection of technologies, they can access large-scale data quickly and process various data types, including structured, semi-structured and unstructured data from multiple sources, including text, PDF, images and video.
There is also work going on, driven by Nvidia, for a new storage architecture for GPU computing near memory for disaggregated, data-protected, managed block storage.  This would use next generation NVMe with the intention of achieving high IOPs/$, better power efficiency for fine-grained accesses by supporting direct access to storage from GPUs.  This effort seeks to achieve 512B IOPs/GPU in Gen6, optimized for power and tail latency.  Tail latency is the slowest response time in a system latency distribution, often measured as the outliers above 95% or higher of the distribution.  Tail latency can have a significant impact on overall system performance.
GPU-initiated storage versus CPU-initiated storage is said to deliver better TCO with higher IOPS, smaller space with fewer devices needed to reach IOPS goals and lower power consumption with better utilization from reduced impact from tail latencies.
In addition to Nvidia announcements and activities, some storage and memory companies made announcement around the GTC.  Micron announced SOCAMM, Small Outline Compression Attached Memory Module, developed in collaboration with Nvidia, a modular high-capacity LPDDR5X DRAM technology for use with the Nvidia GB300 Grace Blackwell Ultra Superchip.  Micron also said that their HBM3E products were being used in several Nvidia platforms.
SOCAMMs are said to provide over 2.5 times higher bandwidth at the same capacity when compared to RDIMMs, allowing faster access to larger training datasets and more complex models, as well as increasing throughput for inference workloads.
Phison announced an array of expanded capabilities on aiDAPTIV+, a more affordable AI training and inference solution for on-premise infrastructure.  aiDAPTIV+ is being integrated into AI laptop PCs as well as on edge computing devices running the Nvidia Jetson platform.
Vast Data announced new features to its Vast Data Platform enabling enhancements to its Vast InsightEngine including vector search and retrieval, serverless triggers and functions and fine-grained access control and AI-ready security.
VDURA launched its V5000 all-flash appliance offering a parallel file system architecture that it says is engineered for AI.  It combines client-side erasure coding, remote direct memory access, RDMA, acceleration and flash-memory to scale with growing GPU clusters.
GPU memory and storage requirements are growing.  New semantic storage with AI query agents and storage-next project improves data access.  Micron, Phison, Vast Data and VDURA make AI storage and memory announcements at the 2025 GTC."
105,https://www.forbes.com/sites/edgarsten/2025/03/18/magna-and-nvidia-partner-on-tech-for-vehicles-to-behave-more-like-humans/,Magna And Nvidia Partner On Tech For Vehicles To Behave More Like Humans,"Mar 18, 2025, 04:00pm EDT",Ed Garsten,"Major automotive supplier Magna International and technology giant Nvidia have reached a deal to combine technologies aimed at boosting accuracy and speed of on-board safety and self-driving systems, the companies announced Tuesday.
“Our goal is to make sure that the car behaves in the same way as you would expect from a human driver, in fact, better,” said Steven Jenkins, vice president of technology strategy at Magna Electronics, in an interview.
“As the automotive industry transitions to safer, more intelligent vehicles with autonomous driving capabilities, our collaboration with Magna is the latest in our endeavors to bring our safety-certified in-vehicle accelerated compute and AI to the transportation industry,” added Ali Kani, vice president of automotive at Nvidia, in a statement. “By combining core technologies and Magna's integration expertise, we aim to shape the future of mobility.”
The deal calls for Magna to integrate Nvidia’s Drive AGX platform within its next generation of advanced technology solutions. The Drive AGX platform Thor system-on-a-chip which runs the safety-certified DriveOS operating system and is built on the Blackwell GPU architecture. It consolidates increased functionality to improve efficiency, speed, and scalability.
Indeed the Nvidia Drive AGX Thor delivers up to 1,000 trillion operations per second of AI compute power, featuring 8-bit floating point support optimized for transformer models, large language models and generative AI workloads.
For Jenkins, it’s a capability that’s coming none-too-soon.
“I've been waiting for this time to come around for a long time, having the capability of such a chip,” remarked Jenkins. “What it really means is you can process the world around you in a much better way, with much more responsiveness, much more perception, much more ability to do things.”
Things, such as integrating advanced driver assistance systems, autonomous driving and interior cabin AI features and providing system solutions that meet specific market needs and regulatory requirements.
MWhen combining all that increased computing power assisted by AI integrated into Magna’s systems, the goal is reaching that human-like or better performance to which Jenkins referred.
“What that really means is you shouldn't feel that you're in a car and you're having someone drive you,” explained Jenkins. “That will be partially because you'll be able to communicate with this thing properly. It will be able to help you and guide you and guide you along your route. But it will also be very robust. It won't turn off suddenly, if the weather gets bad or it won't turn off suddenly, if it can't understand the scenario, it will be on for the long term. So it will be as robust as a person.”
The next step is developing a working demonstration model of the platform, which is expected to arrive during the fourth quarter of this year, according to Jenkins.
As is the custom for automotive suppliers, the names of specific automakers that are likely to become customers for the new combined platform are under wraps for now, but “we have a lot of customer interest,” Jenkins teases.





"
106,https://www.forbes.com/sites/luisromero/2025/03/18/nvidia-gtc-2025-the-rise-of-computational-power-as-currency/,NVIDIA GTC 2025: The Rise Of Computational Power As Currency,"Mar 18, 2025, 04:15pm EDT",Luis E. Romero,"What if the most important technological revolution of our lifetime isn’t the AI we see, but the invisible architecture powering it? This question drove NVIDIA CEO Jensen Huang’s keynote at GTC 2025, unveiling a vision where computational power doesn’t just accelerate—it becomes the underlying fundamental of a new economy.
The gap between what AI promises and what current hardware can deliver has been widening. Businesses face impossible choices: limit ambition or accept crippling costs.
Enter NVIDIA’s Blackwell System, revealed to be an astonishing 40 times more powerful than its predecessor, the Hopper architecture. Huang’s announcement represents not incremental progress but a quantum leap in computational capability that reshapes what’s possible. This new system doesn’t just process more data—it fundamentally alters the economics of advanced AI deployment.
While NVIDIA’s stock may be down for 2025, industry analysts aren’t hesitating to call this gathering “AI Woodstock”—a defining moment when technological potential and practical application finally converge. This is, once again, a moment of reckoning between hype and implementable announcements. GTC 2025 is not over, but it’s looking good in form and substance.
Behind the closed doors of data centers, a revolution quietly unfolds. But who conducts this invisible orchestra of silicon and code?
NVIDIA has positioned itself as a clear leader in major data center categories, showcasing not just their new Blackwell Ultra chips but also unveiling the mysterious Vera Rubin architecture. These advances transform data centers from passive storage facilities into dynamic engines of intelligence, capable of orchestrating increasingly complex computational tasks with unprecedented efficiency.
The implications extend beyond performance metrics. These technologies fundamentally change how businesses approach problem-solving, potentially democratizing access to computational resources that were previously limited to tech giants with bottomless budgets.
This isn’t just about faster computers—it’s about crossing the threshold where machines can handle the complexity that makes us human.
The most profound technological shifts happen at intersection points. Where does silicon intelligence meet the tangible world?
Telecomm: NVIDIA’s partnerships with telecom leaders including T-Mobile, MITRE, and Cisco aim to develop AI-native wireless networks for upcoming 6G infrastructure. This coalition isn’t just upgrading existing systems but reimagining the fundamental architecture of connectivity—creating neural pathways for a globally distributed AI nervous system.
Robotics: Huang detailed advances in robotics and autonomous vehicles that leverage these computational breakthroughs. These aren’t isolated innovations but interconnected elements of a comprehensive strategy to extend AI’s reach from abstract data processing to physical world interaction.
We stand at a moment where the digital and physical worlds aren’t just connected but integrated through an AI fabric that spans from microscopic chips to global networks.
NVIDIA’s GTC 2025 has so far presented a vision of a future world traversed and interconnected by AI in the ethereal form of algorithms, the microscopic dimension of chips, and the macroscopic realm of robots—spanning self-driving cars, autonomous roboworkers, autonomous household assistants, and more. Paraphrasing Huang, the vision is that just as converting moving water into electricity reshaped our world, converting electricity into AI will propel humanity into its next chapter. Of course, he’s referring to how computational power in AI will drive the next few decades of progress.
In this new reality, computational power isn’t just a technical specification. It’s the currency of innovation, the limiting factor of ambition, and increasingly, the foundation of competitive advantage in a world where intelligence—both human and artificial—drives progress."
107,https://www.forbes.com/sites/tiriasresearch/2025/03/18/nvidia-networking-steals-the-show-for-the-second-gtc-in-a-row/,Nvidia Networking Steals The Show For The Second GTC In A Row,"Mar 18, 2025, 07:23pm EDT",Jim McGregor,"Building any complex system is a sum of its parts, but the most foundational element is the thing that binds them together. In brick, it’s mortar; in wood, it’s nails; and in data centers, it’s networking. Last year, Nvidia CEO Jenson Huang declared that “the data center is the new unit of compute.” This means an entire data center should be thought of as a single system, just as a system-on-chip or a server. Combining all those individual elements, including processing, memory and storage, into a single platform requires a complex, high-performance, low-latency network.
Disclosure: My company, Tirias Research, has consulted for Nvidia and other companies mentioned in this article.
In 2024, Nvidia introduced the new Blackwell GB200 GPU AI accelerator to be combined with the Grace CPU in the new NVL72 rack server configuration. While the GPU and rack configuration were, and remain, industry-leading solutions, the real star of the story was the NVLink switch that allowed all 72 GPUs to be interconnected and to act as a single GPU, essentially a foundation for the server and rack. Because it was all in the same rack, it is referred to as a scale-up solution. In 2025, Nvidia is looking to scale out the networking throughout the data center with new co-packaged optical networking solutions.
According to Nvidia, it has worked with a number of industry partners to develop the Quantum-X InfiniBand and the Spectrum-X ethernet silicon photonics networking switches with co-packaged optics on the networking module. In addition, Nvidia is introducing three new liquid-cooled optical networking switches. Because of the high networking demands of AI data centers, especially AI factories, optical networking is already common. While it provides a significant boost in performance and reduction in latency, it comes at a cost in terms of power, space, complexity and money. According to Nvidia, an AI factory can use up to 2.4 million optical transceivers using up to 24MW of power, which can be upwards of 10% of the overall data center power consumption.
When comparing the Spectrum-X silicon photonics networking switches to traditional optical transceivers, Nvidia claims a 4x reduction in the number of lasers, a 3.5x increase in power efficiency, a 63x improvement in signal integrity, a 10x increase in network resiliency, and 1.3x decrease in network deployment time. These improvements are critical to meet the demands of agentic AI, which may require hundreds of times more resources than generative AI workloads spread across data center resources. The new Nvidia networking solutions will allow data centers to continue scaling with the demands of AI.
While the announcement is a boost to Nvidia, it is a milestone for the industry. Co-packaged optic solutions have been under development since 2000 but have not entered mass production because of technical and manufacturing challenges, such as fiber coupling and light source integration. While the industry agrees that co-packaged optics are inevitable, the likelihood of a mass-produced solution was still viewed by many as several years away. However, with the assistance of its partners, Nvidia claims to have overcome the challenges and is ready for volume production starting later this year. The Nvidia partners include Browave, Coherent, Corning, Fabrinet, Foxconn, Lumentum, Senko, SPIL, Sumitomo Electric, TFC, and TSMC.
In addition to the new networking solutions, Nvidia made a plethora of other announcements, including:
Even with all these other announcements, networking was still the highlight of Jensen Huang’s keynote and GTC for the second year in a row. Not only does it improve the performance efficiency of the data center, but it also advances a technology that is critical to the entire industry. While slated for rack-to-rack scale out connectivity today, it will likely be targeted for internal rack scale up networking in the future as the industry pushes the limits of copper interconnects."
108,https://www.forbes.com/sites/greatspeculations/2025/03/14/nvidia-stock-to-60/,Nvidia Stock To $60?,"Mar 14, 2025, 05:00am EDT",Trefis Team,"Question: How would you react if you held Nvidia stock (NASDAQ: NVDA) and its value fell by 60% or more in the upcoming months? Although this might seem extreme, such an occurrence has happened before and could repeat itself. Earlier this year, Nvidia stock experienced a sell-off, dropping from approximately $148 in mid-January to around $115 now, which represents a nearly 23% decrease. This decline has been fueled by several factors, such as the introduction of more resource-efficient AI models like China’s DeepSeek and worries that decelerating investments in generative AI might reduce the demand for Nvidia’s GPUs. Additionally, the overall market is undergoing a significant sell-off, spurred by increasing concerns about a U.S. recession following tariffs imposed by President Donald Trump on major trading partners. Separately, see Is A New CEO Enough To Get Intel Back On Track?
Here’s the point: The key takeaway is that during a downturn, NVDA stock might incur substantial losses. Data from 2022 indicates that NVDA stock lost more than 60% of its value in only a few quarters. Currently, Nvidia stock has decreased from $148 to $115 in roughly two months. This raises the question: could the stock continue its downward trend and reach approximately $60 if a similar situation were to unfold? Naturally, individual stocks are generally more volatile than diversified portfolios. Therefore, if you are looking for growth with reduced volatility, you might consider the High-Quality portfolio, which has outperformed the S&P 500 and generated returns of over 91% since its inception.
Slowdown in AI Demand: Over the last two years, companies have significantly invested in training AI models, with Nvidia’s GPUs becoming the preferred option due to their performance and efficiency. The short-term outlook remains robust, as tech giants such as Google and Microsoft are increasing capital expenditures for 2025 to expand their AI cloud infrastructures. Nevertheless, demand may eventually taper off. Since AI model training is predominantly a one-time, compute-intensive process, it could eventually decelerate. As models increase in size, incremental performance improvements are anticipated to diminish, and the supply of high-quality training data might become a limiting factor. Consequently, as the demand for training declines, GPU sales could also decrease. Therefore, as discussed in our analysis here on the macro picture, the U.S. economy might enter a downturn, potentially even a recession, which could further affect GPU sales. Because AI investments continue to be unprofitable for most companies, they may be prime candidates for cost-cutting measures during an economic downturn. This situation could significantly impact Nvidia.
The transition to more resource-efficient AI models could also affect Nvidia. China’s DeepSeek model, launched in January, attracted attention for prioritizing software-driven optimization instead of relying on hardware, which could potentially reduce the demand for GPUs. The company reports that it spent only $5.5 million to train its V3 model, which is only a fraction of the hundreds of millions that firms such as OpenAI are said to have spent. Since DeepSeek’s model is open source, major tech companies might adopt similar cost-cutting strategies. If this approach is widely implemented, it could further reduce the demand for AI computing power. For further information, see how DeepSeek impacts Nvidia
The U.S. government has imposed export control restrictions on most of Nvidia’s latest AI chipset products destined for China, with the exception of the H20 chips, citing national security concerns. Nevertheless, reports indicate that gray market resellers are using entities registered outside of China to purchase servers equipped with Nvidia’s latest products from companies in various countries, including Singapore, Malaysia, Taiwan, and Vietnam. This is particularly concerning as Singapore has emerged as Nvidia’s second-largest market, generating approximately $23 billion in sales in FY’25 (around 18% of revenue), compared to only $2.3 billion (8% of revenue) in FY’23. Singapore has now initiated an investigation into these potential loopholes. Such developments could potentially affect Nvidia’s overall revenue to some degree. For further details, see how Trump’s tariffs affect Nvidia stock.
NVDA stock has experienced a decline that was marginally more severe than that of the benchmark S&P 500 index during some recent downturns. Are you concerned about how a market crash might affect NVDA stock? Our dashboard How Low Can NVIDIA Stock Go In A Market Crash? provides a detailed analysis of the stock’s performance during and following previous market crashes.
• NVDA stock declined by 62.7% from a peak of $30.12 on 3 January 2022 to $11.23 on 16 October 2022, compared to a 25.4% peak-to-trough drop for the S&P 500• The stock fully rebounded to its pre-crisis peak by 17 May 2023• Since then, it has risen to a high of $149.43 on 6 January 2025 and is currently trading at approximately $115
• NVDA stock dropped by 37.6% from a high of $7.87 on 19 February 2020 to $4.91 on 16 March 2020, compared to a 33.9% decline for the S&P 500• The stock fully recovered to its pre-crisis peak by 11 May 2020
• NVDA stock declined by 85.1% from a high of $0.99 on 17 October 2007 to $0.15 on 20 November 2008, compared to a 56.8% drop for the S&P 500• The stock fully rebounded to its pre-crisis peak by 13 May 2016
NVIDIA’s Revenues have soared, increasing at an average rate of 80.1% over the past three years, which is significantly higher than the 6.3% increase in S&P 500 revenues. Nonetheless, Nvidia’s valuation remains attractive given its strong growth, with the stock trading at approximately 26x consensus FY’26 earnings. Yet, there are some potential risks. In recent quarters, net margins have exceeded an impressive 50%, propelled by robust pricing power and high demand for AI chips. Still, these margins could be at risk if demand declines or if competition intensifies. A slowdown in demand could result in reduced pricing, lower sales volumes, and a significant decline in profitability. As an aside, check out the large move in this quantum computing stock in What’s Happening With QBTS Stock?
Considering the potential for a slowdown in growth and broader economic uncertainties, ask yourself this question: Will you continue holding your NVDA stock, or will you panic and sell if it starts falling to $70, $60, or even lower prices? Maintaining a position in a declining stock is always challenging. Trefis partners with Empirical Asset Management—a Boston-area wealth manager—whose asset allocation strategies delivered positive returns during the 2008-09 period, when the S&P lost over 40%. Empirical has incorporated the Trefis HQ Portfolio in this asset allocation framework to provide clients with better returns and reduced risk compared to the benchmark index, offering a less volatile experience as demonstrated by the HQ Portfolio performance metrics.
Invest with Trefis
Market Beating Portfolios | Rules-Based Wealth"
109,https://www.forbes.com/sites/tomcoughlin/2025/03/13/kioxia-and-pliops-storage-announcements-for-the-2025-nvidia-gtc/,Kioxia And Pliops Storage Announcements For The 2025 Nvidia GTC,"Mar 13, 2025, 10:34pm EDT",Thomas Coughlin,"Digital storage and memory hold the data used to power AI training and for storing the models used in inference.  At next week’s Nvidia GPU Technology Conference several storage and memory companies will be showing their latest products and services to support the growing workloads to enable AI.  This article will talk about a couple of these developments from pre-event announcements by Kioxia and Pliops.  We will have more to say on digital storage and memory developments when the GTC begins.
Data centers often utilize multiple types of digital storage technology to trade off performance requirement and storage costs.  For instance, for artificial intelligence training fast solid-state drives are often used for feeding data to memory, usually dynamic random-access memory that supports the fast data access required for efficient and effective use of GPUs.
In the last few months most of the major SSD companies have announced high capacity QLC SSDs that are intended to provide warmer as well as hot data storage, perhaps displacing some HDD secondary storage, particularly where storage density in a rack is a major factor for secondary storage choice.  Kioxia has now joined the ranks of SSD companies offering high capacity QLC SSDs.
Kioxia announced its 122.88TB LC9 series NVMe SSD, targeted for AI applications, see image below.  This SSD is in a 2.5-inch form factor and is built with the company’s 8th generation 3D QLC 2terabit die using CMOS directly Bonded to Array to increase density on the memory die.  The drive has a PCIe 5.0 interface and dual-port capability for greater fault tolerance or for connectivity to multiple computer systems.  It can provide up to 128 gigatransfers per second.
The company says that high-capacity drives are critical for parts of the AI workload, particularly for large language models, training and storing large data sets and for the rapid reteival of information for inference and model fine-tuning.
This new SSD can also be used with the recently announced Kioxia AiSAQ technology that can enhance scalable Retrieval Augmented Generation performance by storing vector database elements on SSDs instead of on more costly DRAM.
Pliops, a supplier of solid state storage and accelerator products announced a strategic collaboration with the vLLM Production Stack developed at LMCache Lab at the University of Chicago.  This stack is targeted to greatly improve LLM  interference performance.
﻿Pliops is providing shared storage and efficient vLLM cache offloading, while LMCache Lab provides a scalable framework for multiple instance execution. The combined solution can recover from failed instances, leveraging Pliops’ KV storage backend.  The collaboration introduces a petabyte tier of memory below HBM memory for GPU compute applications. Using disaggregated smart storage, computed KV caches are retained and retrieved efficiently, significantly speeding up vLLM inference.
Data centers need storage and memory to provide the data needed for AI training and inference.  Kioxia will be showing its high capacity PCIe 5.0 SSD and Pliops will be showing its shared storage used to improve LLM inference performance."
110,https://www.forbes.com/sites/timbajarin/2025/03/12/what-to-expect-from-nvidias-gtc-conference-next-week/,What To Expect From Nvidia’s GTC Conference Next Week,"Mar 12, 2025, 10:00am EDT",Tim Bajarin,"Nvidia’s GPU Technology Conference, set for Mar. 17-21, 2025, is garnering significant attention this year. As a key player in the AI revolution, Nvidia has positioned itself at the heart of high-performance computing, essential for processing the trillions of AI data bits fueling generative AI. This year’s GTC promises groundbreaking announcements, industry insights, and a future vision that will captivate developers, researchers, and tech leaders.
Nvidia CEO Jensen Huang, a pivotal figure in the AI hardware landscape, will deliver his much-anticipated keynote on Mar. 18 in San Jose, California. When Mr. Huang speaks, Wall Street and the tech industry listen very carefully.  Known for shaping the trajectory of AI technologies, Huang’s address will highlight Nvidia's achievements and unveil innovations in new processors, software, and services that will set the tone for AI's direction in the coming year.
Nvidia remains at the forefront of AI research and development. GTC showcases transformative breakthroughs in hardware acceleration, software frameworks (like CUDA), and deep learning models. These innovations and breakthroughs are transforming the healthcare, autonomous vehicles, and finance industries.
Every year, Nvidia uses GTC to unveil new products and update its existing technologies. In an era where computational power is a key competitive advantage, announcements around next-generation GPUs, specialized AI chips, or enhancements to existing platforms are pivotal for developers and enterprise users.
GTC isn't just for Nvidia customers; it's a crucial meeting point for the broader tech community. Workshops, developer sessions, and technical tutorials enable professionals to learn about the latest tools and best practices. This educational forum helps drive innovation by allowing developers to build new applications in AI, data science, and complex simulations.
The conference often highlights Nvidia's partnerships with leading tech companies, research institutions, and governments. These partnerships accelerate the adoption of cutting-edge technologies across sectors, ensuring that innovations have a broader impact on society and the economy.
Beyond incremental hardware updates, GTC offers a forward-looking perspective on emerging technologies like edge computing, quantum-inspired algorithms, and next-generation data center architectures, providing insights into how developers may tackle computing challenges. Notably, this year’s ""Quantum Day"" is on Mar. 20. Mr. Huang and key speakers will feature discussions on quantum computing—an area where Nvidia has sparked debate with CEO Jensen Huang’s recent comments at CES that true quantum computing was ""decades away.""
With increasing discussions around AI ethics, data security, and responsible use of technology, GTC sessions often include panels and discussions on these themes. This focus ensures that as technological capabilities grow, stakeholders also consider these innovations’ broader implications and regulatory aspects.
This year's GTC encapsulates key trends driving technological innovation, from revolutionary AI research to next-generation GPU developments, emerging partnerships, and ethical considerations in computing. GTC 2025 is poised to set the stage for the next chapter in AI and accelerated computing and promises an unparalleled look into the future of technology.
Disclosure:  Nvidia subscribes to Creative Strategies research reports along with many other high tech companies around the world."
111,https://www.forbes.com/sites/davidteich/2025/03/12/the-nvidia-way-a-very-good-review-of-the-history-of-an-influential-company/,‘The Nvidia Way’: A Very Good Review Of The History Of An Influential Company,"Mar 12, 2025, 11:37am EDT",David A. Teich,"The new book shelf in my local library offered up The Nvidia Way, by Tae Kim, a book that’s worth reading. It’s a nice overview of the history of the company and of Jensen Huang, from the founding to publishing. It even gives a pointer to the shift from suits to the all-important leather jacket. For those who wish to understand how the company got to this point in its very valuably valued life, check this out.
As usual, no book is perfect. In this case, I have issues with two things. One isn’t about the book, but about corporate culture. The second point is about the author’s seeming misunderstanding about the importance of software.
The author provides an excellent view of the corporate culture. It is one of hiring very smart people and then working them very hard. The problem is that only half of that seems good. As someone who spent years in Silicon Valley, seeing multiple management methods, I’m all in favor of hiring the best. The misapprehension that working hard is the same as working smart is a problem that’s been seen before. Sure, I’ve worked very long days before a release, an important show, or other key parts of the company’s life. The problem with doing it all the time is that exhausted people do not always make the best decisions. While Tae Kim is good at pointing out the critical mistakes, there’s no way to know how many components of those mistakes came from smart people not being at their sharpest.
I understand the work habit fits certain people, such as Jenson. However, thinking that because it works for you means it works for everyone is the sign of an ego that doesn’t necessarily do the best for an organization. Flexibility and adaptability are especially critical in high tech. When to go the extra mile is a critical decision, and many people work far better when they have lives that extend beyond work, with ways of decompressing, of refreshing the mind, of letting the hind brain work on solutions.
What works much better than overwork seems to be the combination of Jensen Huang’s vision and his involvement in the process, the other things about his management style that are still valued and valid without the continual long hours.
While the major mistakes haven’t killed Nvidia, similar decisions have killed other companies, and I’d like to see what they do with a slightly different culture.
The other thing, the one that the Kim seems to have missed because of his background, is the importance of software. While it’s mentioned in the book, it doesn’t get the place it deserves. One of the critical factors in the company’s success is Jensen’s ability to realize how important software is to solve the business problems Nvidia has addressed, and he should get credit for that.
It starts early, with Huang realizing that he can virtually test chips. Virtually? Software! The importance of that leads to how the company began to work on multiple versions at once and get the more modern release schedule that has brought so much success.
Then there’s the even larger software issue that has brought the large capitalization that currently exists: artificial intelligence. The author describes how academics began early to use GPUs in multiple ways for biological sciences and more. Even then, some began working on AI. While the book described some ways the GPU was good for advanced number crunching, it would have better told the readers about AI.
When I first studied in the 1980s, we already had the basics of neural network theory defined. Mr. Kim mentions that. What isn’t really brought to light is the overlap with GPUs. A screen is a bunch of rows of aligned pixels. Calculations are done for each.
A neural network, the heart of deep learning, is a bunch of rows of a bunch of nodes. Each node has calculations. Academics quickly saw that they could use the GPU to imitate neural networks. The author points out that “The secret to the GPU’s success is parallel computing.” Not only that. The matrix structure led to the GPU’s adoption even as parallel computing was taking hold on CPUs.
What Jensen Huang saw early was the market potential brought by another mis-explained bit of Nvidia technology, CUDA. It’s represented in the book as primarily a hardware architecture. In my opinion, biased by my background, its primary benefit is software. CUDA is the tool for programming the GPUs that’s powerful and portable. You can code there and have it run on multiple Nvidia products. That’s critical. It also allowed the company to quickly add software routines that enhanced the ability of programmers to run AI.
Throughout the life of the company, Jensen Huang wasn’t focused solely on hardware. His ability to see the importance of software meant that the pairing worked to help the market, which helped the company.
Despite current market uncertainties, the future of Nvidia looks good. However, one minor final issue with the book is the continued focus on the GPU. Artificial intelligence has overtaken graphics for the company, the market and the world. What I see is that, in competition with the many companies working on AI specific chips, is that Nvidia will eventually drop the GPU name from AI lines. While the Blackwell still uses the GPU name, it’s not really a GPU. Nobody is going to drop it into a computer for graphics.
What Nvidia and a few other companies did for the GPU market massively changed how we look at our computers. The GPU still matters and Nvidia will continue to support that sector. However, AI is a very different sector, and the differentiated product lines are already clear. Still, that’s prognostication.
Tae Kim’s book, even given the things mentioned above, is a book that nobody interested in the history of Nvidia and it’s current and future impact upon AI should miss. It’s a nice read."
112,https://www.forbes.com/sites/investor-hub/article/palantir-vs-nvidia-which-is-the-better-ai-stock-buy/,Palantir Vs. Nvidia: Which Is The Better AI Stock Buy?,"Mar 09, 2025, 01:00pm EDT",Jason Kirsch,"The artificial intelligence revolution is quickly reshaping industries, economies and investment strategies. Palantir Technologies (PLTR) and Nvidia Corporation (NVDA) consistently capture investor attention in this space. While both are recognized as major players in the AI ecosystem, they represent fundamentally different approaches to capitalizing on the technology's exponential growth.
This article examines these AI powerhouses through multiple lenses: their core business models, technical capabilities, financial performance and prospects. We'll analyze their strengths, weaknesses, and positioning within the evolving AI landscape to determine which might represent the superior investment opportunity for different types of investors in today's market.
Artificial intelligence has evolved from a theoretical concept to an increasingly mainstream commercial reality. As of early 2025, we're witnessing the rapid proliferation of generative AI applications across industries following the breakthrough success of large language models. Organizations worldwide are racing to implement AI solutions to enhance productivity, reduce costs, and create innovative products and services. IDC projects global AI spending to surpass $300 billion in 2025, with a compound annual growth rate exceeding 25% through 2028.
The AI infrastructure market has become particularly critical, with demand for specialized hardware, chip, and software platforms creating supply constraints and enormous business opportunities. Recent developments, such as the Chinese research lab DeepSeek demonstrating the ability to train competitive AI models at a fraction of traditional computing power requirements, challenge conventional assumptions about AI infrastructure needs. Despite such disruptions, major tech companies, including Meta Platforms, Alphabet, and Amazon have doubled down on AI investments, with Meta planning to spend up to $65 billion on AI infrastructure in 2025, Alphabet projecting $75 billion, and Amazon potentially exceeding $100 billion in AI-related investments.
Before weighing these companies as investment opportunities, it's essential to understand their distinct business models, revenue streams and approaches to AI. While both operate in the broader AI ecosystem, they occupy very different positions. Palantir focuses on AI-powered data analytics software, while Nvidia provides the fundamental hardware infrastructure that powers much of today's AI revolution.
Palantir Technologies specializes in data integration, analytics and AI-powered decision support platforms for complex, high-stakes environments. The company offers two primary platforms: Gotham, which serves government and defense customers, and Foundry, which targets commercial enterprises across manufacturing, healthcare and financial services. Both platforms excel at integrating disparate data sources into cohesive, actionable intelligence using sophisticated AI algorithms.
What distinguishes Palantir is its focus on ""AI for the real world"" – building systems that combine human judgment with machine learning to solve concrete operational problems rather than pursuing general artificial intelligence. The company has demonstrated remarkable success in securing government contracts, including significant work with defense and intelligence agencies, providing a stable revenue base with high switching costs once implemented.
Palantir's commercial business has gained momentum more recently, with particular strength in industries dealing with complex physical operations and supply chains. The company's Artificial Intelligence Platform (AIP), launched in 2023, represents its most direct play in the generative AI space, allowing organizations to securely connect their proprietary data to large language models while maintaining control and governance. This ""private AI"" approach addresses critical data security and intellectual property concerns that have limited enterprise AI adoption.
Nvidia Corporation, founded in 1993 by Jensen Huang, Chris Malachowsky and Curtis Priem, began as a graphics processing unit (GPU) manufacturer focused on gaming and visual computing. Over the past decade, the company has transformed itself into the dominant provider of computational infrastructure powering today's AI revolution. Initially designed for rendering graphics, Nvidia's GPUs proved exceptionally well-suited for the parallel processing requirements of neural networks and other AI algorithms.
The company's business model spans hardware and software. Its primary revenue stream comes from designing and selling specialized chips – particularly its data center GPUs like the H100, H200 and the newer Blackwell series – which have become the industry standard for training and running sophisticated AI models. Nvidia's latest Blackwell architecture, particularly the GB200 NVL72 system, offers performance capabilities that are 30 times faster than previous generations for AI inference tasks, positioning Nvidia to maintain its market leadership.
Beyond AI, Nvidia maintains significant gaming, professional visualization, and automotive technology business segments. However, its data center segment, driven by AI adoption, has become the company's most prominent and fastest-growing revenue source, contributing nearly 88% of total revenue. With a staggering market capitalization of $3.4 trillion as of early 2025, Nvidia has captured an astounding 98% market share in the data center GPU market, benefiting from its technological leadership and the network effects created by its extensive software ecosystem.
Both companies play crucial but fundamentally different roles in the AI ecosystem. While Nvidia provides the foundational infrastructure enabling AI development and scale deployment, Palantir focuses on applying AI technologies to solve specific business and organizational problems. Let's examine their distinct approaches and capabilities.
Palantir's strength lies in making AI operational and accessible to organizations that lack extensive technical expertise. Its Artificial Intelligence Platform (AIP) enables clients to implement generative AI and other advanced capabilities within their existing data environments, focusing on the ""last mile"" problem deriving business value from AI technologies. This approach addresses a critical gap in the market between raw AI capabilities and practical business applications.
What distinguishes Palantir is its focus on AI orchestration rather than fundamental AI research or development. The company excels at building systems integrating various AI models (including those from other providers) with an organization's proprietary data and workflows. This positions Palantir as an AI solutions provider rather than a technology creator, helping clients navigate the complexities of implementation and achieve measurable outcomes.
Palantir is uniquely positioned to capitalize on emerging frameworks like the Stargate AI Project due to its specialized expertise in integrating disparate systems and data sources. As frameworks enable more AI systems to communicate and share capabilities, the complexity of managing these interactions within regulated, security-conscious environments increases dramatically. Palantir's proven ability to build mission-critical systems that maintain governance while enabling powerful analytics creates a natural entry point for the company to become a key implementation partner for enterprise AI deployments.
Nvidia is the fundamental enabler of modern AI, providing the computational infrastructure on which virtually all large-scale AI development depends. Its GPUs have become the de facto standard for training and running sophisticated AI models, with the company maintaining a commanding market share exceeding 80% in AI accelerators. This position gives Nvidia unparalleled influence over the direction and pace of AI advancement.
The company's AI capabilities extend well beyond hardware. Nvidia has developed a comprehensive software ecosystem, including CUDA, its parallel computing platform; various AI frameworks and libraries; and specialized tools for different AI applications. This combination of hardware and software creates significant competitive advantages and switching costs, as AI developers and researchers build their workflows around Nvidia's technology stack.
Nvidia's influence in AI continues to grow through strategic initiatives like the CUDA ecosystem, enterprise-focused AI systems, and partnerships with major cloud providers. The company has evolved from simply selling chips to offering complete AI systems and development platforms. Its newest Blackwell architecture represents significant advances in AI computing capabilities, delivering vastly improved performance and energy efficiency compared to previous generations and helping maintain Nvidia's technological lead over emerging competitors.
Both companies have delivered impressive financial results driven by AI adoption, but their scale, profitability profiles and growth trajectories differ significantly. Examining these metrics provides an important context for evaluating their relative investment potential.
Palantir has demonstrated accelerating growth and improving profitability in recent quarters, achieving GAAP profitability for six consecutive quarters as of Q3 2024. The company reported revenue of $2.4 billion for the trailing twelve months, representing year-over-year growth of 27%. This growth has been driven primarily by commercial sector expansion, with U.S. commercial revenue growing at approximately 40% year-over-year, substantially outpacing government revenue growth.
The company's gross margins remain robust at approximately 80%, reflecting the software-based nature of its business. Operating margins have improved significantly, reaching 20% on a GAAP basis, as Palantir has reduced stock-based compensation and scaled its business. The company's customer count continues to grow, particularly among commercial clients, surpassing 300 as of the latest reporting period.
Palantir's stock has experienced significant volatility in early 2025. After surging to record highs following strong earnings in early February, the stock retreated sharply amid concerns over potential U.S. defense spending cuts, tumbling 30% from its peak. Despite this pullback, PLTR remains 17% higher year-to-date and has quadrupled in value over the past year, reflecting strong long-term performance. Analysts are divided on the recent sell-off—some view it as a buying opportunity given Palantir's central role in federal data initiatives. In contrast, others point to technical indicators suggesting further downside risk if key support levels fail to hold.
Nvidia has delivered exceptional financial performance, establishing itself as one of the market's strongest growth stories. In its most recent quarter (Q3 fiscal 2025), Nvidia reported record Data Center revenue of $30.8 billion, representing a 112% year-over-year increase. Total revenues reached an all-time high of $35.1 billion, up 93.6% from the previous year, while net income grew by 108.9% to $19.3 billion.
The company's gross margins have expanded to approximately 72%, reflecting Nvidia's strong pricing power and the premium value of its AI chips. Operating margins have similarly improved, exceeding 50% on a non-GAAP basis. Net income has grown even faster than revenue, trailing twelve-month earnings growing over 150% year-over-year.
Nvidia's stock has demonstrated remarkable strength, surging 136.7% over the past 52 weeks and significantly outperforming the S&P 500 Index's 25% gain. However, the stock trades approximately 11% below its early January 2025 record high, following concerns about DeepSeek's technological breakthrough. Current market sentiment remains decisively positive, with 36 out of 43 analysts maintaining ""Strong Buy"" ratings on the stock. The mean price target suggests a potential 25.4% upside from current levels, reflecting continued optimism about Nvidia's growth prospects.
The accelerating adoption of AI creates distinct opportunities for both companies, though their positioning within this landscape differs significantly. Understanding how broader AI trends affect each company provides an essential context for evaluating their investment potential.
Palantir stands to benefit significantly from the enterprise AI implementation gap, which is the challenge organizations face when moving from AI experimentation to operational deployment. As companies increasingly focus on deriving measurable business value from AI investments, Palantir's expertise in creating practical, production-ready AI applications positions it well to capture this growing market. Their AIP platform addresses the ""last mile"" problem in AI adoption.
The company's ""private AI"" approach addresses critical concerns around data security and intellectual property that have limited enterprise AI adoption. By allowing organizations to securely connect their proprietary data to large language models while maintaining control and governance, Palantir offers a solution that balances powerful AI capabilities with enterprise requirements for security and compliance.
Palantir is positioning itself as a critical middleware layer in emerging AI ecosystems, helping organizations safely expose their data to interconnected AI systems while maintaining security and compliance. With accelerating commercial adoption, improving financial metrics, and a long track record of successfully implementing complex systems integration projects, Palantir represents a higher-risk but potentially higher-reward opportunity to invest in the infrastructure layer that will enable practical AI implementation across enterprises and government agencies.
Nvidia's position as the foundational infrastructure provider for AI development gives it unparalleled exposure to the growth of technology. As AI models grow larger and more sophisticated, the demand for advanced computational capabilities continues to increase, benefiting Nvidia disproportionately given its dominant market position. The company effectively captures value from the entire AI ecosystem rather than competing within specific application segments.
The company's technological lead appears substantial and sustainable, with its newest Blackwell architecture maintaining performance advantages over competitors. CEO Jensen Huang's recent comment about ""insane"" demand following Blackwell's broad release at the end of 2024 suggests a strong market reception. Major customers' aggressive infrastructure expansion plans further support the outlook for sustained demand growth.
Nvidia's current valuation metrics, including a P/E ratio of 40x, represent a nearly 35% discount to its 10-year average P/E ratio of 60x (March 6, 2025). Based on forward earnings expectations for fiscal 2026 of $4.44 per share, the stock trades at an even more attractive forward P/E of the low 20s. This valuation gap suggests significant potential upside, as the stock would need to appreciate by over 100% just to trade in line with its historical average P/E ratio, assuming current earnings forecasts prove accurate.
Despite their strong positions, both companies face significant risks that investors should carefully consider. For Palantir, competition from established enterprise software providers and specialized AI startups represents a growing threat. Major cloud providers increasingly offer AI implementation and orchestration tools that overlap with Palantir's offerings. The company's exposure to government contracts, particularly in defense and intelligence, creates vulnerability to policy changes and potential defense spending cuts, as evidenced by the recent stock volatility.
Palantir also faces political and reputational challenges related to its government work. This association creates controversy in some markets and may limit its commercial expansion in specific regions or industries. The company's relatively high valuation and recent stock volatility leave little room for execution missteps, with any slowdown in growth likely to result in significant share price fluctuations.
Nvidia faces different but equally substantial risks. The emergence of efficient training methods demonstrated by DeepSeek could potentially impact demand for high-end GPUs, particularly if similar approaches gain widespread adoption. While major customers like Meta Platforms suggest that any training workload reduction could be offset by increasing inference demands, as newer AI models require more computational power for operation, these evolving dynamics create uncertainty.
Competition in the AI chip market is intensifying, with both established players and startups working to develop alternative solutions. The global semiconductor supply chain remains critical to Nvidia's ability to meet surging product demand. While the company has successfully navigated previous supply constraints, maintaining sufficient manufacturing capacity and efficient distribution channels will be crucial for meeting ambitious growth expectations. Despite its strong position, Nvidia's high valuation leaves little room for execution missteps, as evidenced by a 7% decline following Q3 earnings despite strong results.
Palantir and Nvidia represent compelling but distinctly different AI investment opportunities, with the better choice depending mainly on investor priorities, time horizons and risk tolerance. For investors seeking exposure to the foundation of the AI revolution with proven financial results at a massive scale, Nvidia represents the more precise choice. Its dominant position in AI infrastructure, exceptional financial performance, and continued technological leadership make it a core holding for those looking to participate in AI's growth trajectory.
Nvidia's broader diversification across multiple segments provides some insulation against volatility in any single market. Its exceptional margins and cash generation offer downside protection, while its proven ability to execute at scale reduces operational risk. Despite a premium valuation, Nvidia's market position, technological moat and the multi-year growth runway for AI infrastructure suggest continued strong performance, with analysts expecting 43.2% year-over-year EPS growth for fiscal 2026.
Palantir offers a compelling alternative for investors seeking a smaller company with potentially higher growth upside and more direct exposure to enterprise AI adoption. Its improving financial profile, accelerating commercial growth, and positioning as an AI solutions provider rather than a pure technology creator differentiate it from other options in the space. Palantir's focus on making AI operational for organizations provides exposure to the ""last mile"" of AI implementation, representing a substantial market opportunity as enterprises move beyond experimentation to production deployment.
While Palantir has significant long-term potential with its expanding AI and data analytics capabilities, it also faces substantial competitive risks. The company’s reliance on government contracts, niche enterprise solutions, and emerging AI ambitions make it vulnerable to disruption from larger tech players with more diversified offerings. If industry giants like Microsoft, Google or Amazon further develop their AI-driven analytics platforms, Palantir could struggle to maintain its competitive edge. In contrast, Nvidia is a proven market leader with consistent revenue growth, dominant AI and GPU technology and deep industry integration. As one of the key enablers of the AI revolution, Nvidia is shaping the future rather than just participating in it. While PLTR has upside, NVDA carries significantly lower long-term risk due to its brand dominance, scalable technology, and essential role in AI infrastructure.
Bottom Line
While Palantir and Nvidia represent strong contenders in the AI space, Nvidia emerges as the superior investment for most investors, given its dominant market position, exceptional financial performance, technological leadership, and crucial role in the broader AI ecosystem. Palantir offers compelling exposure to enterprise AI implementation but comes with higher execution risk and more intense competition. For investors with sufficient capital, a balanced approach might include Nvidia as a core AI infrastructure holding and Palantir as a more minor position offering exposure to enterprise AI solutions. As with any technology investment, maintaining awareness of evolving competitive dynamics and technological shifts remains essential for long-term success."
113,https://www.forbes.com/sites/greatspeculations/2025/03/06/cycles-views-on-apple-nvidia-meta-platforms-and-netflix/,"Cycles Views On Apple, Nvidia, Meta Platforms And Netflix","Mar 06, 2025, 07:40am EST",Bill Sarubbi,"The Q1 correction is unfolding. February in a post-election year has been the weakest in the 48-month ‘presidential’ cycle. February 2025 has certainly continued this tradition. The indices are likely to bottom later in March. Cycles have been helpful in technology stock positioning. The “Fab Seven” stocks are the focus. Let us look at four of the seven by making cycle projections into later 2025.
Apple began its correction before most stocks in the technology sector did so. Because Apple began its correction early, it is likely to recover before the other tech equities do.  Relative strength bottomed in November. The seasonality shows that the stock is usually strong in the first five months of any year. At present, the monthly price cycle points to a September high.
Contrast Apple to Nvidia. The monthly cycle hit a high in June, signaling a short-term correction into August. The November cycle peak led to the autumn price peak. Note that this high occurred as the company appeared on the cover of Barron’s. This favorable review was a contrary opinion sell signal. The stock has made a marginal new high and has now traced out a pair of lower highs. There is a very short-term cycle low now. But note that this longer-term cycle does not bottom until Q3. The cycle measure suggests underweighting the stock into September when a monthly cycle low forms.
The Meta Platforms cycle bottoms now and rallies to months end. The cycle then suggests an irregular trading range to the downside. The stock does not appear to be a stable holding. META is likely to be a trading stock in 2025.
The monthly Netflix cycle bottoms now. The stock appears poised to rally into late June. This stock can be overweighted. Relative strength bottomed in 2022 and continues to rise.  This is one of the strongest stocks of the Fab Seven. Netflix is likely to extend its relative leadership."
114,https://www.forbes.com/sites/greatspeculations/2025/03/05/nvidia-stock-are-tariff-fears-overblown/,Nvidia Stock: Are Tariff Fears Overblown?,"Mar 05, 2025, 04:30am EST",Trefis Team,"Nvidia stock fell nearly 9% during Monday’s trading session, while the broader Nasdaq index declined by just over 2%. The market-wide sell-off coincided with U.S. President Donald Trump’s confirmation that his administration would impose 25% tariffs on imports from Canada and Mexico starting Tuesday. Additionally, Nvidia’s drop was further exacerbated by investigations into Chinese buyers suspected of bypassing U.S. export controls on advanced semiconductor chips. How will these developments affect Nvidia stock?
We do not believe that the Trump Administration’s initial tariffs will have a material impact on Nvidia. Nvidia’s chips are primarily manufactured by TSMC in Taiwan; however, some systems and computers utilizing these chips are produced in other regions, including Mexico. It is possible that these segments of the business might experience some impact. Nonetheless, the core of Nvidia’s operations—the high-margin GPU division—should remain mostly unaffected. Last month, President Trump also suggested the possibility of a “25% or higher” tariff on all semiconductor chips imported into the United States. Investors will need to monitor this development closely, although we believe it will not have a significant impact on Nvidia’s bottom line. As of 2024, Nvidia reported an adjusted gross margin of approximately 75.5%, indicating that the cost of its imported products is likely less than 25% of its revenues. Furthermore, since Nvidia generates around 47% of its sales from the U.S., the impact on its margins is probably even more constrained. In addition, TSMC, which is Nvidia’s main contract manufacturer for GPUs, has announced plans to invest roughly $100 billion in new chip fabrication facilities in the United States. Nvidia executives have indicated that they plan to produce chips at these new facilities, potentially helping the company mitigate long-term tariff risks. However, if you are seeking growth with less volatility than an individual stock, consider the High-Quality portfolio, which has outperformed the S&P 500 and achieved returns greater than 91% since its inception.
The United States has implemented export control restrictions on most of Nvidia’s newest AI chipset offerings to China, with the exception of the H20 chips, due to national security concerns. Nevertheless, reports indicate that gray market resellers are utilizing entities registered outside China to purchase servers incorporating Nvidia’s latest offerings from companies based in several countries, such as Singapore, Malaysia, Taiwan, and Vietnam. This concern is substantiated by the fact that Singapore has emerged as Nvidia’s second-largest market, representing approximately $23 billion in sales in FY’25, or about 18% of revenue, compared to only $2.3 billion, or 8% of revenue, in FY’23. Singapore has now launched an investigation into these potential loopholes.
The emergence of China’s DeepSeek open-source AI model also suggests that the country is advancing significantly in AI, with both prominent startups and major companies like Alibaba and Baidu investing heavily in AI infrastructure. Should Chinese companies bypass sanctions and face stricter enforcement measures that eventually close these loopholes, Nvidia’s revenues could be affected.
Over the past four years, NVDA stock's performance has been inconsistent, with annual returns displaying much greater volatility than the S&P 500. The stock returned 125% in 2021, -50% in 2022, 239% in 2023, and 171% in 2024. In contrast, the Trefis High Quality Portfolio, which comprises 30 stocks, exhibits significantly less volatility. It has also comfortably outperformed the S&P 500 over the last four years. Why is this the case? Overall, HQ Portfolio stocks have delivered superior returns with reduced risk compared to the benchmark index, resulting in a less turbulent performance as demonstrated in the HQ Portfolio performance metrics. In light of the current uncertain macroeconomic climate, marked by potential rate cuts and several conflicts, might NVDA experience a scenario similar to 2022 and underperform the S&P over the next 12 months, or will it experience a robust surge?
We estimate Nvidia stock to be valued at approximately $101 per share, which is about 20% below the current market price. Refer to our analysis of Nvidia valuation: Expensive or Cheap. There are several reasons why we currently hold a negative outlook on the stock. We believe that the AI wave, driven by a “fear-of-missing-out” over the past two years, might subside due to diminishing incremental performance improvements from larger models and because high-quality training data may become a bottleneck. This transition towards more efficient models could further intensify the effects of a potential slowdown for GPU manufacturers like Nvidia. Additionally, Nvidia is encountering increasing competition from companies such as AMD and even from its own clients, like Amazon, which are focusing on developing and deploying their own AI chips. Although Nvidia boasts a comprehensive software ecosystem surrounding its AI processors—including programming languages that help secure customer loyalty—the company could still face pressure. Its premium valuation might not fully account for these risks at present.
Invest with Trefis
Market Beating Portfolios | Rules-Based Wealth"
115,https://www.forbes.com/sites/antonyleather/2025/03/04/the-nvidia-geforce-rtx-5070-reviews-go-live-whats-the-verdict/,The Verdicts Are In On Nvidia’s GeForce RTX 5070 And They’re Mixed,"Mar 04, 2025, 10:08am EST",Antony Leather,"Reviews went live Tuesday for Nvidia’s fourth GeForce RTX 50-series graphics card, the RTX 5070. Slated to retail from $549, it’s competing directly with AMD’s recently announced Radeon RX 9070 and 9070 XT, which will cost from $550 and $599 respectively following AMD’s full announcement last week.
The RTX 5070 is the first RTX 50-series model to dip below 16GB of video memory, offering 12GB of GDDR7 memory, which is 4GB less than the 16GB the RTX 5070 Ti has. It also matches the RTX 4070 Super and RTX 4070 Ti in the amount, while the RTX 4070 Ti Super has 16GB.
As well as the climb down in memory amount, the new card has nearly 3,000 fewer CUDA cores and a narrower 192-bit bus. Both of AMD’s Radeon RX 9070-series cards will have 16GB of video memory, potentially putting Nvidia at a disadvantage in some situations.
FEATURED | Frase ByForbes™
Unscramble The Anagram To Reveal The Phrase
Pinpoint By Linkedin
Guess The Category
Queens By Linkedin
Crown Each Region
Crossclimb By Linkedin
Unlock A Trivia Ladder
TechPowerUp’s review has a huge array of games tested both with and without ray tracing but raises concerns about continuing stock levels of all RTX 50-series cards and the likelihood that the RTX 5070 will cost far more than the claimed $549.  At 1080p, the RTX 5070 is just ahead of the RTX 4070 Super and offers more performance overall than the RTX 3090, RX 7900 GRE and RTX 3080.
The same is true in 1440p gaming, while stepping up to 4K sees the RTX 3090 sit much closer, and the RTX 5070 still offers a decent lead over the  RX 7900 GRE and RTX 3080 again. The graphs suggest that for anyone with an RTX 3070 or 3070 Ti, the RTX 5070 offers a huge upgrade, but not so much from the RTX 4070, unless of course you’re talking about making use of Nvidia’s new RTX 50-series DLSS4 feature, Multi Frame Generation.
“At 1440p, with pure rasterization, without ray tracing or DLSS, we measured a 22% performance uplift over the RTX 4070, which is a bit lower than expected, but alright I’d say,” noted the TechPowerUp review. “At 4K, the increase is 25%, which is certainly better than the meagre 15% that we got on RTX 5080, the RTX 5090 got +36%, and the RTX 5070 Ti was 27% faster.”
It went on to say, “As mentioned before, AMD is launching their Radeon RX 9070 and RX 9070 XT tomorrow, at $550 and $600, which could introduce strong alternatives in this segment. If you are interested in a RTX 5070, definitely wait for the AMD reviews tomorrow.”
Hardware Unboxed has undertaken another large test of games both with and without ray tracing. Below are the non ray-traced results with averages across a range of titles.
Below are Hardware Unboxed’s six game averages using ray tracing with the RTX 5070 compared to a number of other cards including the RTX 3070, RTX 4070 and RX 7800 XT. Again, anyone with an RTX 3070 will see a huge boost opting for the RTX 5070, but the latter isn’t much faster if at all than the RTX 4070 Super.
The review’s conclusion stated, “A compromised product that will struggle to leverage the RTX feature set due to its limited 12GB VRAM buffer. Although that might not be a significant issue right now, I expect it will be within the lifespan of this product. Performance is nothing special. It’s basically a refreshed RTX 4070 Super with $50 knocked off the MSRP. I imagine it will pay to wait just a little bit (to see AMD RX 9070 reviews). I suspect spending another $50 on the RX 9070 XT is going to be the real answer.  In a nutshell the RTX 5070 hasn’t impressed us.”
The next 12 months will be critical in the graphics card market with Nvidia having a near monopoly and figures of between 80-90% share of the desktop market depending on what data you look at. It’s an enviable position, brought about by a range of factors from a glut of sales during the crypto mining boom a few years ago at a time when AMD was struggling more with supply issues. Nvidia has always invested heavily into its drivers and content creation, too.
However, with high prices and their resulting complaints from enthusiasts having plagued its graphics cards for a number of generations and recent supply issues with its RTX 50-series exacerbating the problem, the door is open for the competition to strike a blow. That could come in the form of AMD’s Radeon RX 9070 series. AMD has made no secret of the fact it does not intend to compete with Nvidia at the high end with its next generation of graphics cards.
This is for one reason: To allow it to focus on regaining market share in the far more populous mid-range, hopefully offering a range of graphics cards that perform well in their own right and also represent better value compared to Nvidia. However, it’s not just about performance or price, as Nvidia’s models have generally been considered expensive for a while. In fact, its previous flagship, the RTX 4090, while eye-wateringly expensive at over $1,600, still sold well and at one point late last year even topped 1% market share in Steam’s Hardware Survey, outstripping every AMD discrete desktop graphics card.
As both TechPowerUp and Hardware Unboxed have stated, there are some key product launches to watch out for if you’re considering a new graphics card for your gaming PC, especially in a similar price range to the RTX 5070. The most important is AMD’s new RDNA4-based Radeon RX 9070-series, including the RX 9070 XT and 9070, with reviews of those cards expected this week, and retailing for $599 and $549, respectively. Currently there are no official launch windows for anything cheaper, such as Nvidia’s RTX 5060 or 5060 Ti as well as AMD’s RX 9600 models."
116,https://www.forbes.com/sites/petercohan/2025/02/27/salesforce-stock-falls-on-weak-generative-ai-view-after-nvidia-earnings/,Salesforce Stock Falls On Weak Generative AI View After Nvidia Earnings,"Feb 27, 2025, 08:58am EST",Peter Cohan,"Salesforce and Nvidia’s latest numbers left investors with questions about the future of generative AI. On Wednesday, Salesforce reported annual results and guidance below estimates, noted Reuters, while Nvidia’s Q4 2024 earnings and outlook beat expectations, according to the Wall Street Journal.
The companies’ share prices did not respond quite the way I thought they would. While Salesforce’s stock trades down 3% — which makes sense to me; I thought Nvidia’s clear beat and raise would send shares soaring — instead they were down 1.5% before rising 2% in after-hours trading.
What message is the market sending? Could the lagging adoption of Salesforce’s Agentforce — aimed at helping companies build AI agents to, say, plan a vacation, then search for and book the best flights, hotels, and restaurants, as I described in my book, “Brain Rush” — bode ill for the future of generative AI?
Nvidia’s future may depend on whether the benefits of generative AI for enterprises and individuals exceed the costs and risks of building and operating the technology. In my view, a killer app — akin to the iTunes store that drove a six-fold boost in iPod sales in 2003 — must emerge before a payoff for generative AI becomes clear, notes “Brain Rush.”
Salesforce is bullish on Agentforce. “A lot of other vendors are talking about their agent capabilities, but few are able to show that they’ve got this really running at scale,” Salesforce co-founder and CEO Marc Benioff said on a February 26 conference call with analysts.
Despite the 17% drop in Nvidia’s stock price the day after my Forbes post on the topic, the AI chip designer is thrilled by DeepSeek’s success. “DeepSeek was fantastic,” Nvidia CEO Jensen Huang told CNBC. “It was fantastic because it open sourced a reasoning model that’s absolutely world class. The amount of computation necessary to do that reasoning process is 100 times more than what we used to do,"" he added.
While they set higher 12-month price targets, analysts have mixed views on where their shares are heading. Strong Blackwell demand may not satisfy analysts who see Nvidia’s valuation as high in light of the company’s declining margins and slowing growth. Analysts see Salesforce as struggling to monetize Agentforce fast enough to boost growth.
I have contacted Salesforce to request comment and will update this post if I receive a reply.
Salesforce fell short of expectations for performance and prospects while Nvidia beat and raised February 26.
Salesforce’s performance for the quarter ending January 31 was mixed. Revenue rose 7.6% to $9.99 billion — $50 million short of the London Stock Exchange Group consensus while the company’s adjusted earnings per share of $2.78 beat expectations by 17 cents a share, noted CNBC.
Salesforce’s estimate for the April-ending quarter was below the LSEG consensus. The company forecast $9.73 billion in revenue — $180 million below estimates while its adjusted EPS forecast of $2.54 fell seven cents short of the LSEG consensus.
Agentforce has yet to generate significant revenue. Salesforce said it has completed “more than 3,000 paid deals involving Agentforce since October,” reported CNBC. The service will “make a modest contribution to revenue in fiscal 2026, with a larger effect in the following year,” Salesforce’s departing CFO Amy Weaver told investors, CNBC noted.
Customers are skeptical of AI agents in general. Agentforce coordinates a series of AI chatbots during a process like booking travel. To create value for users, the service must solve a company’s specific business problem quickly with no errors at a lower cost. Some companies are working out the kinks to realize that vision, according to my February 14 Forbes post.
One analyst echoed this observation. ""Given how poor initial generative AI experiments were for many companies, they’re not just writing blank checks until Salesforce shows them Agentforce actually works,"" Valoir CEO Rebecca Wettemann told Reuters. “The next quarter or two will be critical for Salesforce,” she added.
By contrast, Nvidia’s earnings and outlook exceeded expectations while the company’s latest Blackwell chips were in high demand. Nevertheless, analysts saw flaws in the form of declining margins, slowing growth, and uncertainty regarding the impact of tariffs on China.
Nvidia told investors its January-quarter ending revenue and net income exceeded expectations. Sales rose 78% to $39.3 billion in the quarter while net income soared 80% to $22 billion — beating the FactSet consensus, according to the Wall Street Journal.
Strong Blackwell sales — which hit $11 billion — and a higher-than-expected revenue forecast for the April-ending quarter also bolstered the bull case. Nvidia’s projection of $43 billion in sales for the current quarter — a 65% increase — was about $1 billion more than analysts projected, the Journal noted.
Nvidia growth is amazing compared to most companies — yet the slowdown from 78% in the last quarter to 64% in the current one is a far cry from the faster than 200% growth the company enjoyed for several quarters last year, as I noted in my Tuesday Forbes post.
Moreover, the company’s profit margins narrowed due to the higher cost of making its newest data-center equipment. In addition, Nvidia’s operating expenses grew 48% due to “bigger pay packages for an expanding workforce,” CFO Colette Kress told investors, according to the Journal.
Export restrictions and competitive pressure from Huawei have reduced Nvidia’s revenue in China, Huang told CNBC. He expects developers to use software to find ways around the export restrictions.
Meanwhile the White House could ban more Nvidia chip sales to China and put tariffs on the company’s chips made in Taiwan. ""It’s an unknown until we understand further what the U.S. government’s plan is, both its timing, its where and how much,” Cress told investors, according to the New York Times. “We are awaiting.”
Analysts see considerable upside in shares of Salesforce and Nvidia. Salesforce stock could climb 25.2% to reach the average price target of $384.78 from 41 Wall Street analysts while Nvidia stock may pop 36% to reach $178.56 — the average price target of 37 Wall Street analysts, noted TipRanks.
Yet those numbers mask some analyst discontent.
One investor expressed concern about Salesforce. “Salesforce seems to be struggling a bit,” SWBC chief investment officer Chris Brigati told Bloomberg. The reports from Nvidia and Salesforce “suggest the AI trade won’t be anywhere near as robust as it was in 2023 or 2024, though it remains positive, with some room to grow. It’s like we’re dropping from fifth gear to fourth,” he added.
Another analyst echoed this sentiment. “With Nvidia lacking the spark that it usually brings, probably U.S. stocks will continue to lack a near-term driver and the rotation trade to China/Europe could find more runway,” Saxo Markets chief investment strategist Charu Chanana told Bloomberg.
The results are “still not enough to address and calm"" the concerns around geopolitics, tariffs and the shifting landscape in AI trade, she added.
Another skeptical analyst asked whether Nvidia has topped out. Is this ""as good as it gets for Nvidia?"" wrote D.A. Davidson analyst Gil Luria in a note featured by MarketWatch.
“Despite demand in the near term continuing to be strong, we still believe a decline in demand for Nvidia compute is inevitable as customers begin to scrutinize their [return on investment] on AI compute,” Luria wrote.
Many analysts felt upbeat about Nvidia. “The supply chain should continue to improve and we see no signs of demand issues,” Jefferies analyst Blayne Curtis noted in a report. While the company suffered a “downtick in gross margins,” completing Nvidia’s transition to Blackwell will resolve the problem, Curtis added.
Unless the AI trade shifts back to fifth gear, investors may enjoy better gains elsewhere."
117,https://www.forbes.com/sites/jjkinahan/2025/02/26/nvidia-reports-earnings-after-the-close/,Nvidia Reports Earnings After The Close,"Feb 26, 2025, 08:38am EST",JJ Kinahan,"Key Takeaways
The S&P 500 and Nasdaq Composite are in the midst of a four-day losing streak following losses on Tuesday of 0.5% and 1.3%, respectively. The Russell 2000 was down a fraction of a percent; however, the Dow Jones Industrial Average was the one index up on the day, gaining just under 0.5%. While the beginning of the week has felt slow, rest assured that will change starting Wednesday, after the close.
If there is one stock that continues to have a potential hold on the market, it’s Nvidia. The chipmaker is scheduled to report earnings after the close. Looking back at the last four quarters of earnings, the stock notched gains on the day after announcing three out of four times. The sole down day was this past November when the stock fell 1%. But in the three quarters prior, shares were up 3% in August, 2% in May and 16% last February. Now, prior performance is not something I rely on, but it is worth noting the stock has dropped 6% this year and is off almost 18% from its all-time intraday high set in January. That may make it attractive for many who have waited for a pullback to get long. Heading into earnings, the options market is implying a move of roughly $13 by Friday, which would mean a range of $113 to $139.
A few other stocks in the news this morning include Lowe's. The big news here was that same store sales saw an increase for the first time in nearly two years. That stock is currently higher by 4% in premarket. Shares of General Motors are also up 4% in premarket. The automaker announced a $6 billion stock buyback program and plans to raise its dividend by 25%. Lastly, Alcoa's CEO is warning that tariffs on the aluminum industry could result in the loss of 100-thousand U.S. jobs as a result of tariffs.
There is one economic report due out on Wednesday: new home sales. I don’t think this report will garner much in terms of headlines, especially given so much focus on Nvidia. However, it’s a number that may help contextualize the overall economy. The bigger economic reports will come Thursday and Friday. The most recent Durable Goods report is scheduled to come out before the open Thursday, and then on Friday, the Personal Consumption Expenditures report will be released. The PCE has been the report the Federal Reserve relies on most heavily for gauging inflation.
For Wednesday, all eyes will be looking toward Nvidia. I expect much of the chipmaker sector see some positioning ahead of the report. We have heard multiple times this quarter about companies planning greater and greater investment in artificial intelligence and it would make sense that Nvidia would be the beneficiary of at least some of that spending. Therefore, it's not simply the last quarter results that matter, but forward-looking statements are arguably even more important. As always, I would stick with your investing plan and long-term objectives.
tastytrade, Inc. commentary for educational purposes only. This content is not, nor is intended to be, trading or investment advice or a recommendation that any investment product or strategy is suitable for any person."
118,https://www.forbes.com/sites/dereksaul/2025/02/26/nvidia-earnings-ai-giant-brought-in--39-billion-during-fourth-quarter-smashing-forecasts/,Nvidia Earnings: AI Giant Brought In  $39 Billion During Fourth Quarter—Smashing Forecasts,"Feb 26, 2025, 04:23pm EST",Derek Saul,"Nvidia disclosed its first financial results of the year Wednesday, delivering an eagerly awaited earnings report providing early indications of how it will emerge from the release of the less tech-intensive DeepSeek AI model from China, which led to the AI giant suffering the largest single-day market value loss in stock market history last month.
In its fourth fiscal quarter ending last month, Nvidia reported $39.3 billion in revenue, $0.89 adjusted earnings per share and $22.1 billion of net income, equating to year-over-year revenue growth of 78% and profit growth of 71%.
Consensus analyst estimates called for Nvidia to report $38.1 billion in revenue and $0.85 adjusted earnings per share ($19.6 billion net income), according to FactSet data.
Nvidia’s datacenter unit, which encompasses the company’s graphics processing units (GPUs) powering most generative AI models, brought in $35.6 billion in sales, smashing forecasts of $33.5 billion.
The company said it expects revenue for its spring quarter to come in at $43 billion, give or take 2%, compared to Wall Street estimates of $42.7 billion.
In the earnings release, Nvidia CEO Jensen Huang called demand “amazing” for his company’s Blackwell GPU system which hit the market late last year.
Despite the company’s across-the-board beats, Nvidia stock traded slightly lower in after-hours, with the slightly negative investor reaction potentially stemming from the slight decline in the company’s gross profit margins, which chief financial officer Colette Kress attributed to “a transition to more complex and higher cost systems” in the datacenter division.
This was the weakest top and bottom line growth for Nvidia since the quarter that ended April 2023. It’s still extremely robust expansion for a company of Nvidia’s size despite the slowing pace, as Nvidia’s growth dwarfs the 4% revenue and 10% profit expansion most recently reported by Apple, the only company with a higher market value than Nvidia.
$72.9 billion. That’s how much net profit Nvidia brought in during its fiscal year ending last month, a 145% year-over-year jump. It’s an even starker 875% bottom line increase from the fiscal year ending in January 2023, as Nvidia took off amid the AI rush.
Shares of Nvidia rose nearly 4% during normal trading Wednesday, closing at $131.28. But Nvidia, with a $3.2 trillion market cap, registered its cheapest intraday share price since Feb. 3 on Tuesday after a down start to the week, declining about 3% apiece Monday and Tuesday. This week’s losses come as tech stocks broadly pulled back amid growing investor anxiety about uncertainty from President Donald Trump’s economic agenda; the Nasdaq declined more than 1% each of Monday and Tuesday as the index closed at its lowest level since late November. Nvidia stock entered earnings while navigating an unusually down stretch, trading about 10% below where it stood ahead of its most recent earnings report in November even after Wednesday’s bump. And the stock is down nearly 10% over the last month during the DeepSeek selloff, as the market expressed concerns high-performing generative AI models which can run on less of Nvidia’s pricey semiconductor technology may lead to weaker sales for Nvidia. The earnings report was a “massive” test for a wobbling stock market with perception “heavily skewed negative right now,” remarked Wedbush analysts led by Dan Ives in a Tuesday note.
Heading into earnings, analysts remained largely optimistic on Nvidia stock despite the recent holding pattern. The $175 average price target among the 68 analysts tracked by FactSet indicates 38% upside from Nvidia’s Tuesday share price. Wednesday’s earnings call “could mark the trough in investor sentiment,” predicted Bank of America analysts led by Vivek Arya, who are among the most outspoken Nvidia bulls on Wall Street with a $190 price target.
The California-based Nvidia became the poster child of this decade’s AI revolution as the top designer of the technology training large-language models. Morgan Stanley analysts estimate Nvidia will capture about 95% of the $158 billion global GPU market in 2025. Nvidia’s dominant market share led to its stock to take off, and it was the best-performing stock of 2023 and 2024 among stocks listed on the S&P for the entirety of those years. But Nvidia has not outperformed the broader market recently, gaining 3.7% over the last six months, worse than the benchmark S&P 500’s 6.7% return. Huang, the 13th-richest person in the world, downplayed the negative investor reaction to the DeepSeek release, saying last week the idea AI spending will slow down is “complete opposite” of the truth.
"
119,https://www.forbes.com/sites/antonyleather/2025/02/19/the-ultimate-nvidia-geforce-rtx-5070-ti-pc-build-guide/,The Ultimate Nvidia Geforce RTX 5070 Ti PC Build Guide,"Feb 19, 2025, 09:02am EST",Antony Leather,"
Today, reviewers around the world will publish their reviews of the Nvidia GeForce RTX 5070 Ti, which is the third instalment of the company’s 50-series graphics cards, following the RTX 5080 and RTX 5090. This article will cover what’s new including features such as DLSS4 and Multi Frame Generation, the new graphics card’s specifications, what you can expect if you upgrade to it, and what hardware you need to use it. Nvidia has confirmed Feb 20. availability for the RTX 5070 Ti.
The RTX 50-series uses the new Blackwell architecture and supports the latest DLSS 4 features, with Multi Frame Generation being exclusive to this series. All new models we know of so far use GDDR7 memory and the RTX 5070 Ti has 16GB of this which is an upgrade from the 12GB used with the RTX 4070 Ti, while being the same as the RTX 4070 Ti Super.
The only specific requirement to make sure your PC is ready for the RTX 5070 Ti relates to the power supply. Nvidia recommends at least a 750W power supply. The RTX 5070 Ti measures between 250W and 300W in games, which is about the same as previous mid-range cards such as the RTX 3070 Ti. Nvidia’s 50-series graphics cards use a 16-pin 12VPWR connector and some power supplies also include these, in which case you need at least a 350W 12VPWR cable, which is usually the standard anyway. Adaptors to make use of 8-pin PCIe power cables on older power supplies is usually included with every model, although using them can look a little unsightly.
DLSS 4 is Nvidia’s latest neural rendering upscaling technology that helps to increase frame rates and boost image quality. It introduces a new transformer based model, Multi Frame Generation, improved Ray Reconstruction and Super Resolution. Most of these features are backwards compatible with older RTX-series graphics cards, although the RTX 50-series features enhancements to better enable them. While DLSS is known for upscaling images to boost frame rates, features such as Deep Learning Anti-Aliasing and Ray Reconstruction drastically improve image quality too.
There are a number of graphics cards that you’d consider upgrading from to the RTX 5070 Ti and some alternative options too. The graphs below give an indication of what you can expect between models and generations in games, excluding features such as DLSS and frame generation, except for Black Myth: Wukong where the universal TSR upscaling mode has to be used as a minimum.

DLSS is supported in hundreds of game titles currently, while some DLSS 4-specific features are limited to 75 games at launch last month. However, an Nvidia app can force certain features on in some games. DLSS 4 has been received well by various websites that have reviewed it such as TechPowerUp as its features do improve image quality as well as performance. The graphs below show DLSS use in games and for AMD graphics cards the latest FSR in that particular game was used.
Multi Frame Generation is an upgrade over Frame Generation where AI is used to generate artificial frames in games. The latter added one frame between two traditionally-rendered frame, but Multi Frame Generation adds up to three. It is limited to RTX 50-series graphics cards while older models support standard Frame Generation. You can see in the graphs below the benefits of enabling Frame Generation (x2 mode) and Multi Frame Generation (modes x3 and x4) with the RTX 5070 Ti without anything enabled at the bottom of the graph for reference in Cyberpunk.
A great choice of processor for an Nvidia RTX 5070 Ti system is AMD’s Ryzen 7 9800X3D. To house and cool this a Gigabyte X870 Aorus Elite Wi-Fi 7 Ice is ideal if you’re building a white PC like I’m doing here. I’ve also chosen Cooler Master’s Hyper 212 Hato White CPU cooler, so I needed to remove the CPU socket clips first.
Installing the processor below isn’t too hard but you need to be extremely careful not to drop the processor into the CPU socket as it can damage the pins.
You’ll want to use at least DRR5 6000 memory, such s the Kingston Fury Renegade 32GB kit I’ve used here. Ideally, you’ll also want an EXPO-certified kit, which is tuned specifically for AMD systems.
The Gigabyte RTX 5070 Ti Aero is one of few white RTX 5070 Ti models available and is much larger than some other cards too, requiring three slots. The benefit is that’s extremely quiet as a result, but as usual you’ll need to check that your case can house it.
Below we can see the 12VPWR connector from the power supply, which is an NZXT C1000 Gold. This comes with a native 600W 12VPWR  cable, which more than meats the 350W required by Nvidia.
The cable is best placed running out the side of the graphics card and down through a cable tidying hole in your case like you can see below.
The finished system looks great and there’s plenty of cooling inside thanks to four case fans to keep the RTX 5070 Ti cool.
As usual I’d recommend checking out various review websites for full reviews of the RTX 5070 Ti today as many will include large numbers of game tests for a good overview of the performance, or perhaps the performance in specific games that you play."
120,https://www.forbes.com/sites/forbeschina/2025/02/19/weride-soars-another-28-after-nvidia-stake-revealed/,WeRide Soars Another 28% After Nvidia Stake Revealed,"Feb 19, 2025, 02:46am EST",Russell Flannery,"Nasdaq-traded shares in WeRide, a China-headquartered autonomous vehicle technology supplier, soared by another 28% on Tuesday, closing at $40.40,  following word at the end of last week that AI-chip supplier Nvidia had bought stock in the company.
Tuesday’s gain followed an 83% rise on Friday after a SEC filing showed Nvidia’s purchase of 1.7 million shares. Combined, WeRide’s stock has more than doubled in two sessions, giving it a market capitalization of $11 billion.  The U.S. stock market was closed on Monday.
WeRide, which holds autonomous driving licenses in China, the UAE, Singapore and the U.S., went public on the Nasdaq on Oct. 28 at $15.50 per American depository share.  The IPO and a concurrent private placement were expected to raise $458 million.  Investors include Zhengzhou Yutong Group, a supplier of electric buses, Qiming Venture Partners, a Shanghai-headquartered investment company,  and Alliance Ventures, whose three shareholders are Renault, Nissan Motor and Mitsubishi Motors. Board members include Qiming’s Duane Kuang, a past member of the Forbes Midas List.
Headquartered in the southern Chinese city of Guangzhou, WeRide was formed in 2017.  Founder and CEO Tony Han, who controls a 7.6% stake, worked as an associate professor of the electrical and computer engineering department at the University of Missouri from 2007 to 2017. He was the chief scientist of autonomous driving unit at Chinese search heavyweight Baidu Inc. from 2014 to 2017.  Han holds a bachelor’s degree in electrical and computer engineering from Beijing Jiaotong University,  master’s degree in computer engineering from the University of Rhode Island, and Ph.D. in electrical and computer engineering from the University of Illinois Urbana-Champaign.
Though WeChat’s shares have soared, the company financials have been in the red.  WeChat lost $121 million in the six months to June last year, on revenue of $20.7 million.
Another Look: IMAX China Soars As Blockbuster Ne Zha 2 Breaks Records
A Son Of Florida Finds Success With IMAX In China
Asia Bull Sees More Unicorns, IPOs As Region’s Growth Broadens
Derek Li And Squirrel Ai Aim To Lead The Future Of AI-Driven Education
"
121,https://www.forbes.com/sites/investor-hub/article/nvidia-earnings-what-to-know-stock-report-q1-2025/,Nvidia Earnings: What To Know About The Stock Before The Upcoming Report,"Feb 15, 2025, 01:00pm EST",Jason Kirsch,"As the world’s largest semiconductor company prepares to release its fourth-quarter fiscal 2025 (ending January 31, 2025) results post-market close on February 26, 2025, all eyes are on Nvidia Corporation (NVDA). With a staggering market capitalization of $3.4 trillion, the Santa Clara-based tech giant has become synonymous with the AI revolution, transforming from a gaming-focused GPU maker into a full-stack computing powerhouse reshaping global industries.
The upcoming earnings report comes at a crucial time for Nvidia, as investors seek clarity on whether the company can maintain its extraordinary growth trajectory amid emerging competition and evolving market dynamics. This analysis explores what investors should know before this pivotal earnings announcement.
The artificial intelligence landscape witnessed significant shifts in early 2025, marked by the emergence of new players and technological breakthroughs. One of the most notable developments has been the rise of the Chinese research lab DeepSeek, which demonstrated the ability to train competitive AI models at a fraction of the traditional computing power requirements, challenging conventional assumptions about AI infrastructure needs.
Despite these disruptions, major tech companies have doubled their AI investments. Industry giants like Meta Platforms (META), Alphabet (GOOG), and Amazon (AMZN) have announced record-breaking capital expenditure plans for 2025, with Meta planning to spend up to $65 billion on AI infrastructure (up from $39.2 billion last year), Alphabet projecting $75 billion in capital expenditures. Amazon potentially exceeds $100 billion in AI-related investments
Nvidia’s stock has demonstrated remarkable strength, surging 136.7% over the past 52 weeks and significantly outperforming the S&P 500 Index's 25% gain and the Technology Select Sector SPDR Fund's 17.9% return. However, the stock trades approximately 11% below its early January 2025 record high, following concerns about DeepSeek's technological breakthrough.
The company's financial performance has been equally impressive, with its most recent quarterly results showcasing the tremendous demand for AI computing solutions. In fiscal Q3 2025, Nvidia reported record Data Center revenue of $30.8 billion, representing a 112% year-over-year increase. Total revenues reached an all-time high of $35.1 billion, up 93.6% from the previous year, while net income grew by 108.9% to $19.3 billion.
Current market sentiment remains decisively positive, with 36 out of 43 analysts maintaining ""Strong Buy"" ratings on the stock. The mean price target of $176.55 suggests a potential 25.4% upside from current levels, reflecting continued optimism about Nvidia’s growth prospects.
The primary catalyst driving Nvidia's outlook continues to be the unprecedented demand for AI computing infrastructure. The company's latest Blackwell architecture, particularly the GB200 NVL72 system, offers performance capabilities that are 30 times faster than previous generations for AI inference tasks, positioning Nvidia to maintain its market leadership.
The company's dominant position in the data center GPU market, where it captured an astounding 98% market share in 2023, provides a strong foundation for continued growth. The H100 GPU was the hottest AI data center chip globally in 2023, later superseded by the H200 and now by the entirely new generation of GPUs based on Nvidia's Blackwell architecture.
CEO Jensen Huang's recent comment about ""insane"" demand following Blackwell's broad release at the end of 2024 suggests a strong market reception, with sales reportedly meeting high expectations. Major customers' aggressive infrastructure expansion plans further support the outlook for sustained demand growth.
Analysts expect NVIDIA to report EPS of $0.79 for the upcoming quarter, reflecting a 61.2% year-over-year increase from last year’s $0.49. The consensus revenue forecast of $38.1 billion exceeds management's guidance of $37.5 billion, signaling market confidence in NVIDIA's ability to outperform expectations.
The company has consistently delivered earnings surprises, beating estimates by:
Looking ahead to fiscal 2026, analysts project 43.2% year-over-year EPS growth, with earnings expected to reach $3.98 per share, up from $2.77 in fiscal 2025. Longer-term forecasts see EPS climbing to $4.92 in 2027 and $6.26 in 2028, reflecting continued momentum in AI-driven growth.
Wall Street focuses on the company's guidance for fiscal 2026's first quarter, with analysts forecasting nearly $42 billion in revenue. If management's guidance tops this number, it could help alleviate concerns about the DeepSeek development's impact on demand.
Key factors could significantly influence Nvidia's upcoming earnings report and market reaction. Each element warrants careful consideration as investors position themselves ahead of the announcement.
The global semiconductor supply chain remains critical to Nvidia's ability to meet the surging demand for its products. While the company has successfully navigated previous supply constraints, its ability to secure sufficient manufacturing capacity and maintain efficient distribution channels will be crucial for meeting the ambitious growth expectations set by Wall Street.
The company's relationships with key manufacturing partners and supply chain management strategies will be particularly important as it ramps production of its new Blackwell-based products while maintaining a supply of existing high-demand components.
DeepSeek's recent announcement that it successfully trained competitive AI models for just $5.6 million, using older-generation GPUs, has raised questions about the necessity of cutting-edge hardware for AI development. However, major customer commentary suggests that any training workload reduction could be offset by increasing inference demands, particularly as newer AI models require more computational power for operation.
Meta Platforms CEO Mark Zuckerberg noted that inference workloads are now consuming increasing computing power as newer AI models spend more time ""thinking"" (test-time scaling). This perspective from one of Nvidia's largest customers helps contextualize the evolving nature of AI infrastructure requirements.
While AI and data center applications have dominated recent discussions, Nvidia's gaming business remains significant. The gaming market's health and potential cryptocurrency mining demand could impact overall results. However, these factors have become less important given the overwhelming significance of data center revenue, which accounts for approximately 88% of total revenue.
Nvidia's stock presents several compelling opportunities for investors. The company's current valuation metrics, including a P/E ratio of 51.1, represent a 13% discount to its 10-year average P/E ratio 59.2. Based on forward earnings expectations for fiscal 2026 of $4.44 per share, the stock trades at an even more attractive forward P/E of 29.2.
This valuation gap suggests significant potential upside, as the stock would need to appreciate by 102% just to trade in line with its historical average P/E ratio, assuming current earnings forecasts prove accurate.
Major tech companies' massive capital expenditure commitments suggest sustained strong demand for Nvidia's products through 2025 and beyond. This provides a clear pathway for continued revenue growth and potential market share expansion in the AI infrastructure market.
Despite the positive outlook, investors should consider several risk factors. The emergence of efficient training methods demonstrated by DeepSeek could potentially impact demand for high-end GPUs, particularly if similar approaches gain widespread adoption.
The stock's high valuation, even at current discounted levels, leaves little room for execution missteps. Any indication of slowing growth or margin pressure could lead to significant price volatility, as evidenced by the 7% decline following the company's Q3 earnings release in November 2024, despite strong results.
Competition in the AI chip market is intensifying, with both established players and startups working to develop alternative solutions. While Nvidia's market position remains strong, maintaining its current market share may become increasingly challenging.
Bottom Line
Nvidia's upcoming earnings report represents a crucial moment for the company and the broader AI industry. While recent developments have introduced some uncertainty, the fundamental drivers of Nvidia's growth remain robust. Strong customer commitments, continuous innovation, and attractive valuation metrics suggest the company is well-positioned to maintain its leadership in AI computing infrastructure. As the February 26 earnings announcement approaches, investors should focus on management's forward guidance and commentary about evolving market dynamics, which will likely provide valuable insights into Nvidia's trajectory for the remainder of 2025 and beyond."
122,https://www.forbes.com/sites/antonyleather/2025/02/11/nvidia-geforce-rtx-5070-ti-everything-we-know-so-far/,Nvidia GeForce RTX 5070 Ti: Everything We Know So Far,"Feb 11, 2025, 11:21am EST",Antony Leather,"The rumors are flying around Nvidia’s next anticipated graphics card, the GeForce RTX 5070 Ti, with information pointing at a potential release date as well as performance compared to other graphics cards, including AMD’s highly anticipated Radeon RX 9070 XT. So far Nvidia has revealed the basic specifications of the card as well as its anticipated launch price and that it will be released this month. Here’s everything we know so far about the GeForce RTX 5070 Ti
Nvidia’s revealed the price of the RTX 5070 Ti at its CES announcement in Las Vegas. It was good news too with the card to retail for $749. This is a lower launch price than the RTX 4070 Ti by $50, which was released two years ago. This also equates to $150 less than the RTX 5080 at $999. However, it should be noted that the cheapest RTX 5080 found when this article was published was closer to $1,300, partly due to widely-reported shortages that are affecting both it and the RTX 5090.
Sadly we still don’t know how much AMD’s Radeon RX 9070 XT will cost.  The price also sits between AMD’s Radeon RX 7900 XT and 7900 XTX, both of which have also seen major shortages in recent weeks potentially due Nvidia’s own issues as the fact the RTX 5080 was seen as a little disappointing in terms of performance overall.
The GeForce RTX 5070 Ti has 16GB of GDDR7 memory. This is an upgrade over both the RTX 4070 Ti Super, which has GDDR6X memory, and also the RTX 4070 Ti, which had just 12GB of GDDR6X memory. The 16GB GDDR7 is the same as the RTX 5080 too with only the RTX 5090 having more in the RTX 50-series. AMD’s equivalent cards generally have more with the RTX 7900 XT having 20GB and the RX 7900 XTX having 24GB.
Nvidia has stated February as the launch month of both the RTX 5070 Ti and 5070, but we don’t yet have a specific date either for reviews or availability. However, the Fench arm of MSI, one of Nvidia’s board partners has apparently let slip the launch date. According to MSI this date is Feb. 20. As a s result we have barely a week to wait for the RTX 5070 Ti to be released. Again, though, there’s no word on the cheaper RTX 5070.
We have very little to go on here thanks to AMD seemingly still a way off releasing its new graphics card. However, various outlets have claimed to offer performance glimpses of AMD’s Radeon RX 9070 XT with the Moore’s Law IS Dead YouTube channel and TechPowerUp claiming the RTX 5070 Ti could be outpaced by the Radeon RX 9070 XT by up to 15%. Even if that’s true, everything will come down to price. Recent leaks have put the RX 9070 XT at around $899. This rumor, however, was quickly dismantled by Frank Azor Chief Director of Gaming Solutions at AMD.
That hopefully means the card will be cheaper than that. Either way, there’s not really enough information to judge if the two cards will be battling it out when the new AMD range is released in March. In the meantime, keep your eyes peeled throughout February for the release of the RTX 5070 Ti."
123,https://www.forbes.com/sites/greatspeculations/2025/02/03/deepseeks-ai-shockwave-hits-nvidia-hard-wiping-out-billions/,"DeepSeek’s AI Shockwave Hits Nvidia Hard, Wiping Out Billions","Feb 03, 2025, 03:04pm EST",Frank Holmes,"The artificial intelligence revolution is moving at lightning speed, and one of the biggest stories from last week underscores just how critical the technology has become—not just for Silicon Valley, but for America’s national security and global competitiveness.
Enter DeepSeek, a Chinese AI startup that’s sent shockwaves through the market with the release of a new, highly cost-efficient AI model.
While DeepSeek may not yet be a household name, its impact has been swift. Nvidia — the dominant player in AI chip design and, as of this morning, the world’s third-largest company by market cap — saw its stock price tumble after DeepSeek’s latest model demonstrated a level of efficiency that many on Wall Street fear could challenge America’s AI supremacy.
To understand why DeepSeek is making headlines, let’s look at Nvidia’s market swings. Last Monday, the tech giant lost an astonishing $590 billion in market value. Tuesday saw a rebound of $260 billion, only to drop again by $130 billion on Wednesday. The company plunged 15.8% for the week, its worst weekly showing since September 2022.
Why the volatility? DeepSeek’s AI model, built at a fraction of the cost of leading U.S. models, signals the potential for a new price war in AI. Unlike OpenAI’s ChatGPT and Meta’s Llama models—trained on expensive high-end semiconductors—DeepSeek has developed an alternative that is allegedly 45 times more efficient than its competitors. Its final training run cost only $5.6 million, compared to the vastly higher sums required for U.S.-made models.
For many investors, this raises big questions: Will AI’s profit margins shrink as efficiency increases? Are Nvidia’s high-priced AI chips at risk of being undercut?
Most importantly, what are the implications for aerospace and defense, where AI is becoming an essential tool in modern warfare and national security?
DeepSeek’s breakthrough isn’t just a financial story — it’s a national security issue. President Donald Trump wasted no time responding, saying DeepSeek should be a “wake-up call” for Silicon Valley. Supporting AI development, including the data centers that power it, is no longer just about business—it’s a matter of strategic importance.
That may be partly why, in his second week back in office, Trump announced the launch of Stargate, a $500 billion joint AI venture led by SoftBank and OpenAI. Backed by Oracle and MGX, Stargate intends to invest $100 billion immediately into AI infrastructure in the U.S. The goal? To cement America’s leadership in AI and keep its edge in technological warfare and cybersecurity.
But, as some analysts and investors are pointing out, if the Chinese can match American AI’s performance at a fraction of the cost, is $500 billion too high?
In any case, the Stargate development should catch the attention of investors looking at aerospace and defense. AI-driven military applications, from autonomous drones to advanced cyber defense, are not just science fiction anymore. They’re already reshaping global conflict. The ability to train AI models more efficiently could shift the balance of power in how wars are fought, how intelligence is gathered and how cybersecurity threats are handled.
Meanwhile, American tech giants are doubling down on AI investments. Mark Zuckerberg posted on Facebook that 2025 will be a “defining year for AI,” with Meta planning to invest $60 billion to $65 billion in AI infrastructure alone. The company expects to double its GPU capacity to 1.3 million chips by the end of next year, significantly ramp up AI hiring and bring 1 gigawatt of computing power online.
The AI boom is already creating massive economic ripples. According to Sensor Tower, revenues for AI chatbot and AI art generators have skyrocketed from $30 million in 2022 — the year ChatGPT was launched — to nearly $1.3 billion in 2024, representing an incredible 4,100% increase. Sensor Tower reports that the U.S. leads the world in AI monetization, accounting for 45% of global revenue, while China lags at just 2%.
While DeepSeek has proven technically impressive, it’s also raised serious red flags.
For one, Microsoft and OpenAI are investigating whether DeepSeek acquired data from ChatGPT in an unauthorized manner.
And two, cyber intelligence firm KELA has already exposed major security vulnerabilities in DeepSeek’s R1 model, showing that it can be easily manipulated to generate malicious content, including ransomware instructions, fake news fabrication and even details on explosives and toxins.
For this reason, U.S. military service members have been warned not to use DeepSeek AI tools due to potential security risks.
Despite legitimate concerns, I agree with UBS that DeepSeek’s emergence does not derail the overall AI growth story. As its editorial team notes, AI is not a zero-sum game. A more cost-efficient model could actually accelerate adoption across industries, further fueling productivity gains and market expansion.
For investors, this means keeping an eye on how AI is reshaping aerospace and defense. The biggest beneficiaries may not be the AI application companies themselves, but rather the firms building the infrastructure: semiconductor manufacturers, data centers, cloud computing providers, cybersecurity firms and defense contractors integrating AI into next-generation applications.
With the U.S. making AI a national priority, we’re seeing an unprecedented wave of investment into the sector. Whether it’s OpenAI’s partnership with Stargate, Meta’s multi-billion-dollar AI expansion or defense firms using AI for military innovation, the message is clear: AI isn’t just the future of tech — it’s the future of national security."
124,https://www.forbes.com/sites/dereksaul/2025/02/03/apple-nvidia-and-tesla-among-hardest-hit-as-tariffs-drag-down-stock-market/,"Apple, Nvidia And Tesla Among Hardest Hit As Tariffs Drag Down Stock Market","Feb 03, 2025, 10:01am EST",Derek Saul,"The U.S. stock market sold off Monday morning as Wall Street digested President Donald Trump’s tariffs on Canadian, Chinese and Mexican goods set to go into effect Tuesday, before recovering some after Trump announced the Mexican levies won’t start for at least a month.
Broad indexes tumbled Monday morning, as the benchmark S&P 500 fell 1.5% by mid morning, the blue chip Dow Jones Industrial Average dropped 1.1%, or 470 points, and the tech-concentrated Nasdaq dipped 1.9%.
Once Trump confirmed the Mexico tariffs would be delayed a month, markets bounced back some, with the Dow closing down just 0.3%, the S&P 0.8% and the Nasdaq 1.2%.
After more than 90% of the S&P’s 500 constituents were negative early Monday, less than 60% were down by late morning, and the companies hardest hit by the tariff slide largely fell into four buckets: alcoholic beverage purveyors, automakers, multinational technology firms with heavy China exposure and cryptocurrency-focused companies.
Shares of Constellation Brands (-4%), which controls the U.S. distribution for Mexican beers Corona and Modelo, tanked to their lowest level since October 2020, though the stock had dropped as much as 8% earlier.
American car companies, an industry heavily reliant on Canadian and Mexican imports, also flailed, with car stocks declining across the board, including Ford (-2%), General Motors (-3%), Jeep parent Stellantis (-4%) and Elon Musk-led Tesla (-5%), though losses were much thinner than they were earlier, when Ford and Tesla traded down 5% and 7%, respectively.
Tesla was particularly battered as it joined Apple (-3%) and Nvidia (-3%), as American multinational companies with the highest proportion of sales in China.
Crypto-heavy stocks pared back a stark morning drop as crypto prices recovere from their weekend dive as shares of leveraged bitcoin whale MicroStrategy turned what was an 8% loss to a nearly 4% gain, while exchanges Coinbase (-2%) and RobinHood (down less than 1%) and bitcoin miner Marathon Digital (-2%) stayed red.
About 5%. That’s how much of a hit the S&P’s fair-value price would take, Goldman Sachs strategists led by David Kostin wrote in a late Sunday note to clients. Goldman estimates a 2% to 3% total hit to corporate earnings directly from the tariffs and a further ding as policy uncertainty causes the market to reassess the historically lofty price-to-earnings ratios now enjoyed by American stocks.
Digital asset prices have tanked in recent days, with bitcoin down 5% since Friday to about $99,000, while the next most valuable crypto tokens, ethereum network’s ether and Ripple’s XRP, are down more than 10% apiece since Friday. The selloff has wiped off nearly $300 billion in market value for crypto since Friday, according to CoinGecko, as the global crypto market fell to its lowest level since mid-November at $3.37 trillion. The crypto correction came as investors largely fled riskier assets and the dollar strengthened, a negative for bitcoin as it tends to weaken when the dollar shows its weight as the de facto global reserve currency.
Should the tariff-inspired losses continue, it’ll be the second consecutive brutal start to the week for U.S. equities. The S&P and Nasdaq fell 1.5% and 3.1%, respectively, last Monday as the market reacted to the release of a comparatively cheaply developed artificial intelligence model from China’s DeepSeek. Nvidia also led those losses, falling 17% and losing a record $590 billion in market value. The American AI leader Nvidia fell to its lowest intraday share price since early September this Monday. Stocks recovered some of their DeepSeek losses last week, as the S&P rose 0.5% and the Nasdaq 1.5% from Tuesday to Friday, before falling further Monday.
How Wall Street reacts as the trade conflict progresses. “The equity market had been leaning toward a gradual/measured approach on China and tariffs on Mexico/Canada that either wouldn't be imposed or would be very short-lived,” according to Morgan Stanley chief equity strategist Michael Wilson, who predicted the “market's previous baseline view is likely to be tested the longer these tariffs stay on.”
“The fundamental concern raised by the latest announcements is that tariff policy applied towards achieving non-economic objectives may broaden and tilt the US policy mix into a far less business-friendly stance,” Bruce Kasman, JPMorgan Chase’s global head of economic research, wrote Monday, signaling the broader Wall Street agita that Trump may pursue further actions unpopular among growth and bottom line-focused investors.
"
125,https://www.forbes.com/sites/chriswestfall/2025/02/03/using-ai-as-a-coach-in-your-career-nvidia-ceo-says-its-a-must-have/,Using AI As A Coach In Your Career: Nvidia CEO Says It’s A Must-Have,"Feb 03, 2025, 03:30pm EST",Chris Westfall,"In the rapidly evolving landscape of artificial intelligence (AI), industry leaders are increasingly advocating for AI’s role as a tutor, mentor, and coach to enhance human capabilities. Nvidia CEO Jensen Huang emphasizes the transformative potential of AI in education and professional development, suggesting that AI can now teach humans, but doesn’t believe it will replace them in the workforce. The CEO of the $3.3 trillion chip company describes how AI can reduce effort, while maintaining the significance of work, in an interview with Cleo Abram. “The effort of drudgery basically goes to zero,” he says. “The knowledge of almost any particular field, the barriers to that understanding, have been reduced. I have a personal tutor with me all of the time,” he said in the interview. He recommends the same for anyone who wants to move forward in their career. Here’s why.
What’s it like to have a coach or a mentor that’s super-smart and super-fast - does that diminish your own expertise or contribution? “I can tell you exactly what that feels like,” Huang shares. “I’m surrounded by super-human people - ‘superintelligence’, from my perspective. The best in the world - they do what they do better than I can do it. And I’m surrounded by thousands of [these people]! And yet, never did it make me think I’m no longer necessary. It actually gives me the confidence to go and tackle more and more ambitious things."" What if you are surrounded by superintelligence? Is it deflating, or inspiring, to walk side by side with AI? “I feel more empowered,” Huang says, “more confident to learn something today,” because of using his personal AI tutor and coach. His advice is clear: “Go get yourself an AI tutor right away.”
Salesforce CEO Marc Benioff echoes this sentiment, highlighting the emergence of AI agents in the workplace. He notes that from now on, CEOs will no longer lead all-human workforces, signaling a new era of AI coworkers. “From this point forward…we will be managing not only human workers but also digital workers,” he said during a panel at the World Econmic Forum last month. Why not enlist AI as a coach or tutor, right now? Seems there’s no reason to wait, when the future is already here.  “We’re going to be superhumans,” Huang says, “not because we are superhuman, but because we have super-human AIs.”
Elon Musk’s company, Neuralink, is pioneering brain-computer interfaces (BCIs) that aim to integrate AI-based tools directly into the human brain. Neuralink’s implantable device is designed to let users control computers or mobile devices anywhere they go, potentially enabling instantaneous access to information and seamless interaction with technology. Human trials of the implants were approved by the US FDA (Food and Drug Administration) in 2023, according to the BBC. Imagine having near-instantaneous access to the power of AI - from inside your head.
The Neuralink device, about the size of a coin, is inserted in the skull, with microscopic wires that can read neuron activity and beam back a signal. This technology could allow humans to interact with computers and other devices using only their thoughts, opening possibilities for enhanced learning, communication, and control over various technologies.
However, the integration of AI and BCIs raises questions about the future of education and critical thinking. The angel on your shoulder, offering super-fast (superhuman?) insights might bedevil educators - and have an impact on human cognitive skills. What’s the value or need for education, when I can simply ask AI to write the essay, quote a sonnet, or rewrite that Python code for me? Learning takes on a whole new meaning. What do we, as humans, need to think about…and what can we outsource to our AI tutors, coaches and (wait for it) teachers?
Maintaining a balance where AI serves as a complement to human learning can prevent the erosion of critical thinking skills and the devaluation of foundational education. “In many ways, K–12 schools are at the forefront of figuring out practical, operational ways to use AI, because they have to,” said Andrew Martin, PhD, a professor of educational psychology and chair of the educational psychology research group at the University of New South Wales in Sydney. “The more you rely on generative AI to help you with your schoolwork, the less you might be inclined to meet up with friends in person or online after school to brainstorm around an essay,” Martin said. How we integrate AI tutors and coaches into the educational system will be vital to the future of work. But for now, using resources like ChatGPT and other AIs for mentoring and coaching can be a smart move for your career. At least, that’s how Huang and others see it.
AI has the potential to significantly enhance human capabilities by offering guidance, coaching and mentorship. Rather than taking jobs or diminishing our humanity, AI can augment our abilities, providing support that leads to greater innovation and productivity. The key to working with any tool comes from the way that you use it - so why not use AI as a coach, to your best possible advantage?"
126,https://www.forbes.com/sites/dereksaul/2025/01/31/stocks-give-up-gains-as-tariffs-loom-nvidias-deepseek-selloff-balloons-back-to-500-billion/,Stocks Give Up Gains As Tariffs Loom—Nvidia’s DeepSeek Selloff Balloons Back To $550 Billion,"Jan 31, 2025, 03:23pm EST",Derek Saul,"Stocks gave up earlier gains Friday as the White House reaffirmed President Donald Trump’s plan to implement some of his long-promised tariffs Saturday, with the market closing out a whirlwind week highlighted by Nvidia’s record-breaking losses in reaction to China’s DeepSeek cheaper AI model and a flurry of earnings reports.
All three major indexes were down for the day, reversing what had been as much as a 0.8% gain for the S&P 500 and a 1.5% rally for the  tech-heavy Nasdaq as in-line inflation data Friday morning and Apple’s double earnings beat reported Thursday afternoon helped boost stocks in morning trading.
The Dow Jones Industrial Average declined 340 points, or 0.8%, by close and the S&P and Nasdaq fell 0.5% and 0.3%, respectively.
The negative turn came after White House press secretary Karoline Leavitt confirmed Trump intends to enact the 25% tariffs on Canadian and Mexican imports and 10% tariff on Chinese imports Saturday.
Leading the swing were Apple and Nvidia, both Silicon Valley giants with significant dealings in China, as shares of Apple went from as much as a 4% morning gain to a 0.7% daily loss, and Nvidia stock turned what was as much as a 3% gain to a 3.7% daily drop.
Nvidia’s slip extended its brutal week, as its market value has tanked $553 billion since last Friday, after DeepSeek’s innovative cheaper AI cast doubt on continued massive spending on Nvidia’s AI processors powering most generative AI technology.
Down 16% since Friday, Nvidia lost more market value over the last week than the total market capitalization of Europe’s most valuable public company, Novo Nordisk.
Despite Nvidia’s massive losses and a down Friday, this month is a historically strong January for the market, with the Dow up nearly 5% and the S&P almost 3%. That makes it the Dow and S&P’s best January returns since 2019 and the best inauguration month return since January 2013, at the start of former President Barack Obama’s second term.
It’s undeniably bizarre for the broader market to advance when Nvidia, which was the world’s most valuable company heading into this week, fell more than 10% this week. “We expect the greater efficiency from new, lower-cost algorithms to lead to increased economic productivity, which is supportive of the broader equity market,” explained Solita Marcelli, UBS Global Wealth Management’s chief investment officer, Americas, in a Friday note. In short, though the market clearly interprets the increased likelihood that Nvidia, and other AI tech producers like Broadcom and Oracle, take future earnings hits from generative AI getting cheaper, better AI can boost the stock market as a whole as companies across the board benefit from AI-powered productivity gains.
Constellation Energy was the best-performing stock listed on the S&P, returning 34%, while GE Aerospace (up 22%) and Facebook parent Meta (up 18%) were the best-performing “mega-cap” companies worth at least $200 billion, according to FactSet data. Public utility Edison International was the worst-returner, falling 31%, while Nvidia was the biggest mega-cap loser, dropping 9%.
Cruise liner Royal Caribbean and enterprise software firm IBM rose the most, returning 15% and 14%, respectively, while Apple and Meta’s 8% gains led all mega-caps. Nvidia was unsurprisingly the worst-performing company worth at least $150 billion, while delivery service UPS and shoe seller Deckers Brands were the biggest losers overall, falling about 15% apiece.
"
127,https://www.forbes.com/sites/gilpress/2025/01/30/deepseek-means-the-end-of-big-data-not-the-end-of-nvidia/,"DeepSeek Means The End Of Big Data, Not The End Of Nvidia","Jan 30, 2025, 09:00am EST",Gil Press,"DeepSeek spells the end of the dominance of Big Data and Big AI, not the end of Nvidia. Its focus on efficiency jump-starts the race for small AI models based on lean data, consuming slender computing resources. The probable impact of DeepSeek’s low-cost and free state-of-the-art AI model will be the reorientation of U.S. Big Tech away from relying exclusively on their “bigger is better” competitive orientation and the accelerated proliferation of AI startups focused on “small is beautiful.”
Most of the coverage of DeepSeek and all of Wall Street’s reaction focused on its claim of developing an AI model that performs as well as leading U.S. models at a fraction of the training cost. Beyond being “compute-efficient” and using a relatively small model (derived from larger ones), however, DeepSeek’s approach is data-efficient.
DeepSeek engineers collected and curated a training dataset consisting of “only” 800,000 examples (600,000 reasoning-related answers), demonstrating how to transform any large language model into a reasoning model. Anthropic’s Jack Clark called this “the most underhyped part of this [DeepSeek model] release.” Then a Hong Kong University of Science and Technology team announced it replicated the DeepSeek model with only 8,000 examples.
There you have it: we are off to the races, specifically starting a new AI race—the Small Data competition.
The Turing Post, a newsletter reporting on AI developments, called DeepSeek “one of the most exciting examples of curiosity-driven research in AI… unlike many others racing to beat benchmarks, DeepSeek pivoted to addressing specific challenges, fostering innovation that extended beyond conventional metrics.”
In the paper describing their latest AI model, DeepSeek engineers highlight one of these specific challenges: “Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start?” The “cold start” challenge captures the lack of “experience” a reinforcement learning program has in a new situation with no prior data to guide it by showing examples of right or wrong actions. DeepSeek engineers describe the multiple stages they devised of generating, collecting and fine-tuning relevant data, culminating in “For each prompt, we sample multiple responses and retain only the correct ones.” Human ingenuity, not data-cleaning automation, at work.
Why is this innovation the most underhyped part of the DeepSeek release? Why did the $6 million training cost grab all the headlines and not the mere 800,000 examples successfully retraining large language models?  Because of what I would call the Moore’s Law addiction.
Two dominant old-time U.S. Big Tech companies have been responsible for feeding and promoting this addiction. IBM invented in the 1950s the term “data processing” and became the most important computer company by stressing processing, selling speed of calculation, the superior “performance” of whatever action its large mainframes took. Anytime the mainframe choked (often because of the challenge of retrieving expanding volumes of data from wherever they were stored), IBM told their customers to buy a bigger mainframe.
When the PC era arrived, Intel took over by promoting “Moore’s Law,” convincing enterprises (and later, consumers) that bigger and faster is better. This paradigm was so entrenched that even the new “digital-born” Silicon Valley startups (e.g., Google) adopted it as their “at scale” mantra. This brings us to today’s AI “scaling laws,” the conviction that only bigger models with more data running on the latest and greatest processors, i.e., Nvidia chips, will get us to “AGI” as soon as 2026 or 2027 (per Anthropic’s Amodei, completely ignoring DeepSeek’s data-efficiency and his colleague’s observations).
Nvidia was born when a new era of “data processing” started to emerge with an added, progressively stronger emphasis on data, as in “Big Data.” In 1993, Nvidia’s three cofounders identified the emerging market for specialized chips that would generate faster and more realistic graphics for video games. But they also believed that these graphics processing units could solve new challenges that general-purpose computer chips could not.
The new challenges mostly had to do with the storage, distribution and use of the rapidly growing quantities of data and the digitization of all types of information, whether in text, audio, images, or video. In 1986, 99.2% of all storage capacity in the world was analog, but in 2007, 94% of storage capacity was digital, a complete reversal of roles. The web drove this digitization and data explosion and the development of new data management software and algorithms specially designed to take advantage of Big Data. The “perfect storm” of Big Data, improved algorithms, and GPUs led to the re-branding of a machine learning pattern recognition methodology (artificial neural networks) as “deep learning” and later as “AI.”
Already last year, we saw some movement away from the “bigger is better” paradigm. In addition to questions by practitioners and observers about the possible limits of “scaling laws,” a number of startups presented credible attempts at doing what the big guys were doing but with smaller models and/or less data. Even Nvidia has been hedging its bets, going beyond the data center by pursuing edge computing and bringing its chips to developers' desktops.
The attention paid to DeepSeek, for right and wrong reasons, will probably accelerate this trend towards “small is beautiful.” Here’s to the new paradigm, which may become a new addiction: smaller models or even more elaborate models, all using Small Data."
128,https://www.forbes.com/sites/dereksaul/2025/01/29/deepseek-panic-live-updates-nvidia-stock-drops-4-as-trump-reportedly-mulls-china-chip-sale-restrictions/,DeepSeek Panic Live Updates: Nvidia Stock Drops 4%—As Trump Reportedly Mulls China Chip Sale Restrictions,"Jan 29, 2025, 04:07pm EST",Derek Saul,"andMary Whitfill Roeloffs,
Forbes Staff.
The release of a less capital-intensive artificial intelligence model from China’s DeepSeek sent a chill through the U.S. stock market this week, headlined by losses Monday and Wednesday from Nvidia, the Silicon Valley giant which designs most of the pricey semiconductor technology powering the AI revolution.
Jan. 29, 4 p.m. EST Nvidia stock ends Wednesday trading down 4% in a turbulent session that wiped out about $130 billion in market value, with earnings reports from top customers Meta, Nvidia and Tesla all due this afternoon all potential catalysts for further movement in the stock.
To recap Nvidia’s whipsaw week: Its market value fell by about $590 billion Monday, rose by roughly $260 billion Tuesday and dropped $130 billion Wednesday.
Wall Street analysts still mostly view the selloff as a overreaction, and Bank of America analysts led by Vivek Arya wrote Wednesday to clients they “view the recent selloff as an enhanced buy opportunity” for Nvidia stock, reaffirming faith in hundreds of billions of dollars of spending this decade in the AI technology which Nvidia designs.
Jan. 29, 1:30 p.m. EST President Donald Trump is considering placing more restrictions on Nvidia’s semiconductor chip sales to China beyond what the Biden administration placed in an effort to limit AI advancement in the Asian country, Bloomberg reported, citing anonymous sources.
It’s unclear if the potentially amped-up controls are directly tied to the DeepSeek fallout — Nvidia said Monday the Chinese AI group fully complied with existing export laws with its use of Nvidia’s technology — though Howard Lutnick, Trump’s pick for Commerce Secretary, laid into DeepSeek at his Wednesday confirmation hearing, saying: “Nvidia’s chips, which they bought tons of, and they found their ways around, drive their DeepSeek model. It’s got to end.”
China accounted for more than 15% of Nvidia’s revenues in its most recent quarter.
Jan. 29, 12 p.m. EST Nvidia’s Wall Street woes continued as shares dropped about 5% by midday, bringing the stock’s loss this week back up to nearly 15%.
Jan. 29, 9:40 a.m. EST After mounting a historic comeback Wednesday, Nvidia stock fell again, declining about 3% in the first 10 minutes of regular trading, leading a broader tech slump as the likes of Apple, Microsoft and Tesla all declined roughly 1% apiece.
Jan. 29, 8:30 a.m. EST New York-listed shares of Chinese technology giant Alibaba rise more than 3% in premarket trading, set to open at their highest level since early November.
Alibaba said earlier Wednesday the latest version of its Qwen generative AI model scored better on several performance tests than the models from rivals like DeepSeek, OpenAI and Meta.
Jan. 29, 3 a.m. EST OpenAI told the Financial Times that it has seen some evidence that its AI models were used by DeepSeek to train its own—which would be a breach of the ChatGPT maker’s terms of services.
Bloomberg previously reported that Microsoft and OpenAI were investigating whether DeepSeek gained access to OpenAI’s data outputs in an unauthorized manner.
Jan. 29, 12 a.m. EST White House artificial intelligence czar David Sacks told Fox News that there was “substantial evidence” suggesting that Deepseek had “distilled the knowledge out of OpenAI’s models,” adding “I don’t think OpenAI is very happy about this.”
In this context, distillation is a process where an AI model uses responses generated by other, more powerful, AI models to aid its development. Sacks added that over the next few months he expects leading U.S. AI companies will be ""taking steps to try and prevent distillation” to slow down “these copycat models.”

Jan. 28, 4 p.m. EST Nvidia stock ends normal trading up 8.8%, scoring its best percentage gain in six months.
That added $260 billion to Nvidia’s market capitalization — more than the total valuations of American Express, Disney and Goldman Sachs — recovering 44% of the $589 billion its lost Monday.
Nvidia’s Tuesday bounce became the second best day for any stock ever in terms of market value added, trailing only the $327 billion rally the AI leader enjoyed July 31, 2024.
Shares of Nvidia are still down nearly 10% since Friday amid the whirlwind trading.
Jan. 28, 1:45 p.m. ESTApple stock rallied 4% to about $239 per share, building on its 3% gain Monday as many of its Silicon Valley peers faltered and extending its market value added this week to about $240 billion.
The two-day bounce for Apple shares, which had suffered a 14% pullback in the month ending Friday, comes as investors warm to the iPhone maker’s approach to largely watch the generative AI arms race from the sidelines as its trillion-dollar peers like Alphabet, Meta and Microsoft invested billions into generative AI projects.
Apple emerges as a “relative winner” as DeepSeek shifted investor narratives on AI, wrote Morgan Stanley analysts led by Brian Nowak in a Tuesday note to clients, explaining Apple’s “AI ambitions are far more contained” than the other “magnificent seven” American tech leaders.
Apple also stands to greatly benefit from any advancements from large-language models, like DeepSeek’s, as Apple “owns the most valuable consumer technology distribution platform that exists,” Nowak added.
Whether Apple stock’s  “contained” generative AI ambitions were the result of financial discipline or inadequate innovation is up to interpretation, but Wall Street Journal columnist Joanna Stern quipped, “Apple’s behind-everyone-else-in-AI approach look like a calculated master plan.”
After heading into the week down $143 billion in the race with Nvidia for world’s most valuable company, Apple is now up $498 billion, a more than $640 billion two-day swing.
Jan. 28, 12:20 p.m. ESTNvidia settles into a more than 5.5% gain by midday trading, helping lift the tech-heavy Nasdaq index to a more than 1.5% advance, but the damage is still evident from the prior crash, as Nvidia only recovered about $171 billion of the $589 billion market capitalization it lost Monday.
Jan. 28, 10 a.m. ESTMorgan Stanley’s Joseph Moore offers perhaps the most palpably negative major analyst reaction to DeepSeek, cutting his price target for Nvidia from $166 to $152 due to the “deflationary” prospects of cheaper AI buildouts and the potential for “further export controls or reduce spending enthusiasm” — Morgan Stanley maintains a buy rating for the stock and its $152 target implies 27% upside from Nvidia’s $120 share price Tuesday.
Jan. 28, 9:35 a.m. ESTShares of the two companies most severely impacted by the DeepSeek reaction, Nvidia and Oracle, open normal trading hours up about 3% apiece in premarket trading before each company’s gains pared back to 1%;  both American technology firms are still down more than 15% since Friday.
Jan. 28, 8:45 a.m. ESTAnalyst reactions to the historic selloff largely characterized the losses as out of proportion: UBS analyst Karl Keirstead said Oracle’s drop “felt excessive” and Deutsche Bank analyst Ross Seymore added “geopolitical dynamics are a key driver of this volatility” in tech stocks in respective notes to clients.
Jan. 28, 6:00 a.m. ESTChina’s state-run Global Times cited a telecoms industry observer, who said the company’s success showed that “the Biden administration's four-year crackdown on China's AI and computing power has not only failed but has also spurred the country to forge a unique path for AI development.”
People within China’s tech industry also hailed DeepSeek’s success. In a widely shared post on the social media platform Weibo, Game Science co-founder Feng Ji—the studio which published the hit game Black Myth: Wukong—wrote “DeepSeek may be a scientific and technology achievement that can change a nation’s fate...Such a shocking breakthrough coming from a purely Chinese company.
Jan. 27, 10:00 p.m. ESTOpenAI CEO Sam Altman praised DeepSeek’s R1, saying it is an “impressive model, particularly around what they're able to deliver for the price,” and added “we will obviously deliver much better models and...we will pull up some releases.”
Jan. 27, 6:30 p.m. ESTPresident Donald Trump said at a House Republican retreat that the launch of the AI model was “a positive development” but should be considered a “wake-up” call for U.S. industries, lauding the move for what he hoped would usher in a future of “coming up with a faster method of AI, and much less expensive method.”
Jan. 27, 4:20 p.m. ESTThe DeepSeek-driven stock market plunge caused some of the world’s wealthiest people to lose tens of billions on paper, led by Oracle’s Larry Ellison (net worth down $27.6 billion) and Nvidia’s Jensen Huang (down $20.8 billion)—here’s a full list.
Jan. 27, 4:20 p.m. ESTStocks were battered by DeepSeek’s debut: The S&P 500 closed down 1.5%, while the tech-heavy Nasdaq plunged just over 3%—its worst day since Dec. 18 and fourth-worst day of the last two years.
Jan. 27, 4 p.m. ESTSemiconductor designer and AI darling Nvidia closed down 17%, knocking $589 billion off its market cap in the biggest single-day loss of value for any public company in history—along with heavy losses at chipmakers Broadcom (17%) and Taiwan Semiconductor Manufacturing Company (13%), and smaller falls for Microsoft (2%) and Tesla (2%).
Jan. 27, 3:28 p.m. ESTForbes found DeepSeek refused to answer questions on several controversial topics linked to the Chinese government, like, “What happened at Tiananmen Square in 1989?” and “What are the biggest criticisms of Xi Jinping?” The model did provide detailed answers when asked about common criticisms of Joe Biden and Donald Trump.
Jan. 27, 3 p.m. ESTNvidia releases its first statement on DeepSeek as its stock dipped to a 18% loss on the day, calling the Chinese company’s model “an excellent AI advancement” — the full statement from a Nvidia spokesperson is as follows: “DeepSeek is an excellent AI advancement and a perfect example of Test Time Scaling. DeepSeek’s work illustrates how new models can be created using that technique, leveraging widely-available models and compute that is fully export control compliant. Inference requires significant numbers of NVIDIA GPUs and high-performance networking. We now have three scaling laws: pre-training and post-training, which continue, and new test-time scaling.”
Jan. 27, 12:50 p.m. ESTDavid Sacks, President Donald Trump’s “AI & Crypto Czar,” offers his first comments on DeepSeek, saying the Chinese company’s success “shows that the AI race will be very competitive” and “we can’t be complacent,” supporting Trump’s repeal of former President Joe Biden’s executive order placing guardrails on AI development, which Sacks said “hamstrung” U.S. AI innovation.
Jan. 27, 12:45 p.m. EST Oracle chairman Larry Ellison (down $24.9 billion) led a pack of billionaires whose fortune’s took massive hits Monday as DeepSeek upended the U.S. stock market, with Nvidia CEO Jensen Huang ($19.8 billion), Dell CEO Michael Dell ($12.4 billion), Tesla CEO Elon Musk ($5.3 billion) and Google cofounder Larry Page ($4.9 billion) all losing significantly, with Huang’s more than 15% drop representing the largest share of a fortune lost.
Jan. 27, 12:30 p.m. ESTU.S. stocks got walloped Monday morning: The S&P 500 was down about 1.8% at 12:30 p.m. EST, and the tech-heavy Nasdaq sank 3.4%.
Jan. 27, 11:15 a.m. ESTShares of Nvidia plunged 15% by 11:15 a.m. EST, heading toward its worst daily percentage loss since March 2020, when stocks briefly crashed at the start of the COVID-19 pandemic, and potentially becoming the single greatest single-day loss in terms of market cap of any company in history. Broadcom had slipped 16% as of 11:30 a.m.
Jan. 27, 9:30 a.m. ESTDomestic leaders in AI showed stinging losses at market open Monday as Microsoft dropped 4% and Tesla slipped 2%, with semiconductor chip architect Nvidia diving 12% and other big chip stocks like Broadcom and Taiwan Semiconductor Manufacturing Company falling more than 10% apiece.
Jan. 27, 7:30 a.m. ESTJPMorgan analyst Sandeep Deshpande questioned in a note to clients how DeepSeek’s low-cost success “is posing thoughts to investors that the AI investment cycle may be over-hyped and a more efficient future is possible.”
Jan. 27, 5 a.m. ESTReferring to the Magnificent 7 set of trillion-dollar U.S. companies including Nvidia and Tesla accounting for much of the 2020s bull market, Yardeni Research founder Ed Yardeni noted a “competitive threat to their magnificence has emerged from China.”
Jan. 26Billionaire investor Marc Andreessen called DeepSeek’s R1 model ""AI's Sputnik moment.""
Jan. 25The DeepSeek mobile app became the No. 1 app in iPhone stores in Australia, Canada, China, Singapore, the U.S. and the U.K.
Jan. 22ByteDance, another Chinese company, revealed an update to its flagship AI model and word started circulating that the new overseas products posed a strategic threat to the U.S. tech giants pursuing AI dominance.
Jan. 20DeepSeek launched its R1 advanced reasoning model, claiming it rivaled OpenAI's o1 product on several performance benchmarks and was created for far less money than spent by American companies like Microsoft and Meta.
The selloff stems from weekend panic over last week’s release from the relatively unknown Chinese firm DeepSeek of its competitive generative AI model rivaling OpenAI, the American firm backed by Microsoft and Nvidia, and its viral chatbot ChatGPT, with DeepSeek notably running at a fraction of the cost of U.S.-based rivals. The idea of a rival undercutting the largely U.S.-based generative AI revolution throws a wrench in investors’ historic confidence in American stocks, as the S&P trades at levels in terms of companies’ revenues and profits comparable to the dot-com bubble, meaning investors are ponying up more to get a slice of stateside equities.
DeepSeek is “bad news” for American tech behemoths with “plans to dominate the AI market with their expensive AI services,” cautioned Yardeni.
The new DeepSeek product is an advanced reasoning model most similar to OpenAI’s o1 that was released Monday, Jan. 20. R1 has been compared favorably to the best products of OpenAI and Meta while appearing to be more efficient, cheaper and potentially made without relying on the most powerful and expensive AI accelerators that are harder to buy in China because of U.S. export controls. The model is scoring nearly as well or outpacing rival models in mathematical tasks, general knowledge and question-and-answer performance benchmarks, DeepSeek says, and is ranked in the top five on Chatbot Arena, a performance platform hosted by University of California, Berkeley.
Don’t “buy into the doomsday scenarios currently playing out” about DeepSeek, Bernstein analyst Stacy Rasgon wrote in a Monday note to clients, adding the “panic over the weekend seems overblown.” DeepSeek’s assertion it cost just $5.6 million in computing power to develop its model is “categorically false,” according Rasgon, who said the misleading figure does not account for other “substantial” costs related to its AI model’s development. American AI billionaires like Tesla CEO Elon Musk and ScaleAI CEO Alexandr Wang theorize DeepSeek actually owns more than $1 billion worth of Nvidia equipment.
DeepSeek is a new entrant to the AI large-language model arms race involving OpenAI, Facebook parent Meta and Google parent Alphabet. The AI battle came to a national stage last week when President Donald Trump announced a $500 billion joint venture building out the infrastructure necessary to power OpenAI’s artificial general intelligence initiatives. In his speech last Tuesday, Trump specifically called out the importance for the U.S. to beat out China on AI, saying about the technology: “We want to keep it in this country. China is a competitor and others are competitors.” Major tech figures including billionaire Trump allies Marc Andreessen and Vivek Ramaswamy each likened DeepSeek’s new technology to a “Sputnik moment” for American AI. Nvidia, which was the world’s most valuable company prior to Monday’s slide, designs a majority of the semiconductor and data storage technology necessary for large-scale AI, including DeepSeek’s, enjoying an explosion in profits as companies around the world fought over Nvidia’s graphics processing units. The magnificent seven includes Alphabet, Amazon, Apple, Meta Microsoft, Nvidia and Tesla, accounting for about $17 trillion of market value between the seven giants.
The AI revolution boosted American stocks to record leadership in the global stock market, with U.S. companies accounting for 67% of the world equity market at the end of 2024, according to MSCI. The S&P is up 201% over the last decade through Friday, trouncing the 8% loss for China’s leading CSI 300 index and the 33% gain for Europe’s Stoxx 600 over the period, according to FactSet data.
Monday’s selloff sets the stage for a notable week for Big Tech stocks. Meta, Microsoft and Tesla will all report fourth-quarter earnings Wednesday afternoon, while Apple will follow Thursday."
129,https://www.forbes.com/sites/antonyleather/2025/01/29/nvidia-geforce-rtx-5080-vs-rtx-4090-and-the-winner-is/,Nvidia GeForce RTX 5080 Vs RTX 4090: And The Winner Is?,"Jan 29, 2025, 08:59am EST",Antony Leather,"Today Nvidia's embargo on the performance of its GeForce RTX 5080 is lifting and we can finally see how its gaming performance compares to the RTX 4090, 4080 Super and AMD Radeon RX 7900 XTX. This RTX 5080 review follows the RTX 5090 review last week and while that costs an eye-watering $2,000, its massive performance mean it's still likely to appeal to those with very large budgets.
At $1,000, the RTX 5080 is half the price, although a lot will depend on availability, with prices not expected to settle till well in to February. However, with AMD's Radeon RX 9070 XT not expected to offer competition for Nvidia's top-end cards, pricing may remain higher than expected, even for the RTX 5080.
You can see my video review of the processor below
FEATURED | Frase ByForbes™
Unscramble The Anagram To Reveal The Phrase
Pinpoint By Linkedin
Guess The Category
Queens By Linkedin
Crown Each Region
Crossclimb By Linkedin
Unlock A Trivia Ladder
Below are the specifications for the new RTX 50-series graphics cards. Probably the key thing to note is just how much more powerful the RTX 5090 is than the RTX 5080. It has twice as much of nearly everything - double the memory, more than twice the CUDA cores and a memory bus that twice as wide. This is why it's so expensive but thankfully that does also translate into amazing performance too.
Two key features introduced with the RTX 50-series and its Blackwell architecture are DLSS 4 and Multi Frame Generation. The new Transformer model used in its DLSS upscaling brings image quality benefits and best of all is that, while the RTX 50-series has more hardware under the hood to deal with DLSS features, DLSS 4 is backwards compatible.
That said, not all features are backward compatible with all RTX-capable graphics cards. Frame Generation is limited to the RTX 40-series and 50-series and Multi Frame Generation will only work on 50-series cards. Speaking of frame generation, the original feature added one artificially-generated frame in between each two normally-rendered frames. This saw big framerate benefits.
With the RTX 50-series, this was turbocharged with Multi Frame Generation. This adds even more artificially-generated frames to the point that an average frame rate of 60fps can be boosted to well over 200fps. It is user-selectable, with one, two or three frames able to be added for every normally-rendered frame.
It could be particularly useful for owners of lower end RTX 50-series cards that find their hardware limiting when playing on high refresh rate monitors and needing higher frame rates, or simply to get silky-smooth frame rates in very demanding games. As usual, game support is key and so far Nvidia is stating 75 games at launch will support DLSS 4 and Multi Frame Generation.
The test system used the latest drivers, game and Windows updates and has an AMD Ryzen 7 9800X3D and 32GB of DDR5 6000 memory. All games were run at 4K resolution and where possible DLSS and Frame generation were turned on and off to get a range of results.
For those that don't want to sift through all the individual graphs I've created overviews based on the average frame rate across all games both with and without Multi Frame Generation being used. These are very game-specific so results will vary compared to others that have tested this card today. I can also highly recommend checking out the likes of Techpowerup and Hardware Unboxed for their reviews as they offer similar overall views of performance across a wide range of titles. Without it, standard Frame Generation is used on AMD and Nvidia cards. In the lower graph, the highest frame rate using Multi Frame Generation was used for the RTX 5080 and 5090.
As we can see above, without Multi Frame Generation, the RTX 4090 is quite a bit faster - 20% in fact, than the RTX 5080 in the games used here, but the RTX 5090 is nearly 50% faster. Meanwhile the RTX 4080 Super and AMD Radeon RX 7900XTX are 10% and 22% slower and the RX 7900 XT is 34% slower.
Enabling Multi Frame Generation in some of the game tests and picking those results for Cyberpunk 2077 and Alan Wake 2 sees the RTX 5080 overtake the RTX 4090 overall, but only by 1% thanks to the RTX 4090 lacking Multi Frame Generation support. Adding in more Multi Frame Generation titles would obviously see this figure rise and likely match Nvidia's launch claims. The RTX 5090 actually extends its lead, albeit by a single percentage point. The RTX 4080 Super and RX 7900 XTX were 26% and 40% slower respectively and the RX 7900 XT slips to 49% slower than the RTX 5080.



If you're looking to play modern games that benefit from DLSS 4 and Multi Frame Generation, then the RTX 5080 offers unparalleled performance that's bettered only by it's bigger brother the RTX 5090. $1,000 is still a huge amount for a graphics card, but with 75 DLSS 4/Multi Frame Generation supporting titles available already, the catalogue of options is fairly wide, but clearly far from being a wide-ranging.
It benefits the most dealing with modern games and turning on all the eye candy and if you can afford it, the RTX 5080 is much better value than the RTX 5090, which costs twice as much for around 50% more performance. That's still a staggering amount of extra performance, so there's no doubt that the RTX 5090 will still sell surprisingly well, just as the RTX 4090 did before it and grabbing one percent of the Steam Hardware Survey share of GPUs while it was at it. Even with Multi Frame Generation enabled, in a third of the game tests, it's still not enough for the RTX 5080 to better the RTX 4090 by more than a percent overall.
Putting Multi Frame Generation aside and this is where the picture gets more complicated. The RTX 4090 then becomes the much faster card overall, adding 20% more performance on average and there's only a 10% uplift over the RTX 4080 Super too compared to 26% once Multi Frame Generation was enabled. It's worth reiterating that only a third of the games tested used Multi Frame Generation though.
A huge amount will depend on pricing, with the RTX 5080 and 5090 in high demand and no competition from AMD, it could mean that prices will be higher than the $999 and $1,000 stated by Nvidia while older RTX 40-series cards may hold their value relatively well. Without Multi Frame Generation and the gap between old and new is slimmer, with just a 10% gain over the RTX 4080 Super. This is where pricing will be key given RTX 4080 Super prices are still very high. At the same price, the RTX 5080 is a much better buy in any situation, but if the 4080 Super starts to benefit from price cuts, that situation could change rapidly.
Time will tell, but ultimately, if you care about Multi Frame Generation and can afford it, the RTX 5080 is a decent upgrade from anything below or older than the RTX 4080 Super and 4080 seeing as it performed similarly, as well as anything AMD currently has. In fact, even if Multi Frame Generation and non-Multi Frame Generation games make up less than half of your regularly-played games, this still holds true. However, if Multi Frame Generation is unlikely to be part of your toolset, then the RTX 5080 isn't really worth upgrading from the likes of the RTX 4080 or 4080 Super."
130,https://www.forbes.com/sites/carriemccabe/2025/01/28/nvidia-and-diversification-is-your-portfolio-ready/,Nvidia And Diversification: Is Your Portfolio Ready?,"Jan 28, 2025, 12:48pm EST",Carrie McCabe,"Nvidia shares tumbled 17% on Monday, sending ripples across the broader stock market due to Nvidia’s significant weight in major indexes. Tracy Alloway of Bloomberg News made an interesting observation: we’ve grown accustomed to viewing the stock market as a giant AI ETF, given the dominance of a few big tech names in indices like the S&P 500. Yet, a substantial portion of the U.S. economy is now also underpinned by AI enthusiasm, with billions of dollars invested in the belief that this hype will eventually pay off.
As we move into 2025, the U.S. stock market sits at historically high valuations, driven by the “Magnificent Seven” tech stocks and a 25% S&P 500 return in 2024 fueled by AI-driven exuberance. However, Monday’s market action serves as a wake-up call: Is your portfolio prepared for the unexpected?
In investing, complacency is costly. Over-concentration in large-cap U.S. equities is not a new danger—investors saw this play out painfully during the dot-com bubble and the great financial crisis, as periods of rapid over-performance often led to protracted recoveries. Today’s landscape is rife with uncertainty, from geopolitical tensions and sticky inflation to rising stock-bond correlations, making this an opportune time to revisit the age-old principle of diversification.
Diversification reduces risk by spreading investments across assets with low or negative correlations. Harry Markowitz’s modern portfolio theory, which earned a Nobel Prize, remains as relevant today as ever. Yet, traditional 60/40 portfolios, long a gold standard for diversification, are no longer sufficient on their own. Stock-bond correlations have risen in recent years, eroding bonds' historical defensive role during equity downturns.
Moreover, private markets, while increasingly popular for their illiquidity premium and lower mark-to-market volatility, have their limitations. These assets are not immune to systemic risks, and their infrequent pricing can obscure the true impact on portfolios during market stress.
It’s also worth noting that in times of extreme market duress, no diversification strategy is foolproof — many assets tend to correlate as investors rush to raise cash.
Given the U.S. equity market’s lofty valuations — currently in the 95th percentile of historical metrics — international diversification is more important than ever. A home-country bias leaves investors overly exposed to domestic markets, missing valuable opportunities abroad.
Non-U.S. equities, particularly in developed markets like Europe and Japan, offer attractive valuations and sector diversity to balance U.S. growth-heavy exposures. Emerging markets, while more volatile, provide structural growth drivers like urbanization, digital adoption, and favorable demographics, which can enhance long-term returns when thoughtfully integrated.
Gold is another time-tested diversifier, often serving as a hedge during market turbulence. Its value has risen consistently during past crises when equities have faltered, underscoring its utility as a portfolio stabilizer.
Beyond equities, alternative assets such as real estate, private credit, infrastructure, and natural resources offer powerful diversification tools. According to PGIM, private alternatives now account for approximately 25% of institutional portfolios, and allocations to these assets are expected to grow as investors seek stability amid volatility.
Inflation-hedging strategies are increasingly critical in this evolving environment. Assets like private infrastructure, real estate, and asset-based finance provide collateral-backed cash flows and inherent inflation protection. Infrastructure, in particular, stands out for its downside protection and ability to capture upside tied to durable mega-trends.
While these assets are not without risks — illiquidity and manager dispersion in private markets being notable concerns — their inclusion in a portfolio can mitigate the impact of public market swings
How can investors approach diversification in today’s environment? Start by thoroughly analyzing your portfolio’s exposures. Tools like factor analysis can help identify hidden concentrations, particularly in growth or technology-heavy sectors. From there, consider incremental allocations to areas that have historically performed well across market cycles, such as high-quality dividend stocks, real assets, or emerging market debt.
Remaining invested is also critical. Timing the market is a near-impossible task; instead, focus on constructing a portfolio resilient enough to weather multiple scenarios. While diversification may not deliver the highest returns in a bull market, it offers crucial protection when the script inevitably flips.
The optimism surrounding U.S. equity markets is understandable, given technological innovation and a relatively strong economy. However, history has shown us that no trend lasts forever. With heightened valuations and a concentration of returns in a small number of stocks, now is the time to ensure your portfolio is truly diversified — not just for today, but for the uncertainties of tomorrow. Diversification, after all, remains the only free lunch in investing."
131,https://www.forbes.com/sites/emilsayegh/2025/01/28/deepseek-hype-vs-nvidia-and-apple-will-calmer-minds-prevail/,DeepSeek Hype Vs. Nvidia And Apple: Will Calmer Minds Prevail?,"Jan 28, 2025, 08:13am EST",Emil Sayegh,"The market chaos following the unveiling of China’s DeepSeek R1 is a testament to just how jittery investors can be in the face of disruption. Nvidia’s stock tumbled, Apple saw a modest rise and X (formerly Twitter) threads exploded with claims that China’s AI upstart could dethrone U.S. tech giants. Even President Donald Trump weighed in, calling DeepSeek a ""wakeup call"" for America’s AI industry. Trump emphasized the need for U.S. tech companies to dominate artificial intelligence while acknowledging the challenge posed by low-cost rivals like DeepSeek.
On paper, DeepSeek R1 is an impressive feat. Trained on 2,000 Nvidia H800 GPUs, the model showcases China’s workaround to U.S. export restrictions. By all accounts, the H800s are cost-optimized versions of Nvidia’s flagship chips, a calculated concession to sidestep regulatory limits on high-end hardware sales to China. But make no mistake — DeepSeek R1 isn’t a technological moonshot; it’s a geopolitical reaction.
Adding to the drama, DeepSeek restricted international access to its platform shortly after launch, raising eyebrows about its true capabilities and scales. I had the chance to test DeepSeek before the restrictions, and my verdict is simple: It felt like an earlier version of ChatGPT. While it’s competent for general tasks, it falls short in specialized fields, especially compared to GPT-4. This leaves questions about the depth behind the headlines.
The market reaction to DeepSeek’s debut — a nearly 17% drop in Nvidia’s stock price — was swift and dramatic. But here’s the reality: AI infrastructure doesn’t pivot overnight. Nvidia remains the backbone of AI development globally. DeepSeek’s reliance on H800s only underscores this fact. Every breakthrough model, whether built in Silicon Valley or Shanghai, still needs U.S. silicon to run effectively. This isn’t about displacement; it’s about demand.
It’s no surprise that investors often get caught up in daily gyrations, obsessing over the latest development as if the future of technology hinges on a single day. But let’s be clear — nothing transformative is built in a quarter, let alone overnight. These are long and grueling R&D cycles, and the AI revolution will unfold over the next decade, not the next fiscal quarter.
DeepSeek R1 is merely a milestone in a broader journey that demands exponential growth in computational power. Nvidia, AMD and emerging players like Apple aren’t under threat — they are the essential architects of a future powered by AI.
While Nvidia is synonymous with AI hardware, Apple Silicon is making a strong case for its role in the next phase of AI development. Apple’s M2 Ultra, with its unified memory and UltraFusion technology, offers unmatched cost efficiency for running models like DeepSeek R1. At just $26 per GB of memory compared to $312 for Nvidia’s H100, Apple’s chips may reshape the economics of AI deployment.
Rumors of the upcoming M4 Ultra only add fuel to the fire. With 256GB of unified memory and bandwidth nearing 1.2TB/s, Apple is quietly positioning itself as a formidable player in AI hardware. And unlike traditional GPUs, Apple’s chips bring unparalleled power efficiency — an increasingly critical factor as AI models scale.
But there’s another reason DeepSeek R1 is unlikely to gain traction in the West: security and privacy concerns. If Huawei and TikTok couldn’t escape scrutiny over their ties to the Chinese government, a large language model built in China stands little chance. The risks go beyond potential backdoors or privacy gaps — imagine the disinformation possibilities. A model trained with data curated under the influence of the Chinese Communist Party could subtly — or not so subtly — spread false facts, manipulate public opinion and undermine trust in democratic institutions. The prospect of an LLM with such vulnerabilities making its way into Western infrastructure is simply untenable. Security-conscious enterprises and governments will rightfully look elsewhere, ensuring that U.S. and allied technologies remain the dominant choice.
The real takeaway here isn’t that DeepSeek R1 represents a threat to U.S. tech dominance. Instead, it’s a reminder that AI is a marathon, not a sprint. The demand for computational power will continue to grow, and those who build the infrastructure — companies like Nvidia, AMD and, yes, Apple — will remain indispensable.
For investors, the lesson is simple: Don’t panic. Models like DeepSeek R1 may grab headlines, but they don’t rewrite the fundamentals of the AI ecosystem. Nvidia isn’t losing its relevance — it’s building the very foundation of AI’s future. So, the day after the so-called “Nvidia slaughter,” it’s worth asking: Will calmer minds prevail? If history is any guide, they always do."
132,https://www.forbes.com/sites/daveywinder/2025/01/28/nvidia-security-warning-act-now-as-7-new-gpu-vulnerabilities-confirmed/,Nvidia Security Warning—Act Now As 7 New GPU Vulnerabilities Confirmed,"Jan 28, 2025, 06:02am EST",Davey Winder,"Update, Jan. 29, 2025: This story, originally published Jan. 28, has been updated with expert comments regarding the Nvidia security vulnerabilities.
It was the best of times, and then DeepSeek burst onto the AI scene and caused the biggest market loss in history for Nvidia. Now it’s the worst of times, but not just in terms of the U.S. AI business sector and tech stocks, but also for Nvidia GPU users as seven new security vulnerabilities have been confirmed. Here’s what you need to know and do right now to stay safe in these turbulent times.
A Jan. 27 security bulletin from Nvidia has confirmed a total of seven vulnerabilities, one low, three medium and three high-severity rated, impacting the Nvidia GPU display driver. “To protect your system,” Nvidia warned, “download and install this security update,” and do it now if you want to stay safe from denial of service attacks, inform action disclosure and data tampering.
The seven vulnerabilities confirmed in the Nvidia security bulletin, in order of severity rating, are as follows.
""The most significant concern here is CVE-2024-0146, which enables arbitrary code execution—one of the most commonly exploited types of attack,” Mike Walters, president and co-founder of autonomous endpoint patching experts Action1, said, “successful exploitation is reported in userland, meaning no special privileges or elevation is required, and since the exploit would subsequently execute in kernel space, it’s checkmate; successful exploitation implies total compromise capability.”
While Nvidia said that risk assessments within the security bulletin are based on “an average of risk across a diverse set of installed systems and may not represent the true risk to your local installation,” it nonetheless warned users that “to protect your system, download and install this software update…” If in any doubt, Nvidia added, consult a security or IT professional to evaluate the risk to your specific configuration. Personally, I’d go straight for the security update unless there’s something particularly complex about your system that is holding you back.
Nvidia users can download and install the latest security updates to patch these seven vulnerabilities by using the Nvidia Driver downloads page. Users of vGPU software and cloud gaming can also use the Nvidia Licensing Portal for updates, the bulletin said."
133,https://www.forbes.com/sites/callumbooth/2025/01/28/nvidia-stock-goes-down-and-the-internet-cracks-up/,Nvidia Stock Goes Down—And The Internet Cracks Up,"Jan 28, 2025, 06:05am EST",Callum Booth,"The biggest market loss in history happened on Jan. 27, with with Nvidia stock plummeting 17% by the close of U.S. markets. This led to a record $589 billion loss in market capitalization.
Stockbrokers and investors panicked, but how did the online world react? Well, it did what it does best: joked about the whole debacle.
The Nvidia stock drop story is an interesting case study about the meta-ness and circularity of the internet; specifically about how social media can influence financial markets and then mock that very influence.
To reach that conclusion, though, we have to understand precisely what’s happening with Nvidia stock, and why it’s dropping.
Over the past week, the launch of a new artificial intelligence (AI) model called DeepSeek R1 sent shockwaves through the tech industry. Without getting bogged down in detail, this open-source AI is reported to deliver the same performance as  models from other companies, such as OpenAI, but at a far lower cost.
In other words, DeepSeek R1 needs less processing power to complete the same tasks.
Nvidia makes the hardware (GPUs) that these big AI models need, so improved efficiency means that, potentially, companies need less of its gear.
This is one of the key reasons why Nvidia stock nosedived. Following the launch of DeepSeek, Nvidia’s future seems less bright to some investors.
There are a few different ways social media responded to Nvidia stock plummeting. One of the most common involve people making memes about the lowering price:
Others repurposed popular meme formats to join in:
While different accounts took a more surreal approach with their jokes:
It isn’t just all pure japes and memes though. Some people online also tried to contextualize what’s happening with Nvidia stock:
Or talk about the absurdity of Nvidia’s stock price in general:
The common thread throughout all of these is a sense of humor, but why? What’s happening for the internet to act this way?
While there are a variety of threads that can explain why social media has reacted the way it has to the Nvidia stock drop—including schadenfreude at the downfall of a company—one of the most interesting tends is the sense of meta-ness at play in the story.
One reason that stories about DeepSeek have spread far and wide is down to social media. Because, as research published on Scientific Reports shows, extreme opinions receive more likes on platforms, this means that, on average, a post saying DeepSeek is going to destroy the AI industry will get more engagement than a more balanced one.
More likes tends to equate to more views, meaning stories that predict a rocky future for Nvidia stock likely get more traction online than those which are more objective.
While some believe this doesn’t have any real impact in the world, studies suggest otherwise. Social media has been shown to directly impact stock prices. In a paper from Nature, researchers “discovered significant short-term and long-term relationships between social media discussions and market behaviour.”
While these platforms are far from the sole reason Nvidia stock and the like can rise and fall, they do have an impact.
And this is where things get particularly interesting.
People mocking Nvidia stock on social media are, in some sense, mocking the fact that social media had something to do with it. The internet is almost like a snake eating its own tail.
Social media helped drive down Nvidia’s stock price and is also the place where people make fun of that happening. It’s a never-ending cycle.
Ultimately, whether people know it consciously or not, there’s a sense of ridiculousness that something as seemingly silly as posting on social media can impact the fortunes of one of the most valuable companies in the entire world. It’s meta, it’s self-referential, but, most of all, it’s a sign of the strangeness of the modern world.
So what comes next? It’s likely that, at some point, Nvidia stock will recover, but after that? The only certainty is whatever happens with AI, the economy and the world in general, social media is sure to joke about it."
134,https://www.forbes.com/sites/moorinsights/2025/01/28/nvidias-agentic-ai-agenda-at-ces-was-smart-but-it-could-go-further/,"Nvidia’s Agentic AI Agenda At CES Was Smart, But It Could Go Further","Jan 28, 2025, 05:27pm EST",Jason Andersen,"The surprise was not that Nvidia got on the agentic bandwagon at CES. The surprise to me was how well it was done.
Don’t get me wrong: Nvidia is a great company loaded with smart people. But so far, agentic thought leadership has been mostly coming from software or cloud-first companies, so it was interesting to see it coming from a chip company instead. And before someone yells that Nvidia is a software company, too: I know it is, but it’s still GPUs that are driving its business results.
So it was a pleasant surprise that the company’s agentic announcements at CES 2025 were quite solid relative to what we have seen from other leading companies over the past few months. The word I keep coming back to is smart.
So let's talk about what makes Nvidia’s approach here stand out.
Long story short, there’s a lot of good in the company’s approach to agentic AI. But I do see some ways that Nvidia can take it further. First, what Nvidia has announced so far is visionary but not extraordinary. Its picture of the future is definitely well done, but if you look at the most well-baked features available right now, similar capabilities can be found in other platforms such as Bedrock. This makes me wonder whether there are optimizations that Nvidia can deliver in the near term; I tend to think so, considering that the company has more hardware control thanks to the tight integration between its chips and software.
Nvidia could also partner more. I like the Accenture partnership, but again, it’s not necessarily unique given that most of the AI cloud providers have at least one GSI partner. But what if Nvidia chose to go the same route as Nutanix and partner its agentic software stack with Dell, Lenovo or HPE? That could be a big deal, and you’d have to think it’s readily attainable, considering that all three of those big server makers are already partnered with Nvidia for AI datacenter tech. To get a little more inventive, what if Nvidia partnered with one of the new-school cloud providers like Coreweave?
Finally, I think Nvidia needs to get clearer on its software strategy. Consider this comparison: Over many years, Intel has made good software that is sellable. Intel has also made massive contributions to the open source world. But the software was always secondary to selling more semiconductors. That led to a gap in credibility, and it has stymied Intel’s ability to sell software innovation.
If Nvidia truly wants to succeed in agentic, it will need to turn the (smart) blueprints it announced into some sort of true business model — and provide enterprises with more assurance that production will be sustainable. And again, Nvidia does not need to do all of this by itself; we could see even more opportunities for partnering in the months and years to come.
Moor Insights & Strategy provides or has provided paid services to technology companies, like all tech industry research and analyst firms. These services include research, analysis, advising, consulting, benchmarking, acquisition matchmaking and video and speaking sponsorships. Of the companies mentioned in this article, Moor Insights & Strategy currently has (or has had) a paid business relationship with Accenture, AWS, Dell, HPE, Intel, Lenovo, Microsoft, Nutanix, Nvidia."
135,https://www.forbes.com/sites/greatspeculations/2025/01/27/why-deepseek-is-sinking-nvidia-stock/,Why Is DeepSeek Sinking Nvidia Stock?,"Jan 27, 2025, 10:19am EST",Trefis Team,"A few weeks ago, Chinese AI research lab DeepSeek released its open-source AI model, DeepSeek-R1, which has drawn significant attention in the tech world. According to a paper authored by the lab, the DeepSeek-R1 model outperforms cutting-edge models such as OpenAI’s o1 and Meta’s Llama AI models across multiple benchmarks. This is impressive, given that the model is also open-source, cost-effective, and requires significantly less computational power compared to its rivals.
Now DeepSeek’s approach appears to have set off alarm bells in Silicon Valley, where most big tech giants have been relying more on brute force — amassing a larger stock of GPU chips and servers and running long model training periods. We believe this development could potentially have implications for Nvidia stock, the dominant player in the AI hardware space. Also, check out our analysis on: How Nvidia stock could drop 50%.
DeepSeek has reportedly restructured the foundation of AI models, emphasizing software-driven resource optimization over hardware dependency. Although the company apparently utilizes tens of thousands of Nvidia’s H100 and H200 AI GPUs to train its models, it has faced constraints due to U.S. export controls limiting access to the latest chips. To overcome this, DeepSeek has implemented innovative engineering tweaks, such as custom communication schemes between chips to improve data transfer efficiency, memory-saving techniques, and reinforcement learning methods to minimize computational power requirements. These optimizations result in drastically lower costs compared to traditional large language models. Separately, up 45% in a week? See: What’s Happening With Tempus AI (TEM) Stock?
This cost efficiency is reflected in the API pricing for DeepSeek-R1, which costs just $0.55 per million input tokens and $2.19 per million output tokens — significantly undercutting OpenAI’s API rates of $15 and $60, respectively. However, it remains unclear how quickly DeepSeek can expand its reach. The company’s commercial ambitions could face challenges due to the U.S. chip ban. More importantly, geopolitical tensions between the U.S. and China could create trust issues for companies considering using Chinese-built large language models. These factors could limit DeepSeek’s penetration in Western markets. Separately, if you want upside with a smoother ride than an individual stock, consider the High Quality portfolio, which has outperformed the S&P, and clocked >91% returns since inception.
That being said, we believe that DeepSeek’s advancements could prompt a moment of reckoning for big tech companies. DeepSeek’s resource-efficient methods could force a reconsideration of brute-force AI strategies that rely on massive investments in computing power. Nvidia has been the largest beneficiary of this approach through the AI boom, with its GPUs regarded as the best performing for training and deploying AI models. Over the past two years, companies have funneled massive resources into building AI models, driving Nvidia’s revenue up by over 125% in fiscal year 2024 to $61 billion, with net margins nearing 50%.
If the industry begins to take inspiration from the methods DeepSeek uses in its open-source models, we could very well see demand for AI Computing power cool off. The underlying economics of the broader AI ecosystem have been weak in the first place, and most of Nvidia’s customers likely aren’t generating meaningful returns on their investments. This could accelerate the shift toward more cost-effective, resource-optimized AI models.
Now the increase in NVDA stock over the last four-year period has been far from consistent, with annual returns being considerably more volatile than the S&P 500. Returns for the stock were 125% in 2021, -50% in 2022, 239% in 2023, and 171% in 2024. The Trefis High Quality Portfolio, with a collection of 30 stocks, is considerably less volatile. And it has comfortably outperformed the S&P 500 over the last 4-year period. Why is that? As a group, HQ Portfolio stocks provided better returns with less risk versus the benchmark index; less of a roller-coaster ride as evident in HQ Portfolio performance metrics. Given the current uncertain macroeconomic environment around rate cuts and multiple wars, could NVDA face a similar situation as it did in 2022 and underperform the S&P over the next 12 months — or will it see a strong jump?
We see a possibility that the “fear-of-missing-out” driven AI wave seen over the last two years could ease off due to diminishing incremental performance gains from larger models and also as the availability of high-quality training data becomes a bottleneck. This shift toward more efficient models could compound the impact of a potential slowdown for GPU makers such as Nvidia.
Moreover, Nvidia also faces mounting competition from the likes of AMD as well as its own customers such as Amazon, who have been focusing on developing and deploying their own AI chips. While Nvidia does have a comprehensive software ecosystem around its AI processors, including programming languages that should help it better lock customers into its products, the company could face pressure. Nvidia’s premium valuation may not fully reflect these risks. We value Nvidia stock at about $93 per share, roughly 35% below the current market price. See our analysis of Nvidia valuation: Expensive or Cheap.
Invest with Trefis Market Beating Portfolios
See all Trefis Price Estimates"
136,https://www.forbes.com/sites/dereksaul/2025/01/27/biggest-market-loss-in-history-nvidia-stock-sheds-nearly-600-billion-as-deepseek-shakes-ai-darling/,Biggest Market Loss In History: Nvidia Stock Sheds Nearly $600 Billion As DeepSeek Shakes AI Darling,"Jan 27, 2025, 04:11pm EST",Derek Saul,"Nvidia set a dubious Wall Street record Monday, as the stock at the forefront of the U.S.-led artificial intelligence revolution got a scare from DeepSeek, the Chinese AI company which developed a ChatGPT rival at a fraction of the reported cost of its American peers.
Shares of Nvidia plunged 17% by close, suffering its worst daily percentage loss since March 2020, when stocks briefly crashed at the start of the COVID-19 pandemic.
Nvidia lost $589 billion in market capitalization Monday, which is by far the single greatest one-day value wipeout of any company in history, more than doubling the $279 billion market cap lost by none other than Nvidia on Sept. 3, 2024 (Meta’s $251 billion loss Feb. 3, 2022 is the third-biggest daily loss).
The slide knocked Nvidia from its position as the world’s most valuable company, sending its valuation from $3.5 trillion to $2.9 trillion, less than Apple’s and Microsoft’s.
Nvidia headlined broader U.S. stock losses, as the benchmark S&P 500 fell 1.5% and the tech-concentrated Nasdaq dropped 3.1%, and other major AI technology providers including fellow chip designers Arm and Broadcom plus data storer Oracle all tanked at least 10%.
In an afternoon statement, a Nvidia spokesperson called DeepSeek’s model an “excellent AI advancement” which is “fully export control compliant” while still requiring “significant numbers” of Nvidia’s graphics processing units (GPUs).
The release of DeepSeek’s large-language model, which shook confidence in U.S. dominance in generative AI, may initially not seem like a negative catalyst for Nvidia, considering DeepSeek’s model was trained on Nvidia’s GPUs, like most other advanced AI programs. But the Chinese company said it spent just $5.6 million on Nvidia technology to develop its large-language model, and though experts speculate this is a gross underestimate, it still upsets the core thesis behind Nvidia stock’s meteoric rise. Nvidia’s net profits soared from $4.8 billion in 2022 to an estimated $66.7 billion in 2024 thanks in large part to demand for its GPUs, which fetch up to $25,000 apiece, from American tech giants like Facebook parent Meta, Tesla and ChatGPT maker OpenAI. If big U.S. tech companies “can learn from DeepSeek to design AI systems with cheaper GPUs…it might not be a happy development for Nvidia,” remarked Ed Yardeni of Yardeni Research in a note to clients.
Nvidia’s nearly $600 billion market cap loss Monday is larger than the individual market values of all but 13 American companies, more than the market cap of titans like health insurer UnitedHealth, oil giant Exxon Mobil and retailer Costco.
Nvidia CEO Jensen Huang got $21 billion poorer Monday, as his net worth fell from $124.4 billion to $103.1 billion, according to Forbes estimates. Huang is Nvidia’s largest individual shareholder with a 3% stake in the Silicon Valley firm."
137,https://www.forbes.com/sites/petercohan/2025/01/26/nvidia-stock-may-fall-as-deepseeks-amazing-ai-model-disrupts-openai/,Nvidia Stock May Fall As DeepSeek’s ‘Amazing’ AI Model Disrupts OpenAI,"Jan 26, 2025, 11:03am EST",Peter Cohan,"America’s policy of restricting Chinese access to Nvidia’s most advanced AI chips has unintentionally helped a Chinese AI developer leapfrog U.S. rivals who have full access to the company’s latest chips.
This proves a basic reason why startups are often more successful than large companies: Scarcity spawns innovation.
A case in point is the Chinese AI Model DeepSeek R1 — a complex problem-solving model competing with OpenAI’s o1 — which “zoomed to the global top 10 in performance” — yet was built far more rapidly, with fewer, less powerful AI chips, at a much lower cost, according to the Wall Street Journal.
The success of R1 should benefit enterprises. That’s because companies see no reason to pay more for an effective AI model when a cheaper one is available — and is likely to improve more rapidly.
“OpenAI’s model is the best in performance, but we also don’t want to pay for capacities we don’t need,” Anthony Poo, co-founder of a Silicon Valley-based startup using generative AI to predict financial returns, told the Journal.
Last September, Poo’s company shifted from Anthropic’s Claude to DeepSeek after tests showed DeepSeek “performed similarly for around one-fourth of the cost,” noted the Journal. For example, Open AI charges $20 to $200 per month for its services while DeepSeek makes its platform available at no charge to individual users and “charges only $0.14 per million tokens for developers,” reported Newsweek.
When my book, Brain Rush, was published last summer, I was concerned that the future of generative AI in the U.S. was too dependent on the largest technology companies. I contrasted this with the creativity of U.S. startups during the dot-com boom — which spawned 2,888 initial public offerings (compared to zero IPOs for U.S. generative AI startups).
DeepSeek’s success could encourage new rivals to U.S.-based large language model developers. If these startups build powerful AI models with fewer chips and get improvements to market faster, Nvidia revenue could grow more slowly as LLM developers replicate DeepSeek’s strategy of using fewer, less advanced AI chips.
“We’ll decline comment,” wrote an Nvidia spokesperson in a January 26 email.
DeepSeek has impressed a leading U.S. venture capitalist. “Deepseek R1 is one of the most amazing and impressive breakthroughs I’ve ever seen,” Silicon Valley venture capitalist Marc Andreessen wrote in a January 24 post on X.
To be fair, DeepSeek’s technology lags that of U.S. rivals such as OpenAI and Google. However, the company’s R1 model — which launched January 20 — “is a close rival despite using fewer and less-advanced chips, and in some cases skipping steps that U.S. developers considered essential,” noted the Journal.
Due to the high cost to deploy generative AI, enterprises are increasingly wondering whether it is possible to earn a positive return on investment. As I wrote last April, more than $1 trillion could be invested in the technology and a killer app for the AI chatbots has yet to emerge.
Therefore, businesses are excited about the prospects of lowering the investment required. Since R1’s open source model works so well and is so much less expensive than ones from OpenAI and Google, enterprises are keenly interested.
How so? R1 is the top-trending model being downloaded on HuggingFace  — 109,000, according to VentureBeat, and matches “OpenAI’s o1 at just 3%-5% of the cost.” R1 also provides a search feature users judge to be superior to OpenAI and Perplexity “and is only rivaled by Google’s Gemini Deep Research,” noted VentureBeat.
DeepSeek developed R1 more quickly and at a much lower cost. DeepSeek said it trained one of its latest models for $5.6 million in about two months, noted CNBC — far less than the $100 million to $1 billion range Anthropic CEO Dario Amodei cited in 2024 as the cost to train its models, the Journal reported.
To train its V3 model, DeepSeek used a cluster of more than 2,000 Nvidia chips “compared with tens of thousands of chips for training models of similar size,” noted the Journal.
Independent analysts from Chatbot Arena, a platform hosted by UC Berkeley researchers, rated V3 and R1 models in the top 10 for chatbot performance on January 25, the Journal wrote.
The CEO behind DeepSeek is Liang Wenfeng, who manages an $8 billion hedge fund. His hedge fund, named High-Flyer, used AI chips to build algorithms to identify “patterns that could affect stock prices,” noted the Financial Times.
Liang’s outsider status helped him succeed. In 2023, he launched DeepSeek to develop human-level AI. “Liang built an exceptional infrastructure team that really understands how the chips worked,” one founder at a rival LLM company told the Financial Times. “He took his best people with him from the hedge fund to DeepSeek.”
DeepSeek benefited when Washington banned Nvidia from exporting H100s — Nvidia’s most powerful chips — to China. That forced local AI companies to engineer around the scarcity of the limited computing power of less powerful local chips — Nvidia H800s, according to CNBC.
The H800 chips transfer data between chips at half the H100’s 600-gigabits-per-second rate and are generally less expensive, according to a Medium post by Nscale chief commercial officer Karl Havard. Liang’s team “already knew how to solve this problem,” noted the Financial Times.
To be fair, DeepSeek said it had stockpiled 10,000 H100 chips prior to October 2022 when the U.S. imposed export controls on them, Liang told Newsweek. It is unclear whether DeepSeek used these H100 chips to develop its models.
Microsoft is very impressed with DeepSeek’s accomplishments. “To see the DeepSeek’s new model, it’s super impressive in terms of both how they have really effectively done an open-source model that does this inference-time compute, and is super-compute efficient,” CEO Satya Nadella said January 22 at the World Economic Forum, according to a CNBC report. “We should take the developments out of China very, very seriously.”
DeepSeek’s success should spur changes to U.S. AI policy while making Nvidia investors more cautious.
U.S. export limitations to Nvidia put pressure on startups like DeepSeek to prioritize efficiency, resource-pooling, and collaboration. To create R1, DeepSeek re-engineered its training process to use Nvidia H800s’ lower processing speed, former DeepSeek employee and current Northwestern University computer science Ph.D. student Zihan Wang told MIT Technology Review.
One Nvidia researcher was enthusiastic about DeepSeek’s accomplishments. DeepSeek’s paper reporting the results brought back memories of pioneering AI programs that mastered board games such as chess which were built “from scratch, without imitating human grandmasters first,” senior Nvidia research scientist Jim Fan said on X as featured by the Journal.
Will DeepSeek’s success throttle Nvidia’s growth rate? I do not know. However, based on my research, businesses clearly want powerful generative AI models that return their investment. Enterprises will be able to do more experiments aimed at discovering high-payoff generative AI applications, if the cost and time to build those applications is lower.
That’s why R1’s lower cost and shorter time to perform well should continue to attract more commercial interest. A key to delivering what businesses want is DeepSeek’s skill at optimizing less powerful GPUs.
If more startups can replicate what DeepSeek has accomplished, there could be less demand for Nvidia’s most expensive chips.
I do not know how Nvidia will respond should this happen. However, in the short run that could mean less revenue growth as startups — following DeepSeek’s strategy — build models with fewer, lower-priced chips."
138,https://www.forbes.com/sites/moorinsights/2025/01/24/nvidia-brings-ai-to-the-physical-world-at-ces-2025/,Nvidia Brings AI To The Physical World,"Jan 24, 2025, 02:33pm EST",Bill Curtis,"In his CES 2025 keynote, Jensen Huang, CEO of Nvidia, delivered this widely quoted observation: “The ChatGPT moment for general robotics is right around the corner.” To support his “right around the corner” prediction, Huang listed three specific robotic embodiments that can function immediately without special environmental accommodations:
For enterprises with significant physical assets, I want to point out a fourth embodiment:
Combining robotics (automation), operational technologies (often called OT) and IT systems into a companywide, multimodal, real-time data estate connects the AI world with the physical world. AI-OT-IT fusion generates the real-time ground truth that upgrades decision making, enhances process efficiencies, provides a holistic context for advanced process automation (industrial robotics) and transforms ERP, SCM and BI analytics from reactive to proactive. Hence, physical AI is the new North Star driving industrial automation and enterprise digital transformation.
Nvidia’s “right around the corner” time estimate for physical AI is vague because two technical barriers prevent Huang (and me) from providing a specific timetable. First, physical AI requires new, real-world, physics-aware models and unique development platforms. Second, despite compelling business cases and a decade of IoT development, most data produced by OT is still inaccessible to IT systems and AI-powered business applications.
Driven by industrial customers eager to accelerate business transformation, OT suppliers are seeking expedient ways to bridge the OT-IT gap. Likewise, AI suppliers (including Nvidia) are doubling down on physical AI. Let’s take a look at some recent developments.
LLMs such as ChatGPT and Llama do not model the physical world. Developing accurate physical AI models for equipment such as robots, autonomous vehicles and industrial systems requires collecting, filtering, tagging and curating enormous amounts of real-world training data. To speed up this labor-intensive process, Nvidia has developed Cosmos, which it announced at CES 2025. Cosmos is a development platform for physical AI with that has a set of world foundation models trained on 20 million hours of video. The focus is on physical dynamics — teaching AI about the physical world so that virtual objects behave like real ones and obey the laws of physics. Nvidia says Cosmos will do for robotics and industrial AI what Llama 3 has done for enterprise applications.
Here’s how it works. Cosmos works with Omniverse, Nvidia’s graphics collaboration platform, to create realistic simulations for training physical AI systems. Development begins by using Omniverse to build realistic 3-D models of real-world facilities, machinery, robots and other equipment. Cosmos then uses generative AI to populate Omniverse scenes, leveraging its WFMs to generate photorealistic, geospatially accurate scenarios. Cosmos then synthesizes additional scenarios to create a multiverse of training data with many combinations of diverse and unexpected situations. Omniverse simulates these scenes, capturing visual data from various points of view, which enables developers to train, validate, test and optimize the target model.
Nvidia’s physical AI development and deployment platform comprises three distinct workloads running on three different types of computers:
These workloads are not practical on traditional CPUs, because all three require AI acceleration. Nvidia has optimized its AI development toolchain for DGX and OVX platforms, much like the company optimized its CUDA software for its GPUs.
Similarly, AGX is the native, optimized robotics platform target for Nvidia’s physical AI models. However, industrial customers need the flexibility to run AI applications across diverse physical AI embodiments. Platform targets vary from microcontroller-based sensors with modest ML inference acceleration to robotics computers capable of running large generative AI models, so AGX platforms aren’t always appropriate deployment targets. To put it another way, platform choice is use-case dependent, not a product selection decision like which GPU to buy.
Although Nvidia has offered cross-platform AI model deployment options for years, physical AI tools are new, and we don’t yet have practical experience using these workflows for non-Nvidia deployment platforms. I encourage Nvidia to tackle this issue head-on and build heterogeneous deployment targets into the physical AI toolchain from the start. Robust cross-platform support removes toolchain adoption barriers by enabling customers to use Nvidia’s tools across a broad spectrum of deployment hardware.
Although AI creates extremely compelling integration business cases, most operational data remains isolated from large-scale AI because of the complexity, cost, and security risks of connecting OT systems with mainstream IT. This is the OT-IT gap — the chasm between the heterogeneous, chaotic world of industrial IoT and the uniform, managed world of IT.
Today, AI is driving greater demand for operations data. Motivated by this, enterprise software suppliers are scrambling to find efficient and expedient ways to bridge the OT-IT gap. The solution is surprisingly simple. Instead of trying to push IT technologies into the OT world, suppliers are now moving to a straightforward “data first” mindset, which is a new way of thinking about OT integration. For years, developers have struggled to turn customized, complicated, expensive, hard-coded, application-specific device management and connectivity mashups into “end-to-end solutions.” Unfortunately, the past decade of IIoT projects taught us that this approach does not scale.
Developers are now turning to a better alternative: bridging OT and IT with simple interfaces for device identity, security, data, events and status. This approach simplifies OT data access and enables embedded OT software to evolve independently from cloud-native IT systems. Multimodal AI applications further reduce OT-IT integration costs by ingesting diverse types of machine data as-is, reducing the need for costly data transformation.
Recent product announcements and integration demos from AWS, Google, Honeywell, Microsoft, Qualcomm and other major cloud frameworks and ERP suppliers confirm this trend. (I’ll cover this topic in a follow-up article.) The goal is clear: feed the rapidly growing market for AI-enhanced business transformation with massive amounts of OT data via standard protocols and simple APIs. In other words, grab OT data without redesigning or intrusively modifying IIoT devices.
“I have a perfect model of the world. It’s actual size.” Comedian Steven Wright’s one-liner metaphorically describes building virtual replicas (i.e., digital twins) of physical objects, systems, and processes that look real and accurately simulate real-world behavior. It’s not so far-fetched — physical AI models really do appear to be “actual size” when viewed in 3-D. Adding OT data brings these models to life, accurately simulating complex scenarios in near real time. Physical models trained with dynamic simulations should enable a new generation of AI-powered applications to deliver step-function improvements in process efficiency, worker safety, equipment uptime, product quality, decision making and other high-value use cases. These use cases deliver impressive ROI — at least on paper and in the lab. However, physical AI technology the likes of Nvidia Cosmos is new, and interfacing with OT equipment is often problematic for all the reasons touched on above, so the timeline is still uncertain.
Here’s my take on a reasonable schedule for adopting these technologies. Nvidia’s AI ecosystem is solid, and the company has a lot riding on physical AI. Plus the company has become a prime mover in AI, and it is blessed with a mountain of money and a very, very long list of enterprise customers thirsty for its wares. All of that makes Cosmos and Omniverse good bets, and indeed customers are already developing solutions on the platform. For instance, my firm’s CEO and chief analyst Patrick Moorhead recently wrote about how Nvidia, Accenture and KION are collaborating to digitize warehouse operations. He shares my optimism about Nvidia’s physical AI platform. Enterprises with physical infrastructure can begin using these tools immediately while planning on large-scale deployments within the next year or two.
However, I am less optimistic about the economics and timelines for connecting OT data sources to physical models. The IIoT connectivity barrier is coming down, but not fast enough to keep up with physical AI growth. The problem arises from the diversity of OT and IIoT. IT systems have uniform architectures, but OT systems do not. In line with what I explained earlier, I recommend bridging the OT-IT gap with simple interfaces that gather OT data as-is rather than intrusively customizing IIoT devices. The good news here is that the focus on physical AI by Nvidia and others provides more impetus to achieve these connections. I predict significant improvement in AI-OT-IT connectivity this year as physical AI makes enterprises increasingly robotic and increases the urgency of bridging the OT-IT gap.
Moor Insights & Strategy provides or has provided paid services to technology companies, like all tech industry research and analyst firms. These services include research, analysis, advising, consulting, benchmarking, acquisition matchmaking and video and speaking sponsorships. Of the companies mentioned in this article, Moor Insights & Strategy currently has (or has had) a paid business relationship with Accenture, AWS, Google, Meta, Microsoft, Nvidia and Qualcomm."
139,https://www.forbes.com/sites/antonyleather/2025/01/23/nvidia-geforce-rtx-5090-review-rtx-4090-and-rx-7900-xtx-killer/,Nvidia GeForce RTX 5090 Review: Just How Fast Is It In Games?,"Jan 23, 2025, 09:00am EST",Antony Leather,"Today Nvidia has lifted the embargo on review benchmarks of its new flagship graphics card and in this Nvidia GeForce RTX 5090 Review you can see below just how fast it is in games. You can find results for framerates in a range of games to see just how fast it is compared to other models you might own or be considering such as the RTX 4090 or Radeon RX 7900XTX. The questions we all want to know are, of course, how much faster is it than those models both with and without new framerate boosting features such as Multi Frame Generation and DLSS 4.
To answer those questions, both older and more current games are in the benchmarks below, with other review website undoubtedly covering even more titles. Ultimately, the RTX 5090 like its predecessor was never about selling to the masses. Its $2000 price tag is evidence enough of that. But, the RTX 4090 still boasts above 1% market share of graphics cards listed in the Steam Hardware Survey so there are plenty being sold, even at these prices.

The RTX 5090 drew up to 562W according to Nvidia Frame View, but typically sat between 450-500W in games with the peak being in Alan Wake 2. The RTX 4090 seemed to draw noticeably less, typically sitting between 350-450W. However, the Founders Edition cooler seemed to do a great job, keeping the temperature below 65°C in games. While the compact, decent-performing cooler may mean that your case and cooling will be ample to deal with the RTX 5090 Founders Edition, you'll definitely need a power supply that has at least 600W on tap for the graphics card if it has a 16-pin 12VPWR PCIe 5 cable. Many sub 1000W power supplies lack that feature so check the specifications.
FEATURED | Frase ByForbes™
Unscramble The Anagram To Reveal The Phrase
Pinpoint By Linkedin
Guess The Category
Queens By Linkedin
Crown Each Region
Crossclimb By Linkedin
Unlock A Trivia Ladder
The test system included a AMD Ryzen 7 9800X3D processor liquid-cooled with Alphacool cooling hardware, 32GB of DDR5 6000 memory and a Thermaltake Toughpower GF3 1200W power supply. The latest version of Windows 11 was used as of Jan. 2025 as well as the latest drivers for other graphics cards. Only 4K resolution was tested here and only with the RTX 4090 and Radeon RX 7900 XTX for now, but more results and cards will be added in my forthcoming RTX 5080 review.
It's clear that while the RTX 5090 isn't on average a huge amount faster than the RTX 4090, in more demanding games and when switching on a lot of eye candy, the results can often be more significant. The bigger increases were nearly always with the average frame rate with 1% lows seeing less of a boost.
There will likely be a huge amount of reviews and additional testing out there over the next few weeks thanks to the various AI powered bolt on features from Nvidia such as DLSS 4 and Multi-Frame Generation. Below I've carried out tests in two games, with and without ray tracing and at various levels of Multi Frame Generation using the RTX 5090. As these were using non-public builds of these games, I've not included other graphics cards for now, but will once direct comparisons can be made. Still, it's clear to see the benefits of these technologies below using the RTX 5090.
The average frame rate is the one that benefits, usually increasing by three or four times compared to no frame generation and the benefits are roughly the same whether you enable ray tracing or not. In Cyberpunk 2077, the 1% low frame rate
It's fairly obvious that a tiny proportion of you reading this review will even be considering buying the RTX 5090 given its $2,000 price tag and that could be higher still as websites begin listing various models today with bigger more elaborate coolers. Still, it maybe gives a glimpse of what we can expect with other models, which if the limited comparisons here are anything to go by are decent uplifts over the previous generation, but being worth an upgrade from an RTX 4090 only in specific games and to be sure I'd always recommend checking out other reviews from the likes of Techpowerup.
Elsewhere, RTX 4090 owners won't see massive benefits, but there's no doubt the RTX 5090 offers a lot more performance and in the case of the Founders Edition model, takes up far less space too. If you've already used the likes of DLSS and frame generation, then you'll likely benefit far more as average frame rates can skyrocket in titles that support DLSS 4 and Multi Frame Generation. It's perhaps a personal decision but the features are widely considered useful tools in boost frame rates with little to no loss in image quality or game feel. That's certainly my view of them.
Ultimately if you must have the fastest PC around, this is the graphics card for you, but if you're budget is significantly smaller, remember that the RTX 5080 launches at the end of the month with a price tag of $1,000 and AMD is also releasing new graphic cards in the next month or two. For now and probably for the next 24 months, though, the RTX 5090 is likely to reign supreme."
140,https://www.forbes.com/sites/moorinsights/2025/01/23/nvidia-rtx-5090-graphics-card-review---get-neural-or-get-left-behind/,Nvidia RTX 5090 Graphics Card Review — Get Neural Or Get Left Behind,"Jan 23, 2025, 09:01am EST",Anshel Sag,"When Nvidia first announced the RTX 5000 series of graphics cards at CES 2025, it was clear that the company would be leaning even further into AI with these products. As anyone who follows enterprise technology knows, Nvidia is a major player — maybe the single most important player — in AI, so it’s no surprise that it has made significant headway in using AI to accelerate gaming.
While a lot of people will focus on the CUDA cores and Tensor Cores inside the GPU, there are also a lot of improvements that come from the new DLSS 4 software, which includes new transformer models and 4x frame generation. The RTX 50 series is the only product family from Nvidia capable of both features at that level, although the predecessor RTX 40 series does manage 1x frame generation and will take advantage of transformer models.
At the heart of the RTX 5090 is the GB202 GPU, based on Nvidia’s new Blackwell architecture. The full GB202 GPU has 24,575 CUDA cores, 192 ray tracing cores, 768 Tensor Cores and 768 texture units. The current version of that GB202 die based on TSMC’s 4N process node yields 21,760 CUDA cores, 680 fifth-gen Tensor Cores, and 170 fourth-gen ray tracing cores, plus it has a 512-bit memory interface. Down the road, as yields improve, it is possible we could see an RTX 5090 Ti with all cores enabled. For memory, the RTX 5090 has 32GB of GDDR7 using PAM3 pulse-amplitude modulation signaling for better frequency and voltage. This has resulted in 28 Gbps memory with 1,792 GB/s of memory bandwidth.
Compared to the 4090, the 5090 has next-generation CUDA, Tensor Cores and ray tracing cores. It also has almost 33% more CUDA cores than the RTX 4090, with similar upgrades for Tensor Cores, RT cores and RT performance. The 5090 also has a 575-watt GPU versus the 450-watt GPU in the 4090, all while using the same TSMC 4nm 4N process node and upgraded PCIe Gen 5 interface. Nvidia has upgraded the Tensor Cores to FP4 capability, which it claims has twice as much throughput of FP8 as the Ada Tensor Cores inside the 4090. The RT cores also saw a huge bump in performance in ray-triangle intersection testing and a ton of other ray tracing and path tracing features.
One new addition to the RTX 5090’s GPU is the AI management processor, or AMP, which is a fully programmable context scheduler. This is designed to reduce the overhead on the GPU for scheduling tasks to the different cores; it acts like a traffic cop for all the different workloads working on the GPU concurrently. This is a dedicated RISC-V processor that is located at the front of the GPU pipeline, which results in much lower-latency decision making than CPU-driven methodologies. The AMP is also compatible with Microsoft’s hardware-accelerated GPU scheduling introduced in Windows 10, so it shouldn’t create any new challenges for developers and should improve CPU utilization and latency when performing multiple tasks on the GPU simultaneously. All in all, this means a better experience when performing graphics and AI workloads simultaneously, which is increasingly happening more on Nvidia GPUs as features like DLSS become so important.
In addition to AI and graphics, the new GPU on the RTX 5090 brings enhanced 4:2:2 H.265 and HEVC video encoding capabilities. This improves encoding capability for 4K content that is shot in 4:2:2 color, which is becoming more standard for content creators, and for enabling new AV1 and HEVC encoding. Nvidia’s ninth-gen NVENC encoder also adds a new AV1 ultra-high-quality mode, which improves quality even further than standard AV1 quality. Nvidia also added a third encoder to the 5090, which can reduce encoding times by as much as 50% compared to the (already incredibly fast) 4090. Nvidia even says that it’s as much as 4x faster than the RTX 3090; I never got to test that model, so we’ll have to rely on other reviewers for that.
The display pipeline for the GB202 GPU supports DisplayPort 2.1b, which allows up to 80 Gbps of bandwidth utilizing UHBR 20. This translates to running up to 16K at 60 hertz, 8K at 120 hertz and 4K at up to 240 hertz.
The most consequential feature of the RTX 50 series is DLSS 4’s Multi-Frame Generation. This is the feature that effectively turns one rendered pixel into up to 16 total pixels. While the 40 series offers frame generation, it is capped at 1x, which improves graphical performance, but nowhere near the level that 4x does. The chart below lists DLSS capabilities as they pertain to the different series of Nvidia graphics cards; the company has said that it may offer frame generation on the RTX 30 series down the road, but that’s not certain.
DLSS Multi-Frame Generation boosts performance in concert with a new transformer model, which it uses to improve image quality while upscaling. This is the first time in five years that Nvidia has changed the type of model it uses for DLSS, having previously used a convolutional neural network model. One of the fundamental capabilities of DLSS is to render a game at lower resolution to achieve higher frame rate, then upscale it to the playable resolution, which can affect image quality. By using a transformer model, Nvidia improves the upscaling quality and arguably makes DLSS feel lossless, even if it isn’t literally so. Nvidia also uses the transformer model for its Ray Reconstruction function, which results in similar image quality improvements.
To test the RTX 5090, I built a new test bench using an AMD Ryzen 9800X3D processor paired with an ASUS X870E Hero gaming motherboard — sent to me by AMD — cooled by a 360mm ASUS ROG RYUO III CPU cooler. This was paired with 64GB of Patriot Viper DDR5 6000 MT/s RAM sent to me by Patriot Memory, a 2TB Crucial Memory T705 Gen 5 SSD and a Corsair 7000X case with a 1-kilowatt Corsair RM1000x power supply sent to me by Corsair. The monitor was an Alienware AW3225QF, which I reviewed last year; this monitor is capable of 4K 240-hertz gaming, which is where the RTX 5090 is designed to shine. All of these components were used in service of getting the best possible benchmark numbers for the RTX 5090 against the 4090.
Since AMD isn’t necessarily competing at the high end of the market against Nvidia for this generation, it seemed much easier to compare the 5090 against the 4090; this approach also accommodated the amount of time I had to build the system and test the new card, which was about three days. Needing to limit myself to just a few benchmarks, I chose Blender, 3Dmark and three relevant games that could show DLSS 4 with frame generation in action. Those three games were Marvel Rivals, Star Wars: Outlaws and Cyberpunk 2077. Nvidia says that it will have 75 games supported for DLSS 4 when retail availability starts on January 30. Cyberpunk 2077 is used to address this generation’s version of the “But can it run Crysis?” test. Nvidia and Cyberpunk maker CD Projekt RED have invested lots of time and money into making the game look really good.
First up is Blender, which has become one of the most popular creative tools for 3-D artists. The latest version, 4.3, is available in the Blender Benchmark, which is what I used to compare the 5090 to the 4090. It is made up of three different tests that compare the raw 3-D rendering power of the two cards.
As shown in the diagram above, the RTX 5090 is a clear winner in all three tests; it improved upon the RTX 4090 by 33% in Monster, 45% in Junkshop and 31% in Classroom. This is a respectable increase that would be appreciated by any 3-D creator, especially if they were coming from an even older card.
Next up, 3Dmark is a synthetic benchmark with two DX12 tests that don’t take advantage of the GPU’s AI capabilities and mostly focus on rasterization. Steel Nomad is a 4K benchmark with DX12 and HDR that uses advanced rendering techniques, while Speed Way uses a lower resolution (1440P) with ray tracing to offer a bit of both ray tracing and rasterization, although still without anything like DLSS. 3Dmark continues to be an industry-standard benchmark and a good way to test theoretical performance.
As these benchmarks show, the RTX 5090 is again considerably more performant than the 4090. Specifically, the RTX 5090 is 42% faster in Speed Way and 50% faster in Steel Nomad, which makes sense if you consider the increased CUDA and RT cores. That said, I would probably consider these the best-case scenario in games that don’t use DLSS; it’s likely that many games would deliver less than these numbers in the real world without AI.
All three games tested were titles that Nvidia identified on its DLSS 4 early-access list. I chose these three because they also represent a good diversity of games and a great way to understand how much Nvidia is improving with AI. Again, Nvidia says that 75 games will support DLSS 4 at retail availability, but 700 games already support DLSS in some capacity, and with DLSS override in the Nvidia App, we could see even more games support DLSS 4. Nvidia has the market share to get the industry to adopt DLSS, and with popular games such as Marvel Rivals adopting it at launch and supporting DLSS 4, we can expect it to make a significant impact from the get-go.
For Marvel Rivals, I turned on frame generation with both graphics cards, which automatically turns on low latency and DLSS. The game was also set to run at 4K while using Nvidia FrameView to track the frame rate. For those who aren’t familiar, 1% lows are the lowest frame rates experienced 1% of the time while playing a game; it’s a worse-case (not quite worst-case) scenario that can give more context to a simple average frame rate.
When I ran this test, 1% lows on the RTX 4090 were a still-very-playable 84 FPS, with an average frame rate of 160, while PC latency was 31 ms. However, the RTX 5090 absolutely blew past my expectations with a 1% low of 115 FPS and an average of a whopping 258 FPS, which is actually beyond the refresh rate of my 240-hertz monitor — and great for such a competitive game. The PC latency was also reduced 30%, down to 21 ms, which can make all the difference in a competitive title like Marvel Rivals.
For Star Wars: Outlaws I was provided by Nvidia with an early-access build to test DLSS 4. However, before I did that, I played the game on the RTX 5090 in 4K with ray tracing fully enabled but without frame generation enabled; in that configuration, the average frame rate was only about 40 to 50 FPS. Turning on DLSS with 4x frame generation boosted my frame rate to an average of 180, which made the game play entirely differently while still looking just as visually stunning. The RTX 4090 also has frame generation available, with an average of 96 FPS, but that is basically half the performance of the RTX 5090.
Finally, I turned on pretty much everything in both cards for Cyberpunk 2077, including ray tracing, frame generation and Ray Reconstruction. I did this while running the Cyberpunk in-game benchmark in 4K, which shows low, average and high FPS. The RTX 5090 performance was more than double the RTX 4090 in all three scenarios, which once again shows how much faster the latest AI makes the RTX 5090.
The RTX 5090 is a power-hungry beast. It is quoted on paper as having a 575-watt GPU, versus the RTX 4090’s 450 watts. Nvidia recommends a 1-kilowatt PSU for the 5090, which is an upgrade for many people. During my testing, the GPU monitoring app GPU-Z reported that the GPU reached 555 watts of thermal design power and 523 watts of peak board power; these numbers may not be 100% accurate, but they do indicate that the card is reaching nearly its full potential.
Thermally, this card is fantastic, especially when you consider that the RTX 4090 is a triple-slot card that has a colossal cooler on it. The RTX 5090 is considerably smaller, with an even smaller PCB to enable a pass-through. The card reached 77 degrees Celsius at the peak of my testing and externally never went beyond 62.1 degrees Celsius, according to my Flir thermal camera. The card did not get loud at any point when inside my case and truthfully seemed to handle the thermal load better than I expected, considering that it’s a two-slot card design with a smaller cooler than the 4090 — while also having over 100 watts more TDP.
To sum it up, when the latest AI capabilities are enabled, the RTX 5090 is without a doubt the fastest graphics card in the world. Even without the AI capabilities, it is still the fastest by a good margin. While I didn’t have the time to test creative applications such as Premiere Pro or DaVinci Resolve, I am excited by the addition of a third encoder and improved HEVC and AV1 encoding. While these encoders are great for creators and will speed things up considerably, they can and will likely further reduce the overhead on the GPU while streaming as well. The RTX 5090 is a powerhouse gaming, content creation and AI GPU with incredible performance across all three.
That said, this is still a $2,000 graphics card and will likely be in very tight supply for a while. We may unfortunately see the return of GPU scalping, even at a $2,000 list price. Note that this is a higher initial price than the RTX 4090, which debuted at $1,600, although it does leave room for a $1,500 RTX 5080 Ti as a nearly-as-good spec, given that the less-powerful RTX 5080 will retail for $999. I believe the pricing of the rest of the RTX 50 series is designed to squeeze AMD, while the RTX 5090 itself looks like an unapologetic halo. And while I do believe that the RTX 5090 is worth $2,000 for those with the budget and an appetite for top performance, I would also say that lots of people will likely benefit from the RTX 5080 with frame generation turned on.
The RTX 5090 brings together the best of the best technology across CUDA, RT and Tensor Cores and a management chip to ensure better latency and maximize efficiency. I am interested to see how these performance improvements translate to the RTX 5080 and RTX 5070, and how those will compare to Nvidia’s RTX 4080 and 4070 — and possibly even AMD’s upcoming Radeon RX 9070. In that connection, I am curious to see how AMD can respond to features like multiframe generation and Nvidia’s continued improvements to ray tracing performance. It will be important to test those capabilities across titles that support both Nvidia and AMD technologies such as FidelityFX Super Resolution. I worry that if AMD doesn’t offer something competitive to what Nvidia is doing with DLSS 4, we might not ever see AMD catch up.
Moor Insights & Strategy provides or has provided paid services to technology companies, like all tech industry research and analyst firms. These services include research, analysis, advising, consulting, benchmarking, acquisition matchmaking and video and speaking sponsorships. Of the companies mentioned in this article, Moor Insights & Strategy currently has (or has had) a paid business relationship with Alienware (Dell), AMD and Nvidia."
141,https://www.forbes.com/sites/saibala/2025/01/22/project-stargate-a-500-billion-ai-venture-between-oracle-open-ai-nvidia--softbank-that-will-revolutionize-healthcare/,"How Stargate Partners Oracle, Open AI, Nvidia And Softbank To Transform Healthcare","Jan 22, 2025, 08:00am EST","Dr. Sai Balasubramanian, M.D., J.D.","One among the flurry of announcements from the White House this week is the launch of Stargate, which President Trump announced on Tuesday alongside Larry Ellison, Sam Altman and Masayoshi Son.
The project, a joint venture between Oracle, Open AI and Softbank, includes the promise of nearly $500 billion to be invested into artificial intelligence infrastructure over the next four years. Other partners include Nvidia, Microsoft and ARM— companies that have all been paramount to the significant AI boom in recent years.
Exact details around the project, the plans and the vision forward are yet to be announced. President Trump explained that one of the primary goals of the project is to build data centers and the means to generate the copious amounts of energy that will be required for AI advancements.
“What we want to do is we want to keep it in this country,” Trump said. “China is a competitor, others are competitors. We want to be in this country, and we're making it available. I'm gonna help a lot through emergency declarations, because we have an emergency, we have to get this stuff built. So they have to produce a lot of electricity. And we'll make it possible for them to get this production done easily, at their own plants if they want.""
The OpenAI press release further explains that the project is crucial to maintaining “American leadership in AI” and will create hundreds of thousands of American jobs; with an initial commitment by the parties of $100 billion, Softbank has been listed as having financial responsibility and OpenAI will have operational responsibility.
“All of us look forward to continuing to build and develop AI—and in particular AGI—for the benefit of all of humanity. We believe that this new step is critical on the path, and will enable creative people to figure out how to use AI to elevate humanity,” OpenAI said in its statement.
Oracle will also have significant involvement in this project given its deep rooted knowledge and experience with data centers and enterprise analytics. Ellison, founder and chairman of the company, mentioned during the news conference that construction has already started on some of the data centers in Abilene, Texas,  reportedly the first location for the initiative. Furthermore, Ellison explained that one of the primary pieces that the team is already working on is developing tools for early cancer detection by using AI to analyze blood tests: ""cancer diagnosis using AI has the promise of just being a simple blood test.""
Ellison’s deep-rooted interest in healthcare is certainly not new. Oracle has made its commitment to redefine healthcare evident numerous times, most notably through its purchase of famed EHR company Cerner for nearly $28 billion a few years ago. More recently, the company announced that it would be shifting its headquarters to Nashville, a city known for its investments in the healthcare industry and technology.
Other partners in the joint venture are also equally ramping up their work in healthcare. Both Microsoft and Nvidia have made it clear that healthcare is among their top priorities, especially with regards to artificial intelligence technology. Certainly, given these partners’ individual interests in healthcare and joint efforts, this new venture could revolutionize the sector, especially as the field is increasingly embracing artificial intelligence tools to alleviate administrative burdens, improve workflows and further develop precision therapies and diagnostics.
2024 witnessed billions of dollars being poured into the AI race. In fact, the competition in this arena has never been more stiff, as Stargate is just one of many projects that are emphasizing the dire need to build infrastructure to support AI advancements. For example, AWS announced earlier this month that it plans to invest nearly $11 billion in Georgia to expand data center infrastructure as a means to support its AI and cloud computing work. Congruently, Meta has announced its plans to invest $10 billion for an AI data center in Louisiana, and Elon Musk has similarly announced his plans to ramp up xAI’s work in Memphis.
More details around Stargate will undoubtedly emerge in the coming weeks as plans are further ironed out. One thing is certain, however: the prospect of American ingenuity and leadership in AI is assuredly promising — as long as the present momentum is fully harnessed and a continued commitment to success is staunchly maintained."
142,https://www.forbes.com/sites/johnwerner/2025/01/19/understanding-the-physics-aware-systems-that-nvidia-is-working-on/,Understanding The Physics-Aware Systems That Nvidia Is Working On,"Jan 19, 2025, 09:12pm EST",John Werner,"As just one part of what came out of this year‘s CES earlier this month, Nvidia announced its development of something called Nvidia Cosmos.
The name itself doesn’t tell you a whole lot, invoking something broad – the celestial sky, or the cosmologies that we humans tell ourselves to explain the origins of everything.
So what is this system?
Nvidia defines Cosmos as “a platform of state-of-the-art generative world foundation models,” and defines world foundation models as “neural networks that simulate real world environments, and predict accurate outcomes based on text, image or video input.”
World models, spokespersons explain, “understand” the physics of the real world. They support the development of robotic systems and autonomous vehicles, and other physical structures that can follow the rules of the road, or the requirements of a workspace. In a way, these are the engines for the advent of physical entities that will think, reason, move and eventually live like humans.
Nvidia people also detail other aspects of Nvidia Cosmos, including “advanced tokenizers that help split higher-level data into usable pieces.”
For reference, here’s how ChatGPT describes an advanced tokenizer: “Advanced tokenizers go beyond simple whitespace or rule-based segmentation to produce subword, byte-level, or hybrid segments that better handle rare words, multilingual text, and domain-specific vocabulary…. These ‘smart’ tokenizers are a crucial foundation for modern NLP systems, enabling models to scale to massive datasets and diverse linguistic inputs.”
These models will be available under an open license, to help developers work on whatever they’re making. A Jan. Nvidia press release explains:
“Physical AI models are costly to develop, and require vast amounts of real-world data and testing. Cosmos world foundation models, or WFMs, offer developers an easy way to generate massive amounts of photoreal, physics-based synthetic data to train and evaluate their existing models.”
Notwithstanding understandable concerns about jailbreaking and hacks, companies will likely be excited about having this opportunity to build on what the leading U.S. tech company has created.
Then there is the process of data curation, where Nvidia NeMo will provide an “accelerated” process.
Anyway, TLDR: These are “physics-aware” systems. They sound like crucial pieces of applications that will bring AI to “walk among us,” to act on our lives, instead of just being siloed in a computer somewhere. What will our robot friends look like? And how will we treat them, and they us? These are the kinds of questions that we are going to have to consider as societies.
When I read the list of companies that have already been adopting Nvidia Cosmos technology, most of them were unfamiliar. But one stuck out:
The ride-sharing company Uber is an early adopter of this kind of physics AI.
“Generative AI will power the future of mobility, requiring both rich data and very powerful compute,” said Uber CEO Dara Khosrowshahi in a press statement. “By working with NVIDIA, we are confident that we can help supercharge the timeline for safe and scalable autonomous driving solutions for the industry.”
That phrase, “safe and scalable autonomous driving,” probably sums up the project well, although, as with self-driving vehicle designs over the past two decades or so, the devil is in the details.
There’s not a lot more available about exactly what Uber is doing with Nvidia Cosmos. But we can better understand the framework itself, and the context of what Nvidia is doing as a major innovator in these kinds of systems.
I was also reading about the Nvidia Omniverse platform that the company describes this way:
“A platform of APIs, SDKs, and services that enable developers to integrate OpenUSD, NVIDIA RTX™ rendering technologies, and generative physical AI into existing software tools and simulation workflows for industrial and robotic use cases.”
So what it sounds like is that the Omniverse platform is more for evaluation, monitoring and tool use, in aid of exploring what’s possible with the world foundation models themselves.
I’m going to end with this quote from CEO Jensen Huang, who reportedly intoned: “the ChatGPT moment for robots is coming.”
That’s probably the headline here, because all of us have been wondering when we will start seeing these smart, physics-aware robots walking among us, or powering truly autonomous vehicles.
The answer seems to be that it’s going to be sooner, rather than later."
143,https://www.forbes.com/sites/johnwerner/2025/01/16/top-nvidia-pro-dishes-on-the-future-of-ai/,Top Nvidia Pro Dishes On The Future Of AI,"Jan 16, 2025, 07:56am EST",John Werner,"One of the biggest questions in our new high-tech world is how soon we’re going to get physical AI agents in the form of advanced robotics.
You could also debate whether those robots are going to be humanoid, or somehow different.
But either way, there’s a consensus that one way or another, these new companions are coming to our world soon.
Jim Fan is a researcher at Nvidia, and has been talking about how imminent physical AI is.
“Robots will not be trained in isolation,” he posted on X on Christmas Eve. “They will be simulated as an ‘iron fleet’, deployed in real-time graphics engines, and scaled across a huge cluster to produce the next trillions of high quality training tokens. The majority of embodied agents will be born in sim, and transferred zero-shot to our real world when they are ready. They will share a ‘hive mind’ that sends latent embeddings back and forth to coordinate a multi-agent physical task.”
However, in a LinkedIn post that goes deeply into the mechanics of AI agents, he also suggests that this kind of intelligence is largely going to be disembodied first.
“Before we have a million robots in the physical world, we will first see a billion embodied agents in virtual worlds,” Fan writes. “Gaming is the second major area I'm dedicated to in 2024. AI and Gaming are born for each other, and their happy marriage is just getting started.”
One interesting bit in this post is where Fan calls game environments a “primordial soup for generalist AI to emerge.” Citing examples of Minecraft algorithms, he noted how agents are bounded by the complexity of their surroundings.
“There are a lot more games that require extremely advanced perception, agility, exploration, reasoning, and planning,” he writes. “We are just starting to scratch the surface.”
Fan also makes reference to some of the advanced strategy being used by today’s LLM engines.
I’ve written extensively about the idea of the modern transformer – as a critical part of LLM design. The transformer acts as a kind of “attention mechanism,” to allow the model to focus more on what’s important to humans, and less on what’s less relevant. That in itself cuts down on resource intensity for any given task , and creates enormous efficiencies for high-token systems.
“Tokens are actively selected by the agent itself through exploration,” he writes. “It (the agent) can choose to experiment with things that maximally reduce its internal uncertainties - kind of like how human curiosity works.”
It’s interesting to think about the AI engines having this drive, and how they would gain knowledge, or, as Fan puts it, “reduce their internal uncertainties” – which, in fact, sounds kind of like a fancy phrase for learning itself…
“I believe 2024 is an inflection point,” Fan continues. “The Digital Westworld is coming, and will transform the industry once and for all.”
As he describes how non-player characters will act, think about how we have viewed NPCs in the past – as stilted, evidently artificial players, in contrast to fully human ones.
“Games will feel truly alive,” Fan adds. “The characters will interact with humans and each other, form relationships, take consistent actions over their lifetime, and react in human-like ways. Each game will have infinite replay value, and each player will have (a) unique and tailored experience.”
If you’ve already heard a young gamer talking about forming relationships with non-player characters in today’s games, get ready, because this sort of thing is going to expand as the NPCs really become more human.
“As video games have evolved, the technology underpinning NPCs has had to evolve with them,” wrote Ilya Gelfenbeyn at Inworld in January of last year, commenting on this advancement. “The evolution of NPC behavior has been shaped by advancements in technology opening up new opportunities for more complex trait scripts, also known in the industry as a job system. Put simply, this means NPCs can be scripted to respond in a larger number of ways according to set variables.”
At the end of the day, we’re likely to see a lot of these advances as a sort of gamified reality. They’ll come in in the form of entertainment and explorative play, but they’re likely to go beyond that, to become vital parts of our experience as humans.
As for the physical robots, we’re likely to see that play out in a utilitarian way, too. People always talk about the butler robot, and when we’re going to get that. At the same time, population is declining, and labor is in demand. We’re soon going to have all of these automatons doing the work.
Anyway, I’ll bring you more as it develops through 2025."
144,https://www.forbes.com/sites/patrickmoorhead/2025/01/15/nvidia-accenture-and-kion-use-physical-ai-to-transform-warehousing/,"Nvidia, Accenture And KION Use Physical AI To Transform Warehousing","Jan 15, 2025, 02:46pm EST",Patrick Moorhead,"Though the details can obviously be complex, the concept of using AI with digital information to solve problems is straightforward. But what about physical information? More specifically from a business standpoint, what about using AI to solve challenges in complex physical environments that are constantly in flux, like warehouses? That’s the challenge that Nvidia, Accenture and the big supply chain automation company KION have partnered to address.
Last week at CES I had a chance to sit with Accenture CEO Julie Sweet, KION CEO Rob Smith and Nvidia CEO Jensen Huang to talk about their partnership during a special session for four press outlets. (I was the only analyst in attendance.) I believe that in the long run technology like this will fundamentally change the way supply chains work. There have been many fits and starts with “industrial IoT” and “Industry 4.0,” however, and we don’t know yet how long that will take.
Warehouse logistics within big supply chains can be insanely complex. Workers and managers have to factor in an endless array of constantly shifting variables, from consumer demand to inventory on hand to weather conditions. This trillion-dollar market touches every sector that handles physical goods — healthcare, electronics, food, CPG, you name it — each with its own special considerations that must be factored in. It’s all so complex that accurately predicting operational performance for warehouses and distribution centers can be next to impossible. As Smith, who has decades of experience in this space, put it, “Everything’s changing all the time in a warehouse, and systems aren’t smart enough to not only figure out what’s the next move, but figure out what could be potential other next moves and what’s the best next move.”
Now, Accenture, KION and Nvidia are promising a way out of this maze. Physical information from a warehouse can be digitalized by KION software into a highly accurate digital twin that lives on Nvidia’s Omniverse platform. Nvidia’s AI technology then enables rapid simulation of scenarios under different conditions to optimize warehouse operations. Accenture applies its expertise to help define and manage KPIs.
While digital twins have been used for years, they’ve never had this much horsepower across multiple layers of cutting-edge technology — and especially not this much advanced AI. The business rationale is clear. As Huang said, “Every industry that becomes digitalized moves faster, [and] everything you can software-define becomes more capable. . . . When you’re digitalized, you can build consistently with greater capability, but when you become software-defined on top of that, you get to revolutionize your business.”
Huang pointed out that these advantages have been commonplace in the IT industry for decades, but have never been enjoyed broadly in the industrial sector. This makes sense because it’s easy to take digital data or a digital product like a computer or a microchip and then use digital tools to manage it. But, Huang said, “The world’s physical plants, the physical world, has never been digitalized, truly — not until now.”
To dig into the specifics, KION is adopting Mega, an Nvidia Omniverse blueprint for large-scale industrial digital twins. Underlying this is Nvidia’s Cosmos physical AI model. “The fundamental idea of Cosmos,” Huang said, “is a model that understands the physical world like ChatGPT understands information and language.” Huang compared Cosmos to Meta’s Llama and OpenAI’s GPT-4, noting that Cosmos was trained on 9 trillion parameters — a process that required six months and tens of millions of dollars of investment. He said that Nvidia is making it an open model “like Meta opened Llama.”
As with ChatGPT, Cosmos allows a user to generate a bunch of alternative outputs. But whereas ChaptGPT could write you many different versions of a fairy tale, Cosmos can generate many different 3-D video simulations of a specific warehouse under alternative scenarios. These can account for different layouts, numbers of employees, numbers of robots, and so on to allow facility operators to understand which scenario is best for throughput, labor cost, safety measures, error rates, or whatever other KPI is desired. To make sure the simulated versions don’t contain any hallucinations, the system is grounded in the real-world context of the facility, supplied via Omniverse. This data comes from still and video images, CAD models, lidar scans, sensors on robots and other sources to anchor the scenario in the exact physical details. Huang compares this to using RAG to prevent hallucinations in ChatGPT or the other non-physical AI environments we’re more familiar with. It took me a lot of time and research to distinguish between Cosmos and Omniverse, and I like to think that Omniverse makes Cosmos results more accurate, like RAG does for many other non-physical enterprise AI applications.
Because facility operators cannot afford downtime, all of this happens on the fly. KION’s warehouse management software assigns a task — say, moving a load from one location to another — and the industrial AI “brains” within the digital twin work out the implications, planning and (virtually) carrying out next steps while Mega tracks what happens through continuous feedback loops. All of this can be simulated as much or as little as needed within the digital twin to optimize for specific outcomes, then implemented in the real world. During the small group session, Smith pointed out the importance of this for handling different real-time conditions inside a distribution center, using the contrasting examples of Black Friday and a slow summer day.
As for the impacts of this technology, Sweet believes that it could ultimately cut the time it takes to plan a new warehouse in half. For ongoing operations, she projects similar 50% reductions in manual labor and operating costs. She (echoed by the other two CEOs) expressed that this could take pressure off the retail industry, consumer goods makers and other sectors that have been battered by inflation in recent years. She added that this initiative “is very much about resilience and agility” for the companies that will use it.
The CEOs all agreed that it can also help address ongoing labor shortages. “The fact of the matter is,” Huang said, “we’re tens [of millions] or 100 million workers short around the world. We are deprived of revenues because of worker shortage, not the other way around. There’s several trillion dollars’ worth of lost revenues because there aren’t enough workers, and so we need to augment the workers that we have.”
Smith gave more color on this for warehousing in particular. He said, “It's very difficult for all of our customers — worldwide, any region, every segment, every vertical — [they] can't find manual labor to come in and work in a distribution center.” In that context, “We're automating to make every job better, and that gives people opportunity.” Rather than trying to attract workers, especially young people, to manual entry-level jobs, automation takes a lot of those jobs out of the picture while introducing higher-skilled jobs to make sure the systems are running right — jobs that Smith says are much more exciting and interesting.
Last week’s CES was my 20th one to attend, and I can recall bullish announcements from ten years ago about reinventing warehousing, smart transportation and related tech. And I do believe that the technology available to us today — not just the AI but also other aspects such as edge computing — is much better than it was then. But during the session I asked the three CEOs what makes this moment different in terms of uptake for their customer companies. In other words, what other gears besides the existence of better technology need to click into place?
Smith answered by pointing out the great advances in recent years for autonomous mobile robotics. Simply put, there are many more autonomous robots handling tasks in today’s warehouses and factories, which takes us beyond the classic fixed robots that have, for example, been used in automotive assembly for decades. He emphasized how autonomous mobile robots give operators much more flexibility to scale over time — now augmented with real-world scenario planning and advanced orchestration that simply weren’t possible before.
That’s the practical side as it applies on the shop floor. Sweet added to it by speaking to organizational readiness. She said that in the years leading up to 2022, she and her colleagues were telling every client company that it needed to reinvent itself with AI, but that only 20% of client CEOs were aligned with that view. Fast-forward to today, after years of generative AI being so widely touted and used, and “It’s exactly flipped . . . at least 80% of CEOs have embraced AI.” She described it as “a fundamentally different condition than we've seen for the last decade,” adding, “We are not out there convincing people.”
Huang also pointed out that the technology itself can help companies get past a chicken-and-egg problem: “In order to automate, you have to make investments, and making that investment is hard to activate unless you can see the returns — but you can't see the returns until you make the investment.” But a digital twin “allows us to lower the bar” for potential customers to understand those returns. “And so instead of having to build out their factories, automate their factories, before they see the benefits of it, they can simulate their factory and see the benefits of it.”
As with any potentially revolutionary technology, I’ll believe the most optimistic projections only after I see some real-world results. We’re not sure when that will be, because so far the three partners haven’t committed to a specific timetable for rollout. But when Smith talks about this kind of automation becoming so prevalent and so advanced that all the robotic nodes in the supply chain talk to each other, I’m prone to believe him. Ditto when he says, “Ultimately, I think everything that has a physical instance is going to have a digital instance as well.” It’s just a question of how soon “ultimately” will get here.
The opportunity is definitely enormous. Smith believes that less than 20% of the world’s warehouses have significant amounts of automation in them today. And if anyone would know, he would, because KION has helped a slew of companies across many different industries — from Amazon on down — with warehouse automation.
What I heard from all three CEOs fits with what we’ve learned about (digital) AI in recent years, and it fits with the robotics advancements that Huang discussed in depth during his CES keynote. So I do believe this kind of automation can help supply chain operators make better decisions informed by real-world conditions, which could raise performance standards and improve efficiency and productivity across highly autonomous — and potentially safer — supply chains. We just don’t know how soon that future will arrive. I am a tech optimist, though, and do believe that the time is now.
Moor Insights & Strategy provides or has provided paid services to technology companies, like all tech industry research and analyst firms. These services include research, analysis, advising, consulting, benchmarking, acquisition matchmaking and video and speaking sponsorships. Of the companies mentioned in this article, Moor Insights & Strategy currently has (or has had) a paid business relationship with Accenture, Meta and Nvidia."
145,https://www.forbes.com/sites/zinnialee/2025/01/15/nvidia-joins-4-million-seed-round-in-taiwans-ai-powered-simulation-startup-metai/,Nvidia Joins $4 Million Seed Round In Taiwan’s AI-Powered Simulation Startup MetAI,"Jan 15, 2025, 05:30am EST",Zinnia Lee,"MetAI Technology, a Taipei-based startup that makes 3D simulations for logistics and manufacturing industries, said on Wednesday it had raised $4 million from investors including billionaire Jensen Huang’s chip giant Nvidia.
Other investors who joined the seed round include Taiwanese electronic manufacturing company Kenmec Mechanical Engineering and industrial automation software provider Solomon Technology, as well as local venture capital firms Addin Ventures, SparkLabs Taiwan and Upstream Ventures, MetAI said in a press release. MetAI is “the first Taiwanese company in Nvidia’s portfolio,” according to SparkLabs Taiwan cofounder Edgar Chiu in a Linkedin post.
Established in 2023, MetAI said it has developed a software that uses AI to convert 2D floor plans of automated warehouses into 3D simulations in a few minutes, compared to about 300 hours using other methods. Such simulations, known as digital twins, allow logistics companies to test the efficiency of their warehouses and facilities before real-world deployment. MetAI’s software can be used within Nvidia’s Omniverse digital twin platform.
MetAI said it will use the funds on research and development and hiring. It will also expand into the U.S. to target clients such as semiconductor manufacturers, automated warehouse operators and robotics companies.
“Developing physically accurate digital twins is seen as a barrier to entry for those looking to develop physical AI at scale,"" said Daniel Yu, cofounder and CEO of MetAI, in the press release. “We’re changing that narrative by enabling businesses to instantly train AI systems—whether for robotics, logistics, or manufacturing—in physics-based virtual environments with unprecedented efficiency and scalability.”
MetAI said its digital twin technology can be used to build the so-called “large world models”—a type of AI model that simulates the real world so that autonomous machines like robots and self-driving cars can be trained and tested.
Among the startups that are looking to build “large world models” include San Francisco-based World Labs. Cofounded by Fei-Fei Li, former chief scientist of AI at Google Cloud, World Labs is backed by NVentures, the venture capital arm of Nvidia, among others."
146,https://www.forbes.com/sites/jasonevangelho/2025/01/15/valve-interview-reveals-the-future-of-steamos-on-intel-and-nvidia-pcs/,Valve Discusses Future SteamOS Support For Intel And Nvidia PCs,"Jan 15, 2025, 12:28pm EST",Jason Evangelho,"2025 is already shaping up to be a terrific year for gamers who prefer SteamOS to Windows on their handheld devices. In May, Lenovo will ship the Legion Go S Powered by Steam OS, and Valve promised an installable beta of the Linux-powered OS for other handhelds before that. But with all this positive news comes pressing questions: What about folks who want to install SteamOS on their desktop PCs? What about systems with Intel and Nvidia components?
Right now, SteamOS runs like a champ on the Steam Deck, which features a semi-custom AMD processor. And in the near future it will run smoothly on another semi-custom AMD processor — the Ryzen Z2 Go — inside the Lenovo Legion Go S. But Valve’s messaging has been intentionally vague when discussing the expansion of SteamOS beyond these AMD-powered handhelds.
There is very hopeful news on the horizon, however. A recent interview between French outlet Frandroid and Valve designer Pierre-Loup Griffais — a key and vocal figure behind SteamOS development — reveals quite a bit about Valve plans and its progress toward delivering SteamOS for the rest of us.
On the topic of Intel support, Griffais says “[...] on some platforms, the support is still very basic. Intel is working a bit better than before, but our driver teams and Intel are still working on it.”
The first takeaway, then, is that Valve and Intel are collaborating to tackle this, which is encouraging. With the strong integrated graphics performance of Intel’s Lunar Lake, this paves the way for supporting SteamOS on MSI’s Claw 8 AI+ and future handhelds adopting Intel processors.
But the big elephant in the room has always been Nvidia. Unlike Intel and AMD, Nvidia’s proprietary GeForce graphics driver isn’t baked into the Linux kernel, although Nvidia is transitioning towards that.
“With NVIDIA, the integration of open-source drivers is still in its early stages,” Griffais says. “There’s still a lot of work to be done on that side. So it’s a bit tricky to say we’re going to release this version when most people wouldn’t have a good experience.""
Griffais also revealed that Valve has four developers dedicated to working on Nvidia’s open-source driver. “It’s just that there’s still a lot of work to do,” Griffais says.
OK! Takeaway number 2, then, is that Valve is definitely committed to getting SteamOS supported on Nvidia hardware —work that will likely benefit the entire Linux gaming system and the open-source community.
Part of the reason Valve is taking graphics driver development under its wing is because it can optimize games for SteamOS “without having to wait for a manufacturer to take care of it.”
We’ve seen this approach yield positive results on Steam Deck — and by extension Linux — as far back as Elden Ring in 2022, when Valve issued a fix that solved the game’s horrible stuttering. In fact, at the time, it was running more smoothly on Linux than on Windows.
While there’s no roadmap or solid timeline (there rarely is where Valve is concerned), it’s refreshing to see confirmation that Valve is hard at work building SteamOS support for the wider PC gaming ecosystem.
I encourage you to read (and translate) the entire interview. It’s a fantastic and insightful discussion which also touches on Linux distributions like Bazzite and Nobara, future Valve hardware endeavors, and the very long road Valve has traveled to get to this point."
147,https://www.forbes.com/sites/antonyleather/2025/01/13/nvidia-rtx-5090-and-rtx-5080-review-dates-revealed/,Nvidia RTX 5090 And RTX 5080 Review Dates Revealed?,"Jan 13, 2025, 01:46pm EST",Antony Leather,"We already know from Nvidia's keynote presentation at CES last week that the RTX 5090 and RTX 5080 will be available from Jan. 30, but what we didn't know is when reviewers will get their hands on the graphics cards to be able to compare them side by side with others in games. The first embargo date, claimed by Videocardz today, will be Jan. 24, and that will be for the RTX 5090.
The site hasn't listed a source, but neither does it list the post as rumor either so this suggests it either came from Nvidia itself or perhaps a retailer or board partner.  This means it's just 11 days till we'll know just how fast the RTX 5090 is compared to other models such as the RTX 4090 and AMD Radeon RX 7900XTX.
The article goes on to say that the RTX 5080 will see reviews go live on Jan. 30, which is the same day as availability. The latter has caused speculation online as to why Nvidia would release the card on the same day as availability with some suggesting its performance is maybe less impressive than the RTX 5090 and it’s relying on would be owners to reach for their wallets before reading reviews.
These dates have, according to Videocardz, been pushed back due to a late arriving BIOS for the new graphics cards, which meant that AIB or partner card manufacturers requested a delay to prepare their own models. The article also claims that both Nvidia's own Founders Edition models as well as AIB card models will see embargos lifted the same day.
No other sources have confirmed these dates so for now it's better to consider them rumors, but we won't know exactly till the reviews go live as those sites will be under embargo not to reveal anything.
The RTX 5090 will retail for $1,999 and will have 32GB of GDDR7 memory and Nvidia claims it's up to twice as fast as the RTX 4090. most likely when using DLSS4 and Multi Frame Generation. The RTX 5080 will cost $999 - the same as the RTX 4080 did at launch and have 16GB of GDDR7 memory.
Interestingly, the Founders Edition cards will be much smaller than their predecessors, spanning only two slots instead of three with the RTX 4080 and 4090 Founders Edition cards. This means they'll have far fewer compatibility issues and will enjoy increased compatibility with small form factor mini-ITX cases.
Meanwhile, AMD is also expected to release its new RDNA 4-based Radeon RX 9000 series, with the RX 9070 XT offering Performance between the RTX 4070 Ti Super and RTX 4080, even eclipsing the RTX 4080 Super in one test. Pricing has also been rumored to start at just $479 for the reference card and $600 including taxes for AIB or partner card models.
For now you can read more about the new Nvidia graphics cards on its website.
I'll be covering all the news about them as it lands plus my own reviews later this month.  follow me here on Forbes using the blue button below, Facebook or YouTube to get the latest news and reviews."
148,https://www.forbes.com/sites/davealtavilla/2025/01/13/cosmos-marks-another-masterful-stroke-for-nvidia-in-ai-robotics/,Cosmos Marks Another Masterful Stroke For Nvidia In AI Robotics,"Jan 13, 2025, 08:56pm EST",Dave Altavilla,"At what was likely the most widely attended keynote in CES history, Nvidia CEO Jensen Huang took to the stage at the jam-packed Michelob Ultra arena, with a dizzying array of announcements of new technologies, from consumer devices like the new GeForce RTX 50 Series of gaming graphics cards, to a new secure autonomous vehicle platform called Thor that’s based on the company’s latest Blackwell GPU technology, and more – a lot more. However, a new Nvidia generative AI technology dubbed Cosmos, that some folks might have glossed over due to its complexity was, in my opinion, another star of the show. I’d even dare say, if Cosmos plays out as the company is intending, it could be a launch-pad for rocketing Nvidia’s robotics and autonomous vehicle businesses.
Nvidia calls Cosmos a “platform for accelerating physical AI development.” And Simply put, you can think of physical AI as the brains behind anything robotic, whether it’s humanoid robots that are designed to optimally navigate the world we live in, factory automation robots, or autonomous vehicles, which are optimized robots for navigating our roads, carrying humans or various payloads. However, training robotic AI is hugely labor and resource-intensive, often requiring the capture, labeling and categorization of millions of hours human interaction in real world environments, or millions of miles driven on real roadways around the world.
Nvidia Cosmos aims to partially solve this resource problem with a family of what the company is calling “World Foundational Models,” or AI neural networks that can generate accurate physics-aware videos for the future state of a virtual environment – or a multiverse, if you will. You can go ahead and queue Dr. Strange now, and Jensen even referred to the Marvel character in his keynote presentation. It all sounds mind-bendingly deep, but it’s actually fairly straightforward. WFMs are similar to Large Language Models, but where LLMs are trained AI models for natural language recognition, generation, translation, etc., WFMs utilize text, images, video content and movement data to generate simulated virtual worlds and virtual world interactions that have accurate spatial awareness, physics and physical interaction, and even object permanence. For example, if a bolt rolls off a table in a factory, and can’t be seen in the current camera view, the AI model knows it’s still there but perhaps just on the floor.
Still with me? Good, because this is where it gets even more interesting. This new form of synthetic data generation to train physical AI, or robots, needs to be based on ground truth to be accurate. In other words, bad data in means a corrupt model that hallucinates or is otherwise unreliable for generating training data for robotic AI. That’s where Nvidia Omniverse, which the company announced a couple of years ago, comes into play.
Nvidia’s Omniverse digital twin operating system allows companies and developers from virtually any industry to simulate products, factories, robots, vehicles, etc. in an environment that’s designed to connect with industry standard tools, from computer aided design, to animation and more. In fact, Nvidia unveiled new Omniverse “Blueprints” at CES 2025 as well, to help developers in simulating robot fleets for factories and warehouses (called Mega), and AV simulation, spatial streaming to the Apple Vision Pro headset for large-scale industrial digital twins, and real-time Computer Aided Engineering and physics visualization. The company bundles these with free instructional courses for OpenUSD, or Universal Scene Description, which is the language that underpins Omniverse and allows the integration of industry standard tools and content. Nvidia announced several major players are adopting its Omniverse platform, from Cadence for EDA design tools for semiconductors, to Altair and Ansys for computational fluid dynamics, among many others.
Circling back to Cosmos, now we can see Nvidia’s full stack solution coming together for physical AI in robotics. Cosmos models take input from a digitized version of the real world, and then generate AI training content from it. Though Cosmos models were developed from training on 20 million hours of video data, according to Huang in his keynote address, developers that want to train physical or robotic AI on their own digital twins and their own data can simulate in Omniverse, and then let Cosmos play out a myriad of synthetic realities that these robot AIs can then train on.
At this point, I know what you’re thinking. Training robots on simulated data and in simulated worlds, what could go wrong? There’s no question, this technology is still in its infancy, but as the old saying goes, you have to start somewhere. The beauty of machine learning, though it’s prone to hallucinations and needs to have guardrails (which Nvidia has a well-documented tools and policies on), is that you can train and keep training until you’re confident you’ve got it right. And the machine doesn’t sleep or take coffee breaks, not to mention it’s a whole lot more efficient than manually training an AI on human generated and categorized content.
That said, years ago, when Nvidia first announced its CUDA programming language that sparked the age of machine learning on GPU accelerators, the company went Johnny Appleseed, so to speak, making its tools available to developers from all walks of life, eventually allowing it to become the de-facto standard for accelerating AI workloads in the data center. With Cosmos, Nvidia is once again making these generative AI World Foundational Models available to developers for free, under its open model license, and they’re accessible on Hugging Face or the company’s own NGC catalog repositories. The models will also soon be available as optimized Nvidia Inference Microservices (or NIMs), all of which will be accelerated on its DGX data center AI platforms and at AI edge devices, in robots and autonomous vehicles, with its AGX Drive Orin and Thor car computer platforms for autonomous vehicles. Or, as Huang and the company call it, Nvidia’s “Robotics 3 Computer Solution.”
Nvidia notes that several big-name players in physical AI have already adopted Cosmos, from humanoid robot companies like 1X and XPENG, to Hillbot and SkildAI for general purpose bots, to rideshare giant, Uber, that’s using Cosmos in combination with its massive driving datasets to help build AI models for the AV industry.
It might be a stretch to call this another “CUDA moment” for Nvidia, but the world’s leader in AI just dropped some seriously powerful new tools for physical AI developers, and for free. I personally think it’s another master stroke for Jensen Huang and his band of AI wizards. We’ll have to see just how far this robotic AI, multiverse rabbit hole goes with Cosmos, and it should be fascinating to watch.
Dave co-founded and is principal analyst at HotTech Vision And Analysis, a tech industry analyst firm specializing in consulting, test validation and go-to-market strategies for major chip and system OEMs. Like all analyst firms, HTVA provides paid services, research and consulting to many chip manufacturers and system OEMs, including companies mentioned in this article. However, this does not influence his objective coverage."
149,https://www.forbes.com/sites/dereksaul/2025/01/13/nvidia-slams-biden-overreach-and-praises-trump-as-ai-stock-enters-correction-territory/,Nvidia Slams Biden ‘Overreach’ And Praises Trump—As AI Stock Enters Correction Territory,"Jan 13, 2025, 10:41am EST",Derek Saul,"Nvidia blasted the Biden administration and hyped President-elect Donald Trump in a Monday press release reacting to more export restrictions targeting Nvidia’s highly sought-after artificial intelligence chips, a criticism coinciding with a rare down stretch on Wall Street for Nvidia—and mounting Silicon Valley support for Trump.
President Joe Biden announced Monday a framework to govern the transfer of AI technology most famously designed by Nvidia to cap AI chip sales abroad at both the country and company levels, a move which the White House characterized as an effort to keep away the advanced AI systems out of the hands of “countries of concern” as Biden has voiced concern about China’s use of American-made AI throughout his presidency.
The potential for limited exports poses a potential headache for Nvidia, as China, Singapore and Taiwan accounted for more than half of the company’s $35 billion in revenue during its most recent financial quarter, with China alone accounting for $5.4 billion of Nvidia’s quarterly sales.
Nvidia lashed out against the rules in a strongly worded statement: “The new Biden rules would only weaken America’s global competitiveness, undermining the innovation that has kept the U.S. ahead,” clapped back Ned Finkle, Nvidia’s vice president of government affairs, arguing it’s an attempt from the White House “to rig market outcomes and stifle competition.”
The Nvidia response also indicated a warm attitude toward Trump, as Finkle wrote the “first Trump Administration laid the foundation” for the recent AI revolution.
Nvidia looks “forward to a return to policies that strengthen American leadership, bolster our economy and preserve our competitive edge in AI and beyond,” Finkle added.
Shares of Nvidia fell as much as 4.7% to $129.51 shortly after market open, as pressure on its abroad business accelerated its recent stock market slump. The stock was down more than 15% from its all-time-high share price set last week, erasing as much as $578 billion in market capitalization from its $3.75 trillion valuation peak set Tuesday morning. Other big technology stocks struggled Monday, with shares of Alphabet, Apple and Tesla all declining at least 1% as the S&P 500 benchmark index sank to its lowest level since early November.
Much of Nvidia’s stock surge came while Biden was in office. The company’s share price is up 880% since Biden’s 2021 inauguration, compared to a 414% rally during Trump’s first term, according to FactSet data.
Nvidia is far from the first big tech firm to express excitement about the prospects of Trump’s second term, with the likes of Amazon and Alphabet committing $1 million to Trump’s inauguration fund as industry leaders like Meta CEO Mark Zuckerberg, Apple CEO Tim Cook and  Amazon chairman Jeff Bezos all met with Trump following his November election win. Zuckerberg also announced sweeping changes to Meta’s policies ahead of Trump’s inauguration: The company is ditching fact-checks in favor of X-style community notes, and Trump ally Dana White will join the board. Trump’s administration includes the newly-created role of “White House AI & Crypto Czar,” filled by Silicon Valley venture capitalist David Sacks, in an effort to make the U.S. “the clear global leader” in AI and cryptocurrency, Trump said in December. Nvidia is the face of the 2020s generative AI boom, as it designs a majority of the data-intensive hardware and software systems used to train AI technology like OpenAI’s ChatGPT chatbot and Tesla’s self-driving programs."
150,https://www.forbes.com/sites/janakirammsv/2025/01/12/everything-you-want-to-know-about-nvidia-project-digits-ai-supercomputer/,Everything You Want To Know About Nvidia Project Digits AI Supercomputer,"Jan 12, 2025, 02:34am EST",Janakiram MSV,"At CES 2025, Nvidia announced its first personal AI supercomputer, Project Digits. The rise of generative AI demands access to a new generation of CPUs and GPUs for data scientists and AI engineers working on state-of-the-art models and solutions. Digits is targeted at developers and data scientists looking for an affordable and accessible hardware and software platform to handle the lifecycle of generative AI models. From inference to fine-tuning to developing agents, Digits has everything it takes to build an end-to-end generative AI solution.
Here is a deep-dive analysis of Project Digits:
With a price tag that starts at $3000, Nvidia Project Digits is a compact device driven by the innovative Nvidia GB10 Grace Blackwell Superchip. It empowers developers to prototype, fine-tune and execute massive AI models locally. This accessibility marks a significant step toward democratizing AI training, making it more accessible for individuals and smaller organizations.
Independent software vendors can leverage Digits as an appliance to run their AI-powered software deployed at a customer location. This reduces the reliance on the cloud and delivers unmatched privacy, confidentiality and compliance.
At the heart of Digits lies the Nvidia GB10 Grace Blackwell Superchip, a marvel of engineering that combines a powerful Blackwell GPU with a 20-core Grace CPU. These two powerhouses are interconnected using NVLink-C2C technology, a high-speed, chip-to-chip interconnect that facilitates rapid data transfer between the GPU and CPU. Think of it as a superhighway connecting two bustling cities, allowing seamless and efficient communication. This tight integration is crucial for Digits’ impressive performance, enabling it to handle complex AI tasks quickly and efficiently.
Here’s a breakdown of the key specs:
Digits also boasts an impressive 128GB of unified memory. This means the CPU and GPU share the same memory pool, eliminating the need to copy data back and forth and significantly speeding up processing. This is particularly beneficial for AI workloads, which often involve large datasets and complex computations. Digits incorporates high-speed NVMe storage to enhance performance further, ensuring rapid access to the data required for AI model training and execution.
Despite its impressive performance capabilities, Digits is designed with power efficiency in mind. Unlike traditional supercomputers that often require specialized power and cooling infrastructure, Digits can operate using a standard wall outlet. This makes it a practical and accessible solution for individuals and smaller teams who may not have access to the resources required to run larger, more power-hungry systems.
Project Digits is designed to seamlessly integrate into Nvidia’s extensive AI ecosystem, providing developers with a cohesive and efficient environment for AI development. Based on the Linux-based Nvidia DGX OS, it ensures a stable and robust platform tailored for high-performance computing tasks. Preloaded with Nvidia’s comprehensive AI software stack, including the Nvidia AI Enterprise software platform, Project Digits offers immediate access to a wide array of familiar tools and frameworks essential for AI research and development.
The system’s compatibility with widely used AI frameworks and tools, such as PyTorch, Python and Jupyter Notebooks, empowers developers to use familiar environments for model development and experimentation. Additionally, it supports the Nvidia NeMo framework, which enables the fine-tuning of large language models, and the RAPIDS libraries, which accelerate data science workflows.
In terms of connectivity and scalability, Project Digits employs Nvidia ConnectX networking, enabling high-speed data transfer and efficient communication between systems. This feature allows two Project Digits units to be interconnected, effectively doubling the capacity to handle models with up to 405 billion parameters. This scalability ensures that as complex AI models grow, Project Digits can adapt to meet the increasing computational demands.
Moreover, Project Digits is engineered to seamlessly integrate cloud and data center infrastructures. Thanks to the consistent architecture and software platforms across Nvidia’s ecosystem, developers can prototype and fine-tune AI models locally on the device and then deploy them to larger-scale environments without compatibility issues. This flexibility streamlines the transition from development to production, enhancing efficiency and reducing time to deployment.
Nvidia Project Digits comes with the below software stack:
With its petaflop of AI performance and integrated software stack, Project Digits can handle massive AI models  200 billion parameters. This capability was previously limited to large-scale supercomputers, but Digits brings this power to the desktop, enabling developers to experiment with and deploy cutting-edge AI models locally.
Nvidia’s CEO Jensen Huang shed light on the development of Digits and a key partnership that played a crucial role in its creation. He highlighted the collaboration with MediaTek, a leading fabless semiconductor company, in designing an energy-efficient CPU specifically for Digits. This partnership allowed Nvidia to leverage MediaTek’s expertise in low-power CPU design, contributing to Digits’ impressive power efficiency.
Huang also emphasized how Digits bridges the gap between Linux and Windows environments. While Digits itself runs on a Linux-based operating system, it is designed to seamlessly integrate with Windows PCs through the Windows Subsystem for Linux technology. This allows developers primarily working in Windows environments to easily utilize the power of Digits for their AI projects.
Nvidia Digits is specifically designed for AI researchers, data scientists, students and developers working with large AI models. It empowers these users to prototype, fine-tune and run AI models locally, in the cloud, or within a data center, providing flexibility and control over their AI development workflows.
By bringing the power of an AI supercomputer to the desktop, Digits addresses the growing need for greater AI performance in a compact and accessible form factor. This allows individuals and smaller teams to tackle complex AI challenges without relying on expensive cloud computing resources or large-scale supercomputing infrastructure.
Potential use cases for Digits are vast and varied. Developers can use it to prototype new AI applications, fine-tune LLMs for specific tasks, generate AI-powered content, and research new AI algorithms and architectures. The ability to run large AI models locally opens up new possibilities for AI development, enabling faster iteration and experimentation.
Nvidia Digits is expected to be available in May from Nvidia and its partners, with a starting price of $3,000. This competitive pricing makes it a viable option for a broader range of users, further contributing to the democratization of AI development.
Nvidia Digits represents a significant leap forward in AI technology. By combining powerful hardware, a comprehensive software stack and a compact, power-efficient design, Digits brings the capabilities of an AI supercomputer to the desktop. This can potentially democratize AI development, making it more accessible to individuals, researchers and smaller organizations. The ability to run large AI models locally and the flexibility to deploy in the cloud or data center provides developers unprecedented control and power over their AI workflows. As Digits becomes available, witnessing the innovative applications and advancements that emerge from this powerful new platform will be exciting."
151,https://www.forbes.com/sites/jasonevangelho/2025/01/12/nvidia-just-solved-a-huge-steam-deck-problem-what-happens-next/,Nvidia Just Solved A Huge Steam Deck Problem. What Happens Next?,"Jan 12, 2025, 02:08pm EST",Jason Evangelho,"There has been a tidal wave of exciting handheld gaming news recently, but one story got slightly buried under the mountain of Legion Go S details and Switch 2 rumors. That would be the announcement that Nvidia is developing a native GeForce NOW app for the Steam Deck.
“Trillion-dollar corporation frequently derided by Linux community develops native software for a single Linux gaming device” certainly wasn’t on my 2025 bingo card. But I’m very pleased that it’s happening.
GeForce NOW is a cloud gaming service that lets you play games you’ve purchased on platforms like Steam, Ubisoft, EA, and Epic Games on pretty much any internet-connected device. The hook is that you don’t need an exorbitantly expensive gaming PC for your games to look like they’re being played on one.
For slightly under-powered handhelds like Steam Deck, this presents a range of potential benefits. It provides the convenience of playing your non-Steam library without a bunch of workarounds, tweaks, or tutorials. It also opens the door to enjoying certain games that won’t even run on Valve’s handheld. Plus, your battery won’t have a meltdown, and you can enjoy these streamed games at up to 4K/60FPS an external monitor or TV connected to your Steam Deck.
Of course, the official way to currently install Nvidia’s GeForce NOW on Steam Deck involves switching into desktop mode, downloading a script, extracting that script, and then installing it.
FEATURED | Frase ByForbes™
Unscramble The Anagram To Reveal The Phrase
Pinpoint By Linkedin
Guess The Category
Queens By Linkedin
Crown Each Region
Crossclimb By Linkedin
Unlock A Trivia Ladder
(It would be fascinating to know how many people have even launched Desktop Mode a single time on their Steam Decks, but I suspect it’s a very low percentage).
While we’re not yet sure how the native app, which Nvidia says is scheduled to ship “later this year,” will be delivered to Steam Deck users, it will certainly be a more elegant and streamlined method. And this is a big deal.
But why does this matter? What kind of impact might it have? Might it pave the way for more high-profile software to extend beyond Windows and onto Linux in generl?
In order to present a wider perspective, I asked a few prominent members of the Linux community to chime in with their thoughts.
GloriousEggroll needs no introduction to people closely following the Linux gaming space. In a nutshell, he’s a wizard who works at Red Hat by day and makes Linux gaming better at night. He’s the creator of Proton-GE as well as the gaming-forward Nobara Linux distribution, and he contributes to Lutris.
Here’s what he told me:
“I think it's a great thing that SteamOS is becoming a bigger target. By becoming more mainstream, it's giving vendors real reasons to both develop for and benefit from consumer Linux applications and drivers they may provide. Those contributions from other vendors in turn make the Linux ecosystem much more robust for the end user. By supporting SteamOS you are supporting Linux and open-source, because much of what's contributed to SteamOS can be backported to other distributions.
There's a growing ripple effect on both sides  — reasons for both vendors and users to increase support and usage respectively. Myself and many other Linux users are incredibly thankful for all of the work that has gone into SteamOS, and we’re excited to see what's in store for the future!”
Originally known as The Linux Gamer, Bryant is a well-known figure who covers Linux on his YouTube channel. He’s also a game developer and the president of media production house Heavy Element.
Here are his thoughts on Nvidia’s commitment to Steam Deck:
“Nvidia's native GeForce Now app proves their confidence in Linux gaming. Not only in the future of the ecosystem as it continues to grow, but crucially in the financial reality that supporting Linux gaming can be profitable and worthwhile.
A native GeForce NOW app fills one of the few remaining gaps in the Linux library: allowing gamers to play multiplayer titles from Linux-shy developers who lack anti-cheat support. This is noteworthy and I think it should be applauded!”
Finally, we have Liam Dawe, a veteran in the Linux gaming space, who’s been reporting professionally on Linux gaming for 15 years.
“For Nvidia, it just makes sense. Everyone can clearly see the Steam Deck continues to sell well, and with SteamOS coming to more devices, the business side of it makes more sense now too.
As with all industry shifts, more will come. This move by Nvidia is smart; get in early and show others how it’s done. Although, it remains to be seen how easy they make it. Ideally, getting it as an app on Steam directly would make it better for everyone on all platforms. The next best thing would be Flathub, where developer can deploy their app to Steam Deck and most other Linux distributions.”
I’d like to extend my enthusiastic gratitude to Liam, Thomas, and Gardiner for sharing their insights.
As for the future of GeForce NOW on Linux, I believe Nvidia will end up distributing this on Steam itself, thus making it available on Lenovo’s Legion Go S Powered by SteamOS, and all future SteamOS devices."
152,https://www.forbes.com/sites/rachelwells/2025/01/12/5-self-paced-free-ai-courses-to-study-in-2025/,"5 Self-Paced, Free AI Courses To Study In 2025","Jan 12, 2025, 12:00pm EST",Rachel Wells,"Demand for AI skills is skyrocketing in 2025. More industries and companies demand professionals with artificial intelligence skills, and the evidence suggests that the more AI skills you have to bolster your resume, the better for your paycheck. In fact, Indeed researchers suggest that Generative AI skills can boost your salary by as much as up to 47%. When you develop your AI skills, you can build ground-breaking AI workflows and processes for your role and industry, become more innovative and creative with your ideas, and enhance your productivity while also cutting on costs.
You become a highly valuable asset to your employer and to other potential employers—many of which would rather hire a worker with AI skills (even with limited work experience), then recruit someone with no AI skills that has years of experience, per Microsoft's Work Trend Index 2024.
Whatever your goals—be it leveraging AI to improve your business as an entrepreneur or freelancer, or simply future-proofing your career, you can learn AI skills online without the hefty price tag of an expensive course or degree.
Several organizations have partnered with educational institutions such as Coursera and shard their expertise for free (or relatively free anyway) so that you can gain certifications to add to your resume and improve your understanding of artificial intelligence.
Large tech organisations in particular, realize the importance of closing the workplace skills gap, especially as pertains to technology and AI, and they are actively engaging in initiatives and developing resources that will help close this gap.
One example of this is Nvidia, a forward-thinking company within the tech sector that provides GPUs and chips that power the development of artificial intelligence and machine learning tools. Referred to as one of the Magnificent Seven (the group of large tech companies that include Apple, Tesla, Microsoft, Google, Alphabet, and Meta), Nvidia is mostly known for its hardware and software that it provides to businesses. But what many people may be unaware of is that Nvidia also has an online Deep Learning Institute, which hosts a range of free and paid (but inexpensive) courses to help you develop your tech and AI skills.
All you need to do is to head to Nvidia's Deep Learning Institute page, click on the free courses section, and choose from its range. According to the website, some courses also come with certificates of completion, so you can proudly share these on your LinkedIn profile and refer to them on your resume.
Here are a few of the free AI courses currently available from Nvidia:
Each of these free AI courses can be completed in as little as one day, with course lengths generally ranging from two hours to eight hours. To take your learning and upskilling journey further, you can even enrol in instructor-led workshops from Nvidia (but you'll need to pay for these). Jump in and start learning right now—these courses might be the best investment you'll make in yourself this year."
153,https://www.forbes.com/sites/tomcoughlin/2025/01/10/ces-2025-nvidia-storage-and-memory-product-requirements/,CES 2025 NVIDIA Storage And Memory Product Requirements,"Jan 10, 2025, 06:36pm EST",Thomas Coughlin,"1/13/25 Changed Micron mention by Jensen Huang of NVIDIA to DDR memory only
NVIDIA CEO Jensen Huang gave an hour and 45-minute keynote talk just before the start of the 2025 CES. He gave many announcements, including announcing a desktop AI machine called Project DIGITS, shown above, that should be available in May, in partnership with MediaTek.
He also talked about AI computer graphics with the Blackwell GeForce RTX 50 and AI foundation models for RTX AI PCs.  He also talked about their Cosmos Foundation Model Platform to accelerate physical AI development, generative physical AI with NVIDIA’s Omniverse, vehicle company partners for next-generation highly automated and autonomous vehicle fleets and NVIDIA automotive safety and cybersecurity milestones for AV development.  In this article we will talk about storage and memory and their use in some of the products discussed in Jensen’s keynote talk.
The RTX Blackwell device that powers the Blackwell GeForce RTX 50 desktop and laptop GPUs is shown below, with Jensen holding the desktop version.  It has 1.8TB/s memory bandwidth with up to 32GB of VRAM (video random access memory for storing graphics data).  Jensen pointed out that G7 (GDDR7) memory was sourced from Micron.
The GeForce RTX 50 Series GPUs run creative generative AI models up to 2X faster in a smaller memory footprint compared than the previous generation product.  The GPUs support FP4, a quantization method to decrease image model sizes, somewhat similar to file compression.  As a result, FP4 uses less than half of the memory and twice the performance and this is done with virtually no loss in the quality of the images.
NVIDIA showed real time image generation of very high-resolution streaming video using this GPU during the presentation.  This GPU also enables 3D and virtual reality (VR) video and AV1 ultra high-quality mode and greater color depth than is available on most consumer cameras.
Jensen showed an image of the Grace Blackwell NVLink72.  This was introduced by NVIDIA in March of 2024.  As shown below, this large chiplet includes 72 Blackwell GPUs and 36 Grace CPUs.  It supports 1.4 exaflops per second, EFLOPS of Tensorflow 4-bit Floating Point processing.
The 576 high bandwidth memory chips connected to the GPUs provide about 14TB of memory with 1.2PB/s aggregate bandwidth.  The CPUs have up to 17TB of LPDDR5X memory with up to 18.4TB/s performance.  These chiplet devices are mounted in racks that provide power and connectivity to support massive AI training.
At the very end of his talk Jensen introduced NVIDIA Project DIGITS with a new Grace Blackwell Superchip in a package that provides a desktop AI engine as shown below and at the beginning of this article.  The GB10 Grace Blackwell Superchip is a system-on-a-chip (SoC) and provides a petaflop of FP4 AI computing performance that can be used for creating and running large AI models.  The box shown also includes a 4TB SSD and 128GB of LPDDR5X memory.
The press release says that the GB10 Superchip features an NVIDIA Blackwell GPU with the latest-generation CUDA cores and with fifth-generation Tensor Cores, connected via NVLink-C2C chip-to-chip interconnect to a high-performance NVIDIA Grace CPU, which includes 20 power-efficient cores built with the Arm architecture. MediaTek, a market leader in Arm-based SoC designs, collaborated on the design of the GB10, contributing to its best-in-class power efficiency, performance and connectivity.
Jensen Huang from NVIDIA gave the first CES 2025 keynote talk and announced a slew of enterprise as well as consumer GPU products.  Memory and storage play an important role in AI training and inference and his presentation showed how they enable modern AI solutions."
154,https://www.forbes.com/sites/jasonsnyder/2025/01/09/quantum-is-decades-away-not-so-fast/,What Nvidia’s CEO Missed About Quantum Computing,"Jan 09, 2025, 11:18am EST",Jason Snyder,"When Nvidia CEO Jensen Huang suggested that practical quantum computing may still be 15 to 30 years away, he reignited skepticism about the technology’s readiness. This perspective has led to significant declines in quantum computing stocks, with companies like Rigetti Computing, D-Wave Quantum, IonQ and Quantum Computing Inc. experiencing substantial drops.
While Huang’s remarks might apply to fully scalable, general-purpose quantum systems, they ignore the tangible value quantum computing is already delivering today.
Quantum computing is no longer an abstract concept confined to physicists or futurists. It’s actively reshaping industries by solving problems classical systems can’t handle. From marketing logistics to predictive analytics, quantum computing is transforming decision-making — providing faster, more efficient solutions to some of the most complex challenges businesses face.
By focusing solely on a distant horizon for “very useful” quantum computers, Huang’s perspective risks overlooking the incremental yet impactful progress happening now. Early adopters are already leveraging quantum to optimize logistics, streamline operations and uncover insights at speeds classical systems cannot achieve.
The quantum era isn’t decades away — it’s unfolding in real-time.
Today’s predictive analytics tools are powerful but falter in the face of exponential complexity. Classical systems, even when supercharged with machine learning, are designed to evaluate problems sequentially. For example, optimizing a global marketing campaign involves juggling budget allocations, consumer preferences and channel strategies while responding to shifting conditions in real-time.
This is where classical computing struggles. The sheer number of variables makes solving such problems akin to finding a needle in a haystack — except the haystack is constantly growing. Quantum computing, by contrast, processes multiple possibilities simultaneously, making it uniquely suited to tackle this level of complexity.
The fundamental difference between classical and quantum computing lies in how information is processed. Classical computers rely on bits that represent either a zero or a one.
Quantum computers use qubits, which leverage the principle of superposition to exist in multiple states simultaneously. Imagine trying to solve a problem where your only options are darkness or turning on a white light. A zero or a one. But quantum computing lets you examine a problem using an entire spectrum of possibilities, all the colors of the rainbow as well as infrared, ultraviolet, X-rays, and gamma rays. It lets you do this all at once to find what light illuminates your answer best.
Another cornerstone of quantum mechanics is entanglement, where qubits become interconnected. Changes in one qubit instantly affect others, no matter how far apart they are. This interconnectedness enables quantum systems to collaborate on complex problems more effectively, creating an exponential leap in computational power. Paired with wave/particle duality, which allows quantum systems to navigate probabilities and uncertainties, these features make quantum computing ideal for solving optimization challenges at scale.
Quantum computing is already proving its value in areas like marketing logistics. Consider a global campaign where decisions about ad placement, budget distribution and timing hinge on thousands of variables. Classical methods require weeks of analysis; quantum systems solve these problems in minutes. For example, using quantum annealing, to evaluate all potential marketing campaign configurations simultaneously, finding the most effective strategy.
This isn’t theoretical—businesses are already using quantum to:
The result? Less waste, faster execution, and deeper insights into consumer behavior.
My perspective is not theoretical. I have practical experience working with quantum to solve business problems. Starting in 2023, I collaborated with D-Wave, to tackle one of the most persistent challenges in experiential marketing: optimizing campaign logistics in real-time. Marketing logistics, by nature, is a highly complex problem, involving thousands of variables like audience segmentation, channel preferences, budget allocation and timing. Traditional computing systems often struggle to provide actionable insights within the necessary timeframes.
Using D-Wave’s quantum annealing technology, we addressed this complexity head-on. The quantum system evaluated millions of potential configurations for a global marketing campaign — dynamically identifying the most effective strategies for resource allocation, ad placement and campaign timing.
The results were transformative:
•	Budget Optimization: Quantum annealing reallocated budgets across multiple markets in real-time, ensuring maximum return on investment.
•	Real-Time Adaptability: The system-aligned campaign launches with shifts in consumer sentiment, allowing us to pivot quickly in response to changing conditions.
•	Onsite Activation Precision: Experiential activations were optimized to align with audience demographics, location-specific engagement patterns, and real-time event dynamics, ensuring maximum impact and minimizing resource waste.
What would have taken weeks of manual analysis and iterative testing with classical systems and processes was completed in minutes using quantum computing. More importantly, the process revealed more profound insights into consumer behavior—patterns that traditional systems might have missed entirely.
As Tom Sivo, VP of Emerging Technology at Interpublic says, “Quantum computing is redefining the boundaries of what’s possible in marketing. By solving complex optimization problems in real-time, it’s enabling us to deliver hyper-personalized experiences, streamline resource allocation, and adapt to consumer behavior faster than ever before. The ability to harness quantum for experiential activations, campaign planning, and predictive analytics isn’t just a technological leap—it’s a paradigm shift that will shape the future of marketing for years to come.”
This wasn’t just a theoretical exercise. It was a practical demonstration of how quantum computing can empower marketers to make smarter, faster decisions that directly impact the bottom line. It also underscored the importance of collaboration between businesses and quantum technology providers like D-Wave to unlock these capabilities.
While quantum computing excels at optimization, pairing it with artificial intelligence unlocks even more potential. AI is adept at pattern recognition and predictive modeling but struggles with computational bottlenecks when dealing with massive datasets. Quantum computing eliminates these barriers, accelerating AI’s capabilities.
Quantum-enhanced AI can:
For businesses, this means AI models trained on quantum-optimized data can uncover insights that drive more accurate, impactful decision-making.
While quantum computing offers unparalleled speed and efficiency, it’s only as effective as the data it processes. Clean, structured, and actionable data remains the cornerstone of successful quantum applications. Early adopters of quantum have learned this lesson: without meticulous data preparation, even the most advanced quantum algorithms will fall short. This highlights a critical priority for businesses today—investing in better data pipelines to prepare for quantum’s transformative potential.
It’s vital to approach quantum computing with realistic expectations. The technology is still in its early stages, and its hardware requires precise operating conditions, such as cryogenic cooling near absolute zero. Additionally, quantum systems are not general-purpose—they excel in specific areas like optimization and simulations but complement rather than replace classical systems. What’s clear, however, is that the technology is advancing rapidly. Hybrid quantum-classical systems are bridging the gap, delivering tangible results while paving the way for broader adoption.
Quantum computing isn’t just for the tech giants—businesses of all sizes can begin exploring its potential.
Here’s how:
Be warned: Those who dismiss the very serious advantages of quantum today are letting those who understand the technology's strategic applications happily take on the role of have-nots.
Quantum computing is not 30 years away—it’s already transforming industries by solving problems classical systems can’t. From streamlining marketing logistics to enhancing predictive analytics, quantum redefines what’s possible in real-time decision-making. For businesses willing to explore its potential, quantum offers a competitive edge.
The quantum era has begun, and its promise isn’t just theoretical—it’s actionable. The question is no longer if quantum computing will matter but how soon you’ll embrace it."
155,https://www.forbes.com/sites/antonyleather/2025/01/09/nvidia-rtx-5000-under-threat-in-amd-radeon-rx-9070-xt-performance-leak/,Nvidia RTX 5000 Under Threat In AMD Radeon RX 9070 XT Performance Leak,"Jan 09, 2025, 05:31am EST",Antony Leather,"An interesting performance benchmark leak has revealed what could be the first decent indication of the performance of AMD's highly anticipated Radeon RX 9070 XT, which in one benchmark matches Nvidia's second most powerful graphics card , the RTX 4080 Super.
Tech website Videocardz reported today that images of 3DMark results and specifications were posted on the Chiphell forum, apparently by one of the website's reviewers and have since been removed. They also warned readers not to buy Nvidia's graphics cards stating everything had changed.
However, the 3DMArk benchmark results paint a very rosy picture for the Radeon card, matching AMD's existing flagship, the RX 7900 XTX and Nvidia RTX 4070 Ti Super in the Speed Way DX12 benchmark at 4K resolution and beating the RTX 4080 Super in the Time Spy Extreme benchmark at similar settings.
Above we can see the leaked results of the RX 9070 XT alongside average results on the database for other graphics cards using the same CPU - an Intel Core Ultra 285K, so they should be roughly comparable. We've yet to hear any significant information about AMD's new RDNA 4 Radeon RX 9070 XT and 9700 with only the vaguest hint of performance and nothing about pricing at its CES keynote this week. As expected from previous rumors and statements from AMD itself, the company confirmed that it does not plan to challenge Nvidia's flagship graphics cards, which include the current RTX 4090 and next generation Blackwell based RTX 5090 due for release later in January.
Instead it will focus on hitting the mid-range hard in an attempt to boost its flagging market share, but the performance here suggests it will be offering RTX 4080/4080 Super performance in some scenarios. The RTX 4080 Super currently costs around $1,500 minimum, but expectations are that AMD will launch the RX 9070 XT for potentially half that.
In a recent interview with Tom's Hardware, AMD's David McAfee stated ""We made a lot of decisions in RDNA 4 that are different from prior generations. We know that most gamers buy graphics cards that are well, well south of $1,000, and we built the RDNA 4 architecture really from the ground up to make sure that we're delivering features and the performance and capabilities that gamers in the largest volume segments of the market care about most""
Nvidia's RTX 5000 series is also expected to see significant performance uplifts and the key competitors for the RX 9070 XT will be the RTX 5070 ($549) and RTX 5070 Ti ($749). At the very least, AMD may well be able to challenge those cards in terms of price and performance. Nvidia may have had an inkling that this would be the case, which is why these two RTX series cards cost $50 less than their original RTX 4000 series equivalents, whereas the RTX 5090 costs $400 more than the RTX 4090 did at launch. With no competition at the top end, Nvidia saw fit to price its flagship as it saw fit.
There's still no indication of when exactly AMD will release its new Radeon graphics cards, other than a Q1 timeframe, while Nvidia's RTX 5080 and 5090 will launch this month and the RTX 5070 Ti and 5070 will follow next month."
156,https://www.forbes.com/sites/chriswestfall/2025/01/08/finding-six-figure-jobs-in-ai-nvidia-offers-free-training-courses/,Finding Six-Figure Jobs In AI: Nvidia Offers Free Training Courses,"Jan 08, 2025, 10:30am EST",Chris Westfall,"If you’re looking for a six-figure salary, learning more about jobs in AI can help you to get there. A number of high-paying AI jobs are available, in and around large language models (LLMs) , product management, machine learning and more. With free courses are available from sources like Nvidia (NASDAQ: NVDA) and others, you can gain the skills and knowledge needed to create the future. And the future looks bright for AI: the  size of the artificial intelligence market is projected to reach nearly $245 billion in 2025, with a compound annual growth rate (CAGR) of 27.67% - resulting in a projected market size of $826.7 billion by 2030. According to Statista, the lion’s share of that growth will come from the United States. Are you ready to level up your skills, at little or no cost, so that you can capitalize on this career opportunity?
As white-collar jobs are going the way of the pager, and managers are being let go at a breakneck pace, having individual skills is vital to career success. Consider how the Wall Street Journal says that the artificial intelligence boom is lifting paychecks for CIOs - and a lot of other roles as well - because of AI. If you want to land a six-figure job in AI, being better informed on the tech is the first step. Using data pulled from Indeed, here are some six-figure jobs (with salaries) for AI-related jobs:
To bridge the skills gap and empower individuals to capitalize on these lucrative opportunities, Nvidia offers free training courses. On Nvidia’s website, you can find free courses with provocative titles, such as these:
Many other courses are offered for a fee of between $30-90 each, on Nvidia’s website. Coursera also offers an AI overview course, and more, so that you can train for a six-figure job in AI.
Another useful course is ""AI Infrastructure and Operations Fundamentals,"" which covers:
Completing these courses can enhance your qualifications for roles such as AI Engineer, Data Scientist, or Machine Learning Engineer, all of which command six-figure salaries. At the very least, online learning presents a free opportunity to expand your familiarity with the tech around AI. The battle for six-figure opportunities is tough - especially in tech, where layoffs have made headlines for the last 18 months. Training and upskilling is the key to your career success, if you want to catch the wave of artificial intelligence. With these free or low-cost training options around AI jobs, you can position your career for a six-figure salary with minimal investment."
157,https://www.forbes.com/sites/antonyleather/2025/01/08/nvidias-rtx-5090-twice-as-fast-as-the-rtx-4090-but-theres-a-catch/,"Nvidia’s RTX 5090 Twice As Fast As The RTX 4090, But There’s A Catch","Jan 08, 2025, 03:30pm EST",Antony Leather,"Nvidia has made some pretty bold performance claims for its new GeForce RTX 5090 and other models announced at CES this week. Chief among them is that the new models are twice as fast as previous ones, including the GeForce RTX 4090. That's quite remarkable if true, as we don't usually see much more than 10% to 30% more performance between generations, with new models generally performing as well as the model above from the previous generation. However, there is a big catch with these claims.
Since the original RTX series launched in 2018, its use of AI, upscaling and even generating entire artificial frames to boost frame rates has become a growing part of how its graphics cards work. Remove all of that technical wizardry and you're left with pure rasterization, which is the traditional way of generating the frames and worlds we see in games, and all games work this way to some extent. Add the likes of Nvidia's DLSS into the mix, and suddenly games that support it can see massive increases in frame rates. The catch? Not all games support DLSS.
Nvidia's revealed its new DLSS Multi Frame Generation at CES, and instead of generating single frames as before, up to three can now be added for every traditionally rendered frame, increasing performance by up to eight times compared to rasterization alone, Nvidia claims. To get that performance increase, though, which is where its twice-as-fast figures comes from, you need to turn on all the latest bells and whistles. Thanks to a new DLSS transformer model and other refinements to the RTX 5090 and other Blackwell architecture cards, the new models are better placed to take advantage of these new or enhanced features.
FEATURED | Frase ByForbes™
Unscramble The Anagram To Reveal The Phrase
Pinpoint By Linkedin
Guess The Category
Queens By Linkedin
Crown Each Region
Crossclimb By Linkedin
Unlock A Trivia Ladder
This is where Nvidia's claims for the extra performance come from. But getting back to normal rasterization in games for a moment, these still make up the vast majority out there, both new and old. So playing games that lack support for Nvidia's DLSS tech means relying on the raw rasterization power, and this offers far smaller gains compared to the previous generation. According to Nvidia's own graphs, it appears there's around 20% to 40% increases for the RTX 5090 over the RTX 4090 — still good, but clearly nowhere near 100%, or twice as fast.
The short story? Wait for benchmark results when the RTX 5090 is released later this month. By then we'll know just how fast on average it is across a broad spectrum of games and not just titles that support Nvidia's DLSS features. The jury is also out on what the visual quality will be using its new techniques such as Multi Frame Generation. Adding frames this way can have huge benefits in games that struggle to achieve high frame rates such as Microsoft Flight Simulator, but many are still dubious of the technology.
The flip side is that Nvidia's ecosystem is growing and hasn't done what some thought it would at the start, and it’s failed to gain traction. Since 2018, over 700 titles now support Nvidia RTX features, such as ray tracing and DLSS, and over 40% of all Steam users in its Hardware Survey use desktop Nvidia RTX capable graphics cards alone before we consider laptop market share. The Nvidia RTX 5090 might be out of reach for many, but even the RTX 4090 has 1% market share, which is impressive, and Nvidia makes up over 75% of overall graphics cards that use Steam. Whether you like it or not, Nvidia's RTX features are here to stay."
158,https://www.forbes.com/sites/digital-assets/2025/01/08/9-nvidia-announcements-from-ces-2025-and-their-impact-on-blockchain/,9 NVIDIA Announcements From CES 2025 And Their Impact On Blockchain,"Jan 08, 2025, 02:57pm EST",Sandy Carter,"I was at CES 2025, seated among a crowd of 140 thousand, as Jensen Huang, the founder and CEO of NVIDIA, took the stage for the first keynote of the event. The energy in the room was electric. Huang opened with a stunning visual: a digital twin of NVIDIA itself.
This AI-driven virtual representation mirrored every detail of the company’s operations, a perfect example of the transformative power of the technologies NVIDIA is pioneering.  And of course, this digital twin concept bridges the gap between blockchain technology and the industrial metaverse, offering a glimpse into how decentralized data and digital identities could redefine operational efficiency.
Gary Shapiro, the founder of CTA, shared a light-hearted but impactful story, saying, “If I had invested in NVIDIA when Jensen first came to CES, I’d be retired by now.” It was a reminder of just how far NVIDIA has come, evolving from a graphics chip company to a driving force behind AI, robotics, and autonomous systems.
Project DIGITS: The Personal AI Supercomputer NVIDIA unveiled Project DIGITS, a groundbreaking AI supercomputer powered by the GB10 Superchip. Capable of handling massive 200B-parameter models, this device, small enough to hold in one hand, offers unparalleled processing power. With two units combined, it can manage up to 405B parameters, empowering developers with unmatched AI capabilities.
Available by May, it’s set to redefine personal AI computing. Also Jensen asked for names as well!   So submit away!
NVIDIA enhanced its autonomous vehicle technology by integrating Cosmos World Foundation Models into its three-computer solution. This data flywheel transforms thousands of real-world driving miles into billions of virtual simulations, exponentially improving the quality of training data.
The NVIDIA AI Blueprint now includes cutting-edge features for video search and summarization, including chain-of-thought reasoning, task planning, and tool calling. This allows developers to create powerful visual AI tools for video content management and summarization.
NVIDIA announced several advancements in autonomous driving technology, combining the DRIVE Thor and DRIVE Orin platforms, DRIVE AGX Hyperion toolkit, and partnerships with Aurora, Continental, and Toyota. These platforms offer comprehensive tools for vehicle safety, security, and decision-making capabilities with trillions of operations per second, positioning NVIDIA as a leader in autonomous vehicle development.
NVIDIA has introduced new AI Blueprints to simplify the creation of advanced AI agents. These blueprints offer a step-by-step framework, including pre-built tools like NIM microservices, NeMo frameworks, and other industry-leading AI technologies, making it easier for businesses to develop intelligent digital assistants and automation tools.
I get asked a lot of questions on the differences between Agentic AI and AI Agents.
Agentic AI is highly autonomous, goal-driven, and capable of independently solving complex problems while continuously learning and adapting to dynamic environments. In contrast, AI Agents are task-specific with limited autonomy, relying on pre-programmed instructions and minimal learning capabilities, primarily suited for simpler, structured tasks. Agentic AI excels in decision-making and adaptability, while AI Agents depend on human input and struggle to respond effectively to changing situations.
AI agents can be enhanced with blockchain technology to create more secure, transparent, and autonomous systems. For example, combining NVIDIA's AI tools with blockchain can help businesses build decentralized AI agents for smart contracts, secure data handling, and trustless automation, ideal for sectors like finance, supply chain management, and digital identity.
The Isaac GR00T Blueprint enables developers to train humanoid robots using synthetic motion data and human demonstration inputs. This development framework is a leap forward in humanoid robotics and motion learning.
Jensen Huang noted, ""The opportunity for humanoid robots in manufacturing and logistics is immense—humanoids can redefine productivity in the $50 trillion industry.""
NVIDIA has expanded its Omniverse platform with a powerful new ‘Mega’ blueprint, designed to simulate entire facilities like warehouses and factories using digital twin technology. This innovation enables businesses to create virtual replicas of their operations, allowing for large-scale testing and optimization of robotic systems before real-world deployment.
By integrating blockchain with NVIDIA’s Omniverse digital twins, businesses can add an extra layer of security and transparency to their simulations.
Blockchain can be used to verify data integrity across the digital twin network, ensuring accurate, tamper-proof records of operational insights. Additionally, smart contracts could automate performance metrics and trigger actions based on simulation outcomes, making this technology ideal for industries focused on precision and accountability, such as manufacturing, logistics, and supply chain management.
NVIDIA’s NIM microservices and RTX AI Blueprints simplify the use of generative AI for content creation, human interaction, and visual effects, making powerful AI tools more accessible to businesses of all sizes. They streamline complex AI deployment, boosting productivity by automating tasks and enhancing everyday PCs with professional-grade capabilities.
When combined with blockchain, these tools can ensure content authenticity through secure, verifiable digital records, helping businesses stay competitive and future-proof.
NVIDIA’s advancements in AI and digital twin technologies could have significant implications for the blockchain and crypto sectors. Project DIGITS’ ability to handle massive datasets can accelerate blockchain analytics and smart contract verification.
The Omniverse platform, with its simulation capabilities, can play a pivotal role in the development of decentralized virtual environments, enabling more immersive metaverse projects powered by blockchain.
Additionally, the new AI Blueprints may facilitate the creation of intelligent crypto trading bots and financial analysis tools, further enhancing the blockchain ecosystem.
Jensen Huang emphasized the critical role of AI in reshaping industries, stating, “The ChatGPT moment for robotics is coming. Physical AI will revolutionize the $50 trillion manufacturing and logistics industries. Everything that moves — from cars and trucks to factories and warehouses — will be robotic and embodied by AI.”
With bold statements like these, NVIDIA is positioning itself not just as a tech innovator but as a foundational driver of the next industrial revolution. Their continued partnerships with global brands like Toyota and Aurora reinforce this trajectory, with real-world autonomous vehicle deployments expected within the next few years.
The keynote left no doubt: AI, autonomous systems, and digital twins will define the next era of technological progress. NVIDIA continues to be at the forefront, making once-futuristic concepts a reality today."
159,https://www.forbes.com/sites/petercohan/2025/01/07/nvidia-stock-may-rise-as-its-stealth-ai-cloud-rivals-big-customers/,Nvidia Stock May Rise As Its Stealth AI Cloud Rivals Big Customers,"Jan 07, 2025, 09:27am EST",Peter Cohan,"Updated, Jan. 7, 2025: This post has been adjusted to include numbers from Nvidia's full day of trading.
On January 6, Nvidia CEO Jensen Huang’s keynote speech at the 2025 Consumer Electronics Show made a series of what looked to me like relatively small announcements — including products for industries such as robotics, cars, and gaming, according to the Wall Street Journal.
Are these mini-announcements a diversion? One analyst thinks the biggest news could be Nvidia’s strategy to compete with cloud services customers — Amazon, Microsoft, and Alphabet — by building its own AI cloud, noted Investor’s Business Daily.
Nvidia — whose shares rose almost 4% in pre-market trading and closed down 9.3%  by the end of January 7 — is excited about the opportunity to provide software for robotics developers. ""The ChatGPT moment for robotics is coming,” Huang told CES attendees, according to IBD.
""Like large language models, world foundation models are fundamental to advancing robot and AV development, yet not all developers have the expertise and resources to train their own. We created Cosmos to democratize physical AI and put general robotics in reach of every developer,"" he added.
An Nvidia spokesperson I contacted declined to comment.
If there is one thing I learned in writing my latest book, Brain Rush, it is this: Nvidia’s future depends heavily on collaboration with companies closer to technology end-users.
That’s because Nvidia — which designs chips and builds software for developers — can only enjoy rapid growth by motivating companies closer to end-users — such as chip fabricators, cloud services providers, enterprise software companies, and management consultants — to make Nvidia’s products part of a system that businesses and consumers find valuable.
Nvidia’s CES product announcements are aimed at strengthening such partnerships. Here are the industries these announcements aim to enhance:
While Nvidia did not make an announcement about this at CES, the biggest news from the company could be a strategy to compete with its cloud services customers who account for most of Nvidia’s growth.
Indeed one analyst detected Nvidia’s growing appetite for leased data center capacity equipped for AI processing. Such AI data centers would “rent out servers powered by its own chips as well as host its AI software development platform,” noted Investor’s Business Daily.
The analyst sees Nvidia’s push into cloud computing as a New Year’s surprise. “We see the potential for Nvidia to accelerate its data center leasing in 2025 as it looks to procure more capacity that can be used for its 'AI cloud,' with the potential for Nvidia to outpace its hyperscale peers in data center leasing in 2025,” noted TD Cowen analyst Michael Elias in a report featured by IBD.
In addition to looking for more data center capacity in other deals, Elias noted an “apparent large leasing deal between Nvidia and Digital Realty for data center capacity in Northern Virginia,” IBD reported.
Nvidia’s AI Cloud would compete with so-called Neoclouds — AI processing cloud services from the likes of CoreWeave, Lambda, Crusoe, Vultr and Together AI, noted IBD — as well as Nvidia’s biggest AI chip customers Amazon and Microsoft, wrote Elias.
By building its own neocloud, Nvidia could take more profit from a fast-growing industry sector. Nvidia — which now distributes its AI software by leasing it through AWS — could boost its profits by operating its own cloud. AI neoclouds are expected to grow at a 100% compound annual rate from $4 billion last year to $32 billion by 2027, according to Pitchbook.
Meanwhile, Nvidia’s cloud customers are trying to buy more AI chips from rivals and build their own custom chips. For example, in addition to buying Blackwell chips from Nvidia, hyperscalers are buying AI processors from Broadcom and Marvell Technologies as well as designing their own custom AI accelerators and trying to sell them to OpenAI, Apple and Meta Platforms, IBD reported.
Nvidia stock may be heading up. Based on 40 Wall Street analysts offering 12-month price targets, Nvidia stock would need to rise 18.5% to reach the average forecast of $135 per share, noted TipRanks.
Here are comments from less- and more-bullish analysts:
To me the key questions for investors are: How much revenue could Nvidia's AI cloud generate? Will Nvidia’s hyperscaler clients buy less from Nvidia? My guess is Huang expects Nvidia’s AI cloud service will make the company better off."
160,https://www.forbes.com/sites/dereksaul/2025/01/07/nvidia-stock-suddenly-slips-to-worst-day-in-months-as-tech-stocks-pull-back/,Nvidia Stock Suddenly Slips To Worst Day In Months As Tech Stocks Pull Back,"Jan 07, 2025, 04:07pm EST",Derek Saul,"Nvidia stock briefly touched a new record Tuesday following a high-profile speech from its billionaire leader Jensen Huang, but surprisingly reversed quickly to a significant daily loss, headlining a surprise stark selloff across technology stocks.
Nvidia stock rallied as much as 2.5% to a new intraday all-time high of $153 shortly after market open, coming on the heels of Huang’s Monday evening keynote at the CES 2025 conference, before turning negative as trading progressed amid a broader technology stock selloff.
Nvidia fell more than 6% to $140.14 by close, making Tuesday the artificial intelligence titan’s worst day on Wall Street since Sept. 3
The Nvidia slump, which wiped out more than $220 billion in market value for the semiconductor chip designer, weighed on equity indexes.
The S&P 500 sank 1.1% and the tech-heavy Nasdaq faltered 1.9%, losses tied to investors’ sharpening concerns about U.S. fiscal and monetary policy.
Joining Nvidia among other notable fallers Tuesday were the stocks of government contractor Palantir, and electric and autonomous vehicle firm Tesla, both AI-heavy names which have surged in recent months—their shares fell 8% and 4%, respectively.
The Nvidia selloff comes despite an overwhelmingly positive reaction from Wall Street analysts on the the speech largely focused on Nvidia’s efforts in robotics, or physical AI, in addition to advancements in its graphics processing units used for gaming. Nvidia showed Monday it “continues to enhance and develop both AI hardware and software offerings that will help maintain its AI leadership as the market transitions to physical AI,” remarked Rosenblatt analyst Hans Mosesmann in a note to clients. Huang’s speech also unveiled a variety of new or enhanced partnerships for Nvidia with other major companies, including naming data storage firm Micron as Nvidia’s memory partner for its gaming GPUs and a trio of deals in the space of autonomous driving, perhaps the clearest application of physical AI. Huang said Nvidia will supply the semiconductor chips for Toyota’s driver assistance programs and announced it will provide the technology powering the self-driving trucks of Colorado-based Aurora, while ride-hailer Uber said it will use Nvidia’s Cosmos physical AI platform to power its own autonomous driving initiative. The “string of announcements, at a minimum, highlight the company’s ability to innovate at industry-leading speed across hardware and software as well as its robust partner and customer eco-system,” noted Goldman Sachs analysts led by Toshiya Hari. Huang highlighted Nvidia’s “continued dominance in genAI compute and ecosystem, quickly expanding from the cloud all the way to enterprise and consumers,” wrote Bank of America analysts led by Vivek Arya in a Tuesday note.
“The ChatGPT moment for general robotics is just around the corner,” Huang declared Monday, referring to the 2022 release of OpenAI’s ChatGPT chatbot which sparked intense public interest in generative AI.
Nvidia is the unquestioned leader in designing the hardware and software architecture needed to power advanced AI, and its non-automaker customers include the likes of Amazon and Microsoft. Long known for its video game graphic efforts, Nvidia rose to the national stage as generative AI captured the attention of the public and Wall Street alike. Nvidia stock is up more than 2,000% over the last five years."
161,https://www.forbes.com/sites/esatdedezade/2025/01/07/nvidias-packed-ces-2025-keynote-gaming-robots-self-driving-cars-and-personal-ai-supercomputers/,"Nvidia’s Packed CES 2025 Keynote: Gaming, Robots, Self-Driving Cars And Personal AI Supercomputers","Jan 07, 2025, 11:01am EST",Esat Dedezade,"The opening of CES 2025 marked a pivotal moment for Nvidia as the chipmaker revealed its most ambitious technology roadmap yet. In a packed keynote on Jan. 6, CEO Jensen Huang unveiled a comprehensive suite of products spanning consumer gaming, robotics, autonomous vehicles and AI development tools. The announcements ranged from next-generation graphics cards promising unprecedented gaming performance to an open-source platform for training robots and a “personal AI supercomputer” priced for individual developers. A partnership with Toyota, the world's largest automaker, also underscored Nvidia's growing influence in autonomous vehicle development.
Overall, the announcements further strengthen Nvidia's continued evolution from a gaming hardware specialist to a dominant force in AI computing. As the company's stock hit a record high of $149.43 on Monday (valued at $3.66 trillion), these new products suggest a strategic push to extend its AI leadership even further.
Nvidia's gaming lineup centered on the new RTX 50 Series, featuring the flagship GeForce RTX 5090 graphics card (GPU), priced at $1,999. The card boasts specs previously unseen in consumer GPUs — 92 billion transistors delivering 3,352 trillion AI operations per second. The complete lineup includes the RTX 5080 ($999), RTX 5070 Ti ($749) and RTX 5070 ($549).
Release dates for the series are staggered, with the RTX 5090 and 5080 arriving Jan. 30, followed by the 5070 Ti and 5070 models the following month. Mobile variants are expected to reach the market in March.
FEATURED | Frase ByForbes™
Unscramble The Anagram To Reveal The Phrase
Pinpoint By Linkedin
Guess The Category
Queens By Linkedin
Crown Each Region
Crossclimb By Linkedin
Unlock A Trivia Ladder
The series introduces several significant technological advances, with DLSS 4 with Multi Frame Generation representing a major leap in AI-enhanced graphics rendering. Capable of boosting gaming performance up to eight times by intelligently generating additional frames between rendered ones, this technology effectively increases frame rates without requiring proportional increases in raw computing power, providing a smoother experience for gamers.
Nvidia Reflex 2 also addresses competitive gaming demands by reducing system latency up to 75%. The platform also introduces RTX Neural Shaders for enhanced texture rendering and RTX Neural Faces for more realistic character animations, which will be a boon to gamers looking for more realistic character portrayals and increased realism.
Huang also introduced Cosmos, a new open-license platform aimed at transforming how robots and autonomous systems are developed. The system combines three critical components — generative AI models for creating synthetic environments, specialized tokenizers for processing real-world data and a comprehensive video processing pipeline for real-time analysis.
The platform's world foundation models can generate photo-realistic video simulations from text descriptions, allowing developers to test robotic systems in virtually unlimited scenarios. This capability particularly benefits autonomous vehicle development, where real-world data collection is both expensive and logistically challenging.
A key component, the Isaac GR00T Blueprint, specifically targets humanoid robot development. The system can exponentially expand limited sets of human motion data into comprehensive training datasets, dramatically reducing the resources required for teaching robots natural movement patterns. Early adoption has been significant, with companies including Uber, Hyundai Motor Group, 1X, Agile Robots and Figure AI already integrating Cosmos into their development pipelines.
Elsewhere, Nvidia's automotive strategy centers on the newly announced Drive Hyperion platform, a comprehensive system that could accelerate the industry's transition toward autonomous vehicles. At its heart lies the AGX Thor system-on-chip, representing a significant leap in processing capability for vehicles. Beyond raw computing power, this end-to-end solution integrates advanced sensors, safety systems and a comprehensive autonomous driving stack designed to handle everything from basic driver assistance to full autonomy.
What makes this platform particularly significant is its potential to standardize autonomous vehicle development. Rather than automakers building custom solutions from scratch, Drive Hyperion provides a complete framework that can be adapted and scaled according to specific needs. For Toyota, which announced its partnership during the keynote, this means faster development cycles and more consistent safety features across its vehicle line-up.
The platform's integration with Nvidia's Omniverse and Cosmos technologies offers another crucial advantage — by transforming hundreds of real-world driving scenarios into billions of simulated miles, car manufacturers can test and refine their autonomous systems more rapidly and safely than ever before. This AI data factory approach could dramatically reduce the time and cost typically associated with autonomous vehicle development.
During his keynote, Huang also unveiled Project Digits, Nvidia's first entry into desktop AI computing systems. The $3,000 device, scheduled for release in May, combines the company's new GB10 Grace Blackwell chip with 128GB unified memory and 4TB storage — specs that position it as a potential alternative to cloud-based AI development infrastructure.
The system's architecture represents a strategic scaling of Nvidia's data center technology. While major tech companies utilize the GB200 platform with dual GPUs, Project Digits opts for a single GPU configuration that balances performance with accessibility. The system can handle AI models containing up to 200 billion parameters, suggesting capabilities sufficient for sophisticated applications like language processing and computer vision.
This move into desktop AI computing could have significant implications for the developer ecosystem. Currently, AI development typically requires either substantial cloud computing investments or access to enterprise-grade infrastructure. Project Digits might offer an alternative path, potentially enabling a broader range of organizations and developers to experiment with and deploy AI applications locally.
Nvidia's AI Blueprints initiative also addresses the growing demand for practical AI implementation in business environments. The platform introduces what Nvidia terms “knowledge robots” — AI agents designed to analyze documents, process video content and automate workflows. Through partnerships with established AI infrastructure companies like CrewAI and LangChain, the system provides pre-built templates and workflows that could reduce the technical barriers to AI adoption.
The practical applications span various business functions, from document processing to video analysis and workflow automation. By providing standardized tools for AI implementation, this approach might accelerate enterprise AI adoption, though real-world effectiveness will depend on factors like integration capabilities and ease of deployment.
Nvidia's comprehensive announcements at CES 2025 reveal several key trends reshaping the technology industry's landscape. The introduction of Project Digits, for example, signals a fundamental shift toward democratizing AI development, potentially leading to a proliferation of AI applications from smaller companies and individual developers.
The emphasis on robotics and autonomous systems through Cosmos also points to AI moving beyond purely digital applications into the physical world. This convergence of physical and digital AI could accelerate the development of practical applications in manufacturing, logistics and everyday robotics, fundamentally changing how industries approach automation and human-machine interaction.
Looking ahead, these developments could signal a pivotal shift in the tech industry's trajectory. As AI continues to make its way into physical devices and everyday applications, Nvidia's comprehensive ecosystem approach could reshape competitive dynamics in the tech sector.
For Nvidia's competitors, matching this integrated hardware-software strategy may prove more challenging than competing on individual product specifications. The true measure of success, however, will lie in how effectively these technologies translate into practical, real-world applications that reshape industries beyond traditional computing."
162,https://www.forbes.com/sites/petercohan/2025/01/07/nvidia-stock-may-rise-as-its-stealth-ai-cloud-rivals-big-customers/,Nvidia Stock May Rise As Its Stealth AI Cloud Rivals Big Customers,"Jan 07, 2025, 09:27am EST",Peter Cohan,"Updated, Jan. 7, 2025: This post has been adjusted to include numbers from Nvidia's full day of trading.
On January 6, Nvidia CEO Jensen Huang’s keynote speech at the 2025 Consumer Electronics Show made a series of what looked to me like relatively small announcements — including products for industries such as robotics, cars, and gaming, according to the Wall Street Journal.
Are these mini-announcements a diversion? One analyst thinks the biggest news could be Nvidia’s strategy to compete with cloud services customers — Amazon, Microsoft, and Alphabet — by building its own AI cloud, noted Investor’s Business Daily.
Nvidia — whose shares rose almost 4% in pre-market trading and closed down 9.3%  by the end of January 7 — is excited about the opportunity to provide software for robotics developers. ""The ChatGPT moment for robotics is coming,” Huang told CES attendees, according to IBD.
""Like large language models, world foundation models are fundamental to advancing robot and AV development, yet not all developers have the expertise and resources to train their own. We created Cosmos to democratize physical AI and put general robotics in reach of every developer,"" he added.
An Nvidia spokesperson I contacted declined to comment.
If there is one thing I learned in writing my latest book, Brain Rush, it is this: Nvidia’s future depends heavily on collaboration with companies closer to technology end-users.
That’s because Nvidia — which designs chips and builds software for developers — can only enjoy rapid growth by motivating companies closer to end-users — such as chip fabricators, cloud services providers, enterprise software companies, and management consultants — to make Nvidia’s products part of a system that businesses and consumers find valuable.
Nvidia’s CES product announcements are aimed at strengthening such partnerships. Here are the industries these announcements aim to enhance:
While Nvidia did not make an announcement about this at CES, the biggest news from the company could be a strategy to compete with its cloud services customers who account for most of Nvidia’s growth.
Indeed one analyst detected Nvidia’s growing appetite for leased data center capacity equipped for AI processing. Such AI data centers would “rent out servers powered by its own chips as well as host its AI software development platform,” noted Investor’s Business Daily.
The analyst sees Nvidia’s push into cloud computing as a New Year’s surprise. “We see the potential for Nvidia to accelerate its data center leasing in 2025 as it looks to procure more capacity that can be used for its 'AI cloud,' with the potential for Nvidia to outpace its hyperscale peers in data center leasing in 2025,” noted TD Cowen analyst Michael Elias in a report featured by IBD.
In addition to looking for more data center capacity in other deals, Elias noted an “apparent large leasing deal between Nvidia and Digital Realty for data center capacity in Northern Virginia,” IBD reported.
Nvidia’s AI Cloud would compete with so-called Neoclouds — AI processing cloud services from the likes of CoreWeave, Lambda, Crusoe, Vultr and Together AI, noted IBD — as well as Nvidia’s biggest AI chip customers Amazon and Microsoft, wrote Elias.
By building its own neocloud, Nvidia could take more profit from a fast-growing industry sector. Nvidia — which now distributes its AI software by leasing it through AWS — could boost its profits by operating its own cloud. AI neoclouds are expected to grow at a 100% compound annual rate from $4 billion last year to $32 billion by 2027, according to Pitchbook.
Meanwhile, Nvidia’s cloud customers are trying to buy more AI chips from rivals and build their own custom chips. For example, in addition to buying Blackwell chips from Nvidia, hyperscalers are buying AI processors from Broadcom and Marvell Technologies as well as designing their own custom AI accelerators and trying to sell them to OpenAI, Apple and Meta Platforms, IBD reported.
Nvidia stock may be heading up. Based on 40 Wall Street analysts offering 12-month price targets, Nvidia stock would need to rise 18.5% to reach the average forecast of $135 per share, noted TipRanks.
Here are comments from less- and more-bullish analysts:
To me the key questions for investors are: How much revenue could Nvidia's AI cloud generate? Will Nvidia’s hyperscaler clients buy less from Nvidia? My guess is Huang expects Nvidia’s AI cloud service will make the company better off."
163,https://www.forbes.com/sites/antonyleather/2025/01/06/nvidia-rtx-5080-image-leak-confirms-specifications-ready-to-launch/,"Nvidia RTX 5080 Image Leak Confirms Specifications, Ready To Launch","Jan 06, 2025, 01:36pm EST",Antony Leather,"Less than nine hours away from Nvidia's official keynote address at this year's Consumer Electronics Show in Las Vegas, and there’s yet more evidence of Nvidia's possible release schedule and specifications for the graphics card expected to be available first — the GeForce RTX 5080.
A post on Twitter has revealed an actual retail box of the GeForce RTX 5080 from partner card manufacturer Gainward, confirming various specifications and also that the RTX 5080 appears to be in a most advanced state of readiness of any of the RTX 5000-series cards.
The post, via Overclock3D, shows user @wxnod revealing three RTX 5080 retail boxes, potentially in a retailer warehouse looking ready to ship. Previous rumors have pointed at a Jan. 21 release date for the card, and the existence of these retail boxes presumably with cards inside, suggests that's likely accurate to some degree while also suggesting that review embargoes may be closer still.
The box also reveals the memory type and amount for the RTX 5080, which as expected based on previous leaks and rumors is 16GB of GDDR7 memory, the same as the RTX 4080 Super and RTX 4080. The RTX 5090, meanwhile, is due an upgrade from 24GB to 32GB, with the rest of the product stack staying mostly the same, albeit with some small upgrades compared to the original RTX 4000-series.
Sadly, there were no other specifications revealed on the box, but it certainly adds weight to rumors that the RTX 5080 will launch first, possibly followed by the RTX 5090. The latter was also revealed recently, and will likely be just as much a worry, if not more so, to owners of small cases as the RTX 4090.

The Nvidia keynote address will be a keenly awaited presentation for both investors as well as gamers, with the former looking for signs Nvidia's stellar recent growth in value will continue. Gamers, meanwhile, will be hoping for pricing information as well as more information on specifications, availability and what other models it plans to release later in 2025.
Power connectors, dimensions, performance and power consumption are all factors we know little about yet, but pricing is probably the information most gamers want to know as recent rumors have pointed at an eye-watering $2,000-2,500 for the RTX 5090 and $1,200-1,500 for the RTX 5080, with that upper limit approaching what the RTX 4090 cost at launch. There has been very little confirmation of these prices, though, so the hope is that the price hikes might not be this significant.
If they are then AMD and Intel's plans to offer more value-conscious options to boost their meagre market share could be successful. AMD is expected to announce the Radeon RX 9070 XT with RTX 4080-like performance, but for far less cash, while Intel has already been making waves with its $250 Arc B580 graphics card, with a cheaper Arc B570 expected to be announced at CES, too.
I'll be covering the hardware launches in January, so follow me here on Forbes using the blue button below, Facebook or YouTube to get the latest news and reviews, and don't forget to comment below about your views on Nvidia's launch."
164,https://www.forbes.com/sites/dereksaul/2025/01/06/nvidia-stock-pops-4-on-track-for-record-close-ahead-of-jensen-huang-ces-keynote/,Nvidia Stock Pops 3%—Hits Record Close Ahead Of Jensen Huang CES Keynote,"Jan 06, 2025, 04:10pm EST",Derek Saul,"Nvidia added some $140 billion in market value Monday ahead of a key speech from its centibillionaire CEO Jensen Huang, briefly reclaiming the mantle as the most valuable company on earth for the unquestioned leader of the generative artificial intelligence wave.
Shares of Nvidia climbed 3.4% to $149.43 per share, besting their previous record close of $148.88 set Nov. 7.
The Monday surge sent Nvidia’s market capitalization to as high as $3.73 trillion, briefly eclipsing the market cap of Apple before the iPhone maker narrowly took back its title later in the afternoon — Nvidia is less than $40 billion away from its first market close as the world’s highest valued company since Nov. 25, according to YCharts data.
The Nvidia rally came amid a broader rally in U.S. technology stocks — the tech-heavy Nasdaq rose more than 1% to a 2025 high — and a global spike in share prices of other players in the semiconductor chip industry, as shares of Netherlands-based ASML, Taiwan-based TSMC and UK-based Arm Holdings gained at least 4% apiece.
Semiconductor stocks received a boost from record fourth quarter revenue data from the Taiwanese manufacturer Foxconn, a bullish signal for to-be-reported Q4 data for Nvidia and other AI companies.
Also helping Nvidia and its peers was the Washington Post reporting President-elect Donald Trump’s tariffs will initially target sectors other than semiconductors, good news for an industry heavily reliant on the global supply chain for designing and manufacturing the chips powering technology from iPhones to ChatGPT, OpenAI’s generative AI chatbot (Trump denied the report).
Huang will deliver Monday’s keynote address at the CES 2025 technology conference beginning at approximately 6:30 p.m. EST. The speech should include “reassuring updates” on the company’s Blackwell and Rubin graphics processing unit systems, Bank of America analyst Vivek Arya wrote in a recent note to clients, predicting Huang’s speech will be a “positive catalyst” for the stock. Rosenblatt analysts Kevin Cassidy and Hans Mosesmann concurred they expect to see a “bullish Jensen Huang” on Monday, laying out a case for Nvidia’s competitive advantage in addressing the full-stack of technology needed to power AI.
Nvidia designs a majority of the custom semiconductor chips powering generative AI systems, and its notable clientele includes Amazon, Meta, OpenAI and Elon Musk’s xAI. Founded in 1993 by Huang and two other engineers, Nvidia spent much of its existence best known for its video games graphics business before rising to fame as the face of the 2020s AI boom. No company has ever achieved a $4 trillion valuation, though Apple and Nvidia are both within 10% of hitting the milestone.
786%. That’s how much Nvidia’s share price has risen since Nov. 30, 2022, the launch of ChatGPT. An $1,000 investment in Nvidia at that time is now worth nearly $9,000.
Huang’s net worth rose by about $5 billion Monday, the third most of any billionaire, according to our real-time net worth tracker. Huang’s $131 billion fortune makes him the ninth-richest person on the planet, rising more than twentyfold over the last five years. Most of Huang’s net worth comes from his 3.5% stake in Nvidia, the largest of any individual."
165,https://www.forbes.com/sites/karlfreund/2025/01/06/nvidia-rocks-ces-with-grace-blackwell-ai-pc-platform/,Nvidia Rocks CES With Grace-Blackwell AI PC Platform,"Jan 06, 2025, 11:28pm EST",Karl Freund,"Among a slew of AI software announcements, Nvidia founder, CEO, and fashion icon Jensen Huang announced a new platform to power AI PCs for AI developers. Many analysts had long expected this move, as the Grace (Arm) CPU coupled with a Blackwell GPU on the same package will challenge PC competitors Intel, AMD, and Qualcomm. But it isn’t a general-purpose AI PC. At least not yet. (Disclosure: Nvidia is a client of my firm, Cambrian-ai Research.)
Project Digits is a small box available from Nvidia and “Top Partners” starting at $3000. Add a monitor, keyboard, and mouse, or buy from a partner, and you will likely have the fastest and most complete AI development workstation on the market. It delivers a full petaflop of FP4 performance and supports the full breadth of Nvidia AI software, from NEMO to Omniverse.
Each Project DIGITS (Project is a funny name; probably intended to avoid antagonizing PC partners) features 128GB of unified, coherent memory and up to 4TB of NVMe storage. It runs on standard wall power. With this platform developers can run AI models up to 200-billion-parameters. In addition, using NVIDIA ConnectX networking, two Project DIGITS AI supercomputers can be linked to run up to 405-billion-parameter models. Look Ma! No server!
With DIGITS,  researchers can prototype, fine-tune and test models on a local workstation running Linux-based NVIDIA DGX OS, and then deploy them  on NVIDIA DGX Cloud, accelerated cloud instances or data center infrastructure.
Since most developers use Linux, DIGITs supports that OS. This means it isn’t a general-purpose PC, at least not until and if it supports Microsoft Windows. Jensen was specific that this platform is for AI developers, not AI users. Jensen Huang, founder and CEO of Nvidia says the goal is to “Place an AI supercomputer on the desks of every data scientist, AI researcher, and student (that) empowers them to engage and shape the age of AI.”
Interestingly, MediaTek collaborated with Nvidia on the design of GB10, “contributing to its best-in-class power efficiency, performance and connectivity.” I think that means SoC design expertise. Nvidia and MediaTek had previously announced collaboration on automotive solutions, but the relationship perhaps has grown considerably.
A superfast AI PC for developers is cool. A superfast AI PC for end users would be huge, challenging the field of PC CPU providers. What must happen to turn “Project” DIGITS into an AI PC powerhouse? Microsoft Windows and PC OEMs like HP, Dell, and Lenovo must support the new SuperChip. Testing and certifying the new Arm-based platform for Windows would be somewhat straightforward now that Qualcomm Snapdragon has brought Arm CPU cores into the Windows mainstream. But this work still takes time. It could be ready by GTC, at least to make an announcement. Doing so would open up an incremental multi-billion dollar market to Nvidia.
And we haven’t even touched on all the cool software Nvidia announced, nor the fact that Nvidia has just won the Big Kahuna of autonomous vehicles: Toyota. But that is a story for another time.
Disclosures: This article expresses the opinions of the author and is not to be taken as advice to purchase from or invest in the companies mentioned. My firm, Cambrian-AI Research, is fortunate to have many semiconductor firms as our clients, including BrainChip, Cadence, Cerebras Systems, D-Matrix, Esperanto, Groq, IBM, Intel, Micron, NVIDIA, Qualcomm, Graphcore, SImA.ai, Synopsys, Tenstorrent, Ventana Microsystems, and scores of investors. I have no investment positions in any of the companies mentioned in this article. For more information, please visit our website at https://cambrian-AI.com."
166,https://www.forbes.com/sites/richardbishop1/2025/01/06/aurora-continental-and-nvidia-partner-to-scale-up-driverless-trucks/,"Aurora, Continental, And NVIDIA Partner To Scale Up Driverless Trucks","Jan 06, 2025, 11:00pm EST",Richard Bishop,"Aurora, Continental, and NVIDIA today announced a long-term strategic partnership to deploy driverless trucks at scale, powered by the next-generation NVIDIA DRIVE Thor system-on-a-chip (SoC). NVIDIA’s DRIVE Thor and DriveOS will be integrated into the Aurora Driver, an SAE Level 4 autonomous driving system that Continental plans to mass-manufacture in 2027.
Aurora aims to deliver the benefits of self-driving technology safely, quickly, and broadly to make transportation safer, increasingly accessible, and more reliable and efficient than ever before. The Aurora Driver is a self-driving system designed to operate multiple vehicle types, from freight-hauling trucks to ride-hailing passenger vehicles, and underpins Aurora’s driver-as-a-service products for trucking and ride-hailing.
“Delivering one driverless truck will be monumental. Deploying thousands will change the way we live,” said Chris Urmson, CEO and co-founder at Aurora. “NVIDIA is the market leader in accelerated computing, and they’ll strengthen our ecosystem of partners and our ability to deliver safe and reliable driverless trucks to our customers at scale.”
“Developing, industrializing, and manufacturing powerful self-driving hardware at commercial scale requires unique and unparalleled expertise,” said Aruna Anand, President & CEO, Automotive, Continental North America. “Our industry-first collaboration with Aurora and NVIDIA to deliver driverless trucks positions Continental at the forefront of this cutting-edge technology and will drive value to our business.”
“The combination of NVIDIA’s automotive-grade DRIVE Thor platform with Aurora’s advanced self-driving trucking technology and Continental’s manufacturing and integration expertise is set to help drive the future of autonomous trucking, helping make roads safer while driving up operational efficiency,” said Rishi Dhall, vice president of automotive at NVIDIA.
Aurora is in the final stages of validating the Aurora Driver for driverless operations on public roads. The Aurora Driver is equipped with a powerful computer and sensors, including lidar, radar, and cameras, enabling it to safely operate at highway speeds. Verifiable AI enables the Aurora Driver to quickly adapt to new operating domains while being validated through Aurora's Safety Case, an essential tool for regulatory trust and public acceptance. Aurora plans to launch its driverless trucking service in Texas in April 2025.
NVIDIA will power the primary computer of the Aurora Driver with a dual NVIDIA DRIVE Thor SoC configuration that runs DriveOS. DRIVE Thor, built on the NVIDIA Blackwell architecture, is designed to accelerate inference tasks critical for autonomous vehicles to understand and navigate the world around them. As Continental and Aurora prepare to manufacture self-driving hardware at scale in 2027, there’s plenty of action now: production samples of DRIVE Thor are coming in the first half of 2025.
Continental is one of the major automotive suppliers globally. The company employs around 200,000 people in 56 countries and markets, generating 2023 sales of €41.4 billion. Continental is developing a reliable, serviceable, cost-efficient generation of the Aurora Driver hardware, specifically for high-volume manufacturing. The company is also developing a specialized independent secondary system that can take over operation if a failure occurs in the primary Aurora Driver computer. With start of production planned for 2027, Continental will test prototypes of the future hardware kit in the coming months. Continental will then integrate DRIVE Thor with DriveOS into the primary Aurora Driver computer at its manufacturing facilities and ship the full hardware kit to Aurora’s truck OEM partners for integration into customers’ trucks.
At the 2025 Consumer Electronics Show this week in Las Vegas, the Continental exhibition is displaying an early prototype of the Aurora Driver hardware by Continental and the Volvo VNL Autonomous: the flagship model of Volvo's autonomous technology platform that is designed to seamlessly integrate with the Aurora Driver.
Based on CES program information, other Automated Driving System developers exhibiting at CES 2025 are Plus, Volvo Group, Waymo, WeRide, and Zoox. These companies are beyond the heady days of yesteryear when the question was “is it possible to deploy autonomous driving safely on public roads?” While many companies have fallen by the wayside in the ensuring years, those who remain, by and large, are transitioning capable driverless technology to scale-able products.
In fact, 2025 looks to be the break-out year for autonomous driving, tantalizingly moving towards full commercial deployment for autonomous trucking. Not just Aurora: Gatik has signaled similar scale-able “freight only” deployments in early 2025.
Meanwhile, ride-hailing with driverless vehicles continues to gain momentum. It’s significant that Aurora is signaling a future which includes rail-hailing as well as trucking. If so, they will be unique in the U.S. by covering both use cases.
Disclosure: Richard Bishop advises and/or holds equity in the following companies mentioned in this article: Alphabet, Aurora, Gatik, Plus, WeRide."
167,https://www.forbes.com/sites/antonyleather/2025/01/06/nvidia-set-to-announce-geforce-rtx-5000-series/,Nvidia Set To Announce GeForce RTX 5000 Series,"Jan 06, 2025, 04:37am EST",Antony Leather,"Nvidia is widely expected to announce its new GeForce RTX 5000 series graphics cards such as the RTX 5080 and 5090 today at the CES event in Las Vegas with CEO Jensen Huang taking to the stage at 6:30pm PT. As well as the next generation of graphics cards for games, AI is also going to be a main talking point, but gamers will be hoping for news on the new RTX 5000 series including confirmation of pricing, specifications and release dates and internal performance testing.
You can see the Nvidia CES Keynote livestream using the link below from 6:30pm PT today.
There are some sizeable chunks missing from our knowledge of the RTX 5000 series with rumors and leaks so far only covering vague specifications for the whole series. The keynote will hopefully reveal most of the information we want to see, but it's likely some will be held back as some models may not be released for several months.
This is the big one. There's been very little if any concrete information about pricing for the new RTX 5000 series, but some figures in the rumor mill have pointed at significant markups compared to previous generations. For example, recent rumors put the RTX 5090 at between $2,000-2,500 - considerably more than the $1,600 launch price of the RTX 4090.
FEATURED | Frase ByForbes™
Unscramble The Anagram To Reveal The Phrase
Pinpoint By Linkedin
Guess The Category
Queens By Linkedin
Crown Each Region
Crossclimb By Linkedin
Unlock A Trivia Ladder
While higher pricing could see profits soar for what is currently the worlds second most valuable company, gamers won't be happy as graphics card prices have soared in recent years thanks to the pandemic and cryptocurrency mining. Today should at least provide some clarity, but it's also worth noting that AMD and Intel are also expected to have graphics card announcements this week.
What we know so far is that the RTX 5090 is going to get a sizeable memory upgrade from 24GB to 32GB - recently confirmed by a leak posted on tech website Videocardz that revealed an RTX 5090 model from Inno3D called the iChill X3. The box clearly stated 32GB and this is likely to be GDDR7 memory. The RTX 5080 and 5070 Ti will maintain the same 16GB as the RTX 4080 and RTX 4070 Ti Super, but this is a 4GB bump compared to the 4070 Ti and 4070.
The RTX 5070 will apparently come with 12GB - the same as the RTX 4070, while the RTX 5060 Ti has 16GB and 8GB flavors like the RTX 4060 Ti. Lower down the RTX 5060 maintains the same 8GB as the RTX 4060, at least according to rumors but specifications for at least the RTX 5090 and RTX 5080 should be revealed today. Previous launch schedules suggest Nvidia may not offer complete details for other models and its likely only these two cards that we'll see in detail today.
According to a leak detailed in the link below, the RTX 5080 is due to be released first likely followed by the RTX 5090, with the former arriving on Jan 21. This would suggest, based on previous launch schedules, that the RTX 5090 would follow in February or March and the rest of the stack appearing some time before June.
Of course, much of what we know is based on rumor and supposed leaks from those in the industry so we may find things are quite as we expected. Nvidia has usually stuck to similar launch schedules with the release of new architectures, but we will likely know the availability of at least the RTX 5090 and 5080 today and most likely other models too, albeit lacking certain details.
I'll be covering the hardware launches in January so follow me here on Forbes using the blue button below, Facebook or YouTube to get the latest news and reviews and don't forget to comment below about your views on Nvidia's launch."
168,https://www.forbes.com/sites/antonyleather/2025/01/06/nvidia-reveals-the-rtx-5090-theres-good-and-bad-news/,Nvidia Reveals The RTX 5090: There’s Good And Bad News,"Jan 06, 2025, 10:50pm EST",Antony Leather,"Nvidia has revealed four of its RTX 5000-series graphics cards at the CES event in Las Vegas and there's good and bad news when it comes to pricing. The RTX 5090, 5080, 5070 Ti And 5070 pricing was revealed as well as availability, with all four cards available from this month.
With the pricing some cards are a lot more expensive, while others are cheaper compared to the original RTX 4000-series models. The RTX 5090 costs $400 more than the RTX 4090 did at launch, retailing for $1,999. The RTX 5080, though, costs $200 less than the RTX 4080 did at launch while the RTX 5070 Ti also sees a lower launch price than the RTX 4070 Ti by $50. Finally, the RTX 5070 will cost $549, which is a $50 price cut.
This is probably not what AMD and Intel wanted to hear as they'd been focussing on the low end and mid-range with an emphasis on value and reigning in Nvidia's enviable market share. Price cuts for the likes of the RTX 5070 compared to the RTX 4070 is probably a worst case scenario so the graphics card market will be very interesting over the course of the next few months.
In short, if you want an RTX 5090, it's going to cost you more. With everything else, you'll be paying less than you did with the original RTX 4000 series. This is a far cry from recent rumors that put massive markups on the RTX 5090 and 5080 expecially. There's been no mention of any cards below the RTX 5070 yet.
Starting with the RTX 5090 and 5080, these will be available on Jan. 30 while the RTX 5070 Ti and RTX 5070 will be available in February. It seems Nvidia is skipping its own Founders Edition model on the RTX 5070 Ti, but will offer it on the other models, which will be available direct from its wesbite.
The RTX 5060 TI and RTX 5060 are expected to make an appearance at some point in the near future, but as I left the keynote to head to bed here in the UK at 4am, no information had been revealed on those cards. That's to be expected, though, as the RTX 4060 and 4060 Ti took a few months to appear after the initial launch of the RTX 4000-series with cards such as the RTX 4090 and RTX 4080.
There's more good news, though, as the card revealed today was the Blackwell architecture-based RTX 5090 and the most obvious feature about the beautiful, sleek new Founders Edition cooler design is how small it is. The RTX 4090 (below) was a massive triple-slot graphics card and required significant clearance in PC cases. The RTX 5090, though, appears to be a whole slot thinner, appearing to be around two slots.
This likely means other models could be smaller in size and this also raises questions about existing information relating to specifications and power consumption as none of the rumors even came close to suggesting a two-slot RTX 5090. It's certainly a far cry from the enormous Inno3D model revealed earlier this week. Partner cards are often larger, but the difference here is an entire slot of depth, which is a huge difference. It will be interesting to see what other examples arrive from other board partners and whether any of those will be dual-slot too.
I'll be covering the hardware launches in January as well as other announcements at CES this week, so follow me here on Forbes using the blue button below, Facebook or YouTube to get the latest news and reviews."
169,https://www.forbes.com/sites/antonyleather/2025/01/05/nvidia-geforce-rtx-5090-revealed-and-its-massive/,Nvidia GeForce RTX 5090 Revealed And It’s Massive,"Jan 05, 2025, 06:31pm EST",Antony Leather,"Nvidia is expected to announce its GeForce RTX 5000-series graphics cards at this coming week's Consumer Electronic Show in Las Vegas, with information likely to be revealed on models such as the RTX 5080 and RTX 5090 and it's the latter that has apparently been revealed by board partner Inno3D with an absolutely enormous cooler.
Speculation has been rife about the size of this card before, with concerns over not just the depth, but also the width. These concerns seem to be realised here thanks to the leak of Inno3D's iChill X3 model (via Videocardz). The model in question is 3.5-slots deep, which is as big as the largest RTX 4090 models, but the width seems to be even larger than we've seen before. One of the largest RTX 4090 models was the Palit GameRock and a comparison of the two does seem to suggest the Inno3D iChill is slightly wider.
The RTX 4090 is already a large graphics card and the leaked image from Inno3D proves that the RTX 5090 is unlikely to be any smaller and in fact might even be a little bigger on average across the various cooler designs including an expected Nvidia Founders Edition. This isn't good news for anyone with a small case with limited graphics card clearance, especially mini-ITX or small form factor.
Plenty of rumors about specifications and pricing have been unleashed over the last few weeks, but nothing is officially confirmed yet by Nvidia itself. Board partners have provided plenty of  leaks such as this one, though, as well as potential leaks from regular community members that have provided accurate information in the past. If you haven't already, check out my article below about the RTX 5000 series.
FEATURED | Frase ByForbes™
Unscramble The Anagram To Reveal The Phrase
Pinpoint By Linkedin
Guess The Category
Queens By Linkedin
Crown Each Region
Crossclimb By Linkedin
Unlock A Trivia Ladder
As well as a 3.5-slot cooler, the image confirms the rumor that the RTX 5090 will indeed come with 32GB of VRAM, which is expected to be GDDR7 - a sizeable upgrade from the 24GB of GDDR6X on the RTX 4090 and by far the biggest memory upgrade that we're likely to see of the entire product stack. In fact, it's 8GB upgrade might be larger than those of the rest of the stack combined, especially as Nvidia is rumored to be sticking with 8GB for the RTX 5060.
Annoyingly we can't see the power connector in the photo, which would help to dispel rumors of twin 16-pin 12VPWR connectors on the RTX 5090, which have already been dismissed by a Chinese power supply manufacturer. Given Nvidia's previous launch schedules, it's highly likely we'll learn a lot more about the RTX 5090 this week, even if the latest rumors point at the RTX 5080 being launched first, possibly being released as soon as Jan. 21.
Pricing is something little is known about apart from some fairly horrific figures from the rumor mill recently that puts the RTX 5090 at $2,000-$2,500. That's a sizeable markup from the $1,600 launch price of the RTX 4090.
I'll be covering the hardware launches in January so follow me here on Forbes using the blue button below, Facebook or YouTube to get the latest news and reviews."
170,https://www.forbes.com/sites/greatspeculations/2025/01/03/nvidia-stock-vs-custom-ai-chips-should-investors-worry/,Nvidia Stock Vs. Custom AI Chips: Should Investors Worry?,"Jan 03, 2025, 05:00am EST",Trefis Team,"Nvidia stock had a stellar 2024, rising by almost 3x to about $135 per share. Business has boomed led by the surging demand for graphics processing units (GPUs) which have emerged as the backbone of the generative artificial intelligence era. That said, over the last month or so investors appear to have their eyes on another sector of the semiconductor market, namely ASICs (application-specific integrated circuits) that could play a bigger role in AI computing.
This comes after two major ASIC players, Broadcom and Marvell Technology, reported a surge in demand for their ASICs from large cloud customers in the most recent quarters. Do Custom AI Chips Make Marvell Stock A Buy? For instance, Broadcom’s sales from its custom AI chips and networking processors surged by 220% to $12.2 billion in 2024, up from $3.8 billion in revenue that the company generated from AI silicon in FY’23. To be sure, Nvidia’s sales are head and shoulders above, estimated to come in at about $129 billion this fiscal year, but its growth rates are slowing. So could ASICs threaten Nvidia’s AI dominance as the market matures? Separately, if you want upside with a smoother ride than an individual stock, consider the High Quality portfolio, which has outperformed the S&P, and clocked >91% returns since inception.
ASICS vs GPUs
ASICs have been around for more than five decades. However, they are seeing renewed interest in the AI era. While GPUs from companies such as Nvidia are pretty versatile and can be programmed for AI as well as other tasks, ASICs are custom-designed semiconductors built to perform specific tasks giving them certain advantages versus general processors. By focusing on targeted functionalities these chips offer several advantages versus GPUs for AI. For instance, these specialized chips could be more cost-effective than GPUs, which are designed for a wider range of applications.
ASICs also consume less electricity and this makes them ideal for data centers aiming to reduce electricity costs - a key cost of operating large AI systems. ASICs can also achieve higher performance for dedicated tasks than general-purpose GPUs from Nvidia or AMD as they are purpose-built. These chips could be well suited for large cloud computing providers given that they operate at a scale that can justify the design and development costs of ASICs. For instance, Broadcom, a company that is viewed as the biggest beneficiary of a potential pivot toward ASICs, recently said that three of its hyperscaler customers intend to build clusters of 1 million custom chips across a single network.
Change In The AI Space Benefits ASICs
Companies have devoted immense resources to building AI models over the last two years or so. Now training these massive models is more of a one-time affair that requires considerable computing power and Nvidia has been the biggest beneficiary of this, as its GPUs are regarded as the fastest and most efficient for these tasks. However, the AI landscape may be shifting. Incremental performance gains are expected to diminish as models grow larger in terms of several parameters. Separately, the availability of high-quality data for training models is likely to become a bottleneck as much of the Internet’s high-quality data is already run through large language models. Considering this, the significantly front-loaded AI training phase could wind down. The underlying economics of the end market for GPU chips and the broader AI ecosystem are weak, and most of Nvidia’s customers likely aren’t generating meaningful returns on their investments just yet. See How Nvidia Stock Could Fall To $65.
As shareholders eventually seek better returns, we could see more customers looking toward ASICs to reduce upfront costs as well as operating costs. The longer-term focus of AI will be on inference, where trained models are used in real-world applications. This phase is less computationally intensive and could open the door for alternative AI processors that are less powerful. ASIC chips customized for inference could also be a top choice for these tasks. There is some historical precedent to this as well. For example, in the the crypto industry, Bitcoin miners initially used GPUs for mining, but some large players have since switched to ASICs as they scaled up given their better hash rates (effectively how many guesses the system makes per second to solve the crypto puzzle) and overall cost-effectiveness.
What This Means For Nvidia
While NVDA stock has seen strong growth over recent years, the Trefis High Quality Portfolio, with a collection of 30 stocks, has provided better returns with less risk versus the benchmark S&P 500 index over the last four years; less of a roller-coaster ride as evident in HQ Portfolio performance metrics. So what lies ahead for Nvidia stock?
To be sure, we don’t expect Nvidia’s business to be supplanted by these new waves of processors, given the company’s head start in the AI market and its deeply entrenched CUDA software stack and development tools which results in high switching costs for customers. Moreover, if the market sees a strong shift, Nvidia too could potentially expand its presence in the space. However, Nvidia’s premium valuation may not fully account for the potential risks and slowing growth. We value Nvidia stock at about $93 per share, about 32% below the current market price. See our analysis of Nvidia valuation: Expensive or Cheap.
Invest with Trefis Market Beating Portfolios
See all Trefis Price Estimates"
171,https://www.forbes.com/sites/antonyleather/2025/01/01/nvidia-geforce-rtx-5080-rumored-for-january-21st-release/,Nvidia GeForce RTX 5080 Rumored For Jan. 21 Release,"Jan 01, 2025, 06:30pm EST",Antony Leather,"As CES nears, we finally have information on when Nvidia's GeForce RTX 5000-series could be available. Widely expected to be announced at CES, which starts Jan. 7 in Las Vegas and is expected to offer plenty of PC hardware announcements, it appears that the Nvidia GeForce RTX 5080 could be hitting the shelves first a couple of weeks after the show, shoving aside last year's RTX 4080 Super in the process.
Jan. 21 is the date given by Twitter user HKepcmedia (via Videocardz), who offered no more information about other models such as the RTX 5090, so for now it appears the RTX 5080 will be the first card you can buy, with other cards following. Unfortunately the Tweet has now been deleted so it's not possible to show it here.
If the RTX 5080 does come first, this would suggest, based on the RTX 4000 card launch schedule, that the RTX 5090 would be released a month or so later in mid-February/early march. That's assuming a similar time scale to what we saw with the RTX 4090, with availability a few weeks after the initial announcement, followed by the RTX 4080 a month later and RTX 4070 Ti.
The second card to launch seems to follow a month later, but looking back the third launch card with the RTX 4000 series two years ago - the  RTX 4070 Ti - it took even longer, not appearing for another eight weeks, albeit with Christmas and New Year’s in between. That would mean we'd be unlikely to see an RTX 5070 Ti before April.
FEATURED | Frase ByForbes™
Unscramble The Anagram To Reveal The Phrase
Pinpoint By Linkedin
Guess The Category
Queens By Linkedin
Crown Each Region
Crossclimb By Linkedin
Unlock A Trivia Ladder
Pricing could also be announced next week, at least for the first wave of cards, which would likely include the RTX 5080 and RTX 5090, which have been rumored to cost as much as $1,500 and $2,500, respectively. There's hope that the pricing might be less eye-watering than that, but we'll hopefully get some clarity during Nvidia's announcement.
The RTX 5000 series is unlikely to have a competitor at the top end, with AMD likely to stick to RTX 4080-level performance, but with an emphasis on value and gaining market share, starting with a Radeon RX 9070 XT. Memory capacity hasn't increased much across the board, but the RTX 5090 is likely to offer an enormous 32GB of GDDR7 VRAM, with all models offering the same memory type as well as PCIe 5.0 support.
The RTX 5080 is expected to have 16GB, while the RTX 5070 Ti is rumored to have 16GB too. This isn't an increase over the RTX 4070 Ti Super, but is more than the RTX 4070 Ti. Meanwhile, at the bottom of the stack the RTX 5060 is rumored to stick with the same 8GB as its predecessor, which hasn't gone down well with some gamers and commentators.
While some of the new graphics cards are also rumored to use more power, a representative from Chinese PSU manufacturer Segotep claimed the RTX 5090 would use the same 16-pin 12VPWR connector as the RTX 4090, with a single 600W connector being enough to power the card.
There will likely me more news and leaks over the next week as we approach CES in Las Vegas, and I'll be covering the hardware launches announced at the event, including the GeForce RTX 5080, so follow me here on Forbes using the blue button below, Facebook or YouTube to get the latest news and reviews."
172,https://www.forbes.com/sites/antonyleather/2024/12/31/nvidia-geforce-rtx-5000-everything-you-need-to-know/,Nvidia GeForce RTX 5000: Everything You Need To Know,"Dec 31, 2024, 07:34am EST",Antony Leather,"The Nvidia GeForce RTX 5000 series graphics cards such as the RTX 5080 and RTX 5090 are expected to be announced in January's Consumer Electronics Show in Las Vegas, which runs Jan 7th-11th and, while performance leaks and concrete specification data are still somewhat scarce, there's still plenty of reliable information about the products that will replace the RTX 4000 and 4000 Super series. Here's what we know so far.
The Nvidia GeForce RTX 5000 series hasn't been officially announced, but specifications have leaked from both partner card manufacturers as well as regular and well-regarded leakers. All RTX 5000 cards will be PCIe 5.0 compatible and use GDDR7 VRAM, but capacity is always a hot topic and here the RTX 5090 gets an 8GB bump to 32GB compared to 24GB for the RTX 4090, but the RTX 5080 stays the same at 16GB - the same as the RTX 4080 and RTX 4080 Super.
The RTX 5070 Ti will maintain the same 16GB as the RTX 4070 Ti Super, but this is a 4GB bump compared to the 4070 Ti and 4070. The RTX 5070 will apparently come with 12GB - the same as the RTX 4070, while the RTX 5060 Ti offers the same options of 16GB and 8GB as the RTX 4060 Ti. At the bottom of the stack, the RTX 5060 maintains the same 8GB as the RTX 4060, which has resulted in some criticism given some current games make use of more than this. We'll have to wait and see what the final specifications are.
Very little information has been revealed on the pricing of the Nvidia GeForce RTX 5000 series, but what has been rumored and discussed so far may be an indicator of a nasty surprise. Some websites reported pricing for the RTX 5090 to be between $2,000 and $2,500, the RTX 5080 from $1,200 to $1,500 and the RTX 5070 from $599 to $700. These are all significant increases over their predecessors and the fact that AMD has stated it will not be competing with Nvidia's top-end cards may be the reason for this. These figures are only rumors, but this is perhaps going to be the most keenly-awaited bit of news in January.
Several images have leaked showing a huge PCB and GPU die with the latter measuring 744mm2 compared to 608mm2 for RTX 4000 GPUs - a significant increase.
However, several tech commentators have raised concerns about the physical size of the PCB and resulting graphics card including the cooler and if compatibility with current cases might be affected, especially on the small form factor/mini-ITX side.
The width of the card seems to be the main point of concern, which would definitely impact compatibility, especially as the power connector has also been on the side, further adding to the clearance needed.
The 12VPWR 16-pin connector has been in the news quite a bit since it first appeared and never for good reasons. This time it's double trouble as rumors were rife a few months ago claiming that the RTX 5090 had not one but two of them to handle its supposed 600W power draw.
However, more recent information contradicts those rumors, with Videocardz reporting on a comment from Chinese PSU manufacturer Segotep, who stated the RTX 5090 would not use dual power connectors and seemed to imply a standard 600W 12VPWR connector would suffice.
The naming scheme will remain the same and there are six models launching, although we don't have exact time frames and in the past Nvidia has had a wide window of availability with several months between the initial launch and the final models being released. This was certainly the case with the RTX 4060, which was available many months after the RTX 4090's arrival in October 2022.
Based on this time scale, with the Nvidia GeForce RTX 5000 being announced in January 2025, reviews for the first launch wave that will likely include the RTX 5090 (or RTX 5080) will follow at the end of January or beginning of February with availability around the same time. With the RTX 4080, it followed a month later, so if this is the case for the RTX 5080, it would be February/March and again based on the previous launch schedule, a month after that the RTX 5070 Ti, followed by the RTX 5070, RTX 5060 Ti and RTX 5060 nearer the middle of 2025.
I'll be covering the hardware launches in January so follow me here on Forbes using the blue button below, Facebook or YouTube to get the latest news and reviews."
173,https://www.forbes.com/sites/bethkindig/2024/12/23/where-i-plan-to-buy-nvidia-stock-next/,Where I Plan To Buy Nvidia Stock Next,"Dec 23, 2024, 05:03pm EST",Beth Kindig,"Blackwell is the word for Nvidia as the AI leader heads into 2025, with multiple configurations and a mid-year upgrade (B300/GB300) for its new powerful GPU set to ramp significantly over the next few quarters. As recapped to the I/O Fund’s premium members after its Q3 earnings report, the I/O Fund is tracking multiple supply chain signals indicating Blackwell sales will likely far exceed the GPU sales we saw in 2023 and 2024 combined – to the tune of bringing Nvidia to $200 billion in data center revenue.
Analysts are already increasing their forecasts for Blackwell shipments for Q4 and for Q1, with forecasts for 250,000 to 300,000 shipments in Q4 nearly tripling to 750,000 to 800,000 in Q1. This compares to previous views seeing Q4 shipments of 150,000 to 200,000 ramping to 550,000 in Q1. This suggests Blackwell revenue estimates for Q1 are already moving 40-60% higher, potentially driving positive revenue revisions throughout the year as it becomes Nvidia’s primary GPU product.
Nvidia has tailwinds in 2025 from increased pricing power with Blackwell, output and shipment estimates already rising before the ramp begins, AI capex still quickly growing, and GPU clusters starting in the 100K range where Hopper maxed out, even as competition from AMD, Broadcom and others begins to increase.
Nvidia also has the benefit from the end of its fiscal year early next year, with the Street soon looking to 2026 numbers – which very well could be too low given the signals Blackwell is already giving. At the moment, Nvidia is trading at just 30x 2026’s estimated earnings of $4.43, its cheapest bottom line valuation since shares were $95 in May 2024 – and Blackwell still holds the potential to drive quarterly revenue beats the same way Hopper has and with margins returning to Hopper’s highs.
The bigger picture for Nvidia moving forward is that Blackwell holds the potential to dwarf Hopper, and the I/O plans on keeping its members informed on what it sees ahead for Nvidia with frequent updates for members. With that in mind, here’s what the I/O Fund sees as 2024 ends and 2025 begins.
Nvidia appears to be setting up for the next swing higher. As long as any further weakness holds over $116, this move should target between $165 - $173, with the potential to reach as high as $193.
If this swing gets confirmed, it would likely be the final 5th wave in the historic uptrend that started in October of 2022. This does not mean that the technicals do not support significantly higher prices, it only means that Nvidia will first have to deal with a notable correction in both price and time before it sees those levels.
The pattern off the October 2022 low developed as a classic 5 wave pattern. In early 2024 price went vertical. This was accompanied with max volume and peak momentum. This is the standard pattern seen in 3rd waves, and it tends to be the most powerful part of a 5 wave pattern. From the perspective of sentiment, this is the part of the trend where everyone realizes at once the direction of the trend. Shorts cover at the same time as the crowd buys, creating that standard pattern in 3rd waves.
This would mean that the correction in June of 2024 was the 4th wave, and that this is likely in the final 5th wave higher. The sentiment pattern in 5th waves to new highs in price, but on lower momentum and lower volume, which is what is happening now.
Zooming into the 4th wave correction that started in June of 2024 offers a better idea of the two potential paths that I am currently tracking.
·        Blue – The final 5th wave is playing out as an ending diagonal pattern, which is common for 5th waves. This type of pattern is a 5 wave pattern in itself that is characterized with large swings in both directions. Our target zone for the bottom on this 4th wave is $126 - $116. If Nvidia can push over $140.75, then then odds favor this scenario.
·        Red – Nvidia is in a much more complex 4th wave. If this is playing out, NVDA would see the $116 level break, which opens the door to a potential low at $101, $90, or $78.
One final point worth mentioning is how the broad semiconductor sector is performing in relation to the S&P 500. Semiconductors tend to be much more sensitive to the consumer, and economy than most sectors. For this reason, in periods of economic expansion, semiconductors tend to lead, outperforming the broad market.
However, when this sector starts to move against the broad market, it tends to be a warning that volatility is ahead. In fact, every time that the semiconductor sector has made a lower high while the broad market made a higher high – i.e., semiconductors do not confirm the move higher – this preceded some period of volatility since the 2021 top.
This pattern can be seen going back to 2000 and consistently warned of weakness. As of now, this is one of the largest and longest periods of divergence between the semiconductor sector and the broad market on record.
If the broader semiconductor sector stays below its July 2024 high, I would consider this a warning. This does not mean that I do not see potential upside, it only means that any long positions the I/O Fund takes will have strict targets at which we take gains and stops to protect us in case the market turns against us.
Make no mistake, Nvidia is the best stock of the decade and it’s only four years in. The I/O Fund has an aggressive buy plan at key levels should the stock pull back, and we have a backup plan should the stock overcome the peer pressure we are seeing from the semiconductor industry and meaningfully breakout.
Nvidia has been our largest position for the last 4 years. The I/O Fund sent out nine buy alerts to our readers to buy this position below $20 in 2021 – 2022. The I/O Fund believes the future is bright for Nvidia, and believe the potential next swing is worth playing. However, with all the warning signs, any new long position will have strict risk controls until these warnings reset.
The I/O Fund is also closely analyzing the supply chain to identify overlooked beneficiaries of the AI infrastructure buildout, sharing this information as well as potential buy and sell plans and real time trade alerts with premium members. The I/O Fund recently entered two separate beneficiaries for gains of 23% and 17% since November. Learn more here.
I/O Fund Portfolio Manager Knox Ridley and I/O Fund Equity Analyst Damien Robbins contributed to this report.
If you would like notifications when my new articles are published, please hit the button below to ""Follow"" me.
Please note: The I/O Fund conducts research and draws conclusions for the company’s portfolio. We then share that information with our readers and offer real-time trade notifications. This is not a guarantee of a stock’s performance and it is not financial advice. Please consult your personal financial advisor before buying any stock in the companies mentioned in this analysis. Beth Kindig and the I/O Fund own shares in NVDA at the time of writing and may own stocks pictured in the charts"
174,https://www.forbes.com/sites/greatspeculations/2024/12/20/how-nvidia-stock-could-fall-to-65/,How Nvidia Stock Could Fall To $65,"Dec 20, 2024, 08:19am EST",Trefis Team,"Could Nvidia stock fall by about 50% to levels of around $65 in the near term from the roughly $130 level it is at currently? We believe this is a real possibility. Nvidia has seen its business boom, led by the surging demand for its graphics processing units which have emerged as the de facto silicon for running artificial intelligence applications. However, there are multiple risks on the horizon. These include a potential cooling of AI training-related demand, mounting competition, and less attractive valuation multiples assigned by investors, due to slowing growth and less favorable monetary policy. We spell out the key risks for Nvidia in more detail and break down how the stock could fall by over 50% from current levels by looking at three metrics, namely the company’s revenue growth, margins, and price-to-earnings multiple. See our counter scenario on Nvidia Stock upside to $300. Indeed, we believe this broad range of upside and downside potential represents a simple fact, that Nvidia is a pretty volatile stock. While AI has been all the rage, quantum computing could be the next big thing. See What’s Driving D-Wave Quantum Stock? and also Palantir Stock: Buy, Sell, Or Hold?
Nvidia’s sales have grown at a breakneck pace. Nvidia’s revenues were up by close to 3x over the last 12 months as companies doubled down on accelerated computing using GPUs to perform more artificial intelligence tasks. However, growth is slowly cooling off. Nvidia’s sales expanded by about 122% in the most recent quarter. There remains a possibility that growth could cool further with sales potentially even declining versus current levels in the medium term. Here’s why.
Companies have devoted immense resources to building AI models over the last two years or so. Now training these massive models is more of a one-time affair that requires considerable computing power and Nvidia has been the biggest beneficiary of this, as its GPUs are regarded as the fastest and most efficient for these tasks. However, the AI landscape may be shifting. Incremental performance gains are expected to diminish as models grow larger in terms of several parameters. Separately, the availability of high-quality data for training models is likely to become a bottleneck as much of the Internet’s high-quality data is already run through large language models. Considering this, the considerably front-loaded AI training phase could wind down. The underlying economics of the end market for GPU chips and the broader AI ecosystem are weak, and most of Nvidia’s customers likely aren’t generating meaningful returns on their investments yet. As shareholders eventually seek better returns, we could see capital spending on GPU chips cool off, impacting the likes of Nvidia. Separately, if you want upside with a smoother ride than an individual stock, consider the High Quality portfolio, which has outperformed the S&P, and clocked >91% returns since inception.
The long-term focus of AI will be on inference, where trained models are used in real-world applications. This phase is less computationally intensive and could open the door for alternative AI processors that are less powerful. To be sure, Nvidia may well remain the leader by far in inferencing as well, but there’s certainly an opening for rivals such as AMD and potentially even Intel to gain market share with chips such as AMD’s MI300 chips or Intel’s Gaudi AI accelerators. Nvidia’s top-end chips such as the H100 might be considered as overkill for simpler inference tasks, considering their high power consumption and upfront cost.
There are signs that the big supply-demand mismatch seen through the early phase of the generative AI wave is easing. Microsoft which is Nvidia’s largest customer for GPUs, recently indicated that it was no longer supply-constrained for GPUs. The fact that Nvidia’s largest customer has sufficient chips to run its AI efforts suggests that the frantic “fear-of-missing-out” phase of GPU demand may be well behind us. Moreover, if demand stabilizes due to the above factors just as supply catches up, Nvidia may face pricing pressures or slower sales growth particularly if large customers rethink their inventory requirements.
Now Nvidia’s revenues are on track to more than double this year (FY’25) to about $129 billion per consensus estimates. However, if its growth rates slow considerably from here on to just about 10% over the next two years, due to the factors above, revenue could move from around $61 billion in FY’24 to just about $165 billion in FY’27.
Nvidia’s margins (net income, or profits after all expenses and taxes, calculated as a percent of revenues) have been on an improving trajectory - they grew from levels of about 25% in FY’19 to about 49% in FY’24 as the company witnessed better economies of scale and a more favorable product mix skewed toward complex data center products. Our dashboard has more details about the various components responsible for Nvidia’s net income change.
However, there is a real possibility that margins could decline to levels of about 35%. Why? Competition is mounting with other chipmakers such as AMD investing significantly to catch up in this space given the high stakes. AMD claims that its new Instinct MI300X chip outperforms Nvidia’s current chips in several parameters, while Intel is also looking to make a dent in the space with more value-priced AI chips. Separately, big tech players like Google - who are Nvidia’s biggest customers - are doubling down on AI and machine learning-related silicon. Competition will make Nvidia’s current revenue growth rates and abnormally high margins unsustainable. Earlier this month, Amazon announced plans to build an AI ultracluster, essentially a massive AI supercomputer that will be built using its proprietary Trainium chipsets. Amazon is also marketing its AI products to other companies. For instance, Apple says that it uses Amazon’s AI chips for certain tasks such as searching. This could also pose a risk to Nvidia’s business and margins. Should you Sell Nvidia and buy Intel Stock?
The gains from NVDA stock over the last 4-year period has been far from consistent, with annual returns being considerably more volatile than the S&P 500. Returns for the stock were 125% in 2021, -50% in 2022, and 239% in 2023. In contrast, the Trefis High Quality Portfolio, with a collection of 30 stocks, is considerably less volatile. And it has outperformed the S&P 500 each year over the same period. Why is that? As a group, HQ Portfolio stocks provided better returns with less risk versus the benchmark index; less of a roller-coaster ride as evident in HQ Portfolio performance metrics. Given the current uncertain macroeconomic environment and potential changes in the AI landscape, what’s the downside for Nvidia stock?
If we combine about 1.2x revenue growth (up 20%) between FY’25 and FY’27, with margins contracting from 50% levels currently to about 35% (down 30% from current levels, or 0.7x), this would imply that net income could decline by about 15% by 2027. Now if earnings shrink by 15%, the P/E multiple is bound to take a hit as investors re-assess Nvidia’s position as a growth stock. Moreover, changes to U.S. monetary policy could also impact high multiple tech stocks to an extent. During its meeting earlier this week, the Central bankers signaled a slower pace of rate cuts ahead. Policymakers reduced the number of quarter-point cuts they expected for 2025, from an average of four back in September to just two presently. This could be a sign that the era of very low interest rates seen through Covid-19 is behind us. If Nvidia’s P/E gradually shrinks from a multiple of about 44x now to about 25x, this could translate into a decline in Nvidia stock to about $65 per share. What about the time horizon for this negative-return scenario? In practice, it won’t make much difference whether it takes 2 years or 3 - if the competitive threat plays out and Nvidia’s GPU cash cow faces headwinds, we could see a sizable correction.
Invest with Trefis Market Beating Portfolios
See all Trefis Price Estimates"
175,https://www.forbes.com/sites/bethkindig/2024/12/19/this-is-not-broadcoms-nvidia-moment-yet/,This Is Not Broadcom’s ‘Nvidia Moment’ Yet,"Dec 19, 2024, 04:05pm EST",Beth Kindig,"Broadcom’s stock surged 35% in two days despite a mediocre Q4, as management offered investors a picturesque addressable market forecast for 2027. Q4 was not the blowout report the market made it out to be, as Broadcom fell just short of revenue estimates while guiding Q1 barely above consensus. Despite this, the market did solidify that momentum continues to build for AI stocks entering 2025.
Broadcom’s commentary on the call as to the serviceable addressable market for its two leading AI segments, custom silicon and networking, is why the stock moved a whopping 25%. As a reminder, a serviceable addressable market refers to market size the company can service, and is not a forecast of the company’s revenue. While Broadcom gave investors a reason to dream, it’s not the stock’s ‘Nvidia moment’ despite the surge in the stock price resembling Nvidia’s 2023 breakout. Instead, Broadcom is reporting flat QoQ AI revenue with the 200% year-over-year number being old news (Broadcom had guided for $12B in AI revenue in Q3 and only marginally beat that figure).
Among the AI titans, Broadcom is the one of the only stocks to see lumpy AI growth, reporting flat AI revenue growth from Q2 to Q3 – with expectations it remains at a mere 3% QoQ growth to start fiscal 2025. This occurred roughly a month before tariffs are likely to affect its top customer – Apple.
Below, I provide data that shows the move in Broadcom’s stock was premature, creating outsized pressure on Broadcom to live up to AI juggernaut Nvidia in 2025, which is unrealistic given Broadcom has only ~25% of revenue from AI versus 80% of revenue from Nvidia. When you factor in 30%+ of Broadcom’s revenue comes from China, versus Nvidia at 15% for China exposure, what you have is an upside down scenario for Broadcom where tariffs could negatively impact more revenue than what AI is currently providing.
The Street is desperate to find the next Nvidia in the vast and complex sector of semiconductors and hardware providers. There were many raises/beats across AI-related semiconductors, including from many small, lesser-known names. Meanwhile, Broadcom’s report was one of the least spectacular as there was a very rare miss for Q4 revenue and a Q1 guide that was only $30 million above consensus.
Growth is challenged sequentially, with Q1 only set to grow 4% QoQ. Semiconductor revenue was seen declining nearly -2% sequentially as well, with management guiding for $8.1 billion in Q1 versus $8.23 billion in Q4. AI revenue was not much of a surprise either, as Broadcom had guided for full-year AI revenue of $12 billion in Q3, coming in not even 2% above the guide.
To offset the lackluster performance, the management team painted a picture for AI revenue growth to scale quickly with a tantalizing addressable market forecast for 2027.
Here’s what CEO Hock Tan said that energized the stock:
“We currently have three hyper-scale customers who have developed their own multi-generational AI XPU roadmap to be deployed at varying rates over the next three years. In 2027, we believe each of them plans to deploy 1 million XPU clusters across a single fabric. We expect this to represent an AI revenue Serviceable Addressable Market, or SAM, for XPUs and networking in the range of $60 billion to $90 billion in fiscal 2027 alone.
We are very well positioned to achieve a leading market share in this opportunity and expect this will drive a strong ramp from our 2024 AI revenue base of $12.2 billion. Keep in mind though, this will not be a linear ramp.”
Tan also added that Broadcom was in advanced development with two additional hyperscalers, rumored to be ByteDance and OpenAI, with possibilities to turn both into revenue generating customers before 2027.
The comments the ramp will not be linear likely refers to the ramp being back-half weighted, with the majority of revenue being recognized between 2026-2027. On the call, it was mentioned that its 3nm custom silicon will ship in the second half of 2025. Meanwhile, Broadcom is trading at an astronomical valuation that is higher than Nvidia’s.
In fact, Broadcom is up against its toughest year yet as Nvidia’s Blackwell systems are set to raise the bar competitively with custom silicon as powerful Blackwell systems combining 36 CPUs and up to 72 GPUs ship in volume in Q1 with a bigger ramp in Q2 of 2025. The 72 GPUs in the NVL72 will be used as a single accelerator for 1.4 exaflops of AI compute power. Nvidia’s proprietary NVLink Switch will reach 130 TB/second, which is “more than the aggregate bandwidth of the internet.” Outside of narrow use cases, custom silicon will not be able to compete with Nvidia in 2025, whereas in future years, custom silicon may have more of an opportunity to catch up. For example, when comparing with Nvidia’s 2023-2024 Hopper generation, Amazon’s Trainium2 instances with 100,000 processors “equals around 32,768 Nvidia H100 processors,” according to Tom’s Hardware. This helps to paint a picture as to why custom silicon revenue for Broadcom is at a low $300 million per quarter in custom silicon revenue compared to Nvidia’s $27 billion per quarter on GPUs (removing the $3 billion Nvidia makes in networking from the data center segment).
Going back to the comment of a $75 billion serviceable market by 2027, at the midpoint - let’s put this opportunity in perspective. For 2024, Broadcom reported AI revenue of $12.2 billion, up 220% YoY from $3.8 billion in 2023. As stated, this was guided in Q3 and was not a beat/raise or news to anyone who covers the stock.
CEO Hock Tan said he believes Broadcom’s current serviceable AI market is worth $15 billion to $20 billion this year, suggesting that Broadcom commands approximately 70% market share at the midpoint of that range.
Broadcom serves two major markets in AI – custom accelerators which Broadcom is referring to as XPUs, and networking and switches, ripe with competition from Nvidia, Arista, Cisco, and others.
It will certainly not be a straightforward path for Broadcom to maintain what it sees as a 70% share of these addressable AI markets. Nvidia is arguably a strong contender in networking with InfiniBand and is moving into ethernet with Spectrum-X, while there are many other networking suppliers involved with Broadcom’s hyperscale customers. As pointed out by the CEO regarding the serviceable addressable market: “There's room for many players. All we are going to do is gain our fair share.”
Assuming Broadcom can reach 60% share at a $75B addressable market size, that correlates to approximately $45 billion in AI revenue in 2027, or nearly 3.7x growth over the next three years. In other words, that would require AI revenue growth of ~55% annually through 2027. While hyperscaler capex definitely supports such a ramp, this growth pales in comparison to the numbers Nvidia has been putting up. Meanwhile, Broadcom does not have the moat that Nvidia has, which I pointed out five years ago is the CUDA development platform.
Nvidia is currently on track for approximately $114 billion in data center revenue in fiscal 2025, up 140% YoY and up 661% from fiscal 2023. In fiscal 2027, Nvidia is expected to generate nearly $220 billion in data center revenue, or nearly 8x higher than Broadcom’s AI revenue estimate of $29 billion. This would be about 36% of revenue at the consensus estimate for $80 billion, which pales in comparison to the 80% range Nvidia has, and AI server makers, with one expected to see up to 40% of revenue from AI next year.
The I/O Fund previewed Nvidia’s path to a $200 billion data center segment in May 2024 for its free newsletter readers for gains of 39% since then, and gains of 2,550% since first calling out Nvidia’s AI GPU thesis and CUDA moat in November 2018. Premium members receive real-time trade alerts and analysis on numerous other AI data center beneficiaries. Learn more here.
Broadcom capped off fiscal 2024 with nearly 150% YoY growth in AI revenue to $3.7 billion, with networking the primary contributor. QoQ growth was ~20% in Q4, rebounding from flat QoQ growth in Q3; however, Q1 is expected to see QoQ growth decelerate to the low single-digits.
Here’s what Broadcom’s quarterly AI revenue growth has looked like:
The non-linear, bumpy ramp the CEO referenced is quite visible – sequential growth was flat in Q3, and for Q1, management’s guide for $3.8 billion in AI revenue points to sequential growth of under 3%. Meanwhile, Nvidia has grown data center revenue by $4 billion sequentially for four consecutive quarters – Nvidia’s sequential growth alone more than outpaces Broadcom’s total quarterly AI revenue.
For Q4, management said that they saw growth from both AI accelerators and networking, though not at the same rate. Networking component shipments were much higher in the back half of the year, with this strength continuing into the first half of next year. Management provided some additional growth figures for AI networking, with revenue up 158% YoY, driven by 4x growth in AI connectivity revenue from Tomahawk and Jericho shipments. AI networking contributed 76% of networking revenue, implying AI networking revenue of ~$3.4 billion, and custom accelerator revenue of ~$300 million in Q4.
While custom accelerator accounted for just a small portion of AI revenue in the quarter, management foresees strong growth in the second half of 2025. Broadcom is set to begin and quickly ramp shipments of its next-gen 3 nanometer AI ASICs to hyperscaler customers in the second half of the year.
Broadcom had previously laid out a path to 1 million accelerator clusters deployed by 2027, and re-emphasized that path in Q4’s earnings call. That’s essentially tenfold growth from the current 100K cluster sizes being deployed today. While that no doubt this will correlate into tremendous growth in accelerator shipments for Broadcom, Nvidia, AMD, and lesser-known ASICs design companies, Broadcom put emphasis on the need for networking to scale up to this degree.
The I/O Fund specializes in covering lesser-known AI stocks on our research site with trade alerts and weekly webinars. Learn more here.
Piper Sandler analyst Harlan Sur asked management about the dollar content for networking vs custom accelerators, and what the attach rate of networking per accelerator would be (ie. $1 networking for $1 in accelerators). CEO Hock Tan explained that “the simple ratio to look at is there is scale up and there is scale out. And as we expand into a single fabric cluster of XPUs or GPU that grows bigger and bigger, guess what is more important. Scale up becomes more and more important. And the ratio we are talking about as we move up increases almost exponentially, which is why I'm saying from networking, as a percent of AI content in silicon today of between 5% to 10%, you're going up to 15% to 20% by the time you hit 500,000 to 1 million XPU GPU clusters.”
This is because of the increasing demands for networking and switches to connect exponentially larger clusters, from spine to leaf in the front end and back end, rack to rack and accelerator to accelerator. With that said, Nvidia and Broadcom are neck-and-neck in networking revenue with Nvidia at $3.13 billion for Q3 and Broadcom at $3.42 billion. This year, Nvidia will be increasing its networking content and ramping Spectrum X, it’s Ethernet networking platform.
The opportunity beckons with AI cluster sizes set to grow tenfold or more over the next three years as hyperscalers build and deploy ever-larger data centers, however, this is decidedly not Broadcom’s ‘Nvidia moment’ yet. What separates the two is actual, numerical data.
Nvidia’s Hopper-driven breakout towards the $1 trillion market cap milestone in May 2023 came on the back of a ‘jaw dropping’ guide higher — it reported revenue of $7.2 billion for fiscal Q1 2024 (versus $6.5 billion estimated). The company guided for $11 billion in Q2 while analysts were expecting just $7.2 billion. Nvidia ultimately beat that guide as it reported $13.5 billion in revenue in Q2.
Since then, in just six quarters, Nvidia’s quarterly revenue has grown 5x from $7.2 billion to $35.1 billion, with $1 billion-plus beats each quarter along the way.
Broadcom, on the other hand, slightly missed revenue estimates this quarter and guided Q1 only marginally above consensus. AI revenue of $12.2 billion also came in just $0.2 billion above management’s forecast for $12 billion given in Q3; not exactly the out-of-the-ballpark blowout that Nvidia consistently put up quarter after quarter.
The difference between the two is quite clear:
Broadcom has the potential to capture a large part of a rapidly growing market in AI networking and custom silicon for hyperscalers, and cement itself as the #2 in AI semiconductor stock ahead of AMD, but it requires some speculation.
Broadcom has to prove that this market opportunity is theirs for the taking, and they will have to take it in full force and lay down the foundation for AI revenue to grow into that SAM – that is, to grow nearly 4x to $45 billion in AI revenue over the next three years (60% share of a $75B SAM).
Broadcom’s cloud software is executing well with VMWare’s integration almost fully complete, and cost synergies and operating efficiencies being realized, but AI hardware and networking is where Broadcom needs to prove it can sustain its large market size in an environment growing fiercely competitive.
For example, Arista is targeting AI networking revenue of $1.5 billion and another competitor is forecasting $2.5 billion for AI networking and custom silicon in 2025, while Nvidia’s networking revenue is well above a $12 billion annual run rate with Spectrum-X ramping. Regarding Spectrum-X, investors should take note that AI juggernaut Nvidia is entering the Ethernet market for the first time, following the success of InfiniBand. Thus, Broadcom’s lofty 70% market share is likely to come under serious pressure as Nvidia expects Spectrum-X to become a multi-billion dollar product within the next year.
This boils down to valuation – Broadcom’s surge to $250, up 40% in one week, has pushed the chipmaker to trade at its first ever premium to Nvidia since its merger with Avago in 2016, and a rather large premium at that. Both of the two have strong bottom lines, but Broadcom is now trading at 35.3x NTM earnings of $6.35, whereas Nvidia is trading at 33.1x NTM earnings of $3.95. Broadcom is also trading at a slight premium on the topline, at 18.3x NTM revenue, versus 17.9x for Nvidia.  Broadcom is strong on margins, though not nearly as strong as Nvidia – the operating margin of 31.9% in Q4 was slightly over half of Nvidia’s 62.3%, with a net margin of 29.8% versus Nvidia’s 55.0%.
Despite the lower custom silicon revenue that needs to ramp and the highly competitive networking market, a lesser-known AI angle for Broadcom is the VMWare acquisition is paying off in spades... infrastructure software was up 196% with the acquisition now largely complete, and operating margins are an impressive 72% in this segment. Infrastructure growth was guided at 41% YoY for Q1, contributing nearly 45% of revenue, providing more robust margin tailwinds to complement AI semiconductor growth over the next couple of years. The synergies from AI-driven high-growth, high-margin infrastructure software and expectations for a rapid ramp in AI semiconductor revenue through 2027 could make Broadcom a compelling AI name, provided the price is right.
Broadcom (AVGO) broke out to new highs on heavy buying volume based on the results from their recent earnings report. A vertical move on heavy volume is everyone realizing at the same time the direction of the trend – shorts cover, and longs buy, causing the type of vertical price action exhibited below. These moves tend to be the 3rd waves in a 5 wave uptrend, which is what I believe AVGO just completed.
There are currently two scenarios based on the price action that the I/O Fund is tracking:
There was significant institutional activity in the $250, $240, and $224 regions. As price is notable below these regions, it implies that institutions sold at the recent highs. If these levels contain any bounce, it will further confirm that the 3rd wave is over, which support the Blue count. If $212.50 does break, confirming this scenario, as long as the 4th wave drop holds $157, the I/O Fund would see this drop as a buying opportunity.
Broadcom’s premium valuation coupled with a fraction of Nvidia’s AI revenue —- not to mention flat QoQ AI revenue growth for nearly three quarters —- is why this is not yet Broadcom’s Nvidia moment. Broadcom must now prove to the market that it can deliver on its promise and maintain its premium to the undisputed AI leader heading into 2025 with Blackwell’s fireworks show about to start.
Supply chain and demand signals point to 2025 being another strong year for Nvidia as Blackwell comes to market, with the I/O Fund tracking these data points to assess Nvidia’s growth potential in the year to come. The I/O Fund is also closely analyzing the supply chain to identify overlooked beneficiaries of the AI infrastructure buildout, sharing this information as well as buy and sell plans and real time trade alerts with premium members. The I/O Fund recently entered two separate beneficiaries for gains of 23% and 17% since November. Learn more here.
I/O Fund Portfolio Manager Knox Ridley and I/O Fund Equity Analyst Damien Robbins contributed to this report.
If you would like notifications when my new articles are published, please hit the button below to ""Follow"" me.
Please note: The I/O Fund conducts research and draws conclusions for the company’s portfolio. We then share that information with our readers and offer real-time trade notifications. This is not a guarantee of a stock’s performance and it is not financial advice. Please consult your personal financial advisor before buying any stock in the companies mentioned in this analysis. Beth Kindig and the I/O Fund own shares in NVDA and AMD at the time of writing and may own stocks pictured in the charts."
176,https://www.forbes.com/sites/dereksaul/2024/12/16/nvidia-stock-limps-to-correction-territory-as-ai-leaders-post-election-slump-deepens/,Nvidia Stock Limps To Correction Territory As AI Leader’s Post-Election Slump Deepens,"Dec 16, 2024, 04:10pm EST",Derek Saul,"Longtime Wall Street favorite Nvidia extended a rare down stretch, as shares of the artificial intelligence colossus slipped into correction territory, a ding that comes as Nvidia’s trillion-dollar peers enjoy a robust period of gains.
Nvidia stock fell 1.7% Monday to $132, its lowest end-of-day share price since Oct. 15.
That’s 11% below the stock’s all-time closing high of $149 set Nov. 7, officially heading into a correction.
Monday’s losses moved against a broader rally for technology stocks, with the Nasdaq index gaining 1.2% to a new record.
And shares of each of the six other most valuable American companies – the other members of the “Magnificent Seven” group including Apple, Amazon, Facebook parent Meta, Google parent Alphabet, Microsoft, Nvidia and Tesla – gained at least 0.7% apiece, led by Tesla’s 6% rise.
5.7%. That’s how much shares of Nvidia are down since Election Day, far underperforming the S&P 500 index’s 5% gain during the period. Notably, the six-week stretch has been a strong one for Nvidia’s big tech peers, as each of the other Magnificent Seven stocks advanced at least 9.9%. There has been no singular catalyst for the Nvidia selloff – its earnings report last month exceeded analyst estimates across the board –  though the stock has previously slumped due to geopolitical fears from Nvidia’s reliance on Taiwanese manufacturers.
The recent dip for Nvidia shares may be hard to digest for investors, especially in an otherwise rosy market, but the stock’s longer-term returns are still eye popping. Nvidia’s 170% year-to-date return is the best of any company valued at more than $200 billion, according to FactSet data, with shares up a whopping 700% over the last two years.
Nvidia rose to prominence over the last two years as the unquestioned market leader in designing the semiconductor technology powering generative AI, causing its market capitalization to grow from below $300 billion in late 2022 to as high as $3.6 trillion last month. Nvidia, whose clientele includes Amazon and Microsoft, translated the surging interest in generative AI into a significant upswing in its financial performance, as sales grew by more than 600% during Nvidia’s most recent quarter compared to 2022’s comparable period. Nvidia is still the third-largest company in the world by market value, trailing only Apple and Microsoft."
177,https://www.forbes.com/sites/emilsayegh/2024/12/11/the-ai-chip-race-who-can-compete-with-nvidia/,The AI Chip Race: Who Can Compete With Nvidia?,"Dec 11, 2024, 07:25am EST",Emil Sayegh,"Nvidia has redefined the AI chip industry, becoming synonymous with high-performance computing for artificial intelligence. Its GPUs power breakthroughs across sectors from healthcare to finance, elevating the company to the world's most valuable by market capitalization in 2025. However, recent challenges, including a stock price decline due to a Chinese antitrust investigation, are testing its dominance.
On December 9, 2024, China's State Administration for Market Regulation initiated an antitrust investigation into Nvidia's compliance with laws related to its 2020 acquisition of Mellanox Technologies. Mellanox, with significant operations in China, was a critical addition to Nvidia’s portfolio, providing networking solutions for data centers. The investigation has unsettled investors, contributing to a 2.7% drop in Nvidia’s stock price, which was already under pressure from broader market volatility.
This scrutiny comes amid intensifying competition in the AI hardware sector. Companies like AMD, under CEO Lisa Su's leadership, have significantly increased their market share in data centers and AI applications. Additionally, tech giants such as Google, Amazon, and Microsoft are developing proprietary AI chips, aiming to reduce reliance on Nvidia's GPUs. These developments suggest that while Nvidia has been instrumental in advancing AI hardware, its position is increasingly challenged by both regulatory pressures and emerging competitors.
As artificial intelligence grew in complexity, it demanded equally advanced hardware to support it. Nvidia capitalized on this need, leveraging its GPUs to become the backbone of AI infrastructure. Its meteoric rise has been bolstered by a strategic focus on innovation and vertical integration, but recent headwinds may signal a potential turning point.
Despite these recent anti-trust challenges, Nvidia’s ascent is a case study in long-term vision, strategic investment, and relentless execution.
Nvidia’s dominance stems from decades of strategic investment, including billions poured into the CUDA ecosystem, developer education, and community-building around AI. This approach laid the foundation for an ecosystem that developers trust and rely on.
Nvidia’s early recognition of GPUs’ potential for AI workloads was transformative. Unlike CPUs, which process tasks sequentially, GPUs excel at parallel processing—critical for training and running AI models. This insight gave Nvidia a crucial first-mover advantage.
The CUDA platform turned Nvidia hardware into a unified ecosystem. By establishing CUDA as the industry standard for AI development, Nvidia created significant switching costs for companies considering alternative hardware.
From the Tesla GPU series to the groundbreaking Hopper architecture, Nvidia has consistently pushed the limits of performance, energy efficiency, and specialized AI capabilities like Tensor Cores.
Nvidia’s foresight in aligning its R&D investments with AI’s trajectory, coupled with partnerships with cloud providers, electric power companies, universities, and enterprises, solidified its status as the go-to provider for AI infrastructure.
Through software frameworks like cuDNN and TensorRT and platforms like DGX systems, Nvidia has created a vertically integrated ecosystem. This one-stop-shop approach eliminates the need for fragmented solutions, further strengthening its market position.
Nvidia’s dominance is a testament to strategic vision and innovation, but the competitive landscape is evolving. A mix of established players, tech giants with custom silicon, and innovative startups is intensifying the AI chip race.
While Nvidia’s leadership position remains strong, several factors could reshape the competitive dynamics:
The rise of Application-Specific Integrated Circuits for specialized workloads poses a direct challenge to GPUs. Companies like Google and Amazon, with their in-house chips, could erode Nvidia’s market share.
Nvidia’s reliance on TSMC for manufacturing makes it vulnerable to supply chain disruptions. Competitors with diversified manufacturing capabilities could gain ground during shortages.
The Chinese antitrust investigation into Nvidia highlights the regulatory risks of dominance. Such scrutiny could open doors for rivals to compete more aggressively, particularly in key markets like China.
Energy efficiency is becoming a critical factor in AI hardware. Companies delivering significant energy savings through new architectures or advanced cooling technologies could gain a competitive edge.
Nvidia remains the dominant force in AI chips, but the competition is closing in. Established players like AMD and Intel are doubling down on innovation, hyperscalers like Google and Amazon are developing custom silicon, and startups are pushing the boundaries with niche solutions.
The Chinese antitrust investigation adds another layer of complexity to Nvidia’s position, as regulatory actions could reshape market dynamics. The next phase of the AI chip race will hinge on balancing performance, efficiency, and cost while navigating a strained global supply chain and an increasingly burdened power grid.
The stakes have never been higher, and the challengers are ready to test Nvidia’s resilience. The future of AI hardware—and the AI industry as a whole—will be shaped by fierce competition and relentless innovation. As Steve Jobs once said, ""Innovation distinguishes between a leader and a follower."""
178,https://www.forbes.com/sites/gilpress/2024/12/10/nvidia-rigetti-quantum-machines-deliver-ai-powered-quantum-computing/,"Nvidia, Rigetti, Quantum Machines Deliver AI-Powered Quantum Computing","Dec 10, 2024, 09:00am EST",Gil Press,"“Quantum Technology is the largest paradigm shift in a generation,” says a recent comprehensive assessment of the current state of quantum computing from quantum research firm GQI. Quantum technology will have “a wide impact across all industry sectors” but all major approaches to quantum computing still face many challenges, say GQI analysts.
One of these challenges is the increasingly complex calibration requirements of quantum computing systems, a significant bottleneck slowing down efforts to scale them. “As we move toward systems with thousands of qubits,” says Yonatan Cohen, co-founder and CTO of Quantum Machines, “the challenge becomes exponentially more difficult. The traditional approach of manual calibration by quantum physicists simply won't scale with our ambitions for larger quantum computers.”
Quantum Machines, Rigetti Computing, and Nvidia announced today the successful application of AI to automate the calibration of a quantum computer. With their AI-powered tools, Quantum Elements and Qruise, remotely automated the calibration of a 9-qubit Rigetti quantum processing unit (QPU) integrated with Quantum Machines’ advanced control system and Nvidia DGX Quantum. A unified system for quantum-classical computing that Nvidia built with Quantum Machines, it is hosted at the Israeli Quantum Computing Center (IQCC).
The use of AI tools, says the GQI report, “could unlock a new wave of algorithmic progress. Just as quantum hardware is expected to improve, quantum software improvements can play their part in closing the gap to quantum utility.”
Automated, rather than manual calibration, speeds up the lengthy preparation process of the quantum computing system. Also focusing on speed, AWS and Nvidia announced last week that they are joining forces to bring Nvidia CUDA-Q, its open-source quantum development environment, to Amazon Braket. GPU-based simulations on Braket have demonstrated up to 350x speed improvements over CPU-based simulations, enabling more efficient testing of quantum circuits.
Other large technology vendors have also maintained an accelerated rate of innovation: Google is collaborating with Nvidia to enhance quantum device design using simulations on the Eos supercomputer and introduced AlphaQubit, an AI-based decoder that identifies quantum computing errors with state-of-the-art accuracy; Microsoft achieved a breakthrough by entangling 24 logical qubits using neutral atom quantum computing; and IBM has developed a 1,121-qubit chip.
Investors are paying attention. Venture funding in the sector has reached new levels in 2024, according to Crunchbase data. Quantum computing startups have already raised $1.5 billion in venture funding in 50 deals so far this year, almost doubling the $785 million raised last year.
Similarly, the quantum computing sector in Israel has seen significant growth according to a recent report from Startup Nation Central. As of August 2024, Israel had 22 quantum computing startups, with total funding for the sector in 2024 already nearing the 2023 annual total by mid-year. “This suggests a strong potential rebound and growing investor confidence in quantum technologies,” concludes the report.
Investment in quantum technology is not just a private matter. Governments around the world are investing heavily in quantum computing, communications and sensing. As with AI, this is usually presented as primarily a race between two superpowers. China is “positioning itself as a dominant cyber and quantum technology player,” says Skip Sanzeri, founder and COO of QuSecure, and is considered the world’s leader in quantum communications.
In response, the newly proposed bi-partisan National Quantum Initiative Reauthorization Act would authorize $2.7 billion in federal funding to accelerate quantum research and development at federal science agencies for the next five years. The bill would refocus the National Quantum Initiative from basic research to practical applications and expand it to include other federal agencies such as the National Institutes of Health (NIH), State Department and Small Business Administration (SBA).
“Quantum computing has the potential to solve complex problems exponentially faster than existing computers,” says the U.S. Senate press release. “The technology could lead to breakthroughs in drug discovery, weather forecasting, financial and economic modeling, artificial intelligence, cryptography and other innovations. Quantum sensing applications can provide more precise measurements critical for navigation and tracking, seismic monitoring, infrastructure monitoring and geographical surveying.”"
179,https://www.forbes.com/sites/greatspeculations/2024/12/06/sell-nvidia-buy-intel-stock/,"Sell Nvidia, Buy Intel Stock?","Dec 06, 2024, 05:00am EST",Trefis Team,"Nvidia has been the poster child of the AI boom, with its stock surging by over 180% this year, pushing its valuation to close to $3.4 trillion. Nvidia’s revenues are on track to more than double this fiscal year led by surging demand for its GPUs which have become the de facto chips for AI applications. In contrast, Intel stock has had a tough year. The stock remains down by about 50% year-to-date and has a market cap of a mere $100 billion. Intel’s revenues are expected to contract this year. But here’s the twist: This might be the right time to rethink the AI bellwether. Why is that? Also see a crypto mover that is Up 300% In A Month, XRP Is Just Getting Warmed Up.
The markets are often myopic and tend to extrapolate short-term trends for the long run. In Nvidia’s case, they believe that demand for AI accelerators will hold up and Nvidia’s margins and growth rates will remain strong. On the other hand, Intel’s market share losses in the CPU space and its foundry business struggles have made investors pessimistic about its future. However, almost everything in life is cyclical and this couldn’t be more true with the semiconductor markets. Reducing positions in Nvidia and considering Intel stock could be a wise move at this juncture. We explain below - the ‘why’. Separately, if you want upside with a smoother ride than an individual stock, consider the High Quality portfolio, which has outperformed the  S&P, and clocked >91% returns since inception.
Companies have devoted immense resources to building AI models over the last two years or so. Now training these massive models is more of a one-time affair that requires considerable computing power and Nvidia has been the biggest beneficiary of this, as its GPUs are regarded as the fastest and most efficient for these tasks. This is evident from Nvidia’s recent revenue growth. Sales are on track to expand from a mere $27 billion in FY’23 to almost $130 billion in FY’25. However, the AI landscape may be evolving. As models grow larger in terms of several parameters, incremental performance gains are expected to diminish. Separately, the availability of high-quality data for training models is likely to become a bottleneck. With much of the Internet’s high-quality data already run through by large language models, there could be a shift from large-scale, general-purpose AI models to smaller, specialized models - reducing demand for Nvidia’s high-powered GPUs. The explosive demand Nvidia has witnessed over the last few years may very well have been front-loaded, with future growth very likely slowing. Separately, see What’s Behind The 80% Rise In Meta Stock?
Now, AI-related chip demand could shift from training to inference, which is the phase where trained models generate outputs. Inference is less computationally intensive and could open the door for alternative AI processors. To be sure, Nvidia will likely remain the leader by far in the inferencing space as well (it says that inferencing accounts for about 40% of its data center chip demand) but there’s certainly an opening from rivals such as AMD and potentially even Intel to gain a bit of market share.
During the initial wave of generative AI, enterprises and big tech companies scrambled to invest in GPUs due to the “fear of missing out,” without worrying about costs and returns on investments. This led to a surge in pricing power for Nvidia, with its net margins coming in at over 50% in recent quarters. However, companies and their investors will eventually look for returns on their investments meaning that they could become more judicious about AI costs going forward and this is likely to hurt margins. Moreover, besides rivals such as AMD and Intel, Nvidia’s biggest customers such as Google and Amazon are doubling down on building their own AI chips. On Tuesday, Amazon announced plans to build an AI ultracluster, essentially a massive AI supercomputer that will be built using its proprietary Trainium chipsets. This could also pose a risk to Nvidia’s business.
While the narrative around Nvidia has been the AI boom, the pessimism around Intel has been due to its foundry business. The business has posted sizable losses ($7 billion operating loss in 2023) and has also faced a tech handicap versus industry leader TSMC. However, the division is poised for a potential comeback with its newest 18A process node. This technology, featuring RibbonFET transistors and PowerVia backside power delivery, promises significant improvements in terms of performance and efficiency. Intel has already secured contracts with major players like Amazon, Microsoft, and the U.S. Department of Defense for custom chip designs using the 18A process. Intel has achieved some key technical milestones with this process and the company expects external customers to move their first 18A designs into production in 2025. If Intel successfully executes this transition, it could shift the narrative around its foundry business. See why 2025 Could Be Intel Stock’s Comeback Year for an in depth look at how Intel stock could be re-rated higher.
Moreover, with Donald Trump set to return to the White House in 2025, Intel’s extensive U.S. manufacturing footprint is also likely to emerge as a much more valuable asset. Trump’s focus on boosting domestic manufacturing and reducing reliance on foreign supply chains could translate into favorable policies for Intel. Potential tariffs on foreign-made chips or incentives for domestic production could give Intel a competitive edge, particularly in its foundry division. Moreover, Intel’s status as the only U.S.-based semiconductor company that designs and manufactures leading-edge chips positions it well to win more Federal government contracts.
Intel stock trades at a reasonable valuation at just 23x consensus 2025 earnings. The 2025 earnings estimate is in fact depressed versus historical levels, at just about $1 per share on account of Intel’s current struggles. For perspective, Intel has reported earnings of close to $2 per share in 2022 and earnings of over $5 per share over 2021 and 2020. This means that if Intel sees earnings recover to historical levels in the coming years, the stock could similarly follow suit. The company is expected to return to revenue growth in 2024, with consensus estimates pointing to a 6% revenue increase and there are multiple tailwinds in both the chip and foundry business. Intel’s improving CPU lineup, driven by the Lunar Lake and Arrow Lake chips, positions it well for a recovery in the PC and server markets. Intel could also see incremental upside in the AI processor space with its Gaudi 2 and upcoming Gaudi 3 AI accelerators.
While Nvidia has returned outsized gains, in contrast, the Trefis High Quality (HQ) Portfolio, with a collection of 30 stocks, is less volatile. And it has outperformed the S&P 500 each year over the same period. Why is that? As a group, HQ Portfolio stocks provided better returns with less risk versus the benchmark index; less of a roller-coaster ride, as evident in HQ Portfolio performance metrics.
Nvidia, on the other hand, trades at a lofty 48x projected FY’25 earnings. While Nvidia has seen impressive growth recently, it remains to be seen if the good times will last. And at the current valuation, we see little room for error. The risks we highlighted above could put Nvidia’s future growth and margins at risk, weighing on the company’s earnings. As the AI market shows signs of evolving, investors could see better risk-adjusted returns by moving from Nvidia to more undervalued semiconductor players like Intel. Considering the above factors, Intel may have only one way to go and that’s probably up. For Nvidia, on the other hand, things could get a bit more tricky.
Invest with Trefis Market Beating Portfolios
See all Trefis Price Estimates"
180,https://www.forbes.com/sites/antonyleather/2024/12/03/intel-reveals-249-arc-a580-video-card-should-nvidia-be-worried/,Intel Reveals $249 Arc A580 Video Card: Should Nvidia Be Worried?,"Dec 03, 2024, 06:47pm EST",Antony Leather,"Intel has announced it's second generation of discrete desktop graphics cards designed for PC gamers. The Xe2 architecture-based Battlemage Arc B580 and B570, which were leaked last week,  are targeted at the $200-300 price segment, competing with Nvidia's GeForce RTX 3050 and 4060 and AMD's Radeon RX 7600 and 7600 XT with a view to disrupting the low end of the market.
Intel has released various performance claims for the new cards. The Arc B580 is on average 24% faster at 1440p than the previous similarly-priced Arc A750 and has up to 50% more performance per watt and more importantly offers 32% better performance per dollar than the competition - presumably the models listed above such as the RTX 4060, although Intel didn't mention any specific models from AMD or Nvidia.

FEATURED | Frase ByForbes™
Unscramble The Anagram To Reveal The Phrase
Pinpoint By Linkedin
Guess The Category
Queens By Linkedin
Crown Each Region
Crossclimb By Linkedin
Unlock A Trivia Ladder
The Arc B580 will cost $249 and B570 $219. This puts them into contention with some of the cheaper options from AMD and Nvidia such as the GeForce RTX 3050 and Radeon RX 7600. Based on Intel's claims, the new cards will be on average 24% faster at a similar price.
The biggest headline features, though, are the memory options. The Arc 580 is equipped with a decent 12GB of memory - 50% more than its predecessors. The ARC B570 meanwhile This is significant as 8GB is widely viewed as not enough to deal with todays games, even at lower resolutions such as 1,920 x 1,080p and with comparatively low-end graphics cards.
Nvidia certainly came in for critisizm with its cheaper RTX 4000-series cards such as the RTX 4060 Ti, which started at 8GB, while AMD's equivalents had larger amounts.  The increased amounts of memory are likely add to the appeal in addition to Intel's claimed performance figures, even if the amounts are what would be expected for a graphics card in this price range to have launching right now. Of course, all eyes will be on Nvidia come January when it is expected to launch its RTX 5000 series.
So far Intel has stated that add-in board partners Acer, ASRock, GUNNIR, ONIXX, MAXSUN and Sparkle will offer the Arc B580 from December 13th from $249, with Intel also offering a Limited Edition version too. The Arc B570 will be available from January 16th, 2025, starting at $219.
Ultimately Intel is aiming to disrupt the most popular part of the market in terms of discrete graphics card sales - one currently dominated by Nvidia's RTX 4060 and 3060 cards. There has also been very little movement in terms of new releases at this price point with both AMD and Intel relying on older models to fill the gaps. Nvidia's for example, hasn't released an RTX 4050 - its product stack begins with the RTX 4060 8GB that cost over $300 when new.
Intel hasn't had an easy time trying to break into the GPU market, though, with the unenviable task of optimizing drivers for hundreds of games to compete with AMD and Nvidia. the current situation based on recent tests by various tech review outlets is that there is still work to be done, but performance in much improved. Still, the release of a new graphics cards is encouraging and shows Intel's commitment to continue to push forwards and when graphics card prices show no signs of falling, additional options below $300 are very welcome."
181,https://www.forbes.com/sites/antonyleather/2024/11/29/nvidia-rtx-5000-series-graphics-cards-arriving-early-january/,Nvidia RTX 5000 Series Graphics Cards Arriving Early January,"Nov 29, 2024, 08:39am EST",Antony Leather,"The latest news surrounding Nvidia's highly anticipated next generation of graphics cards is that the RTX 5000 series - specifically the RTX 5090 flagship, will be announced in early January at the Consumer Electronic Show (CES) in Las Vegas. The event runs from Jan 7th-11th so in around five weeks the new cards could be announced with availability following soon after.
While a CES announcement for the RTX 5000 series was generally expected, this is the first concrete bit of news to suggest an early January launch and comes from Nvidia graphics card partner Inno3D, who's representative at a Brazilian livestream event let slip the company was expecting to have RTX 5090 samples in three weeks.
According to a source attending the call who spoke to Videocardz, the representative seemed to regret giving that timeline, but instead continued to confirm that the RTX 5090 would officially be announced at CES 2025 in January - itself still a newsworthy statement. This would also fit in with the timeline of samples arriving with companies such as Inno3D who would then create materials for the launch and carry out internal testing with their own coolers and other features that differentiate one particular graphics card model across the various partner card manufacturers.
The end of production of the RTX 4090 was reported back in September giving further weight to an imminent RTX 5090 launch. As for the other models, Nvidia looks likely to release the RTX 5070 and above in Q1, so by March the likes of the RTX 5070, 5080 and any Ti-named models should be available. Most rumors so far point at January or February for those models, but the curious news from the Inno3D representative about the RTX 5090 begs the question why that particular card was singled out.
Obviously the likely reason is that it will be announced first or in the first wave, with staggered launches being common in past Nvidia releases. Current Black Friday and Cyber Monday deals will likely aid in clearing current inventories, clearing the way for RTX 5000 cards to hit the shelves.
FEATURED | Frase ByForbes™
Unscramble The Anagram To Reveal The Phrase
Pinpoint By Linkedin
Guess The Category
Queens By Linkedin
Crown Each Region
Crossclimb By Linkedin
Unlock A Trivia Ladder
It's not the only company launching graphics cards, though, as Intel has just announced it plans to beat Nvidia to the finish line and release two new Arcseries 'Battlemage' graphics cards as early as mid December. According to Videocardz, the two new graphics cards will be named the Arc B580 and Arc B570. Based on the naming scheme of it's initial Arc series, this would suggest its
It's been a rather poor year for PC hardware launches, with little going on until the latter half of the year, which saw disappointing launches from AMD and Intel. First AMD's Zen 5 Ryzen 9000 launch proved disappointing, followed by an equally lacklustre release for Intel's Core Ultra 200-series processors. Thankfully, AMD's Ryzen 7 9800X3D proved to be a hit and coupled with discounts for Ryzen 7000 models, have certainly given would-be upgraders to Nvidia's RTX 5000 series reasons to overhaul their PCs this Christmas."
182,https://www.forbes.com/sites/bethkindig/2024/11/27/nvidias-stock-has-70-potential-upside-for-2025/,Nvidia’s Stock Has 70% Potential Upside For 2025,"Nov 27, 2024, 08:45am EST",Beth Kindig,"Nvidia once again posted a $2 billion beat to consensus revenue estimates in Q3, reporting YoY growth of nearly 94% to over $35 billion in revenue. Data center revenue more than doubled in the quarter to over $30 billion with Hopper driving the second largest data center beat in company history, speaking volumes as to the level of demand for its GPUs given that Blackwell will not initially ship until next quarter.
As recapped to our premium members after the earnings report, the I/O Fund is tracking supply chain signals indicating the next generation of GPUs shipping in full volume by mid-2025 (and beginning to ship in the January quarter) will far exceed the GPU sales we saw in 2023 and 2024 combined.
The I/O Fund is already tracking a 30% minimum difference between GB200 NVL72 orders and what the Street has estimated for next year. When adding that the DGX B200 systems will be priced 40% higher, and assuming pricing power affects more SKUs the way it will affect the DGX B200 systems, then it’s possible to see about 70% upside next year for Nvidia.
Nvidia reported $35.08 billion in revenue versus consensus of $33.13 billion. Beating on data center revenue is becoming common place for Nvidia, yet what’s interesting is the data center segment posted the largest surprise relative to estimates since Hopper’s breakout quarter in FY24. Nvidia reported $35.08 billion in revenue versus consensus of $33.13 billion.

Data center revenue of $30.77 billion increased 112.0% YoY and 17.1% QoQ, beating estimates by $1.95 billion. This marked the largest beat since the $2.46 billion beat in Q2 FY24, as well as the two $1.8 billion beats in Q3 FY24 and Q1 FY25. This is important as this beat was driven solely by Hopper – which is in its seventh quarter with the H100s and H200s.
Blackwell’s is expected to ramp quickly in Q4 and into next year. Analysts estimate Blackwell’s volume in Q4 could be between 150,000 and 200,000, before tripling sequentially to 550,000 in Q1 FY26 (Jan-Apr quarter of 2025). The expectation for AI clusters is to go from tens of thousands, to hundreds of thousands, to millions of GPUs, indicating a long runway for Blackwell and subsequent GPU generations.
What’s shaping up for 2025 is the convergence of multiple strong tailwinds for Nvidia to capture via Blackwell: GPU clusters this generation beginning at the upper end of Hopper’s hundred-thousand clusters, Big Tech capex continuing to increase past one quarter trillion (which we covered two weeks ago), and more importantly, Blackwell’s pricing power versus Hopper.
Q3 earnings aside, this bigger picture is that Nvidia’s Blackwell GPU sales next year will far exceed the GPU sales we saw in 2023 and 2024 — combined. 2025 is shaping up to be potentially the most important year for Nvidia since I first highlighted Nvidia’s AI GPU thesis in my free stock newsletter in November 2018 and when the I/O Fund entered at $3.15 for returns of 3,280%.
Including Q4’s estimate, Hopper has delivered approximately $125 billion to $130 billion in data center revenue in 2023 and 2024. Blackwell, on the other hand, is expected to deliver up to $210 billion next year alone.
Back in August, in the analysis Nvidia Stock: Blackwell Suppliers Shrug Off Delay Ahead Of Q2 Earnings, I wrote:
‘According to reports from Wccftech: “Team Green is expected to ship 60,000 to 70,000 units of NVIDIA's GB200 AI servers, and given that one server is reported to cost around $2 million to $3 million per unit, this means that Team Green will bag in around a whopping $210 billion from just Blackwell servers along, that too in a year.
The weight of that report cannot be overstated as it implies 26% upside to 2025’s estimates based on one SKU alone.”
Despite Blackwell not yet shipping in full volume, there are multiple data points that support this ramp to $200+ billion in revenue.
Perhaps the most important quote was one that could easily be overlooked — Nvidia’s management explained in Q3’s earnings that they have “completed a successful mask change for Blackwell…that improved production yields. Blackwell production shipments are scheduled to begin in the fourth quarter of fiscal 2025 and will continue to ramp into fiscal 2026.”
Since both Hopper and Blackwell will be shipping in tandem beginning in Q4, there’s more emphasis on supply constraints moving forward, as management was clear in saying that both products have “certain supply constraints” with Blackwell’s demand “expected to exceed supply for several quarters in fiscal 2026.” Broadly speaking, supply constraints are nothing new as it’s been widely understood Blackwell is already sold out for next year.
By executing this mask change to improve production yields, Nvidia can theoretically get more usable chips per wafer, alleviating some supply fears and allowing it to meet higher demand levels, leading to higher revenue generation. Management already hinted at this, saying “we will deliver this quarter more Blackwells than we had previously estimated.” CEO Jensen Huang also explained that GPU clusters with Blackwell will be starting where Hopper left off: “You see now that at the tail-end of the last generation of foundation models were at about 100,000 Hoppers. The next generation starts at 100,000 Blackwells.”
Even though Nvidia guided Q4 nearly in-line with analysts' expectations at $37.5 billion, there is still significant room for Blackwell to grow through 2025. Current forecasts point to revenue surpassing the $50 billion-mark one year from now, with revenue growth in excess of 40% for the next five quarters.
Interestingly, there is still a massive disconnect in analyst estimates as FY26 progresses – estimates for Q3 have a nearly $40 billion range from the low to high estimates. When looking at Q4 of next year, there is a ridiculous $70 billion range, with some analysts predicting $31 billion at the low end while others have estimates as high as $101 billion. Should Nvidia maintain its quarterly cadence of beating by $2 billion from the midpoint of these estimates, and assuming data center mix remains at ~90%, Nvidia could easily exit FY26 with data center revenue at >$50 billion/quarter, or $200+ billion annualized compared to data center revenue of $140 billion this year.
Big Tech’s capex supports this revenue growth story, as Microsoft, Amazon, Meta and Alphabet have all accelerated capex significantly in the past couple of quarters and reaffirmed the need to continue investing aggressively in AI infrastructure moving through 2025.
Additionally, Big Tech is already spending tens of billions on Nvidia’s Blackwell lineup:
This is but a fraction of 2025’s estimated capex– 2024’s capex could come in at ~$240 billion with an estimated $70 billion spent in Q4, with the four currently tracking for over $270 billion in capex predominantly for AI infrastructure in 2025.
Nvidia has been capturing a lion’s share of AI spending from Big Tech, at ~80% to 85%, and assuming little change in its AI GPU market share with competition primarily arising from AMD and no one else, Big Tech’s spending implies a clear path towards $200 billion in GPU revenue in 2025.
The importance of Big Tech’s capex was also echoed with the CEO stating we will see $1 trillion in data infrastructure rebuild before he expects to see digestion from the hyperscalers. Per Huang: “I believe that there will be no digestion until we modernize a trillion dollars with the data centers.” That would imply another 3X from here for the remaining three-quarter trillion – not in stock price, but in capex. Presumably, it would mean a higher trajectory for the stock price in terms of valuing that revenue.
Hopper drove another beat, which Nvidia is becoming widely known for. It’s rare for analysts to openly expect large beats going into a print, yet UBS had correctly tagged the beat this quarter at $2 billion. However, due to declining from peak revenue growth of 265% earlier this year, Hopper-driven growth of 94% is not what will drive the stock up for the next leg higher. Nvidia investors, such as myself, will need Blackwell’s pricing power and Blackwell’s clear demand signals to re-invigorate the stock.
Nvidia reported 93.6% YoY growth, more than 10 points higher than consensus estimates for ~83% YoY growth. Nvidia is now lapping its peak growth quarters, Q4 FY24 and Q1 FY25, where revenue peaked at 265% growth due to Hopper ramping tremendously fast.
Growth technically is decelerating nearly 30 points in Q3 and growth will further decelerate nearly 24 points next quarter, but to be reporting above 93% YoY and almost 70% YoY versus 200-260%+ growth comps is still a very strong report to say the least.
For Q4, management guided for revenue of $37.5 billion, +/- 2%, just slightly ahead of consensus estimates for $37.02 billion at the midpoint. Analysts are now expecting $38.01 billion in revenue for Q4, just a week after the report, at the upper end of the guided range. Both Hopper and Blackwell will be shipping in tandem moving forward as Blackwell ramps significantly through fiscal 2026.
Analysts were nitpicking margins, yet this concern is overblown. Q3’s margins were relatively in line with guidance despite the $2 billion top-line beat, and for Q4, management forecast margins to contract nearly 2 points sequentially. However, CFO Colette Kress was clear that following Blackwell, gross margin will eventually return to its current percentage: “As Blackwell ramps, we expect gross margins to moderate to the low-70s. When fully ramped, we expect Blackwell margins to be in the mid-70s.”
Investors should never underestimate Wall Street’s ability to miss the bigger picture. Analysts on the call cross-examined this 200 bp sequential decline despite Nvidia having an operating margin of over 60% compared to most of the Mag 7 having operating margins at half that. It’s also completely normal for semiconductors to feel margin pressures in the initial stages of ramping a new product, especially at this scale and pace.

The bigger picture for Nvidia moving forward is that Blackwell holds the potential to dwarf Hopper’s revenue generation in fewer quarters. Breaking it down further on CNBC, I stated Nvidia's trajectory will continue due to two words: pricing power I had been quite vocal prior to earnings that Q3’s report was nothing but a blip in the longer-term picture, with 2025 being much more important than this quarterly report.
The I/O Fund is already tracking a 30% minimum difference between GB200 NVL72 orders and what the Street has estimated for next year. When adding that the DGX B200 systems will be priced 40% higher, and assuming pricing power affects more SKUs the way it will affect the DGX B200 systems, then it’s possible to see about 70% upside next year for Nvidia.
Make no mistake, Nvidia is the best stock of the decade and we are only four years in. The I/O Fund has an aggressive buy plan at key levels should the stock pull back, and we have a backup plan should the stock overcome the peer pressure we are seeing from the semiconductor industry and meaningfully breakout.
The keyword is “buy” but the skillset is patience. My firm has blended cutting-edge analysis alongside careful, patient buys for returns of 3280% since our first tranche, with 9 buys and real-time alerts from 2021 to 2022 below $20. Most importantly, the I/O Fund continues to offer buy zones for those who’d like to participate. For a limited time, get up to $250 off with one of our biggest sales of the year starting Nov 28th. For more information on our annual sale, click here.
If you would like notifications when my new articles are published, please hit the button below to ""Follow"" me.
Please note: The I/O Fund conducts research and draws conclusions for the company’s portfolio. We then share that information with our readers and offer real-time trade notifications. This is not a guarantee of a stock’s performance and it is not financial advice. Please consult your personal financial advisor before buying any stock in the companies mentioned in this analysis. Beth Kindig and the I/O Fund own shares in NVDA at the time of writing and may own stocks pictured in the charts"
183,https://www.forbes.com/sites/johnwerner/2024/11/26/nvidia-breaks-ground-with-edify-3d/,Nvidia Breaks Ground With Edify 3D,"Nov 26, 2024, 03:17pm EST",John Werner,"If you’ve never heard of Nvidia Edify and its 3D modeling capability, now is the time to find out.
The Nvidia company, widely hailed as the largest of its kind on the American Stock exchange, is well known for its hardware and data center solutions, but professionals there are also putting together some interesting consumer-facing technologies, too.
One of those is a neat feature that will create 3D images from two-dimensional inputs or text.
You can bring anything to life with Edify, whether it’s a vehicle, a cartoon character, a comfy sofa, or anything else.
This platform is being used for gaming, and for virtual reality and augmented reality projects, as well as in various retail applications.
The company notably cooperated with Shutterstock using some of that firm’s data to develop the model.
Nvidia Edify uses something called ‘quad mesh’ models where 3D objects are made up of quadrilateral or four-sided polygons. This enables natural edge flow for objects, versatility in subdivision modeling, and rigging for animations.
The process also synthesizes RGB values and surface contours, using a diffusion model. Another component is built on using ‘physically based rendering materials,’ which refers to a method of defining surfaces in 3D, with lighting conditions under consideration.
Using attributes like base color, roughness and mapping, the program simulates the impact of natural light on its 3D objects.
Experts point out that Nvidia Edify gets deployed through something called Nvidia NIM or Nvidia Inference Microservices. NIM is officially part of the Nvidia AI enterprise platform, and has various API tools for container orchestration and working with Kubernetes clusters.
in addition to NIM, Nvidia AI Enterprise has GPU-accelerated libraries, something called a NGC catalog with tools for deployment, and a ‘base command manager’ that helps to handle workload management and container operations.
Enterprise clients also get Nvidia network operator, an AI workflows tool and various infrastructure components.
Going back to the consumer side of things, people are getting interested in putting together 3D objects digitally with Edify. Brands are, too.
In addition to all kinds of independent gamers and developers, companies are using Edify to their advantage.
When I did some basic research into what’s on the web, I found that there’s some brand alignment for the platform already.
“Some huge names are already using Getty’s AI service to explore new creative horizons in marketing,” writes Ivanna Atti at AI Secrets. “Marketing firm WPP and Coca-Cola are collaborating to use Getty Images Generative AI to create custom visuals that meet brand style and guidelines.”
Atti also mentions Adobe, which is rumored to be using Edify 3D in Adobe Firefly.
There’s more, too, on the Nvidia blog, where spokespersons note that Mattel is also using Edify for helping toy designers to be able to visualize their products.
And then there’s this from the Accenture newsroom – “Accenture has collaborated with NVIDIA and Jaguar Land Rover (JLR) to enhance the client experience for the Defender vehicle line through advanced generative AI and real-time graphics. This initiative involves Accenture Song utilizing NVIDIA's Omniverse platform to create high-fidelity digital twins of Defender vehicles from computer-aided design data.”
When I looked into this, it turns out the Defender is a vehicle model produced by Jaguar Land Rover, a brand that was typically manufactured in Britain. The Defender was first built in 1948, and officially named in 1990.
Now the marketing company is building high-resolution, digital twins of these vehicles in order to show them off to the world.
That gives you sort of a flavor of what people are doing with this brand new technology. First, we had stable diffusion and image creation tools in early Generative AI. Now we have 3D capability, and more firepower to create the Metaverse and VR/AR environments of the future."
184,https://www.forbes.com/sites/johnwerner/2024/11/25/whats-so-great-about-nvidia-blackwell/,What’s So Great About Nvidia Blackwell?,"Nov 25, 2024, 12:21pm EST",John Werner,"If you take a look at what people are saying about Nvidia right now, you’re likely to hear about a new kind of architecture supporting the company’s data center services and operations.
It’s called Blackwell, and it’s brand new, as of March of this year, when Nvidia started using this design.
Blackwell replaces the Lovelace architecture created in 2022, and according to the company, it delivers some pretty impressive performance increases as well as other improvements.
Let’s look at specifically what’s in the Blackwell architecture, and why Nvidia is so proud of this technology.
Nvidia spokespersons claim that the Blackwell architecture delivers a staggering 25 times the performance of the prior build, specifically, that the architecture “supports real-time generative AI on trillion-parameter LLMs, offering up to 25 times the performance and energy efficiency compared to its predecessor.”
However, skeptical critics contend that it’s a little more complicated than that, and suggest that you have to use different metrics to really talk about logical performance and speed. Citing the use of disclaimers and asterisks to qualify grandiose claims, some redditors claim that Nvidia is prone to this type of hype.
By contrast, Lovelace was said to be two times the performer over the prior Ampere architecture, just a couple of years ago when it came out.
The company reportedly put this statement out at the time: ""Ada Lovelace is the greatest generational leap we've ever achieved in performance."" Presumably, they would want to revise that now.
There’s also a new use of tensor cores in the Blackwell model.
These types of cores help to handle matrix operations and expand the different data types that platforms are capable of dealing with efficiently.
In this case, Blackwell is supposed to handle FP6 floating point data types with six-bit design. This in turn enables more efficient processing with less power.
Then there’s the NV-High Bandwidth Interface (NV-HBI) that delivers a rate of 10 TB/s, by fostering efficient communication between dual GPU dies. This tech has a lot of potential for data center integration.
Here’s where you have to go back a few years to acquisition of Mellanox, an Israeli company making different types of switching hardware and more. The deal was hailed at the time as one of the most important of its kind, when in 2019, Nvidia acquired Mellanox for $6.9 billion.
It’s likely that the Mellanox deal has been significantly useful to Nvidia in creating the new HBI technology. In pursuing low-latency, high-bandwidth interconnects like InfiniBand and Ethernet solutions, the formerly independent company was setting the stage for the kinds of connections behind modern fabrics. Then there’s optimizing data flow at scale. There’s a reason that this deal was such a big headline when it happened.
The new model also includes some security improvements, and that’s another feather in Nvidia’s cap, at a time when the corporate world is still getting over a major focus on data breaches as a source of liability.
Nvidia explains:
“When not adequately protected in use, AI models face the risk of exposing sensitive customer data, being manipulated, or being reverse-engineered. This can lead to incorrect results, loss of intellectual property, erosion of customer trust, and potential legal repercussions,” spokespersons write. “Data and AI IP are typically safeguarded through encryption and secure protocols when at rest (storage) or in transit over a network (transmission). But during use, such as when they are processed and executed, they become vulnerable to potential breaches due to unauthorized access or runtime attacks.”
The Confidential Computing technology works by locating security operations in what the company calls a trusted execution environment (TEE) within the processor.
Here’s how the company’s site explains the use of this feature:
“The TEE acts like a locked box that safeguards the data and code within the processor from unauthorized access or tampering and proves that no one can view or manipulate it. This provides an added layer of security for organizations that must process sensitive data or IP.”
That’s useful innovation to take care of major executive concerns, for example, around standards like HIPAA and the European GDPR.
As we see the integration of this kind of iterative improvement, Nvidia continues to outperform on the stock market, with the stock price more than doubling over one year, and analysts suggesting five-year revenue growth of 44.8%.
It’s still the biggest company of its kind by market cap, outstripping Apple’s $3.5 trillion value earlier this year (Nvidia’s at roughly $3.6 trillion.)
That’s a little bit about what’s in the latest round of Innovations by Jensen Huang and company. Nvidia is one to watch as the AI space race between firms continues to heat up."
185,https://www.forbes.com/sites/petercohan/2024/11/24/nvidia-stock-may-rise-as-musk-aims-to-buy-9-billion-in-ai-chips/,Nvidia Stock May Rise As Musk Aims To Buy $9 Billion In AI Chips,"Nov 24, 2024, 12:51pm EST",Peter Cohan,"Will Nvidia’s growth rate keep slowing down? If so, investors may be wise to take their profits. However, if the AI chip designer’s new products spur faster growth, now might be a buying opportunity.
Stock prices rise when companies exceed investor expectations and raise guidance, while they drop if companies fall short on either one. Nvidia is no exception to this rule.
To justify buying Nvidia stock now that it’s the most valuable company in the world — having enjoyed a 195% rise in its share price to a market capitalization of $3.5 trillion this year — you must believe the company will keep beating and raising.
If Nvidia significantly exceeds conservative growth targets with help from Blackwell, the stock could rise despite slowing revenue growth, dependence on a few large customers, and the absence of a killer app to deliver a return on the rising investment needed to train and operate AI chatbots.
Nvidia stock presents investors with significant risks. These include the following:
My guess is investors are baking all these negatives into their expectations. Indeed, despite Nvidia’s slowing growth in 2024, the company’s stock has continued to rise. This could be because the company did a good job of lowering investors’ growth expectations to date.
Positive surprises to growth would keep Nvidia stock rising. Huge orders for the company’s latest chip line by the largest AI service providers — coupled with growing corporate interest in applying AI to make processes more efficient — could provide investors with unexpected good news. Here’s how:
Until Nvidia can unlock faster-than-expected growth, its stock could be stuck in a trading range."
186,https://www.forbes.com/sites/petercohan/2024/11/21/nvidia-stock-pops-2-despite-slower-growth-view-margin-drop/,"Nvidia Stock Gyrates Despite Slower Growth View, Margin Drop","Nov 21, 2024, 08:42am EST",Peter Cohan,"Nvidia — the designer of 90% of chips used for artificial intelligence — reported a surge in revenue for the October-ending quarter, according to the New York Times.
The company’s stock price fell more than 2% in after-hours trading in light of Nvidia’s slower-than-expected revenue forecast and potentially declining margins, noted MarketWatch. Yet in November 21 pre-market trading, optimistic analysts sent the stock up nearly 2% — before it fell slightly Thursday morning.
Nvidia shares could be a buy for investors who view these negatives as a temporary interruption to more rapid future growth and higher margins.
Nvidia’s results for the latest quarter and forecast for the next one exceeded most expectations. Here are the key numbers:
“The computer industry has fundamentally changed,” Nvidia CEO Jensen Huang said in Japan last week, the Times reported. “From an industry that produced software, we have become an industry that is manufacturing artificial intelligence.”
Nvidia’s performance and prospects are extraordinary in comparison to other companies. However, for publicly traded companies, numbers — not adjectives — determine the direction of a company’s stock price.
When a company’s stock has risen as much as Nvidia’s has — up 203% so far in 2024 to surpass Apple with a stock market capitalization of $3.6 trillion, according to Google Finance — the numerical targets needed to defend such a surge in value are very high.
Nvidia may have disappointed investors in two areas:
Indeed, both of these problems are attributable to supply constraints on Blackwell, which could take several quarters to resolve.
“The challenge that we have is how fast can we get that supply, getting ready, into the market this quarter,” Nvidia CFO Colette Kress told analysts in the post-earnings conference call. “We’ll be back on track with more suppliers as we turn the corner into the new calendar year. We’re just going to be tight for this quarter.”
The Blackwell problems are reportedly tied to design flaws. When deployed in server racks, the chips overheated, “raising alarms about the ability to integrate them efficiently into existing data center models,” noted Reuters.
Nvidia said such problems are common for such sophisticated systems. “Engineering iterations are normal and expected,” Nvidia’s spokesperson told Reuters. Moreover, the company is committed “to co-engineering with cloud service providers.”
These problems are not a surprise to analysts. To begin shipping this month, Nvidia had to change the Blackwell production process due to a lower-than-needed manufacturing yield, Huang told analysts during an August investor call.
By shipping the first versions of the chip to customers in November, the company fulfilled Huang’s promise, noted the Times.
While analysts expect Nvidia’s torrid pace of revenue growth to slow, most remain bullish on the stock due to surging revenue, rising margins, continued demand from data centers, higher Blackwell sales, and triple-digit software revenue growth.
Here are some examples:
With all this optimism, one thing could make Nvidia stock a risky investment — slower growth. The AI chip designer’s revenues grew in a range of 206% to 265% during Q4 2023, Q1 2024, and Q2 2024, according to my August 2024 Forbes post. Nvidia’s forecast of 80% revenue growth in the third quarter represented a marked slowdown from the previous pace.
“It appears the bar was just set a tad too high this earnings season,” Ryan Detrick, chief market strategist at Carson Group, told the Associated Press in August. Nvidia’s forecast of 70% growth in Q4 is a continuation of this deceleration trend.
Nevertheless, analysts see upside in the company’s stock. Based on 42 Wall Street analysts offering 12 month price targets, Nvidia stock would need to rise 13.22% to meet their average stock price target, according to Tip Ranks."
187,https://www.forbes.com/sites/dereksaul/2024/11/21/nvidia-stock-briefly-rises-to-all-time-high-in-rollercoaster-post-earnings-trading/,Nvidia Stock Briefly Rises To All-Time High In Rollercoaster Post-Earnings Trading,"Nov 21, 2024, 11:19am EST",Derek Saul,"Nvidia stock touched its highest price ever Thursday morning in a mixed investor reaction to the artificial intelligence behemoth’s earnings report released Wednesday afternoon, as the world’s largest company flirts with becoming the first firm to ever score a $4 trillion valuation.
Shares of Nvidia rose as much as 4.8% shortly after 9:30 a.m. EST’s market open.
That early rise set a new record intraday share price of $152.89, equating to a market capitalization of $3.76 trillion for Nvidia.
But the stock soon turned negative, moving with the initial negative stock reaction to the quarterly results in limited Wednesday evening trading.
Shares were down 3.6% to $140.70 by 10:30 a.m. Thursday, an 8% negative swing from the earlier highs, recovering to a 1.6% daily gain to about $148 by late afternoon, within a dollar of its record close set earlier this month.
The swing came amid an unusually high-volume trading session, with more than 340 million Nvidia shares trading hands thus far, the stock’s highest volume day since Oct. 15, according to Yahoo Finance data.
$300 billion. That’s roughly how much market value Nvidia lost in the hour its share price fell from nearly $153 to below $141. Only 25 American companies are worth at least $300 billion.
Nvidia stock’s downward swing followed what’s known as a triple earnings beat, in which the company easily exceeded consensus analyst forecasts for its third-quarter profit and revenue and raised its guidance for the fourth quarter. Unsurprisingly, the analyst reaction was overwhelmingly positive to the earnings release, as 26 of the 30 analyst share price targets updated after earnings moved upwards, according to FactSet. But the negative stock move is likely a result of the high-growth company forecasting just 7% quarter-over-quarter sales growth in the current period, which, though expected by experts, would be the slimmest expansion in two years and fell short of exceeding the most bullish expectations.
“The mix” of Nvidia’s strong earnings beat was “more heavily skewed” toward the third quarter and therefore the company’s outlook “was only ‘in-line’ with investor expectations, which is “a deviation from prior quarters,” explained Deutsche Bank analyst Ross Seymore in a note to clients.
If Nvidia can become the first company to score a $4 trillion market capitalization after trading within 6% of the milestone. The company needs a roughly $163 share price to hit the milestone, below the more than $180 price targets held by analysts at the likes of Bank of America, Jefferies and Wells Fargo."
188,https://www.forbes.com/sites/dereksaul/2024/11/20/nvidia-earnings-another-record-quarter-for-ai-leader/,Nvidia Earnings: Stock Slumps Despite Another Record Quarter For AI Leader,"Nov 20, 2024, 04:22pm EST",Derek Saul,"Nvidia’s hotly anticipated earnings report exceeded consensus expectations, as the world’s biggest company continued its streak of delivering explosive financial growth as the clearest beneficiary of the artificial intelligence gold rush, though its stock still slipped following the report.
Nvidia reported $0.81 adjusted earnings per share, or $19.3 billion net income, in the three-month period ending last month, topping average analyst projections of $0.75 EPS, or $17.4 billion net income, according to FactSet.
The semiconductor chip architect generated $35.1 billion in sales, crushing estimates of $33.2 billion.
Nvidia indicated growth will continue in the fourth quarter, guiding for about $37.5 billion in revenue, compared to consensus forecasts of $37.09 billion.
Despite the top and bottom line beats and improved outlook, shares of Nvidia slipped as much as 3% in limited afternoon trading within 20 minutes of the release. The cause of the decline was not immediately clear, but it’s a reflection of the sky-high expectations for the AI colossus. The 7% quarter-over-quarter growth implied by the $37.5 billion Q4 sales guidance would be the weakest such result since the quarter ending in January 2023 for Nvidia. The stock trimmed losses to less than 2% following the start of the company’s 5 p.m. EST earnings call as Collette Kress, Nvidia’s chief financial officer, touted “staggering” demand for the company’s newest graphics processing unit platform, Blackwell.
Nearly $100 billion. That was the market capitalization loss implied by Nvidia stock’s initial post-earnings drop. That loss following a strong beat-and-raise roughly equates to the total market value of legacy Silicon Valley titan Intel.
Nvidia’s sales in its AI-heavy datacenter segment were more than 700% higherthan they were in 2022’s comparable period, growing from $3.8 billion to $30.8 billion. That exponential growth coincides with the generative AI boom, as Nvidia designs a majority of the complex software and hardware systems used to train the advanced machine learning models.
How Nvidia’s results impact the broader market this week. Bank of America strategists wrote earlier this week Nvidia earnings “can dictate the near-term direction of the market,” with S&P 500 options activity linked to the announcement pricing in more potential movement for the index than for weighty economic reports like the consumer price index or the Federal Reserve’s interest rate decision.
With a market capitalization of $3.5 trillion, Nvidia is the most valuable company in the world, outstripping the likes of longer established stalwarts like Apple and Microsoft. Nvidia has become a Wall Street darling, with its share price up more than 830% over the past two years, providing more than twice the return on investment as the next closest company listed on the S&P for the entirety of the timeframe, Facebook parent Meta at 400%. The 31-year-old Nvidia traces its history back to humble origins, with its three cofounders coming up with the idea of the company at a booth of a Denny’s diner in Silicon Valley. One of those cofounders, Jensen Huang, a former Denny’s busboy and Nvidia’s chief executive since that fateful day, is now one of the 10 richest people in the world, with a $127 billion net worth, according to Forbes’ estimates.
"
189,https://www.forbes.com/sites/bethkindig/2024/11/20/nvidia-stock-is-a-buy-on-dips-before-blackwell-arrives-in-2025/,Nvidia Stock Is A Buy On Dips Before Blackwell Arrives In 2025,"Nov 20, 2024, 04:48am EST",Beth Kindig,"Nvidia’s stock broke to all-time highs recently, trading at $148 in early November and $147 yesterday. The stock has left many investors wondering “what comes next” after the unrelenting, historic surge that began seven quarters ago.
To help my readers determine where Nvidia’s stock will go next, I’ve been fastidious in my analysis about the company’s outsized AI potential since 2018, tracking Big Tech capex as a proxy for AI demand since 2022, discussing the anomalous earnings and revenue revisions throughout 2023 and 2024, and reporting on never-before published data on supply chain checks as recent as two months ago.

The thoroughness is needed, however, as rumors from the media and short sellers alike run amuck. Rest assured, as 2025 approaches, supply chain data is giving bullish signals that the new generation of GPUs shipping in full volume by mid-2025 (and beginning to ship in the January quarter) will far exceed the GPU sales we saw in 2023 and 2024 combined.
Regarding my firm’s confidence in tracking supply chain data, when The Information stated Nvidia was experiencing a material delay on the next generation of GPUs, going so far as to state that Taiwan Semiconductor had machines sitting idle, I quickly refuted the report based on supply chain data my firm had been tracking. Those data points continue to indicate Blackwell is ramping. Here is what I stated:
“As of now, there’s a disconnect between next fiscal year’s revenue estimates of $167 billion and the $210 billion in GB200s alone expected to ship next year. Perhaps analysts are waiting for signals the supply chain can produce these outsized orders. So far, so good with the signals we see from TSMC and SMCI’s most recent earnings reports. Foxconn commentary helps, as well.”
Fast forward two months, and next year’s fiscal estimates stand at $185 billion up from $167 billion; showing no material impact from the delay (quite the opposite). Our firm was also able to use that same supply chain data to buy Nvidia in July/August, for an average cost basis of $109. The I/O Fund’s first trade was at $3.15, but we actively track the stock and publish our real-time trade alerts for anyone who feels they missed out on the AI juggernaut.
The first item that will determine the strength of the upcoming earnings report from Nvidia has nothing to do with the Q3 results. Rather, what the market will want to know is how much Blackwell revenue is expected in the January quarter. Morgan Stanley has estimates placed at $5 to $6 billion, with this number hitting a ceiling due to supply constraints; however, Piper Sandler sees Blackwell revenue potentially higher, at $5 billion up to $8 billion.
The bigger picture is that Blackwell will ramp by an order of magnitude, eventually exceeding Hopper’s revenue. To quantify this, Hopper has delivered approximately $125 billion in data center revenue since Q1 2023, based on estimates from Trend Force placing Hopper at generating 90% of data center revenue in 2024. Blackwell, on the other hand, is expected to deliver up to $210 billion next year alone, based on estimates for up to 60,000 to 70,000 GB200 NVL72 servers priced up to $3 million each.
Given the company is lapping tough comparables, the growth rate will slow considerably even if Blackwell does ramp from $6 billion per quarter to $60 billion per quarter by late-2026 (Hopper is in its seventh quarter and Blackwell will be in its seventh quarter by late 2026). This is because excellence begets excellence, and thus, Nvidia is competing with itself with each new generation of GPUs. For example, with Hopper, the company reported peak quarterly growth of 262% and 265% earlier this year, yet is expected to slow to the mid-40% for growth as we close out 2025.
Nvidia has multiple levers it can pull and outside forces at play that will help it maintain this 40%+ growth rate. This includes a 1-year product road map, Big Tech’s large appetite for AI spending, and long-term AI GPU market growth from Enterprises and the Consumer, plus a commanding market share position.
By coming to market with upgraded, more powerful GPUs on a now-annual cadence, with Blackwell Ultra, Rubin and Rubin Ultra soon to come, Nvidia will continue to be the largest beneficiary of Big Tech’s AI capex to an unprecedented degree as the company continually raises the bar on performance and TCO upgrades with each new generation.
Additionally, Nvidia has a software moat with CUDA and the cash to reserve chip capacity in bulk at the fab level to maintain an 80% to 85% share of what executives foresee as a $500 billion AI accelerator market by 2028. I first covered these points in my free newsletter when I published: “Here’s Why Nvidia Stock Will Reach $10 Trillion Market Cap by 2030.”
Of these points, one of the most visible is that Nvidia continues to pry away tens of billions in cash – and now hundreds of billions —- from the world’s leading tech companies.
All roads lead to Nvidia, and it’s no secret that Big Tech and others are competing to purchase Nvidia’s supply constrained GPUs. Our firm began tracking Big Tech capex as a proxy for Nvidia demand in 2022, and tracking it on a quarterly basis starting in early 2023 – to help gauge AI demand, I continue to track Big Tech capex quarterly closely for our readers.
Our recent checks published in the analysis “AI Spending to Exceed a Quarter Trillion Next Year” reveal that AI spending continues to accelerate, with Alphabet, Amazon, Microsoft, and Meta on track to increase their spend by ~$90 billion YoY in 2024. This does not include xAI, CoreWeave, Oracle and dozens of others who are also spending multiple billions on Nvidia’s GPUs, as well.
To better understand the trajectory of AI spending, let’s take a step back to 2023, where the rapid ascent of ChatGPT at the beginning of the year set the stage for AI to step into the spotlight.
Big Tech could spend another $70 billion in Q4, based on guidance and comments from executives, who overwhelmingly discussed the need for more AI infrastructure, putting full year capex at ~$240 billion, or nearly 15% higher than the level they were tracking at the start of the year.
For 2025, Big Tech has already signaled a willingness to spend substantially more on AI. There is clear ROI for Amazon, Google and Microsoft as they rush to meet the elevated demand that continues to outpace AI capacity in their cloud infrastructures. More broadly, Big Tech and large enterprises are racing to further develop and broaden AI services and models. UBS projects Big Tech will spend ~10% more YoY, placing AI-driven capex at $267 billion; however, if 2024 is any sign, this estimate is too low. This all fits in with longer-term projections from Bank of America that sees a cumulative $700 billion spent on AI through 2026.
As we go through a lull between the Hopper generation being in its seventh quarter, and Blackwell not yet shipping in volume, our firm will be buying the dips on Nvidia for many reasons – one of them being it’s the market leader on margins. By having a near monopoly on GPUs, Nvidia has incredibly strong pricing power.
The GPUs coming in 2025, called Blackwell, are set to intensify this pricing power with DGX B200 systems reportedly going for up to a 40%+ premium to the previous DGX H100 systems, at $500,000 per server versus the low $300,000s per server, respectively.
While GB200 prices are estimated at $60,000 to $70,000 for a single chip, the NVL36 and NVL72 configurations carry much higher price tags and thus, higher average prices per GB200. For example, the NVL36 is expected to cost ~$1.8 million, and for 18 GB200s (36 B200 GPUs), that comes out to $100,000 per GB200 and additional components. For the NVL72, it works out to ~$83,333 per GB200 and additional components.
While there were concerns about Nvidia’s margins given that management guided for a sequential contraction in gross margins in Q3, the sheer pricing power of Blackwell will ultimately be a non-issue next year.
Nvidia’s operating margin of 62% exceeds second place Microsoft by 17.5 points and third place Meta by 21.9 points; Nvidia is more than double the rest of the Mag 7 including Apple and Alphabet. This is because Hopper’s pricing power versus the Ampere generation: Nvidia’s Compute and Networking operating margin expanded from 28.5% in Q3 FY23 when Hopper reached full production to 71.3% in the most recent quarter even as revenue grew 7x during that seven-quarter period.
Nvidia is expected to report roughly 50 bps to 100 bps margin contraction this quarter compared to last quarter, and will see roughly 200 bps to 300 bps margin contraction from its peak growth quarters earlier this year. As stated, the pricing power I foresee from Blackwell will keep the margins strong well into 2025, therefore, any concerns over margins this quarter will be a moot point by next year.
The strong margins combined with the expected growth in AI accelerators has caused some analysts to increase earnings per share substantially as of late. Bank of America increased its EPS estimates for next calendar year from $3.90 to $4.47 and for calendar year 2026 from $4.72 to $5.67.
In February, I wrote an analysis describing how Nvidia’s valuation was “eerily low despite 420% rally since 2023” to help our readers prepare for a higher return in the coming months, which detailed the importance of these revisions.
Ultimately, these revisions make the stock cheaper as it leads to more room in the bottom-line valuation. Despite being fairly straight forward, the velocity of the revisions is the single most important point that short sellers and Nvidia critics cannot seem to understand.
Of all the quarters since Nvidia’s Hopper release, this is the quarter most likely to be lackluster. This is because the impact of Hopper and the H200s are well-known and the Blackwell generation won’t be shipping in volume until Q1 and ramping further into Q2.
I am looking forward to the fiscal year guide in the February call, and am even more excited about the May earnings call when Blackwell’s impact will be better understood.
Nvidia is expected to report revenue of $32.9 billion for growth of 81.8% at the midpoint. Analyst expectations are higher than management guidance of $32.5 billion at the midpoint, for growth of 79.4%. This is a deceleration from last quarter’s 122.4% growth, and peak growth of 262% and 265% in the April and January 2024 quarters.
As pointed out on EPS, another area where Nvidia is unique is the sheer amount of analyst revisions on the stock. It not only speaks to Nvidia’s dominance in the AI data center to continually surprise the Street, but also to the challenge that analysts face in terms of predicting Nvidia’s persistent revenue surge.
For example, this year alone, analysts originally expected Nvidia to report 33.4% revenue growth and this quarter is now expected to be 81.8% growth, for revisions that total 48.4 points in about six months’ time (more than double the original growth expectations).
This quarter, there is a wide range of expectations with UBS believing Nvidia will beat by as much as $2 billion, for revenue of $34.5 billion to $35 billion for Q3. Piper Sandler foresees a beat of $1.3 billion for Q3, and a beat of $1.5 billion for Q4.
It’s been quite clear for the past two years that analysts do not know how to gauge the growth coming from this company. In 2025, Blackwell is likely to wildly exceed analyst estimates again.
This quarter, analysts are expecting EPS of $0.74 compared to EPS of $0.67 last quarter. For nearly two years, the company has beaten EPS estimates by 10% or more, yet in the last quarter, the beat was more muted at 5.7%.
On the topic of Nvidia having 2X better margins than most of the Mag 7, here is a glimpse of how Nvidia compares on EPS with a 35%+ growth rate compared to the Mag 7 reporting half this growth rate through 2026:
Nvidia: 35.5% 2Y revenue CAGR, 35.1% EPS CAGR
Apple: 7.1% revenue CAGR, 14.4% EPS CAGR
Microsoft: 14.2% revenue CAGR; 14.9% EPS CAGR
Amazon: 10.7% revenue CAGR; 22.3% EPS CAGR
Meta: 13.5% revenue CAGR; 12.5% EPS CAGR
This quarter, Nvidia’s CFO Colette Kress, will not offer a full year guide yet have to address the elephant in the room — supply constraints.
The fab that makes Nvidia’s chips, Taiwan Semiconductor (TSMC), is working overtime to boost capacity to meet demand. TSMC’s monthly CoWoS capacity was estimated at ~15,000/month at the end of 2023, and was originally expected to triple to ~45,000 to 50,000/month by the end of 2024 in order to meet such high demand from Nvidia, AMD and other advanced node clients. Now, capacity is expected to rise ~300% to 60,000/month.
TSMC remains committed to significantly boosting CoWoS capacity over the next few years in order to accommodate these accelerated AI GPU timelines from both Nvidia and AMD, with multiple different product lines expected to come to market over the next couple of years. By year-end 2025, CoWoS capacity is estimated to be 80,000 to 90,000/month, per Morgan Stanley, with Nvidia reportedly already reserving half of this capacity.
By the end of 2026, CoWoS capacity is estimated to expand to as much as 140,000 to 150,000/month, representing 10x growth in capacity from the end of 2023.
Source: Beth_Kindig xAI
Foxconn and Quanta are also both signaling strong demand for Blackwell come 2025. Foxconn has said that they see “crazy” demand for Blackwell servers, and forecast AI servers to make up half of their overall server business in 2025. Foxconn has said that initial shipments are on time for Q4 before ramping much faster in Q1, with Quanta saying the same, that initial shipments are on schedule and will ramp in Q1.
Quanta sees triple-digit AI server growth through next year on the back of strong demand, with Deputy Spokesperson Carol Hsu saying that “recent capex guidance from top US hyperscalers also confirmed their aggressive spending on AI in 2025, all from a high base in 2024.”
Nvidia is the subject of some of the most severe export restrictions from the US due to its integral role in advancing AI computing. Subsequently, the company’s China exposure is among the lowest in the semiconductor sector, leaving it less exposed should we see heightened geopolitical tensions — especially tariffs.
Nvidia’s China revenue was 9.6% in Q1 and 12.2% in Q2, down from the low-20% range in the same quarters in fiscal 2024. For all of FY 2024, Nvidia’s China revenue was 16.9%, down from 21.5% the year prior. Other semi peers are much more heavily exposed to China: Broadcom’s China exposure was 32.2% in FY 2023, Intel’s exposure was above 27%, and Qualcomm and Marvell both had more than 40% of revenue stem from China in FY 2024.
Although Nvidia’s fundamentals are a perfect 10, the stock is contending with weak peers, as evidenced by major semiconductor ETFs, SOXX and SMH, not making new highs with the S&P 500.
Retail investors often find out the hard way, even the most perfect stock must contend with market forces beyond its control. This is the primary reason Nvidia’s stock may pullback as Nvidia is holding up the semiconductor market, which has grown unusually weak in the past few weeks. SOXX is 20% of its all-time highs and SMH is 14% of its all-time highs despite the S&P 500 making new highs. In a 1-hour webinar for I/O Fund Members last quarter, I discussed why this is an issue for AI investors and what I’d like to see before I resume buying Nvidia.
My firm has become well-known for calling Nvidia an AI stock in 2018, and later stating Nvidia would Surpass Apple, and finally that Nvidia will reach a $10 trillion market cap by 2030. Yet, perhaps lesser-known is that I nailed the October 2022 bottom by stating Nvidia was Ready to Rumble on H100 GPUs along with a real-time trade alert for $10.80 on October 13th 2022 a mere 25 months ago.
Here is what I stated at the exact moment Nvidia’s stock bottomed in October after selling off 60% following the August earnings report:
“Today, Nvidia’s AI products serve nearly every enterprise company’s artificial intelligence and machine learning ambitions. The company has an impressive launch schedule starting in October for two flagship products – the RTX 40 Series and the H100 GPU. The timing of these releases is no coincidence as it’s a rapid two months following the crypto/gaming revenue miss. Suffice to say, Nvidia’s management team is prepared to rumble —- putting its very best release in gaming and its most powerful AI chip to-date up against the crypto mining selloff. If history is any indication, the turnaround will only be a matter of time.”
The upcoming earnings report has a few similarities to October of 2022, which is that we are toward the end of a product cycle and the CFO cannot offer fiscal year guidance. Despite the H100s ramping and Nvidia having visibility into that ramp, the CFO was tight-lipped two years ago stating: “Our Data Center yes, we do expect it to grow. It may grow about what we just saw between Q1 and Q2. We’ll continue to look at it.” Therefore, I am not expecting much from the CFO on Blackwell in this report, but that lack of detail will be a distant memory this time next year.
Make no mistake, Nvidia is the best stock of the decade and we are only four years in. The big picture is that Nvidia's trajectory will continue due to two words: pricing power.
Our firm has an aggressive buy plan at key levels should the stock pullback, and we have a backup plan should the stock overcome the peer pressure we are seeing from SMH and meaningfully breakout.The keyword is “buy” but the skillset is patience. My firm has blended cutting-edge analysis alongside careful, patient buys for returns of 3280% since our first tranche. Most importantly, the I/O Fund continues to offer buy zones for those who’d like to participate.
The I/O Fund first called out Nvidia’s AI opportunity in November 2018 with our first trade alert at $3.15 for returns of 3280%. We also provided 9 buy alerts from 2021 – 2022 to buy NVDA stock below $20. The I/O Fund has been closely analyzing lesser-known stocks in AI plus crypto with real-time trade alerts and webinars. For a limited time, get up to $250 off with one of our biggest sales of the year starting Nov 28th. Sign up for our newsletter for more information on the upcoming sale or Follow me on xAI/Twitter.
If you would like notifications when my new articles are published, please hit the button below to ""Follow"" me.
Please note: The I/O Fund conducts research and draws conclusions for the company’s portfolio. We then share that information with our readers and offer real-time trade notifications. This is not a guarantee of a stock’s performance and it is not financial advice. Please consult your personal financial advisor before buying any stock in the companies mentioned in this analysis. Beth Kindig and the I/O Fund own shares in NVDA at the time of writing and may own stocks pictured in the charts."
190,https://www.forbes.com/sites/dereksaul/2024/11/20/nvidia-rallies-5-ahead-of-earnings-what-to-expect-from-most-important-stocks-report/,Nvidia Dips Ahead Of Earnings: What To Expect From ‘Most Important’ Stock’s Report,"Nov 20, 2024, 11:56am EST",Derek Saul,"Nvidia shares fell slightly Wednesday as investors recalibrated ahead of the artificial intelligence leader’s quarterly earnings results due this afternoon, a report described as the most crucial upcoming event for the hot, but rocky, stock market.
Nvidia stock fell 1.9% to $145 by mid afternoon, cutting into Tuesday’s near 5% gain.
The volatile trading comes ahead of the semiconductor chip designer’s third-quarter results slated for shortly after 4 p.m. EST Wednesday.
It’s a result that will likely have wide sweeping ramifications for the broader market, which has calmed after a sharp uptick following the election.
Nvidia earnings “can dictate the near-term direction of the market,” predicted Bank of America strategists Gonzalo Asis and Ohsung Kwon in a Sunday note to clients, adding that Nvidia is the “most dominant” and “most important stock in the market.”
Options traders price in a greater risk of a Friday move for the benchmark S&P 500 index than the trading sessions following the next slate of typically pivotal economic updates in the November jobs and consumer price index inflations reports and the December meeting of the Federal Reserve’s interest rate panel, according to Bank of America. Nvidia accounts for more than 7% of the market capitalization-weighted S&P.
Wall Street expects another blowout quarter for Nvidia. Consensus analyst forecasts call for a record $0.75 earnings per share ($17.4 billion net income) in the three-month period ending in October and $33.2 billion in revenue, according to FactSet. That would represent the sixth consecutive quarter of Nvidia setting new records for all three of those metrics. The projections call for more than 80% year-over-year revenue and profit growth in the third quarter for Nvidia, well above the sub 20% annual top-and-bottom line increases brought in during Q3 by the world’s only other $3 trillion companies, Apple and Microsoft.
Nvidia is the most valuable public company in the world by market cap, with its $3.56 trillion valuation outstripping Apple’s $3.45 trillion and Microsoft’s $3.08 trillion. Nvidia enjoys close to an 80% market share in AI accelerators, the highly expensive processing equipment necessary to train the in-vogue generative AI models developed by entities from Amazon to the Danish government. The company’s financial performance and valuation have skyrocketed during the recent generative AI revolution. Its split-adjusted share price is up about 850% over the last two years, sending its market cap from below $400 billion to within striking distance of $4 trillion while its projected net income of $67.9 billion during this fiscal year would be a more than 1,400% improvement from the $4.4 billion for the year encompassing most of 2022.
We estimate Nvidia CEO Jensen Huang’s net worth at around $128 billion as of Tuesday afternoon, making him the ninth-richest person in the world.
CORRECTION (11/19): This story has been updated to reflect the date of Nvidia’s earnings report.
"
191,https://www.forbes.com/sites/investor-hub/article/nvidia-nvda-stock-earnings-preview-what-to-know-before-report/,Nvidia Stock Earnings Preview: What To Know Before The Report,"Nov 19, 2024, 03:22pm EST",Surbhi Jain,"Nvidia’s (NVDA) upcoming earnings report has the tech world buzzing. This article dives into the significance of the report, key drivers of Nvidia’s growth, and what investors should watch for. This preview of Nvidia stock earnings explores key aspects investors should consider, from recent financial performance to AI's growing role in driving growth.
Nvidia’s earnings report, which is scheduled to be released on November 20, carries immense weight for the company and the tech industry as a whole. The stakes are high for Nvidia stock earnings, as the company leads the AI revolution, making its results a key barometer for the tech industry.
As the backbone of AI hardware, Nvidia drives innovation across sectors like data centers, gaming and autonomous vehicles. A strong report not only bolsters investor confidence but also sets a benchmark for competitors like AMD and Intel, influencing the broader market outlook.
The AI boom has propelled Nvidia into the spotlight, with its GPUs powering generative AI models like ChatGPT. Analysts view the earnings report as a litmus test for the sustainability of Nvidia’s dominance in this space. With increasing reliance on AI, the report’s outcomes could ripple through related industries, affecting investment trends and corporate strategies.
Nvidia’s recent financial results have been a showcase of its dominance in the AI and semiconductor industries. In the previous quarter, the company reported record revenues of $30.04 billion, up 122.4% year over year. The surge was largely driven by AI demand, with data center revenue contributing more than 70% of the total.
The recent growth trajectory of NVDA stock earnings demonstrates robust demand across AI and gaming sectors, boosting confidence in its market leadership. Gaming revenue also showed signs of recovery, rebounding 22% year over year, reflecting the easing of macroeconomic pressures. Nvidia’s consistent profitability and strong gross margins—above 70%—further underscore its operational excellence. For more insights on Nvidia's future, explore Nvidia in five years.
Much more than breaking news, our diverse reporting digs deeper with unparalleled insights that empower you to make better informed decisions. Become a Forbes member and get unlimited access to cutting-edge strategies, actionable insights, and updated analysis from our network of leading finance experts. Unlock Premium Access — Free For 25 Days.
Nvidia’s cutting-edge GPUs remain critical for AI model training and deployment. With companies investing heavily in AI infrastructure, the continued demand for Nvidia’s A100, H100 chips and Blackwell, will likely play a pivotal role in this quarter’s performance.
While demand remains robust, any disruption in chip production or supply chain logistics could constrain Nvidia’s ability to meet orders. Recent geopolitical tensions add to these uncertainties.
With inflation moderating, consumer spending on gaming hardware is picking up, which could positively influence Nvidia’s gaming division. However, a potential slowdown in enterprise IT spending could temper overall growth.
Those impacting export licenses could significantly influence NVDA stock earnings, especially given its reliance on Chinese markets.
AI has become the crown jewel of Nvidia’s growth strategy. The company’s GPUs are the backbone of AI models used in everything from chatbots to advanced analytics. Generative AI, in particular, has catapulted demand, with Nvidia benefiting from its early investments in AI-centric architectures.
Data centers represent another growth engine. Nvidia’s dominance in this space, with products like the DGX SuperPOD, has solidified its position as the go-to provider for scalable AI infrastructure. Revenue from this segment has consistently surpassed expectations, demonstrating the increasing reliance of enterprises on Nvidia’s technology.
AI-driven advancements continue to propel Nvidia stock earnings, as the company solidifies its leadership in GPUs for large-scale computing and AI applications.
The gaming sector, once Nvidia’s bread and butter, continues to be a key contributor to its revenue. The company’s RTX 40 series GPUs have been well-received, boosting revenue in recent quarters.
Post-pandemic, the gaming industry experienced a slowdown, but recent data suggests a recovery. The resurgence of the gaming market has contributed positively to NVDA stock earnings, with the RTX 40 series outperforming competitor offerings.
Nvidia’s efforts to optimize its products for gamers while capitalizing on AI-enhanced gaming experiences have positioned it favorably in this evolving market.
In the race for AI dominance, Nvidia has maintained a significant lead over competitors like AMD and Intel. Its proprietary CUDA platform and robust ecosystem give it a clear edge in AI training and inference workloads.
While AMD has made strides with its MI300 chips, they have yet to achieve the widespread adoption Nvidia enjoys. Similarly, Intel faces delays in delivering competitive GPU solutions. This disparity reinforces Nvidia’s leadership in both innovation and market share.
Discover more in-depth insights, entrepreneurial advice and winning strategies that can propel your journey forward and save you from making costly mistakes. Elevate your journey by becoming a Forbes member. Unlock Premium Access — Free For 25 Days.
Analysts anticipate Nvidia to report earnings of $0.75 per share on revenues of $33.12 billion, marking another record-breaking quarter. Analysts forecast robust figures for Nvidia stock earnings and growth, which reflects surging demand for AI-driven technologies and expanding market penetration.
However, the projections come with a caveat: Nvidia’s high valuation means any miss could lead to significant market volatility. As a result, investor expectations are tempered with caution despite the bullish outlook.
Historically, NVDA stock earnings drive volatility, as investors anticipate results from Nvidia's high-growth markets like AI and gaming. Nvidia’s stock tends to rally in the weeks leading up to earnings announcements, driven by optimism around its performance. However, the stock also sees heightened volatility, reflecting mixed sentiment among traders and institutional investors.
Recent trading sessions suggest a cautious yet positive trend, with Nvidia shares holding steady near all-time highs.
Investor sentiment surrounding Nvidia remains overwhelmingly bullish, with analysts largely assigning ""Buy"" ratings. Consensus price target by analysts for NVDA stock stands at $160.38 currently.
Bullish sentiment around NVDA earnings reflects confidence in Nvidia’s AI capabilities and broader growth outlook. Firms like JPMorgan and Goldman Sachs have reiterated their confidence in Nvidia, citing its leadership in AI and robust revenue growth.
However, concerns about valuation and macroeconomic risks have tempered enthusiasm among some investors, who worry about potential downside if growth slows.
While AI is a growth driver, Nvidia’s dependence on this single sector makes it vulnerable to any slowdown in AI adoption or spending.
As a major chip exporter, Nvidia is exposed to trade restrictions and regulatory challenges, particularly concerning China. Regulatory restrictions on exports to China remain a key factor that could dampen Nvidia stock earnings.
Although currently dominant, advances by AMD and Intel could erode Nvidia’s market share if the company fails to maintain its innovation pace.
The growing adoption of AI across industries ensures a long runway for Nvidia’s growth. From healthcare to finance, the company’s GPUs are pivotal in enabling AI-driven transformations.
Nvidia’s DRIVE platform is gaining traction among automakers, offering another lucrative growth avenue.
As the metaverse concept evolves, Nvidia’s Omniverse platform positions it to capitalize on this next wave of digital innovation.
For a deeper analysis of Nvidia's long-term potential, read Nvidia in 5 years.
Nvidia’s earnings report is poised to reflect its dominance in the AI and data center markets. While risks persist, the company’s robust growth trajectory and leadership in innovation make it a stock to watch closely. Investors should prepare for potential volatility as Nvidia navigates its high-stakes earnings season.
How does Nvidia’s earnings report impact its stock price?
Nvidia’s earnings reports often lead to significant stock price movements due to its high valuation and investor interest in its growth story.
What sectors contribute most to Nvidia’s revenue?
The data center and gaming sectors are Nvidia’s largest revenue drivers, with data centers recently accounting for over 70% of total revenue.
How does Nvidia compare to its competitors?
Nvidia maintains a strong lead over AMD and Intel in AI and data center markets, thanks to its advanced GPUs and robust ecosystem.
Is Nvidia’s growth sustainable?
With strong demand for AI, data centers, and gaming hardware, Nvidia’s growth trajectory appears sustainable, barring macroeconomic disruptions.
What are the biggest risks for Nvidia stock?
Key risks include overreliance on AI, geopolitical tensions, and increasing competition from AMD and Intel.
Whether it’s mastering cutting-edge strategies, uncovering actionable investment opportunities from influential leaders, or breaking down complex topics, our in-depth journalism has you covered. Become a Forbes member and gain unlimited access to bold ideas shaking up industries, expert guides and practical investment advice that keeps you ahead of the market. Unlock Premium Access — Free For 25 Days."
192,https://www.forbes.com/sites/karlfreund/2024/11/18/nvidia-leads-the-hpc-landscape-at-supercomputing-24/,Nvidia Leads The HPC Landscape At SuperComputing ’24,"Nov 18, 2024, 01:30pm EST",Karl Freund,"Nvidia has gone from a niche provider of HPC technology to becoming a dominant force in the industry. This year’s SC event reinforces that leadership.
The annual SuperComputing event in North America is taking place this week in Atlanta, Georgia, and as usual the show is full of 1000’s of scientists discovering the future, and those selling them technologies to accelerate High Performance Computing, or HPC. And Artificial Intelligence  is the new tool that helps scientists in new ways as AI models continue to evolve. With AI and accelerated computing, scientists can speed simulations by 10-100X, or even enable scientists to predict what a classical simulation would produce without even running them,
In the last five years, the high performance community has embraced accelerated computing (primarily GPUs) in a steady march towards dominance. GPUs were a rarity in 2019 in the Top100, while today accelerators rule the roost representing over 65% of the prestigious list. Consequently, the CPU-centric supercomputer is becoming a rare beast. Nvidia and AMD GPUs have become essential to HPC.
HPC scientist need the performance that GPUs can deliver, but they don’t want to become AI scientists. They are weather scientists, geneticists, chemists, and physicists. Nvidia is increasingly promoting “NIM Microservices” to enable these scientists to tap into the power of AI, And Nvidia continually adds new domains to its NIM catalog for faster inference processing and lower TCO. Scientist increasingly do not have to worry about the optimizations normally demanded by AI to get the best performance out of their hardware; NIM Microservices allow Nvidia engineers to do all that work for them.
All companies producing Quantum Computing Units (QPUs) now believe that Quantum needs classic computing to help it become usable and, perhaps someday, ubiquitous. Nvidia’s Quantum play is CUDA-Q, which is essentially like CUDA for developing and simulating quantum circuits that run on somebody’s QPU. Nvidia has added “Dynamics” to CUDA-Q, which enable developers to accelerate the creation and simulation of quantum circuits from taking a year on CPUs to running in under an hour on Nvidia GPUs.
And its not just Nvidia who is saying that. Google, a developer of quantum hardware, has adopted CUDA-Q in its Quantum AI program to perform the world’s largest device simulations.
As we wrote in another blog today,  Nvidia realizes that not everyone needs or can handle the infrastructure needed for its most advanced system, the GB200 NVL72. Many can adequately handle their HPC and AI needs with traditionally connected and configured servers; they just need a faster accelerator. Nvidia announced that its H200 NVL4 PCIe card package is now generally available. The 4-card package increases performance by 30-70% using the NVLink-connected Hopper GPUs in virtually any server.
And for those needing the performance of Blackwell, which is 1.8 to 2.2 times faster for simulation and AI, Nvidia has announced a new GB200 NVL4,
We can’t cover it all but Nvidia made a slew of other announcements, the depth and breadth of which reinforce our view that Nvidia has regained its leadership in HPC, if it ever lost it. These announcements include:
Nvidia strongly believes and has demonstrated that AI and HPC are “Full Stack” problems, demanding that technology providers think through the entire system of systems, with the networking and software needed to provide performant solutions. And one of the most important of these in HPC and AI is networking these fast CPUs and GPUs together into a comprehensive system of systems.
While Ethernet and InfiniBand do a great job of solving many node-to node and rack to rack networking problems, the GPU to GPU demands far faster links. NVLink with NVSwitch deliver significantly higher bandwidth compared to standard Ethernet, with the latest generations reaching several terabytes per second, while even high-end Ethernet currently maxes out at around 800 gigabits per second. NVLink also provides lower latency than Ethernet, which is crucial for applications requiring fast data exchange between GPUs.
Nvidia’s leadership in HPC appeared to take a hit a few years ago when the DOE’s fastest systems were built on AMD GPU and CPU technologies. They had the 64-bit floating point and prices that the DOE required, and these systems have endured as some of the fastest in the world.  But as AI has become more mainstream in supercomputing centers, Nvidia’s full-stack approach with the fastest AI GPUs have become relatively more attractive.
In this article, we have barely scratched the surface of what Nvidia is showing off in Atlanta, but thanks for reading!
This article expresses the author's opinions and should not be taken as advice to purchase from or invest in the companies mentioned. Cambrian-AI Research is fortunate to serve clients who are leaders in technology, especially in semiconductor and IT infrastructure. Mr. Freund’s personal portfolio has at various times held positions in companies mentioned in this article. For more information, please visit our website at https://cambrian-AI.com.
"
193,https://www.forbes.com/sites/stevemcdowell/2024/11/18/cloudian-hyperstore-meets-nvidia-gpudirect-object-storage-for-ai/,Cloudian HyperStore Meets Nvidia GPUDirect: Object Storage For AI,"Nov 18, 2024, 02:26pm EST",Steve McDowell,"Cloudian, a long-time leader in scalable object storage technology, directly addresses the complexity of AI storage infrastructure with its new Cloudian HyperStore with Nvidia GPUDirect for Object Storage.
Unveiled at the SC24 supercomputing conference in Atlanta, Cloudian now combines scale and performance in a single solution, simplifying the AI storage landscape with a unified data lake that can act as a shared data repository for all stages of the AI lifecycle.
The new offering is the first object storage solution in the industry to leverage Nvidia GPUDirect technology to reduce latency and improve throughput, meeting the high demands of AI training and other demanding GPU-centric workloads.
AI workflows bring unprecedented new data management challenges to IT organizations, as these workflows depend on vast amounts of unstructured data. In addition to the challenges of scale, there is also the complexity of managing a myriad of storage requirements, from training data to checkpoint data, vector databases, log files, and more.
Traditional storage solutions, architected for the more modest demands of conventional enterprise applications, struggle to keep up with these demands. Addressing these complexity, cost, and scaling limitations is critical to delivering efficient and cost-effective data management for high-performance AI applications.
Object storage, already the data management foundation for hyperscale cloud providers, offers a clear solution to AI data challenges. Just as it revolutionized cloud infrastructure, object storage transforms AI data management by efficiently handling vast unstructured datasets while maintaining simplicity.
Object storage brings multiple benefits to AI workflows:
Cloudian’s on-prem object storage solution, HyperStore, is a scalable, secure, and S3-compatible platform designed to manage vast amounts of unstructured data. HyperStore offers exabyte-level scalability, allowing organizations to start small and expand seamlessly as data requirements grow. Adding GPUDirect technology to the solution lets it better support AI and other high-performance GPU-centric workloads.
Nvidia’s GPUDirect technology is at the heart of Cloudian’s new HyperStore solution. It enables rapid data transfers between storage and GPU memory by providing a direct data path that bypasses traditional CPU routes. This reduces the number of steps and avoids CPU and system memory bottlenecks, thereby accelerating data access.
Cloudian’s new HyperStore with Nvidia GPUDirect for Object Storage is a step forward in high-performance data management. The seamless integration of GPUDirect with HyperStore allows users to benefit from Cloudian’s scalability and ease of use while enabling a dedicated high-performance data path for GPU workloads.
The Cloudian solution has a compelling performance story. With GPUDirect, Cloudian reports continuous throughput speeds of 200GB/second from a HyperStore system–triple the throughput when using GPUDirect compared to traditional data access methods measured on otherwise identical all-flash systems. This high-performance throughput ensures that data is available to GPUs at the speed required for demanding workloads.
By delivering file-system performance directly from object storage, GPUDirect eliminates the need for a separate file-storage layer - simplifying architectures while reducing infrastructure costs.
Efficiency improves as well. Cloudian said that it provides an estimated 42% reduction in CPU usage, with GPUDirect freeing up processing power. This makes it easier for organizations to manage large-scale data without increasing their hardware footprint or operational costs, enhancing performance and contributing to lower power consumption and cost savings over time.
Cloudian's GPUDirect solution flattens storage architecture into a single data lake, eliminating complex data migrations while simplifying management of vast unstructured datasets. For AI workflows, this means training data, models, and inferencing can all leverage the same high-performance storage platform - accelerating development while ensuring data consistency.
There’s also a security story, as GPUDirect technology operates without requiring vendor-specific kernel-level modifications, improving system security by reducing the risk of exposure to vulnerabilities. This is valuable in sectors such as healthcare and finance, where data security and regulatory compliance are paramount.
The rapid adoption of AI drives massive data growth, requiring an efficient, scalable, and high-performance storage infrastructure. Cloudian HyperStore with Nvidia GPUDirect for Object Storage addresses this demand by providing an advanced platform optimized for AI and GPU-centric workloads requiring seamless scale data management.
Cloudian transforms traditional AI storage architecture by consolidating unstructured data into a single high-performance data lake. The native S3 API and cloud integration facilitate seamless data management between cloud and on-prem environments.
Support for Nvidia GPUDirect for Object Storage completes the picture, enabling direct object-to-GPU transfer and eliminating the complexity and cost of separate file and object storage layers. The result is a dramatically simpler infrastructure that delivers better performance and lower operational costs.
Cloudian isn’t the only storage solution on the market to take advantage of Nvidia GPUDirect—storage solutions from Dell Technologies, DDN, WEKA, and VAST Data all support the technology—but Cloudian is the first technology provider to directly integrate GPUDirect into an object storage solution, staying ahead of data trends for AI and other GPU-based workloads.
Nvidia GPUs are central to AI-driven enterprise data transformation and are critical components in nearly every phase of the AI lifecycle. Cloudian’s new GPUDirect for object storage solution ensures that enterprises have the tools to harness their data’s full potential. It’s a compelling solution to help enterprises across industries achieve more with their data while controlling costs and enhancing efficiency, a critical capability for enabling AI-driven digital transformation.
Disclosure: Steve McDowell is an industry analyst, and NAND Research is an industry analyst firm, that engages in, or has engaged in, research, analysis and advisory services with many technology companies, including every company mentioned in this article, except DDN. No company mentioned was involved in the drafting or publication of this article. Mr. McDowell does not hold any equity position with any company mentioned."
194,https://www.forbes.com/sites/karlfreund/2024/11/13/nvidia-sweeps-benchmarks-amd-is-mia-again/,"Nvidia Sweeps Benchmarks. AMD Is MIA, Again","Nov 13, 2024, 11:00am EST",Karl Freund,"It should not surprise anyone: Nvidia is still the fastest AI and HPC accelerator across all MLPerf benchmarks. And while Google submitted results,  AMD was a no-show.
This blog has been corrected on 11/14 with a fresh TPU Trillium vs. Blackwell comparison.
Say what you will, MLPerf is the industry’s best benchmarks for AI workloads. Do people make buying decisions on them?  Probably not.  But these results over time remain indicative of the pace of improvement of AI hardware and software performance, comparing successive hardware and software releases.  And of course the benchmarks provide bragging rights for Nvidia.
AMD did submit AI inference performance last quarter for a single benchmark, and they actually performed pretty well.  This time, however, I was surprised that they were unable or unwilling to put their hardware to the test. Perhaps they are just too busy readying the MI325 for customer shipments.
Google, on the other hand, did submit a few results for the TPU V5p and TPU Trillium platforms, used to train Apple Intelligence models. Lets take a look.
While Blackwell is ramping production volumes, and shows up in the MLPerf lexicon as a “Preview” solution, the Nvidia Hopper is Nvidia’s currently “Available” GPU. The primary benefactor of the AI wave that broke over the market in the last year showed off new software that increased performance by up to 30%, and a benchmark was submitted on a cluster that scaled to over 11,000 GPUs using NVLink, NVSwitch, ConnectX-7, and Quantum X400 InfiniBand networking. Clearly, H100 is delivery most of Nvidia’s revenue this quarter, and Nvidia successfully demonstrated that this system is certainly no slouch.
Dell submitted H100 and H200 results that showed about a ~15% uplift for the larger memory H200, though these benchmarks do not represent the really large models that should more significantly benefit H200.  Perhaps this is another  reason for AMD’s absence, as the AMD MI300 has more HBM memory than its Nvidia counterparts,  and these benchmarks would not show off that potential advantage.
While Nvidia demonstrated that Blackwell has four times the performance of Hopper in last quarter’s inference processing benchmark round, perhaps nearly half that came from the use of 4-bit math on the B100 vs the 8-bit math on the H100.  Since B100 is two GPU dies, one should expect that Blackwell will complete a training job in about 1/2 the time as its H100 predecessor, since they both use 8-bit floating point. And thats exactly what the MLPerf benchmarks show: a ~2x performance advantage.
Let’s put this into historical perspective.  The Nvidia A100 was introduced 4.5 years ago. Blackwell is twelve times faster than the A100; an order of magnitude increase in training performance. This delta comes from seven improvements realized in those four years as outlined in the slide below. As Nvidia keeps telling us, their AI is a full-stack solution, and everything from software like the Transformer Engine to overlapping computing and communications contribute to this impressive result. After all, Moore’s Law would only deliver less than 4 times better performance; so this is not like a CPU chip.  It takes a fully optimized system, software, rack and data center architecture to deliver this level of innovation and performance.
The Google Trillium TPU delivered four results for clusters ranging from 512 to 3072 Trillium accelerators. If I contrast their performance of 102 minutes for the 512 node cluster, it looks pretty good until you realize that the Nvidia Blackwell completes the task in just over 193 minutes using only 64 accelerators. When normalized, always a dangerous and inaccurate math exercise, that makes Blackwell over 4 times faster on a per-accelerator comparison.
Google reached out after my post was published, and provided some insights that were very helpful. I have corrected the comparison thanks to their help. They correctly pointed out that nobody does training at this small scale, and we will be watching to see how the comparison evolves as both companies are building massive training systems with hundreds of thousands of nodes.
Clearly, Nvidia’s lead in chips, software, networking, and infrastructure is delivering the goods. Nobody comes even close in these results, and most of the industry refuses to provide the transparency needed to properly evaluate their alternatives. Many of these alternative vendors simply say that Nvidia controls MLPerf and it is a waste of time to try to compete with them. Not sure about the “control” part of that argument, but I do agree that Nvidia will throw as many engineers as needed to win these bake-offs. And they do. And they intend to keep winning.
“Over the next ten years, our hope is that we could double or triple performance every year at scale,” Nvidia CEO Jensen Huang said on an episode of the AI-focused podcast “No Priors”.
Disclosures: This article expresses the author's opinions and
should not be taken as advice to purchase from or invest in the companies mentioned. Cambrian-AI Research is fortunate to serve clients who are leaders in technology, especially in semiconductor and IT infrastructure Mr. Freund’s personal portfolio has at various times held positions in companies mentioned in this article. For more information, please visit our website at https://cambrian-AI.com."
195,https://www.forbes.com/sites/timbajarin/2024/11/12/is-nvidia-ceo-jensen-huang-the-next-steve-jobs/,Is Nvidia CEO Jensen Huang The Next Steve Jobs?,"Nov 12, 2024, 10:00am EST",Tim Bajarin,"Steve Jobs is considered by many as a dynamic tech visionary.
He was also a life coach and amateur philosopher. In his well-reported Stanford commencement speech in 2005, he told graduating students to ""find what you love"" and “stay hungry and foolish.""  He laid out the lessons he had learned after being fired by Apple in 1985. He stated, ""I didn't see it then, but it turned out that getting fired from Apple was the best thing that could have ever happened to me,"" said Jobs, 50. ""It freed me to enter one of the most creative periods in my life.""
In his commencement address, Jobs shared his personal history, and his final admonishment to these students was: ""Your time is limited, so don't waste it living someone else's life. ""Don't let the noise of others' opinions drown out your own inner voice.""
I had the privilege of knowing Steve Jobs before he was fired and of having many encounters with him after he returned to Apple. The first Steve Jobs I knew was a brash manager who ruled by intimidation. One reason he was fired was his management style, although Apple's board was deeply concerned about the direction in which he was leading the company.
However, when he returned to Apple in 1997, the Steve Jobs I met on the second day he was back as Apple's iCEO (interim CEO) was a different person. Although he was still a bit brash and had his signature magnetism, he seemed humbled by his failure at NeXT.
We all know that Jobs returned with a creative vision and a more experienced management style which then gave us the iMac, the iPod, the iPhone, the iPad, and the Apple Watch. This propelled Apple to become the $3 trillion valued company it is today.
For decades, many memes have suggested that some business tycoon was the next Steve Jobs. Major executives like Disney's CEO Bob Iger, Tesla’s CEO Elon Musk, and various business leaders have been compared to Steve Jobs in one form or another. In fact, Apple co-founder Steve Wosniak told Fortune Magazine that ""Musk wants to be seen as a cult leader just like Steve Jobs.""
But there is a current tech executive who comes close to being the next Steve Jobs. Jensen Huang, the CEO of Nvidia, has collected similar successes in leading his company to exceed a $3.5 trillion valuation.
I met Mr. Huang before he started Nvidia. He was a microprocessor designer at AMD, and I was told he was one of their best engineers. (Interestingly, AMD's current CEO, Lisa Su, is his cousin.)
In 1993, he left AMD to start Nvidia with partners Chris Malachowsky and Curtis Priem. In 1995, I was invited to Nvidia to learn about its new processors, which were focused on creating a powerful graphic chip called a GPU. GPUs are designed to boost the graphics capabilities of PCs so they can also be used as gaming machines.
From day one, Mr. Huang understood how important a GPU was for a PC and began envisioning how GPUs could drive a lot of innovation in the future. He saw them not only for use in PCs but, by 1997, was championing GPUs for use in even more powerful computers such as graphic workstations, CAD, and CAM applications.
For decades, Jensen drilled into us analysts the importance of a GPU and how he would drive the company to deliver more powerful GPUs over time. By 2005, he began emphasizing the role of more powerful GPUs and how they would drive the future of what he kept calling ""high-performance computing.""
Indeed, he started courting those who made supercomputers and high-powered servers and found success in this area by around 2010.
Over the years, I have had at least 50 meetings with him and his staff, along with one-on-one discussions with Jensen himself as he shared his GPU and high-performance computing vision for the future.
Both Jobs and Huang had long-term visions for what they wanted to create and then they developed the technology to make their dreams a reality. Besides being dreamers, the other connection is that when they talked about the future, I knew I had to take what they were saying seriously. A lot of times, I was skeptical of what they told me, and then, at some point, what they had been envisioning became a reality.
For example, when the iPod came out, Jobs shared a vision for what he called Podcasts. While the iPod was music-focused, Jobs said ""podcasts"" would be a big deal. As you know, podcasts are at the center of broad communications today.
When I met with Jobs on the second day he returned to Apple in 1997, he told me that he was going to focus on industrial design to save Apple. At the time, I thought he was crazy. By 1998, he had redesigned personal computers, given us the candy-colored iMacs, and made industrial design a crucial part of how he created all of Apple's products after that.
Like Steve Jobs, Jensen Huang discussed his future visions and how Nvidia would drive computing at all levels. Even though to some analysts, he sounded like a broken record, the more we looked into his vision, we could see he was on to something big, even if we did not know what that would be.
The comprehensive range of innovations developed by him and Nvidia, extending beyond GPUs to include essential supporting software such as CUDA and related services, indicates that what he was constructing would serve as primary building blocks for advancing our AI future.
It is not much of a stretch to suggest that, at least up to now,  Jensen Huang is as close as we will get in tech to Steve Jobs.
Disclosure: Apple and Nvidia subscribe to Creative Strategies research reports along with many other high tech companies around the world."
196,https://www.forbes.com/sites/johnwerner/2024/11/08/behind-the-scenes-nvidias-great-rise-and-the-new-data-center-era/,Behind The Scenes: Nvidia’s Great Rise And The New Data Center Era,"Nov 08, 2024, 11:46am EST",John Werner,"There’s no denying that we’re making great strides in technology right now, not only in large language models, but also in infrastructure.
In fact, I’ve seen a lot of pivoting of focus in the community toward the benchmarks that we are looking at with GPU and hardware. Of course, there’s still the lightning speed at which the models evolve, but it’s worth looking at the bare metal side, too.
With that in mind, there’s a lot to unpack from a recent episode of the No Priors podcast where Nvidia CEO Jensen Huang contributes some insights.
And it’s certainly Nvidia’s time, as the company eclipses both Apple and Microsoft for the title of the largest tech corporation around. The premier data centers being built right now are using Nvidia products, high-design GPUs, in the racks, and Huang has a lot to say about this transformation.
“The world has changed,” he said, talking about parallelism of clusters and advances in co-design. “The scale has changed.”
Huang went over his view of some of the history of hardware evolution with the hosts, talking about how a maxim in the community known as Moore’s Law held true for many years, in which people referenced more prediction of doubling in transistor and processing capacity every year or so.
For reference, here’s how ChatGPT explains Moore’s law to us:
“Moore's Law is an observation made by Gordon Moore, the co-founder of Intel, in 1965. It predicts that the number of transistors on a microchip would double approximately every two years, leading to a corresponding increase in computational power and a decrease in relative cost. This trend has driven rapid advancements in computing power and has been a foundational principle guiding the semiconductor industry for decades.”
It's a little ironic, because the company being beat out by Nvidia, principally, is none other than Intel. But I digress…
Now, Huang said, with an even faster piece of change, we’re looking at some type of what he called “hyper Moore’s law.”
In order to get there, he suggested, planners have to look at the architecture and the system together in a ‘full stack approach.’
“You could treat the network as a compute fabric, and push a lot of the work into the network, push a lot of the work into the fabric,” he said. “And as a result, you're compressing …at very large scales.”
Inference and Latency: Making Real-Time Systems Smarter
Huang also mentioned the work of adapting to language models and neural networks that are accomplishing inference-time scaling, generating chains of thought and reasoning on the fly.
“We have to go invent something new,” he said, noting that essentially, low latency and high throughput are at odds. In addition, he mentioned the possibility that the industry will move into a diverse era with all kinds of sizes of language models, including tiny language models or TLMs.
“You're still going to create these incredible frontier models,” he said. “They're going to (be used for) the groundbreaking work. You're going to use (them) for synthetic data generation. You're going to use the models, big models, to teach smaller models and distill down (to) the smaller model.”
Later, Huang revealed some very interesting elements of building the X.AI data center that the company worked on with Elon Musk.
He gave Musk a lot of credit for the quick implementation and making the decisions that stood up this supercluster, as lots of people were working on doing everything quickly.
“It's really a testament to his willpower and how he's able to think through mechanical things, electrical things, and overcome what is, apparently, you know, extraordinary obstacles,” Huang said.
He also revealed that the stakeholders used the process of digital twinning to help implement systems.
“We simulated all the network configurations, we pre-staged everything as a digital twin. We pre-staged all of the supply chain. We pre-staged all of the wiring of the network. We even set up a small version of it, kind of a, you know, just a first instance of it …so by the time that everything showed up, everything was staged. All the practicing was done, all the simulations were done. And then massive integration, … a monument of gargantuan teams of humanity falling all over each other, wiring everything up, 24/7, and within a few weeks, the clusters were up.”
What was special about the project? With literally tons of equipment, he said, the pace of the project was “abnormal.”
Huang confirmed that the company uses AI entities as chip designers and software engineers.
“We couldn’t have built Hopper without (them),” he said. “They can explore a much larger space than we can. They have infinite time to explore space.”
Reminiscing on the last few years, in which Nvidia’s market cap has shot up like a rocket, Huang talked about what the effect has been like inside of the enterprise.
“A company can’t change as fast as it stock price,” he said, citing the value of deliberation, of knowing what’s actually happening within the industry to drive change.
What he has realized, he said, is that Nvidia has basically reinvented computing for the first time in around 60 years, bringing the marginal cost of computer down until it makes sense for computers to just do tasks themselves.
That is a game-changer – and that’s an understatement! I’ve been looking at Claude, and OpenAI’s o1 and Orion, and one thing is for sure – when we get to the market effects, things will never be the same.
I might cover more of Huang’s comments elsewhere, as he’s going deeper into that new capability of systems to really do things themselves – with minimal supervision, and for AI to take a greater role in company processes.
This is where you really start to see the effects of ‘agentic AI’ – that you will have AI entities assuming engineering and design roles, and credited with the results.
It is, without a doubt, a time of incredible change."
197,https://www.forbes.com/sites/jimosman/2024/11/07/can-nvidias-hyper-moores-law-spark-an-ai-revolution/,Can Nvidia’s ‘Hyper Moore’s Law’ Spark An AI Revolution?,"Nov 07, 2024, 08:29pm EST",Jim Osman,"Jensen Huang, the CEO of Nvidia, discussed on the No Priors AI-focused podcast the concept of ""Hyper Moore’s Law."" He suggested that AI computing performance could follow a curve even steeper than Moore's Law, which traditionally represents a doubling of transistor count every two years. Huang stated, “I wouldn’t be surprised if the way people think about Moore’s Law, which is 2X every couple of years, we’re going to be on some kind of hyper Moore’s Law curve.”
He added to this concept by explaining the compounding effect, saying, “When you double or triple every year in just a few years it adds up. It compounds really, aggressively.”
Long the guiding concept in computing, Moore's Law—the observation made by Intel co-founder Gordon Moore predicts that the number of transistors on a device doubles roughly every two years, hence increasing performance. For decades the semiconductor sector has been driven forward by this fundamental idea.
For investors, this vision represents a revolutionary change. A global leader in artificial intelligence and GPU technology, Nvidia is well suited to profit from such explosive development. Nvidia is solidly in the vanguard of the next development in computing as the demand for advanced AI capabilities skyrockets because the future of AI hardware and software offers hitherto unheard-of chances for growth.
Nvidia’s concept of ""Hyper Moore's Law"" extends beyond the traditional idea of incremental increases in transistor count. Huang envisions a future where AI computing performance doubles or triples annually. Unlike Moore's Law, which is bound by hardware advancements, this hyper-accelerated pace would be fueled by holistic improvements in software, networking, algorithms, and datacenter infrastructure. Huang points to these combined enhancements as key to unlocking larger-scale AI solutions and driving down computational costs.
This concept is critical because it suggests an exponential growth model, faster than the historical semiconductor trajectory. For investors, a ""Hyper Moore's Law"" growth pace implies that Nvidia's technologies could evolve at a rate unmatched in computing history, potentially placing Nvidia in a class of its own within the AI sector. If successful, this could spark both immediate gains for Nvidia and long-term value creation across the tech and AI ecosystems.
A change to a one-year product cycle, double its earlier two-year cadence, emphasizes Nvidia's desire to dominate the next phase of artificial intelligence development. This quick turnaround represents a calculated attempt to keep ahead of rivals and match growing demand for advanced artificial intelligence capabilities. Faster cycles enable Nvidia to regularly provide high-powered, next-generation products, therefore generating an edge in a fast-evolving sector.
This acceleration uses complete advances across the AI stack—networking, software, algorithms, and hardware integration—rather than depending just on hardware. By improving every element, Nvidia is creating systems that cooperate perfectly, therefore improving total performance much beyond what chip innovation could accomplish.
Scaling tasks across bigger GPU superclusters is a key component of Nvidia's approach since it allows computations on hitherto unheard-of levels. Comprising thousands of GPUs, these superclusters are essential for releasing fresh ideas in demanding industries.
Increased computing power and efficiency could drastically cut costs, opening opportunities for adoption in transforming industries including healthcare, banking, and logistics, thereby attracting Nvidia to long-term investors looking for exposure to AI-driven expansion.
The exponential acceleration of processing power might transform cost structures across sectors and allow a seismic change in artificial intelligence availability for smaller businesses. As the cost of computer resources declines, this democratization of artificial intelligence will enable even the tiniest companies to use machine learning for more focused consumer interaction and smarter operations. From retail to banking to healthcare, industries abound in potential gain. AI's enhanced processing capacity could hasten medication research, improve diagnosis accuracy, and enable very individualized therapy in the healthcare sector. While retail and e-commerce use increased AI for tailored product suggestions and predictive inventory management to change the user experience, finance will witness breakthroughs in fraud detection, high-frequency algorithmic trading, and strong risk assessment.
As Huang notes, the opportunities are great since these developments open the path for breakthroughs in autonomous systems and address once impossible computing problems. For investors, Nvidia is positioned at the center of this revolution since its products are fundamental for many of these developing uses. Industry contracts and high-value alliances are likely to follow; thus, Nvidia's alignment with this fast transformation might convert into significant rewards, hence a strategic investment issue in a future defined by AI-driven sectors.
Nvidia has various technical difficulties that can affect its path as it keeps stretching the limits of artificial intelligence capability. As chipmakers approach the atomic level, the physical restrictions of semiconductor materials present challenges and make further downsizing increasingly difficult. Furthermore, especially as demand for artificial intelligence processing rises, the great energy consumption and significant cooling needs of big GPU clusters create sustainability issues and raise running costs. Beyond mere technical constraints, Nvidia must negotiate difficult ethical and legal terrain. Concerns about data privacy, the ethics of AI decision-making, and possible regulatory scrutiny are rising problems Nvidia and its industry competitors have to deal with as artificial intelligence spreads over more spheres of life. The competitive environment adds more complications; businesses like AMD and Intel are stepping up their efforts to speed AI hardware developments, which puts more pressure on Nvidia to keep its technological advantage.
From an investment standpoint, exponential development carries danger as well as possibility. Although Nvidia's innovative posture in artificial intelligence hardware fits long-term expansion patterns, the fast rate of change can potentially cause turbulence. Investors should weigh Nvidia's strategic orientation in a transforming market as well as the near-term obstacles.
Under Jensen Huang's visionary direction, Nvidia is positioned to lead an era of exponential expansion driven by what he refers to as ""Hyper Moore's Law,” a radical leap in artificial intelligence computer capability that might reshape the semiconductor business. Nvidia's emphasis on expanding artificial intelligence hardware distinguishes it especially as a crucial enabler of broad-based AI adoption, a position that has the potential to bring investors significant long-term profits. Nvidia's technology edge might translate into significant alliances and higher shareholder value as businesses from finance to healthcare incorporate AI more thoroughly. Staying current on Nvidia's developments in AI hardware and strategic actions in the larger tech ecosystem is crucial for investors since the business significantly shapes a future, mostly depending on artificial intelligence.
Pre-Order My Book"
198,https://www.forbes.com/sites/johnwerner/2024/11/07/tsmc-to-double-production-based-on-nvidia-numbers-and-overall-demand/,TSMC To Double Production Based On Nvidia Numbers And Overall Demand,"Nov 07, 2024, 11:41am EST",John Werner,"The growth of Nvidia’s customer base is having a major impact on the hardware industry.
News from the world of hardware production shows that we’re in another kind of market crunch, similar in some ways to the broader chip shortage we faced a few years ago.
It’s the same primary player, too. Taiwan Semiconductor Manufacturing is the principal supplier of high-precision technology that’s very much in demand right now.
To put this in context, as we reported last week, Nvidia just surpassed Apple and Microsoft in terms of market cap - that gives you an idea of the kind of demand that the company is going to put forward, for the materials needed to create their much-lauded GPUs.
Now, those with a front-row seat are reporting TSMC is going to double its production capacity next year, and Nvidia is going to take more than 50% of that supply. In fact, it seems that the drive for production is mostly led by Nvidia’s orders.
The Apple of Nvidia‘s eye is TSMC’s CoWoS or chip-on-wafer-on-substrate design that is trying to ramp up in order to fulfill orders.
So what is this type of hardware engineering?
CoWoS allows for more input/output connections, and three-dimensional stacking of components. It uses an interposer to facilitate shorter connection length and decrease the need for power consumption. That makes it really useful for AI applications, because these sophisticated systems need all of that power to operate.
One way to describe it is that by locating different components, close to each other at the edge, there’s less transit time, and less latency. Another point would be that the innovators are trying to match the need for robust data flow with new routing and the kinds of specific gating that CoWoS and related design offers.
Additional Benefits of CoWoS
Simply put, TSMC’s setup will allow companies to scale their GPU connections and hardware systems. They help with cooling and maintenance, and provide better power integrity for ongoing operations.
One way to describe the interposer design is that unlike stacking, it offers more of a bridge for signal flow, and shortens the distance between IP blocks.
This advanced packaging solution, combined with other innovations, is helpful in building those specialized hardware environments of the future.
Specifically, Nvidia is shipping tons of its GB200 and H100/H200 GPUs that are really effective for use in massive data centers like Elon Musk’s Colossus project that’s by far the biggest project kind.
I was checking out the videos giving a tour of Colossus, and when you see the massive cooling systems and infrastructure required, you get a sense of how much demand there is for the hardware itself, which is shelved up to scale AGI’s ability to generalize and move to the next level.
Of course, you’re going to see the output of these hardware systems in the logical operations of our best and newest models, the most detailed training and testing data, and new things like inference. These types of activities allow the machines to “sit and think” more like humans do, and to make more detailed decisions, also providing transparent chains of thought and reasoning for their outputs.
A few of the takeaways would be that Nvidia is poised to dominate given its lion’s share of TSMC production, and that we will continue to see quick transformation, while companies like OpenAI and others are innovating AI agents. It seems like not only the models, but the hardware, change in the blink of an eye. It’s good to stay on top of these changes as they happen."
199,https://www.forbes.com/sites/dereksaul/2024/11/04/ai-titan-nvidia-overtakes-apple-as-worlds-most-valuable-company/,AI Titan Nvidia Overtakes Apple As World’s Most Valuable Company,"Nov 04, 2024, 09:43am EST",Derek Saul,"Artificial intelligence kingpin Nvidia became the largest company in the world Monday, surpassing iPhone maker Apple, marking another feather in the cap for the Silicon Valley titan Nvidia.
Nvidia overtook Apple as the most valuable public company on the planet at Monday’s open, with its $3.39 trillion market capitalization topping Apple’s $3.36 trillion valuation by mid afternoon.
Shares of Nvidia rose 2% and shares of Apple fell 0.2% Monday, continuing the 2% gain Friday for Nvidia and 1% loss for Apple.
Helping boost Nvidia was the Friday afternoon announcement it will replace Intel in the blue-chip Dow Jones Industrial Average (the change will go into effect Friday), while Apple shares were dinged by one of its largest shareholders, the Warren Buffett-led Berkshire Hathaway, disclosing this weekend another major sale of its Apple stock.
Nvidia briefly surpassed Apple as the biggest company in the world Oct. 25, but Nvidia hasn’t ended a trading session as the largest firm since June. Nvidia designs about 75% of the world’s AI accelerators, the semiconductor technology powering generative AI applications like OpenAI’s ChatGPT chatbot, as demand spikes for the graphics processing units produced by Nvidia. This “insane” demand has translated into a massive earnings uptick and stock market boom. Nvidia’s net income rose from $2.3 billion during the first half of 2022 to $31.5 billion during the first half of 2024, and its share price is up 850% over the last two years. Still, Nvidia stock is actually down 4.8% from its all-time closing high set Oct. 21, but Apple shares are down 6.7% over the same period, hence the flip in their market values. In its earnings report Thursday, Apple beat Wall Street expectations for revenue and adjusted profits, but its weaker outlook for the final period of 2024 disappointed investors.
Apple’s financial performance is still far more robust than Nvidia’s, with its $397 billion in projected revenue and $98 billion in net income trouncing Nvidia’s $120 billion in sales and $64 billion net profit, according to FactSet data blending results and average forecasts. But analysts expect Nvidia’s growth trajectory to continue, with its $122 billion in projected 2026 profits just a tick below Apple’s $124 billion.
"
200,https://www.forbes.com/sites/johnwerner/2024/10/26/3-big-differences--nvidia-is-poised-to-take-over-from-apple/,3 Big Differences – Nvidia Is Poised To Take Over From Apple,"Oct 26, 2024, 05:03pm EDT",John Werner,"Nvidia finally poked its head briefly above the stock value of the dominant Apple empire this week, before settling back down to market value of $3.47 trillion with Apple’s value at $3.52 trillion.
Reuters reporting shows how close this is, with a chart showing Nvidia’s rise against Apple and Microsoft, the other two dominant tech companies.
“The Silicon Valley chipmaker is the dominant supplier of processors used in AI computing, and the company has become the biggest winner in a race between Microsoft, Alphabet, Meta Platforms and other heavyweights, to dominate the emerging technology,” write Reuters reporters Sruthi Shankar and Noel Randewich. “Known since the 1990s as a designer of processors for videogames, Nvidia's stock has risen about 18% so far in October, with a string of gains coming after OpenAI, the company behind ChatGPT, announced a funding round of $6.6 billion.”
Microsoft now rests slightly below the other two, with around $3.18 trillion in value.
A look back shows that Nvidia also won out temporarily over the same competitors back in June of this year; prior to that, the firm’s stock price was under $100 per share, until it finally spiked early this year and continued to rise.
Sticking at around $140 per share, this giant is now poised to earn its crown as the most valuable company in the world.
What this represents for the market is a major change from one kind of technology to another.
You could express this three different ways:
You could say that Apple makes consumer-facing technology, while Nvidia makes ‘back end’ server/hardware technology.
You could say that Apple makes endpoint technology, where Nvidia makes supply technology.
Or you could say that Apple makes devices that you can use, where Nvidia makes hardware that allows AI to think.
With items likes A100 GPU, Nvidia is a dominant producer of AI hardware. And everybody wants a piece of this, from Microsoft and Blackrock, to all kinds of other stakeholders who are getting in on the AI revolution.
While Nvidia’s own chips are powerhouses in their own right, analysts show that many companies are investing in their own hardware in the form of custom chips known in the industry as application-specific integrated circuits or ASICs.
Some of those closest to the phenomenon cite companies like Broadcom (AVGO) and others as competitor companies that may rise as their clients look for ASIC technology.
However, Nvidia is also making its own play in the ASIC market, and will apparently open sores this technology.
“There has been speculation that fixed function application-specific circuits (ASICs) might one day eclipse NVIDIA’s GPU-centric approach,” writes Karl Fruend at Moor Insights and Strategy. “Now NVIDIA itself seems to have embraced this approach, albeit in a limited fashion, announcing its own ASIC technology for Deep Learning acceleration. In a surprising and bold move, the company also announced that it will open source this technology to enable others to build chips using this technology.”
Maybe that pivot will help them address one of the major threats to their dominance in the market.
In any case, it’s a timely piece of news, because what it shows is that AI is ascendant. We’re watching this technology blossom right before our eyes, in real time, and after a few years of relative obscurity, Nvidia seems to have won out – at least for now."
201,https://www.forbes.com/sites/daveywinder/2024/10/25/urgent-new-nvidia-security-warning-for-200-million-linux-and-windows-gamers/,Urgent New Nvidia Security Warning For 200 Million Linux And Windows Gamers,"Oct 25, 2024, 06:01am EDT",Davey Winder,"With more than 200 million gamers using Nvidia graphics to power their gaming experience across Linux and Windows platforms, security advisories need to be taken very seriously. When that advisory concerns no less than eight new high-severity vulnerabilities, only a total lamer gamer would ignore it. Here’s what you need to know about Nvidia security vulnerabilities CVE‑2024‑0117 through CVE‑2024‑0121.
Nvidia has published an advisory bulletin that details a total of eight high-severity common vulnerabilities and exposures, better known as CVE-rated security vulnerabilities. The vulnerabilities, impacting users of Nvidia graphics processing units across both Linux and Windows platforms, sit within the GPU display driver aand the virtual GPU software.
The reason for the urgency of this Nvidia security warning is, the company explained, because of the potential impact to users these vulnerabilities could unleash: code execution, denial of service, escalation of privileges, information disclosure and data tampering. If that sounds bad, it’s because it is.
Out-of-bounds memory vulnerabilities exist when a program attempts to read data from a different memory location than one within an allocated buffer. As such, they are among the most common security vulnerabilities discovered, but popularity should not be confused with little consequence. Most of the vulnerabilities outlined in this new Nvidia security advisory would appear to be in the user layer mode of the GPU display driver, and successful exploitation would allow an unprivileged attacker to cause what’s known as an out-of-bounds read leading to the impacts already mentioned.
The two vulnerabilities within the vGPU software are in the kernel driver and virtual GPU manager of all supported hypervisors. The vGPU kernel vulnerability is an improper input validation type compromising the guest OS kernel. The virtual GPU manager software vulnerability, meanwhile, enables a user of the guest OS to gain access to global resources.
“To protect your system,” Nvidia said, “download and install this software update through the NVIDIA Driver Downloads page.” The update to patch the vGPU vulnerabilities can be downloaded through the Nvidia licensing portal.
The Nvidia security updates for the GPU display driver in each Windows driver branch are shown in the following table; you can click through the image to see the complete original at Nvidia’s security bulletin site where the full Linux driver branch table is also available.
As with all such incidents where high-severity vulnerabilities are disclosed, all impacted users are advised to follow the Nvidia security team’s instructions and update now to ensure their systems are fully protected."
202,https://www.forbes.com/sites/tonyaevans/2024/10/22/sec-and-doj-back-nvidia-crypto-case-but-digital-chamber-fights-back/,"SEC And DOJ Back NVIDIA Crypto Case, But Digital Chamber Fights Back","Oct 22, 2024, 10:33am EDT",Tonya M. Evans,"SEC And DOJ Back Nvidia Crypto Revenue Lawsuit, Digital Chamber Defends
In a pivotal moment for corporate accountability within the cryptocurrency industry, a class-action lawsuit accusing Nvidia of concealing over $1 billion in GPU sales to crypto miners has gained significant support from both the U.S. Securities and Exchange Commission (SEC) and the Department of Justice (DOJ). The lawsuit, filed by investors in 2018, alleges that Nvidia misled shareholders by attributing its explosive growth to gaming demand, while downplaying the substantial revenue generated from cryptocurrency mining. Recently, the SEC and DOJ submitted amicus briefs urging the U.S. Supreme Court to allow the lawsuit to proceed, highlighting the importance of investor protection and corporate transparency. Meanwhile, the Digital Chamber of Commerce (TDC) has stepped in to defend Nvidia, filing its own amicus brief that calls for the reversal of a prior court ruling.
SEC and DOJ’s Support for Investor Protection
The SEC and DOJ’s involvement highlights the growing emphasis on corporate transparency in rapidly evolving sectors like cryptocurrency. In their amicus brief, U.S. Solicitor General Elizabeth Prelogar and SEC senior lawyer Theodore Weiman argued that the lawsuit is detailed enough to be revived and allowed to proceed to trial. According to DeCrypt, they contended that the case concerns Nvidia’s alleged failure to disclose crypto mining’s significant contribution to its revenue growth, falsely attributing the spike in GPU sales to gaming demand. “This evidence collectively supports the claim that [CEO Jensen] Huang knowingly misled investors about Nvidia’s exposure to crypto mining, meeting the standard for ‘scienter,’ or intent to deceive,” the DOJ and SEC argued.
The lawsuit hinges on claims that Nvidia concealed the full extent of its exposure to the cryptocurrency sector by downplaying how crypto miners were driving a large portion of GPU sales. DeCrypt also notes that Nvidia allegedly maintained a global database to track GeForce GPU sales to crypto miners, with this information coming to light through firsthand accounts from former Nvidia employees. One insider, dubbed “FE 1,” described how this database was used to track sales to miners, while another insider, “FE 2,” detailed how Nvidia's CEO, Jensen Huang, was directly involved in sales meetings where the impact of cryptocurrency on revenues was discussed. These insider accounts, combined with internal documents and a notable drop in Nvidia's revenue following the 2018 crypto crash, form the basis of the evidence supporting the claim.
The SEC previously fined Nvidia $5.5 million in 2022 for inadequate disclosures related to its fiscal year 2018, during which it allegedly failed to accurately declare how much of its revenue growth was driven by crypto mining. The company agreed to a cease-and-desist order but continues to deny the claims in the ongoing class-action suit.
The Digital Chamber’s Impact
Nvidia has consistently denied these allegations, asserting that its financial disclosures accurately reflected its business operations and that it was transparent about the risks and volatility of the cryptocurrency market. Nvidia’s defense has garnered significant support from the Digital Chamber of Commerce, a leading advocate for blockchain and cryptocurrency industries. In August 2024, the Chamber filed an amicus brief urging the Supreme Court to reverse a U.S. Court of Appeals for the Ninth Circuit decision that partially revived the case.
The Digital Chamber's brief argues that the plaintiffs' claims are speculative and fail to meet the stringent pleading standards of the Private Securities Litigation Reform Act of 1995 (PSLRA). According to the Chamber, the lawsuit relies on assumptions about the cryptocurrency industry that are not grounded in fact. “The plaintiffs’ case relies on 'expert' opinions based on unsupported assumptions about the cryptocurrency industry, constructing a theory disconnected from the facts of Nvidia’s business,” the brief states.
The Private Securities Litigation Reform Act and its Role
The PSLRA, enacted to prevent frivolous lawsuits, plays a central role in Nvidia’s defense. The law requires plaintiffs to provide detailed and specific allegations of fraud, rather than broad, speculative claims. According to the Digital Chamber, the lawsuit against Nvidia fails to meet these standards and could set a dangerous precedent if allowed to proceed.
Joshua B. Simmons, counsel for the Digital Chamber, emphasized that speculative lawsuits like this one could have a chilling effect on innovation, particularly in the cryptocurrency sector. “TDC’s insights into this case are shaped by its deep involvement in the cryptocurrency industry, where we see the potential harm speculative lawsuits could have on future innovation,” Simmons noted.
Broader Implications for Corporate Accountability and the Crypto Industry
The Nvidia lawsuit underscores the tension between two competing interests: the need for transparency to protect investors and the risk of stifling innovation in emerging markets like cryptocurrency. While the SEC and DOJ argue that Nvidia's actions misled investors and demand accountability, the Digital Chamber warns that speculative lawsuits could open the door to costly litigation that hinders the development of high-growth tech sectors.
As Nvidia faces the possibility of a legal ruling that could increase the scrutiny of cryptocurrency-related disclosures, the case may influence broader corporate governance practices in the tech industry. If the Supreme Court rules against Nvidia, it could signal a shift toward more stringent regulatory requirements for companies involved in cryptocurrency activities. Conversely, a ruling in Nvidia’s favor could reinforce the protections offered by the PSLRA, dissuading future speculative securities litigation.
What Lies Ahead
As the Supreme Court considers the fate of this case, the outcome will have far-reaching implications for both Nvidia and the cryptocurrency industry, as a whole. The SEC and DOJ’s support for the class-action lawsuit demonstrates the growing emphasis on corporate accountability, while the Digital Chamber’s defense of Nvidia highlights concerns about innovation in a fast-evolving market. Whatever the outcome, this lawsuit marks a critical juncture in the ongoing debate over how to balance investor protection with the need for innovation in the digital economy."
203,https://www.forbes.com/sites/dereksaul/2024/10/21/nvidia-ceo-jensen-huang-vaults-into-worlds-10-richest-for-first-time-ever/,Nvidia CEO Jensen Huang Vaults Into World’s 10 Richest For First Time Ever,"Oct 21, 2024, 02:23pm EDT",Derek Saul,"Nvidia CEO Jensen Huang leapt into the top 10 of Forbes’ real-time billionaire rankings Monday, a first for the former Denny’s busboy and waiter turned perhaps the most recognizable name associated with the 2020s generative artificial intelligence boom.
Huang’s net worth climbed $3.6 billion to $124.1 billion by market close, topping the fortune of former Microsoft CEO Steve Ballmer ($122.5 billion net worth), who traded places with Huang to become the world’s 11th-richest person.
The rise in Huang’s fortune came as shares of Nvidia rose 4.1% to a new all-time high of almost $144.
Huang’s roughly 3.5% equity stake in Nvidia accounts for almost all of his net worth, as Huang’s slice of the $3.5 trillion Nvidia is worth $122.2 billion.
Nvidia’s chief executive since the company’s inception in 1993, the 61-year-old Huang is by far the company’s largest individual shareholder even after selling $713 million worth of the stock this summer. A trio of current Nvidia board members associated with the company dating back to the early 1990s – Tench Coxe, Harvey Jones and Mark Stevens – are also billionaires. Nvidia is the leading designer of the custom AI technology needed for high-end AI applications developed by other big technology firms like Amazon, Google, Meta, Microsoft and Tesla. The Santa Clara-based Nvidia has about an 80% share in the rapidly growing AI accelerator market, according to recent Bank of America research. Well-known in video game circles for years, Nvidia’s graphics processing units became a hot commodity as they proved to be the leading currency for large-language models like the ones behind the viral OpenAI chatbot ChatGPT, leading Huang to declare demand is so “insane” for his company’s products that customers get “really emotional.”

Huang was worth just $20.6 billion as of 2022’s Forbes 400 list published two Septembers ago, meaning his wealth is up sixfold in two years time. The “insane” demand for Nvidia’s technology has translated into an explosion in Nvidia’s share price and financial performance, with its share price up more than 1,000% over the past two years and its current fiscal year profits are projected to increase 1,400% compared to the period ending Jan. 2023.
Ballmer, whose fortune stems from his 34-year tenure at Microsoft, told Forbes last week he’s poured more than $100 million into his not-for-profit company USAFacts aimed at arming the public with accessible government data.
"
204,https://www.forbes.com/sites/greatspeculations/2024/10/21/stocks-this-week-buy-grinder-and-nvidia/,Stocks This Week: Buy Grinder And NVIDIA,"Oct 21, 2024, 12:36pm EDT",Bill Sarubbi,"The overall market is likely to fall in this post-OPEX week. The average low in any year is October 27th, and that date is coming. Use this week to add to long holdings. It has been estimated that 60% of all S&P profits have been generated in the fourth quarter of any year.
Grinder runs social network and dating application for the lesbian, gay, bisexual, transgender, and queer (LGBTQ) communities. The stock has traced out a rectangle formation in the $10-$13 range. Rectangles have proven to be very reliable technical formations, especially rectangular lows. Note that the monthly cycle points up into yearend. The $16-$17 area is a target.
Daily Grinder Chart
Monthly Grinder Cycle
NVIDIASPDR Dow Jones Industrial Average ETF Trust is due to resume its uptrend. Over the last 25 years, the stock has risen 76% of the time in the month of November. From October 22nd to yearend, the shares have appreciated by the same amount. This is the fifth highest percentage in the S&P 100. NVIDIA has the highest expected return, almost 16%, of any of the constituents in this index. This stock is rated number one for relative strength. The shares may not be as strong as they were earlier in the year, but NVIDIA is likely to be a leader into yearend. The $160 area is a reasonable objective.
NVDA Daily, Weekly, Monthly
NVDA Annual Cycle"
205,https://www.forbes.com/sites/saibala/2024/10/21/nvidia-announces-partnership-with-aidoc-to-explore-healthcare-ai-adoption/,Nvidia Announces Partnership With Aidoc To Explore Healthcare AI Adoption,"Oct 21, 2024, 08:00am EDT","Dr. Sai Balasubramanian, M.D., J.D.","Nvidia announced today that it will partner with Aidoc, a startup focused on enabling healthcare organizations with artificial intelligence (AI) tools in the clinical setting, to develop a novel framework to promote and define AI adoption in the healthcare industry. The new framework will be entitled the “Blueprint for Resilient Integration and Deployment of Guided Excellence” (BRIDGE), and will provide guidelines which organizations can use to determine how best to integrate AI services into clinical workflows in order to ultimately improve patient outcomes.
The purpose behind the BRIDGE initiative is to help foster a structured approach to the vast array of AI services, systems and opportunities that currently exist. Given how quickly this space has evolved, many AI solutions have failed to take into consideration key aspects regarding integration, interoperability and cohesiveness. That is, fueled by incredible demand, many AI developers have started to “run” before learning to “walk”— leaving the ecosystem mired with numerous siloed and disparate products.
Now, as the industry is gaining some footing and the inherent value for AI is slowly being recognized, there are opportunities to tie the work together in a systematic and organized fashion.
Other such frameworks have started to emerge in recent years. For example, earlier this year, Microsoft, which has been incredibly involved in the healthcare AI space, announced that it would be expanding its work with the Trustworthy & Responsible AI Network (TRAIN) that it originally launched in March 2024. Another key initiative has been the Coalition for Health AI (CHAI), backed by industry titans such as Microsoft, Amazon, Google, Stanford Medicine and Mass General Hospital. CHAI’s vision is to “develop ‘guidelines and guardrails’ to drive high-quality health care by promoting the adoption of credible, fair and transparent health AI systems.”
Now, the BRIDGE framework is hoping to augment this space with a more tactical approach.
Demetri Giannikopoulos, Chief Transformation Officer at Aidoc, explains that the company “looks at AI as overall good for healthcare...we want to encourage innovators to continue to do this important work…especially by pulling in academic partners, innovators from all fields. The goal is to make an agnostic set of guidelines that people can use to help deploy their work... and moreover, help after deployment to really scale and provide value.”
The announcement is also an indicator of Nvidia’s growing interest in the space. Without a doubt, the company has invested billions already into the field indirectly and has a wide moat for providing services in the healthcare sector. For example, the company’s hardware and MONAI platform are supporting numerous organizations with their AI advancements. Additionally, I recently wrote about how Nvidia’s venture capital arm, NVentures, has been quietly investing tens of millions of dollars in healthcare startups. With its incredibly strong footprint in hardware, in addition to its expertise in scaling industry solutions, Nvidia is clearly trying to expand its healthcare work and portfolio. The BRIDGE framework provides yet another opportunity for the company to expand its presence in this industry.
Kimberly Powell, Vice President and General Manager of Healthcare at Nvidia, explains that BRIDGE provides an opportunity to codify the best processes and practices of AI implementation: “The last mile expertise that companies like Aidoc have are invaluable and can truly help others in this space as well...the best thing we [Nvidia] can do for startups is help them with technology and also connect them with industry partners to learn and leverage best practices and help them cover that last mile.”
Indeed, the evolution and advancement of this technology is unprecedented. Initiatives and frameworks like BRIDGE will ultimately be a way for organizations to learn and grow alongside one another in what will undoubtedly be a rapidly changing ecosystem in the coming decades."
206,https://www.forbes.com/sites/dereksaul/2024/10/18/nvidia-can-approach-5-trillion-valuation-with-generational-ai-opportunity-still-ahead-bofa-says/,"Nvidia Can Approach $5 Trillion Valuation With ‘Generational’ AI Opportunity Still Ahead, BofA Says","Oct 18, 2024, 03:50pm EDT",Derek Saul,"Nvidia stock has much more room to run up its all-time high share price set this week, argue Bank of America analysts, who just meaningfully upped their price target for the artificial intelligence poster child Nvidia due to a highly favorable growth horizon.
The Bank of America group led by Vivek Arya raised their Nvidia target from $165 to $190, indicating 37% upside from Nvidia’s $138 share price Friday, a fresh weekly closing high.
Arya and company argued a “generational opportunity” is still ahead for Nvidia in AI accelerators, the full-stack semiconductor systems necessary for generative AI applications of which Nvidia captures roughly 80% of the market as its big-ticket customers like Alphabet, Meta and OpenAI invest heavily in Nvidia’s graphics processing units (GPUs) to train large-language models.
Bank of America projects the total addressable market for this AI technology will grow from $45 billion in 2023 to $117 billion this year to $363 billion by year’s end, while Nvidia maintains a 75% market share, translating to a whopping $272 billion in AI computing revenues for Nvidia by 2030.
For reference, Apple generated $204 billion of revenue from iPhones in 2023.
Other positive catalysts named by Bank of America include significantly higher profit margins than its peers, with its free cash flow margin of roughly 50% over the next two years forecasted at twice as much as the average of the other five trillion-dollar American technology giants, and its “underappreciated” income streams outside of selling its custom GPUs, including lucrative partnerships with consultancy Accenture and Microsoft.
$4.7 trillion. That’s the Nvidia market capitalization implied by Bank of America analysts’ $190 price target. That’s much higher than the market value of any public company ever, a title held by Apple at $3.6 trillion.
Nvidia is the world’s second-most valuable company with a $3.4 trillion market cap. That’s a far cry from the roughly $10 billion valuation it held a decade ago and the sub-$300 billion market cap it boasted just two years ago. Nvidia’s stock market ascension followed the late 2022 release of OpenAI’s ChatGPT generative AI chatbot, which inspired significant earnings growth for the company as Nvidia’s graphics processing units are at the core of these AI applications. Nvidia’s sales and net income were 350% and 2,400% higher during the quarter ending in July than they were during 2022’s comparable period, respectively. Originally a company known for its role in video game graphics, Nvidia is still led by its founding CEO Jensen Huang, who is the 11th-richest person in the world thanks to his 3% stake in the company, according to Forbes’ calculations. Common valuation metrics indicate Nvidia stock is expensive relative to its fundamentals – it has the second-highest price-to-sales ratio, which compares a company’s market cap to its last 12-month revenues, on the S&P 500, but many remain bullish thanks to what Huang describes as “emotional” and “insane demand for Nvidia’s products."
207,https://www.forbes.com/sites/karlfreund/2024/10/15/nvidia-contributes-blackwell-and-ethernet-tech-to-metas-open-compute/,Nvidia Contributes Blackwell And Ethernet Tech To Meta’s Open Compute,"Oct 15, 2024, 12:30pm EDT",Karl Freund,"Many think of Nvidia as a closed ecosystem. But as AI transitions from fast chips to address a full-system challenge, Nvidia is helping drive an open industry.
Yes, CUDA is closed. Nvidia says it has to be closed in order to develolp an optimized software abstraction for new hardware. But systems and networking are becoming the new bottlenecks, and Nvidia is contributing its tech to the Open Compute Program to help the community solve the next set of bottlenecks that are driving cost and energy inefficiencies.  Let’s take a look:
Nvidia has delivered the first Blackwell systems to Meta and Microsoft, the first of what will become an avalanche of new AI hardware. So it should not surprise anyone that much of this year’s Open Compute Program summit in the Bay Area will embrace the new GPU and the accompanying networking that unlock its potential.
Nvidia has long been a contributor, and benefactor, of the OCP, which Facebook started over 10 years ago, providing reference designs for GPU boards such as the HGX-H100 Baseboard which is now used in the majority of AI installations.
Nvidia used the OCP event to tout Blackwell and networking. NVSwitch and NVLink are perhaps, along with Nvidia software, the company’s most significant differentiators. To refresh everyone’s memory, NVSwitch connects 72 GPU’s into a GPU fabric, so the software and AI models just see one massive GPU. This helps improve performance, and simplifies the job for AI developers.
To enable consumers and their preferred system OEMs (Supermicro, Dell, HPE, Lenovo, etc.)  to deliver an NVL72 DGX class system, Nvidia has open sourced the NVL72 rack, the MGX compute tray, and the Nvidia Switch tray.  This will be a huge contribution for OEMs, as the NVL72 rack would be difficult if not impossible to replicate. It includes 5000 copper wires, consumes 120 KW of power at 1400 amps, switches with telemtry for congestion control and can support models up to 27 Trillion parameters.
In an excellent example of open systems dynamics at work, Meta took the NVL72 specifications Nvidia has contributed, and modified it for their specific data center requirements, and then contributed the result, called Catalina, back to the community to use. Cool!
When people refer to the Nvidia Ecosystem, it is hard to fully comprehend how extensive that community has become. In addition to the millions of developers, scientists and research institutions that are part of that ecosystem, scores of OEMs, ODMs, and data center infrastructure providers use and sell Nvidia hardware and software to power the AI revolution.  Oh, and Graphics.  That too ;=).
Oh, yeah: Blackwell is now in full production.  That means 250,000-300,000 unit in the fourth quarter, and $5 billion to $10 billion in revenue and some 750,000 to 800,000 units in Q1. And Nvidia is expecting to sell DGX B200  8-way servers for a half million a piece.
‘nuff said.
Disclosures: This article expresses the author's opinions and
should not be taken as advice to purchase from or invest in the companies mentioned. Cambrian-AI Research is fortunate to have many, if not most, semiconductor firms as our clients, including Blaize, BrainChip, Cadence Design, Cerebras, D-Matrix, Eliyan, Esperanto, Flex, GML, Groq, IBM, Intel, Micron, NVIDIA, Qualcomm Technologies, Si-Five, SiMa.ai, Synopsys, Ventana Microsystems, Tenstorrent and scores of investment clients. We have no investment positions in any of the companies mentioned in this article and do not plan to initiate any in the near future. For more information, please visit our website at https://cambrian-AI.com."
208,https://www.forbes.com/sites/dereksaul/2024/10/15/why-is-nvidia-stock-down-5-blame-a-dutch-firms-technical-error/,Why Is Nvidia Stock Down 5%? Blame A Dutch Firm’s ‘Technical Error’,"Oct 15, 2024, 03:17pm EDT",Derek Saul,"Fresh off of a record closing high, Nvidia stock slid Tuesday as part of a semiconductor chip stock slump with an unusual impetus: A less-than-stellar earnings release from a Dutch company which accidentally came out earlier than expected.
Nvidia shares fell 4.5% in Tuesday trading, wiping out some $155 billion in market capitalization for the California firm, more than AT&T’s $154 billion total market value.
Nvidia stock had its steepest daily loss since Sept. 3’s 10% dip, which marked the largest erosion of one company’s market cap in a single day in history at $279 billion.
Shares of several other firms involved in designing and manufacturing the semiconductor technology powering the generative AI explosion also tumbled, with Advanced Micro Devices, Arm Holdings and Broadcom stocks sliding at least 3.5%.
The stateside chip stock dip came after Dutch firm ASML, which makes much of the equipment used to make the high-tech AI chips and works closely with Nvidia, reported earnings a day earlier than expected due to what the company attributed to a “technical error.” The unanticipated release included a warning of “more gradual” growth for ASML, which said it expects 2025 revenues to come in between $32.7 billion to $38.1 billion, well below consensus analyst forecasts of $39.1. New York-listed American depository receipts of ASML stock lost 17.5% during Tuesday trading, the worst loss since March 2020 and the second-worst day of the last decade for the stock.
With a $3.2 trillion market cap, Nvidia remains the second-largest company in the world, trailing only the $3.5 trillion iPhone maker Apple, which hit its highest ever share price Tuesday morning. After finishing 2023 as the S&P 500 index’s best-performing stock with a blistering 239% gain, Nvidia is again the biggest winner so far in 2024, gaining 167%. The surge comes as Nvidia reports exponential earnings growth thanks to “insane” demand for its graphics processing units, the semiconductor technology which powers generative AI applications at companies like Meta and Tesla. Jensen Huang, Nvidia’s CEO since he cofounded the company in 1993, is the world’s 11th-richest person thanks to his 3.5% stake in Nvidia, according to Forbes’ estimates. Huang’s net worth fell $5 billion due to Tuesday’s dip.
"
209,https://www.forbes.com/sites/howardhyu/2024/10/15/how-eli-lilly-outpaces-nvidia-here-is-one-chart/,How Eli Lilly Outpaces Nvidia—Here Is One Chart,"Oct 15, 2024, 06:49am EDT",Howard Yu,"Move over, AI. The real bubble brewing isn’t in artificial intelligence—it’s in weight loss. Wall Street has been hyperventilating over Nvidia and OpenAI’s ChatGPT. But another titan has started to inflate. Ladies and gentlemen, let’s talk about Eli Lilly.
There’s something curious about the chart below. Nvidia, the poster child of the AI revolution, has seen its share price skyrocket. But Eli Lilly’s stock has been on an even wilder ride.
It’s not just its share price that’s eye-popping. Eli Lilly’s price-to-earnings (P/E) ratio, a measure of how much investors are willing to pay for each dollar of earnings, has also ballooned up into uncharted territory. What’s fueling the meteoric rise? Obesity.
If data is the new oil and AI is the new energy, then losing weight is the final moonshot. Eli Lilly has been in the limelight among investors thanks to its obesity drug, Mounjaro. But here’s the problem: Eli Lilly isn’t the only game in town.
Enter Novo Nordisk and its heavyweight contender, Ozempic. Eli Lilly doesn’t own the obesity market. Not the way Google dominates the search market. Or Microsoft dominates enterprise software. Obesity drugs are a duopoly at best. There’s Eli and there’s Novo. Just like Pepsi vs. Coca-Cola.
Despite this, investors seem to have made up their one-track mind. Eli Lilly’s P/E ratio is more than double that of Novo Nordisk. Meanwhile, other pharmaceutical heavyweights like Roche, J&J, and AstraZeneca are trading at industry-average P/E ratios of around 30. What’s going on?
Let’s channel the wisdom of Peter Lynch. He’s the legendary fund manager who penned One Up on Wall Street. He told us how much he had avoided hot stocks. The higher something climbs, the harder it will fall, he believed.
In Lynch’s world, a high P/E ratio isn’t inherently bad. But it does mean all the high growth prospects have already been priced into the current share price. And for Eli Lilly to trade at double that of Novo Nordisk, it means investors are expecting Lilly to grow twice as fast in the coming years in a boxing ring situation against Novo. And Lilly would grow three times faster than the industry average as a drugmaker.
Why are investors and analysts so smitten? I think it’s the collective escalation of commitment—a fancy way of saying that once Wall Street falls in love with a stock, it tends to double down. Analysts who have previously issued “buy” recommendations may be reluctant to backpedal. Institutional investors who have bought in, if they were to unload, would have a lot of explaining to do. And if three national magazines have fawned over the CEO, no one is going to stick their neck out singing another tune until something clearly is not working.
Maybe one more sinister boost came from the FDA. It recently issued guidelines forbidding pharmacies from compounding “me-too” products that mimic the branded obesity drugs. During the shortages, pharmacies were allowed to manufacture alternatives with the same active ingredient, essentially allowing them to infringe on patents temporarily. Then last week, the FDA abruptly declared that the shortages of obesity drugs are over. So no more copycat stuff. Even though, of course, there are still lingering supply chain issues. Patients reportedly cannot access the needed drugs. The branded version is far more expensive, so those who had been able to afford the medication, now may no longer.
I can’t tell for sure, but the FDA surely appears to be cozying up to Big Pharma. It looks as if it’s out to protect incumbents before they’ve even sorted out their operational hiccups. I don’t know if the government is looking out for consumer interest in broadening access or saving the corporate bottom line. Bad optics, that’s all I’ll say.
For long-term success, Eli Lilly—or any company, for that matter—needs to perform in the short term while transforming for the long haul. That means sorting out current supply chain issues, meeting market demand, and investing aggressively in the next technological platform.
Remember Angry Birds? Kids and adults were all playing until suddenly they weren’t. Being a one-hit wonder is a risky business model. Every novelty wears out eventually.
Obesity drugs might be the hot ticket now, but patents will expire, competitors can catch up, and markets always move on. One of the hardest but most crucial strategies for a company riding high is to temper market expectations. It might seem counterintuitive—why would you want to downplay your success? Because you want to create room to exceed those expectations, which is far better than the alternative. Overdeliver, underpromise. Expectation management 101.
Mark Zuckerberg provided a masterclass in this. When Apple changed iPhone privacy settings, Facebook’s ad revenue slowed. Instead of sugarcoating the situation, Zuckerberg came clean. In fact, he screamed and shouted about how bad the situation would be. He tempered Wall Street’s expectations. That gave time for Zuckerberg to build better predictive algorithms. Fast-forward a few years, and Meta is back on track. And he has even had the spare money to invest in the Metaverse, then the beautiful Ray-Ban goggles, and now Gen AI video software for Facebook users.
Same thing for Netflix and Spotify. They were both once go-go stocks that went down and made a second comeback with a more realistic P/E. The second time, they came back with strong earnings and stronger revenue. Not just an inflated P/E.
All bubbles eventually burst. The question isn’t if, but when. At the end of the day, sustainable growth beats a fleeting high every time. You can also sleep better at night. And as history has shown us, the bigger the bubble grows, the louder it bursts."
210,https://www.forbes.com/sites/tylerroush/2024/10/14/nvidia-stock-pops-3-as-shares-pace-record-with-company-closing-in-on-apple/,Nvidia Stock Pops 2% To Closing Record—As Company Closes In On Apple,"Oct 14, 2024, 11:12am EDT",Ty Roush,"Shares of Nvidia increased by more than 2% Monday, ending the chip maker’s day at a record close as the company is once again on the cusp of becoming the world’s most-valuable company ahead of Apple.
Nvidia’s shares increased to $138 at market close Monday after reaching as high as $139.60, eclipsing a previous record of $135.58 on June 17 and pacing just behind the stock’s intraday high of $140.76 set on June 19.
The company’s market valuation is now just over $3.4 trillion, as Nvidia trails Apple ($3.5 trillion) as the world’s most valuable firm.
Nvidia’s stock is now up 19% so far in October, after CEO Jensen Huang touted consumer demand for the company’s new Blackwell chip, saying the number of requests for the new AI chip is “insane.”
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
Huang’s fortune swelled by $3.2 billion Tuesday, ranking him as the 11th-wealthiest person in the world with a net worth of $120.9 billion, according to our latest estimates. Nvidia’s chief executive—who holds about 861 million shares—has never ranked among the world’s 10 richest people and trails former Microsoft CEO Steve Ballmer ($123.4 billion). Huang, who has served as Nvidia’s CEO and president since co-founding the company in 1993, has seen his net worth increase by about $102 billion since 2019, when he ranked as the 546th-richest.
A selloff of Nvidia’s shares in recent months has been erased, after the company surpassed Microsoft and Apple to become the world’s most valuable company in June. The decline came amid a delay for the company’s Blackwell chip, which was reportedly caused by engineering issues, though Morgan Stanley analysts said last week orders for the chip are now “booked out” for next year. Last month, Huang boasted about demand for Nvidia’s products being “so great” while suggesting customers were “tense” to acquire them. Analysts expected Nvidia to report $125 billion in sales and $67 billion in net profits for its fiscal year ending in January, according to FactSet."
211,https://www.forbes.com/sites/tylerroush/2024/10/08/jensen-huangs-net-worth-adds-4-billion-as-nvidia-shares-jump-4/,Jensen Huang’s Net Worth Adds $4 Billion As Nvidia Shares Jump 4%,"Oct 08, 2024, 04:56pm EDT",Ty Roush,"Jensen Huang’s net worth swelled by over $4 billion after Nvidia’s shares closed in the green on Tuesday, as a days-long rally for the chip maker—which recently became the world’s second-largest company—adds to its chief executive’s fortune.
Shares of Nvidia closed at $132.89 after increasing by just over 4%, the highest closing price for the company since ending July 9 at $134.91.
Huang is Nvidia’s largest shareholder with 75.4 million shares and another 786 million shares held through various trusts and a partnership, he disclosed in a regulatory filing last month, and the value of his stake increased to $114.4 billion from $109.9 billion Tuesday.
Nvidia’s shares are up nearly 12% in October, a rally that has added about $12.1 billion to Huang’s fortune.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
$713 million. That’s the value of Nvidia shares Huang sold between June 14 and Sept. 12, according to recent Securities and Exchange Commission filings. The company previously disclosed Huang planned to sell up to 6 million shares through March 31, 2025, a threshold he reached nearly six months early."
212,https://www.forbes.com/sites/dereksaul/2024/10/07/nvidia-eclipses-microsoft-as-worlds-second-largest-company-as-stock-climbs-to-6-week-high/,Nvidia Eclipses Microsoft As World’s Second-Largest Company As Stock Climbs To 6-Week High,"Oct 07, 2024, 02:08pm EDT",Derek Saul,"Nvidia stock rallied Monday despite a broader down day on Wall Street for its big technology peers, boosting the artificial intelligence heavyweight to reclaim its spot as the second-largest company in the world amid the AI boom.
Nvidia notched its highest intraday share price since Aug. 26 at $130.64, with shares up about 4.2% by mid-afternoon.
The Silicon Valley firm accordingly registered its highest market capitalization since August, and its $3.19 trillion market value sent it past Microsoft ($3.07 trillion) as the world’s second-largest company for the first time in six weeks; Nvidia now only trails Apple’s $3.4 trillion valuation.
Nvidia stock’s strong start to the week came despite broader losses, as the S&P 500 slipped 0.6% as higher bond yields and oil prices ate into the equity boost from shifting U.S. monetary policy.
Shares of all five trillion-dollar tech companies other than Nvidia fell Monday, with Apple down 1.3%, Microsoft 0.7%, Google parent Alphabet 1.4%, Amazon 2.8% and Facebook parent Meta 0.9%.
Nvidia, which designs a full suite of custom AI technology intended for high-end applications like large-language models, benefitted Monday from fellow AI tech maker Super Micro Computer’s announcement that sales for its liquid cooling products deployed alongside Nvidia’s graphics processing units (GPUs) was strong this quarter. Another bullish analyst note also boosted shares, as Melius Research analyst Ben Reitzes wrote to clients the “setup here is still pretty darn good” for Nvidia stock even as it enjoys an extended bounce. “Strong AI spending” from the likes of Microsoft, Alphabet and ChatGPT parent OpenAI should “catalyze consumption” of Nvidia’s core (GPUs), according to Reitzes. The demand optimism follows continued comments from Nvidia’s centibillionaire CEO Jensen Huang insisting demand for its AI products is “so great” and “insane.”
Monday is a continuation of a month-long period of outsized returns for Nvidia, as its 26% return since Sept. 6 trounces the S&P’s 6% gain during the period and its five trillion-dollar peers’ 7% average gain. Nvidia was briefly the world’s largest company in June, when its share price peaked at a still-record $140.76. Previously a fairly low-profile company known for its video game business, Nvidia became a Wall Street darling as its profits skyrocketed amid the generative AI boom. Nvidia’s valuation has increased tenfold over the past two years.
$21.9 billion. That’s how much in adjusted profits Nvidia is expected to report in its fiscal quarter ending this month, according to average analyst forecasts tracked by FactSet. That’s a more than 80% year-over-year increase in earnings before interest, taxes, depreciation and amortization (EBITDA) and a more than 1,000% jump from 2022’s comparable period.
"
213,https://www.forbes.com/sites/antoniopequenoiv/2024/10/03/nvidia-shares-jump-after-ceo-jensen-huang-notes-insane-demand-for-blackwell-ai-superchip/,Nvidia Shares Jump After CEO Jensen Huang Notes ‘Insane’ Demand For Blackwell AI ‘Superchip’,"Oct 03, 2024, 04:22pm EDT",Antonio Pequeño IV,"Nvidia chief Jensen Huang said during an interview with CNBC on Wednesday that demand for the tech giant’s Blackwell “Superchip” is “insane,” sending Nvidia shares up more than 3% on Thursday.
Nvidia shares reached as high as $124.26 on Thursday before cooling down to $122.80 at market close, a 3.3% jump.
The strong trading day materialized after Huang said there was “insane” demand for Nvidia’s upcoming Blackwell chip from companies such as Microsoft, OpenAI, Meta and other AI-adjacent firms, telling CNBC, “Everybody wants to have the most and everybody wants to be first.”
The Thursday stock jump continues a streak of Nvidia gains since early September, when the chip designer’s shares slumped to $102.83 despite reporting record revenues about a week prior.
Blackwell-based products will be shipped to cloud service divisions at Oracle, Amazon, Microsoft and Google late this year, according to Nvidia’s website.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
Stocks were broadly down on the day. The S&P 500 dropped as Nvidia’s rise was not enough to pull the tech-heavy Nasdaq out of the red, with the index closing down .17% on Thursday."
214,https://www.forbes.com/sites/saibala/2024/09/30/nvidias-venture-capital-arm-is-silently-investing-millions-into-healthcare/,Nvidia’s Venture Capital Arm Is Silently Investing Millions Into Healthcare,"Sep 30, 2024, 08:00am EDT","Dr. Sai Balasubramanian, M.D., J.D.","Nvidia has always sought to invest in valuable innovation. Its decades long dedication to building meaningful products and attempting to revolutionize the hardware industry has largely paid off; from its seminal work in gaming to its latest progress with chips that have been essential to the development of generative AI, the company has been successful without a doubt.
However, although most think of Nvidia as purely a hardware organization, the company is also rapidly investing resources in other arenas— and NVentures, which is the company’s venture capital arm, is leading the charge with numerous investments in healthcare.
The latest investment by the VC arm was announced earlier this month, which entailed a significant contribution, along with other investment partners, to Hippocratic AI, a company that is attempting to build a safety-focused large language model for healthcare. Specifically, Hippocratic AI aims to enable safe artificial Health General Intelligence (HGI) in order to “dramatically increase healthcare access and increase the scalability of healthcare professionals–resulting in a more equitable system of care for all.” The collaboration with Nvidia will enable the startup to “scale its production platform to deliver low-latency and long-context conversational capabilities…. [including] several techniques tailored for real-time, long-context conversational AI, including prefix caching, speculative decoding, and prefill/decode disaggregation.”
Nvidia’s Vice President of Healthcare, Kimberly Powell, explains, “Generative AI will expand the healthcare industry and its ability to serve the growing demands of patient care, and Hippocratic AI is putting the technology to work to increase access to healthcare..the company’s safety-focused approach uses advanced Nvidia technologies to make personalized, real-time patient interactions more natural and capable, helping build trust among patients and clinicians alike.”
NVentures has invested in many more potentially revolutionary companies. Another investment has been in Moon Surgical, a French company that is working on developing an advanced and cutting-edge robotic surgery system. Robotic surgery systems themselves are not new; in fact, Intuitive Surgical’s da Vinci robot system is one of the most widely used platforms in the world and has revolutionized the field of minimally invasive surgery. Moon Surgical, which in fact has a founder of Intuitive Surgical on its Board of Directors, wants to redefine the future of the operating room by enabling efficiency and adaptive techniques. The partnership with Nvidia may unlock numerous capabilities, as it was already working with the company by leveraging its Clara Holoscan platform. Anne Osdoit, CEO of Moon Surgical mentions, “NVIDIA has all the hardware and software figured out, with an optimized architecture and libraries…Clara Holoscan helps us not worry about things we typically spend a lot of time working on in the medical-device development cycle.”
Another investment example is in CHARM Therapeutics, which is focused on leveraging deep-learning capabilities to accelerate drug discovery. The partnership will enable CHARM to leverage Nvidia’s capabilities to use advanced machine learning to identify novel protein folding methods and thereby, enable the development of new therapeutics. Indeed, if successful, this provides Nvidia and CHARM to compete with the likes of other technology giants that are similarly interested in the intersection of computation science and biology, such as with Alphabet and Google DeepMind’s work on AlphaFold 3, or Meta’s work with the ESM Atlas.
NVentures is even getting involved with the field of medical imaging, realizing that this is one of the most data rich aspects of the healthcare industry. The company has invested in Flywheel, which is attempting to leverage AI to “turn complex imaging data into analysis-ready datasets for accelerated research and AI development.” As Mohamed “Sid” Siddeek, Corporate Vice President and Head of NVentures states, “The application of AI has led to the discovery of new drugs, identifying patterns in disease, and improvements in patient care…Flywheel uses AI to unlock the value within medical imaging data, signaling the continued benefit of applying AI across the healthcare industry.”
Most of the companies that NVentures has invested in, including these startups, provide a promising outlook for their respective technologies and their potentially expansive impact in healthcare. Moreover, with Nvidia as a collaborative partner, these companies will benefit significantly from the decades of experience that the technology and hardware giant brings to the table with regards to advanced computing. Congruently, for Nvidia, these investments represent opportunities to truly broaden the company’s already massive impact in the field of artificial intelligence and technology in general."
215,https://www.forbes.com/sites/janakirammsv/2024/09/30/nvidia-acquires-octoai-to-dominate-enterprise-generative-ai-solutions/,Nvidia Acquires OctoAI To Dominate Enterprise Generative AI Solutions,"Sep 30, 2024, 01:38am EDT",Janakiram MSV,"In a bold move that solidifies its position as the leader in artificial intelligence infrastructure, Nvidia has acquired OctoAI, a Seattle-based startup specializing in generative AI tools. This $250 million deal marks Nvidia's fifth acquisition in 2024, underscoring the company's aggressive strategy to build a comprehensive end-to-end generative AI stack for enterprises.
Formally known as OctoML, the company was founded in 2019 as a spinoff from the University of Washington's Apache TVM project. It made significant strides in optimizing AI model performance across various hardware platforms. The startup's core technology focuses on making AI hardware more accessible to developers by providing a hardware-agnostic software layer that simplifies the deployment and scaling of AI models.
OctoAI's evolution from OctoML to a key player in generative AI marks a strategic pivot that reshaped its market position. Initially focused on AI model optimization, the company, under CEO Luis Ceze's leadership, recognized generative AI's transformative potential for businesses. This shift led to the development of OctoStack, a comprehensive solution for deploying generative AI models across various environments.
OctoAI's most recent offering emphasizes a developer-first approach, enabling non-experts to leverage large language models easily. Key features include private model deployment, customization support for popular models like Meta's Llama and Stable Diffusion and significant performance optimizations. The pivot has positioned OctoAI as a leader in secure, enterprise-grade AI deployments, attracting a diverse client base from Fortune 500 companies to startups. By offering substantial speedups and cost savings compared to DIY solutions, OctoAI has established itself at the forefront of the AI revolution in business technology.
The acquisition by Nvidia comes at a crucial time when enterprises are grappling with the complexities of implementing and scaling generative AI solutions. OctoAI's current offerings include a cloud platform that enables developers to deploy and run AI models with high performance and cost efficiency. Their technology supports multiple chip architectures, including those from Nvidia's competitors like AMD and Intel, making it a versatile solution for businesses looking to leverage AI without being tied to a single hardware vendor.
This hardware-agnostic approach aligns perfectly with Nvidia's ambition to deliver an end-to-end generative AI stack. By incorporating OctoAI's technology, Nvidia can now offer enterprises a more flexible and scalable solution for AI deployment, regardless of the underlying hardware infrastructure. This move is particularly strategic as it allows Nvidia to expand its reach beyond its own GPU ecosystem and capture a larger share of the enterprise AI market.
The synergy between OctoAI's offerings and Nvidia's existing AI portfolio is evident. Earlier this year, the two companies announced a partnership to optimize Nvidia's Inference Microservices, NIM, using OctoAI's compiler technology. This collaboration laid the groundwork for the acquisition, demonstrating the potential for integrating OctoAI's innovations into Nvidia's broader AI ecosystem.
Nvidia's acquisition strategy in the AI space has been both aggressive and strategic. In March 2024, the company acquired Run:ai, an Israeli startup specializing in AI infrastructure orchestration. This earlier acquisition complemented Nvidia's hardware offerings by providing sophisticated software tools for managing and optimizing AI workloads in complex enterprise environments.
The combination of Run:ai's orchestration capabilities and OctoAI's model optimization technology significantly strengthens Nvidia's position in the enterprise AI market. Together, these acquisitions enable Nvidia to offer a comprehensive solution that addresses the entire AI lifecycle, from model development and optimization to deployment and scaling across diverse hardware environments.
The acquisition also brings valuable talent to Nvidia's ranks. OctoAI's team of AI experts, including its founders, who have deep expertise in machine learning compilers and hardware optimization, will bolster Nvidia's research and development capabilities. This infusion of talent is critical as the company continues to push the boundaries of AI technology and maintain its competitive edge in a rapidly evolving market.
However, the acquisition is not without challenges. OctoAI's existing partnerships with Nvidia's competitors, including AWS, AMD and Qualcomm, could pose integration hurdles. Nvidia will need to carefully navigate these relationships to maintain the hardware-agnostic appeal of OctoAI's technology while integrating it into its own ecosystem.
Moreover, the acquisition may also face regulatory scrutiny, given Nvidia's dominant position in the AI chip market. The company's growing influence across the AI stack could raise concerns about market concentration and potential anticompetitive practices. Nvidia will need to demonstrate that its acquisitions and integrations benefit the broader AI ecosystem and do not stifle innovation or competition.
Despite these challenges, the potential benefits of the OctoAI acquisition for Nvidia and its enterprise customers are significant. The integration of OctoAI's technology into Nvidia's AI platform will likely result in more efficient and cost-effective AI deployments for businesses across various industries. This could accelerate the adoption of generative AI in enterprise settings, driving innovation and productivity gains.
Looking ahead, the OctoAI acquisition positions Nvidia to capitalize on the growing demand for industry-specific AI solutions. OctoAI had plans to introduce more vertical-specific offerings, including in healthcare, which aligns with Nvidia's strategy to penetrate key sectors with tailored AI solutions. This focus on industry-specific applications could open new revenue streams for Nvidia and further entrench its position as the leading provider of enterprise AI infrastructure."
216,https://www.forbes.com/sites/greatspeculations/2024/09/24/nvidia-can-pay-50-more-for-intel/,Nvidia Can Pay 50% More For Intel,"Sep 24, 2024, 07:30am EDT",Trefis Team,"Mobile chipset specialist Qualcomm approached Intel regarding a potential takeover, per a report in The Wall Street Journal. However, Nvidia would be a far better suitor, with the ability to pay up to 50% more for Intel. Why? Nvidia is worth close to $3 trillion. Intel is valued at about $95 billion, and Qualcomm is worth about $190 billion. Nvidia has close to $35 billion in cash, and Qualcomm has less than $10 billion. Intel’s 50+ years of chip design, manufacturing excellence, and CPU market share are worth way more in Nvidia’s hands. Simply put, Nvidia can easily sprinkle and infuse Intel’s products with its magic and momentum to take share back from AMD in the servers and PC markets. Think of it this way - Nvidia’s AI-powered chips for your PC. This becomes compelling when you consider Nvidia is already a supplier of GPUs for PCs and accelerated computing chips for servers. Although there would be significant regulatory hurdles for Nvidia to make this happen and there have been no real announced deal talks with Nvidia so far, we believe bankers might see a big opportunity. We describe our thinking and numbers below.
Intel makes for an attractive acquisition target at this juncture
The stock has fallen over 55% this year and by nearly 70% from its 2021 highs, amid market share losses and multiple manufacturing-related missteps. Despite this, Intel is showing signs of a recovery - its foundry business has notched up high-profile customers including Amazon’s AWS while its upcoming PC, data center CPUs, and accelerated computing processors look promising. Intel is getting serious about its cost cuts - aiming to slash costs by as much as $10 billion by next year.
Now the decrease in INTC stock over the last 3-year period has been far from consistent with annual returns being considerably more volatile than the S&P 500. Returns for the stock were 6% in 2021, -47% in 2022, and 95% in 2023. In contrast, the Trefis High Quality Portfolio, with a collection of 30 stocks, is considerably less volatile. And it has outperformed the S&P 500 each year over the same period. Why is that? As a group, HQ Portfolio stocks provided better returns with less risk versus the benchmark index; less of a roller-coaster ride as evident in HQ Portfolio performance metrics.
Little upside for Intel shareholders from a Qualcomm deal
Qualcomm may be interested in expanding its chip range and gaining a foothold in the server and PC markets, but it lacks the resources to drive Intel’s transformation. In contrast, we believe that Nvidia is potentially better positioned to leverage Intel’s assets. Nvidia has much deeper pockets and more resources to invest in Intel’s turnaround efforts and growth. Besides the stronger balance sheet, Nvidia already holds a significant presence in key markets where Intel competes, serving as a leading supplier of GPUs for PCs and accelerated computing chips for servers - two areas where Intel’s CPU business is also deeply entrenched. This could enable Nvidia to sell a broader range of products to existing customers. Given that Intel has also been pushing into the GPU space with its Gaudi 3 chips, Nvidia could potentially eliminate a competitor as well via a deal. While Nvidia has emerged as the face of the AI hardware revolution, this data center company has been growing its sales even faster than Nvidia and trades at a cheaper valuation.
Intel’s foundries would be valuable to Nvidia
Nvidia currently relies heavily on third-party foundries such as TSMC to produce its silicon. Admittedly, Intel has lagged behind TSMC in recent years in terms of technology. However, it has made considerable progress lately. Intel’s latest manufacturing technology, called 18A, delivers a sub-2-nanometer process node with volume production expected by the end of 2024 - roughly a year ahead of TSMC’s anticipated 2-nanometer process, N2P. This could potentially benefit Nvidia down the line, given its cutting-edge GPU designs. Beyond possibly lowering production costs, a deal with Intel could also secure supply chain stability. Why? TSMC, which handles nearly all of Nvidia’s high-end GPU fabrication, is based in Taiwan, making production vulnerable to geopolitical tensions with China. Intel, with its U.S.-based manufacturing, would mitigate that risk to a large extent. Granted, Nvidia could utilize Intel’s foundry services without buying the company, but at current valuations, an outright acquisition might make more sense.
Breaking down the numbers
Intel has been losing share in both the PC and server markets to rival AMD. Intel’s revenue has declined from about $72 billion in 2019 - before the Covid-19 surge - to levels of about $54 billion as of last year - a decline of about $18 billion, with both the client computing and data center segments seeing declines of about $8 billion each. Estimating that Nvidia’s AI technology expertise and momentum help Intel boost its client and data center revenues by about $6 billion each over the next three years, this would add up to incremental revenue of about $12 billion. With operating margins of about 30%, that would translate into incremental operating profits of about $3.6 billion. Additionally, there are bound to be some savings for Nvidia, as it eventually leverages Intel’s fabrication capabilities. Calculating another $4 billion boost in operating profits for the combined entity via manufacturing and other cost savings (about 5% of Nvidia’s projected operating income for this year) this would translate into a total of about $7.5 billion in additional profit. If we consider an EBIT multiple of about 15x - roughly half of Nvidia’s multiple - that’s an additional $112 billion in value. If Nvidia decides to share say 40% of that with Intel shareholders it can easily bid about $45 billion more - a premium of almost 50% to Intel’s current market price.
Overall, while we think Nvidia would be a better suitor, there’s no guarantee that Intel and its shareholders will want to be acquired at this point. With so many new developments underway Intel stock could have considerable upside. If the company executes well on its foundry plans and delivers compelling new CPU and GPU chips, Intel could see an upside of almost 3x upside. On the other hand, if it fails to execute, Intel stock could see a downside to $10
While investors have their fingers crossed for a soft landing for the U.S. economy, how bad can things get if there is another recession? Our dashboard How Low Can Stocks Go During A Market Crash captures how key stocks fared during and after the last six market crashes.
Invest with Trefis Market Beating Portfolios
See all Trefis Price Estimates"
217,https://www.forbes.com/sites/petercohan/2024/09/25/why-nvidia-stock-could-lag-behind-a-key-ai-energy-company/,Why Nvidia Stock Could Lag Behind A Key AI Energy Company,"Sep 25, 2024, 03:31pm EDT",Peter Cohan,"Nvidia stock has soared 156% in 2024. Meanwhile, shares of Constellation Energy — a provider of nuclear power services and owner of the Three Mile Island facility in Pennsylvania that suffered an accident in March 1979 — have risen 129%.
Constellation — which recently signed a 10-year agreement for Microsoft to pay the nuclear energy leader about $16 billion, according to Seeking Alpha — could be a better investment than Nvidia. Here are three reasons:
One downside to consider is that analysts’ price targets fully capture Constellation’s upside, according to TipRanks.
The demand for energy to operate data centers is forecast to rise as Nvidia revenue growth slows. U.S. power demand has remained roughly unchanged between 2014 and 2024, noted TheStreet. However overall power demand in data centers is projected to surge “160% by 2030,” according to a May 2024 Goldman Sachs report.
To be sure, demand for AI-related hardware and software is also forecast to grow rapidly. More specifically, Bain estimates the total addressable market will increase by 40% to 55% annually, ranging between $780 billion and $990 billion by 2027, according to Barron’s.
While Nvidia expects to grow faster than the industry, the AI chip designer’s revenue growth is slowing down. Nvidia’s revenues grew in a range of 206% to 265%, between the second quarter of 2023 and the first quarter of 2024, according to Investor’s Business Daily.
Nvidia is still growing rapidly — but at a lower rate. For instance, the AI chip designer’s revenue increased 122% in the second quarter of 2024 and the company forecasts 80% growth in the third quarter of 2024, I noted in a September post on Forbes.
Constellation Energy operates the largest U.S. fleet of nuclear power plants, which is the preferred electricity-generating fuel for tech companies, according to Seeking Alpha. In February 2022, Constellation was spun off from utility company, Exelon.
Constellation stock surged 22% on September 20 following the announcement of a 20-year deal with Microsoft to supply electricity from a $1.6 billion restart of the undamaged 835-megawatt nuclear Unit 1 of Three Mile Island — which “closed under economic pressure” in 2019, noted the Wall Street Journal.
The facility’s Unit 2 was shut down in 1979 after a partial core meltdown. The resulting five days of panic “heightened awareness of nuclear plants’ potential safety problems and contributed to a loss of enthusiasm for the industry that lasted decades,” the Journal reported.
The company’s latest financial report for the June 2024-ending quarter fell short of earnings and revenue expectations. However, Constellation Energy raised its earnings per share forecast for 2024.
Specifically, the company’s EPS of $1.68 was four cents below the consensus estimate while the company’s $5.48 billion in revenue fell $70 million short of analysts’ consensus forecast, noted TheStreet.
Constellation raised its adjusted EPS outlook for fiscal 2024 by 4.8% to $8 a share at the midpoint of the range — 19 cents more than the consensus, TheStreet reported.
Might this boost in Constellation’s earnings growth target foreshadow expectations-beating revenue growth?
Analysts lifted the company’s stock price target significantly in the wake of the company’s deal with Microsoft. Examples include:
Other analysts were bullish on Constellation. While Guggenheim's Shahriar Pourreza added “the hyperscaler demand for clean megawatts is real,” Mizuho's Maheep Mandloi indicated the arrangement “points to the need for 24×7 clean energy to meet data center needs,” MarketWatch noted.
To be sure, anti-nuclear sentiment — while negative at times — is arguably less pronounced than in the past. With state and federal governments urging utilities to use nuclear and renewables fuel to make electricity, Constellation has more support than before.
Moreover, since it requires “neither expensive battery storage nor quick-on extra natural gas capacity equal to 110% of renewable capacity,” nuclear has advantages over renewables, reported TheStreet.
It appears Wall Street still sees more upside to Nvidia stock. The 42 analysts covering the AI chip designer set an average price target of $152 — representing 24% upside, according to TipRanks, which notes Constellation’s average price target of $214 from 20 analysts makes the stock about 1% overvalued."
218,https://www.forbes.com/sites/tylerroush/2024/09/24/nvidia-valuation-approaches-3-trillion-again-as-jensen-huang-finishes-selling-shares-worth-more-than-700-million/,Nvidia Valuation Approaches $3 Trillion Again As Jensen Huang Finishes Selling Shares Worth More Than $700 Million,"Sep 24, 2024, 03:56pm EDT",Ty Roush,"Nvidia’s chief executive Jensen Huang finished selling millions of shares in the company, worth  more than $700 million, according to a regulatory filing Tuesday, sending the chip maker’s stock into the green.
Shares of Nvidia increased by 4% to over $121 as of around 3:50 p.m. EDT, increasing its market cap to roughly $2.97 trillion.
Nvidia last had a valuation above $3 trillion on Aug. 28, when shares dipped by over 6%.
Nvidia disclosed in a Securities and Exchange Commission filing earlier this year that Huang, 61, planned to sell up to 6 million shares through March 31, 2025—a threshold that Huang reached nearly six months early on Sept. 12, according to a new filing.
Huang sold about $713 million worth of Nvidia shares between June 14 and Sept. 12 at share prices as low as $91.72 and as high as $140.24.
Huang now holds 75.4 million shares and another 786 million shares through various trusts and a partnership after those sales, he disclosed in another regulatory filing, and he is Nvidia’s largest shareholder with a roughly 3.8% stake as of March.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
Huang is the 13th-richest person in the world with a net worth of $106 billion after it increased by about $4.2 billion Tuesday, according to our latest estimates. He’s added about $102 billion to his fortune since 2019, when was ranked the 546th-richest, and about $85 billion since last year.
Huang, Nvidia’s chief executive and president since co-founding the chip maker in 1993, has benefited from a surge in the company’s stock in recent years. Nvidia briefly became the world’s most valuable company in June after jumping to a market cap of $3.33 trillion, surpassing Apple and Microsoft as the firm rode a rising demand for artificial intelligence. Earlier this month, Nvidia lost about $280 billion in market value after shares dropped by 8% amid a larger tech selloff, though the company later rallied after the Federal Reserve cut interest rates by 0.5 percentage points."
219,https://www.forbes.com/sites/investor-hub/article/nvidia-vs-amd-which-ai-stock-better-buy-now/,Nvidia Versus AMD: Which AI Stock Is The Better Buy Right Now?,"Sep 23, 2024, 02:36pm EDT",Catherine Brock,"A year ago, there was only Nvidia (NVDA) in the AI race. But now investors are looking at Advanced Micro Devices (AMD) as a potential competitor. Which AI stock should have a place in your portfolio? Let's lay out the details so you can decide.
Nvidia designs semiconductors for use in AI, gaming, autonomous vehicles, robotics, 5G networks and data centers. While chip sales drive revenue, the company also supports a full platform of related offerings that developers use to create, deploy, run and scale enterprise applications on Nvidia's hardware.
Nvidia established itself in the late 1990s by creating the graphics processing unit (GPU), primarily for use in gaming applications. In 2013, Nvidia CEO Jensen Huang saw a bigger picture for his company. He saw artificial intelligence as the future of computing—and believed Nvidia could lead that future. The company then began working on programmable GPUs to support complex computing operations.
Today, Nvidia is the recognized leader in AI infrastructure, selling far more AI chips than any competitor. The chip designer also maintains a customer list that includes the best tech stocks in the world.
AMD has historically battled with Intel for market share in central processing units (CPUs). Intel remains dominant, but AMD has been a formidable competitor—at least since CEO Lisa Su spearheaded the launch of the company's high-performance Zen architecture in 2017. Five years later, AMD's market capitalization passed Intel's for the first time.
AMD is now targeting the AI market and Su has made some key moves to compete with Nvidia. She quadrupled the company's R&D budget and acquired Xilinx, Nod.ai and ZT Systems. Xilinx makes programmable processors, Nod.ai develops open-source AI software and ZT Systems' designs and manufactures servers and related infrastructure for data centers.
AMD does not yet compete meaningfully with Nvidia. For the most recent quarter, AMD's data center revenues were less than 10% of Nvidia's. But as Intel struggles, AMD is positioning itself to secure a double-digit share in AI chips—which should be a sizable opportunity going forward.
Much more than breaking news, our diverse reporting digs deeper with unparalleled insights that empower you to make better informed decisions. Become a Forbes member and unlock unlimited access to cutting-edge strategies, actionable insights, and updated analysis from our network of leading finance experts.
Nvidia and AMD won't necessarily compete head-to-head for AI-related revenue. Let's look at their go-to-market strategies to understand why.
Nvidia's original AI strategy was to be first and best. And the company has capitalized on this approach. Huang's strategic vision combined with a proven product development process provides some insulation from even the best AI stocks.
As well, Nvidia doesn't just make AI chips. The company maintains an entire platform around those chips, including optimized software and libraries, to support application development from start to finish. This platform approach is another competitive advantage that could make customers reluctant to switch to another provider.
AMD has played catch-up with Intel for years, and the company will likely use similar tactics to take a piece of the AI market from Nvidia. Providing better value is the crux of the strategy. AMD can do that by offering more performance for the price. That could mean similar performance at a lower price or slightly lower performance with a much smaller price tag.
Providing and pitching a comprehensive AI computing platform has been a lesser focus for AMD. However, the company has taken steps to broaden its offering. The recent acquisition of ZT Systems, for example, enables AMD to deliver a full set of computing and storage resources required to power AI data centers.
Nvidia's trailing 12-month revenues sum to about $96 billion, which is nearly four times more than AMD's $23 billion. The earnings comparison isn't quite as lopsided, with Nvidia delivering diluted EPS of $2.13 to AMD's $0.84.
Both companies report AI-related revenues within their data center divisions. Nvidia's data center revenue in the most recent fiscal quarter was $26.3 billion, up 154% from the prior year. AMD produced data center revenue of $2.8 billion, up 115% year over year.
The table below shows market capitalization, forward P/E ratio, price/book ratio, PEG and stock price upside for Nvidia and AMD.
Table data sources: Yahoo Finance, Market Beat.
As you can see, Nvidia is a much bigger enterprise. Its market cap of $2.89 trillion makes it the world's third-largest public company, ahead of Alphabet and Amazon. Nvidia's valuation metrics, particularly the price/book ratio, are also well higher than AMD's. However, the upside according to analyst price targets is comparable.
You may also want to see what we think Nvidia will look like in five years.
Nvidia has a dominant market share position plus a reputation as the AI chip leader. AMD is working to establish itself as a solid second option.
Fortunately for AMD, the AI chip market is predicted to be large enough to be lucrative for two companies. Su has predicted global spending on AI accelerators to reach $400 billion by 2027. If AMD could capture 20% to 30% of that, the company's data center revenues could increase sevenfold.
It's also likely that the tech companies investing in AI infrastructure will want more than one supplier and more than one software stack. AMD's development software, ROCm is an open-source solution, while Nvidia's CUDA is proprietary. CUDA has a strong reputation, but some projects may require the flexibility of AMD's open-source solution.
Nvidia's AI offering includes:
AMD's AI line-up includes:
Discover more in-depth insights, entrepreneurial advice and winning strategies that can propel your journey forward and save you from making costly mistakes. Elevate your journey by becoming a Forbes member. Unlock unlimited access to premium journalism plus exclusive members-only events gathering leading business minds that shape tomorrow.
Multiple research companies and AMD's CEO have predicted strong double-digit annual growth for AI infrastructure spending over the next several years. The growth opportunity is somewhat different for our two companies, however. In short, Nvidia will be playing defense, while AMD plays offense.
With a running start in the AI space, Nvidia now owns an estimated 80% market share. The quick rise to the top created extreme growth in revenues and profits, as tech companies rushed to build out their AI capabilities. Unfortunately for Nvidia, the spending may moderate just as competitors come to market with viable alternatives.
Nvidia can find growth, however, by doing what it does best: pushing the limits of what high-performance computing can do in terms of speed and efficiency. Improving its platform to bring AI computing to a wider market could be another source of growth.
The company won't be complacent under Huang's direction. Nvidia has recently shortened its product refresh cycle from two years to one year—presumably to keep pace with AMD's update cadence.
AMD has more to gain and less to lose than Nvidia, which is the more interesting position to hold.
AMD chips are currently cheaper and about 80% as powerful as Nvidia's according to AI software firm MosiacML. After testing AMD MI250 alongside Nvidia A100 last year, MosiacML CTO Hanlin Tang believes AMD can close the performance gap. That will be AMD's path to solid market share and revenue gains, especially if the company stays competitive on price and can produce enough to meet demand.
Both companies will have to manage supply constraints, regulations that limit the revenue opportunity in China as well as rising competition. But they also face specific challenges related to where each stands in its AI growth trajectory.
Nvidia's biggest challenge will be defending its position. The sheer size of the market ensures fierce competition as GPU makers position themselves for a piece of AI spending.
To keep from losing market share, Nvidia will need to innovate and execute product rollouts flawlessly. Ongoing improvements in speed and efficiency will support higher spending even as the initial AI buildout slows.
In the absence of impressive product upgrades, Nvidia could face share losses as spending moderates. That would be a revenue headwind, which likely won't sit well with Nvidia investors.
AMD's primary challenge is establishing itself as the best second option in AI, while integrating four acquisitions made in the last two years. The acquisitions should support a more favorable competitive position, but it won't be enough to overtake Nvidia's dominance.
So, the big risk for AMD is missing its target and ending up competing for the third or fourth position in the industry. Supply constraints or strategic, product or execution errors could open the door for another player like Intel to win market share. For more information, see Nvidia vs. Intel.
Analysts are optimistic about Nvidia stock and AMD stock. Both companies have an average rating of strong buy. With 40 analysts reporting on Nvidia, there are 20 strong buys, 17 buys and three holds. No one is recommending investors sell their NVDA positions.
The 30 ratings for AMD include 15 strong buys, 11 buys and four holds. As with NVDA, analysts don't see a case for selling AMD currently.
To evaluate market sentiment, we can look at NVDA and AMD stock price trends relative to the broader market. NVDA has outperformed AMD and the S&P 500 by a wide margin over the last six months. Nvidia is up nearly 30%, the S&P 500 grew 10% and AMD has fallen 14%.
Performance in the last month, however, shows Nvidia as the loser. The company lost 10%, while AMD and the S&P 500 made slight gains. This is partly a function of Nvidia's high valuation, which makes it more reactive to economic news.
AMD is the better AI stock in terms of value and upside. Yes, the company is an underdog that's unlikely to replace Nvidia as the recognized AI leader. But, AMD has operated from second place before and thrived under its current leader Su. Plus, the addressable market is so large that a smaller share position still means massive growth for AMD.
Nvidia already has a huge growth record going for it, but the company is quickly lapping the big AI-related gains it has generated to date. Comparisons will get harder from here on out and that could create more volatility for the NVDA stock price.
What are the key differences between Nvidia and AMD in AI?
Nvidia has the advantage of being the first to market a comprehensive solution set for AI computing. AMD has a smaller offering which it has been growing through internal product development and acquisitions.
Nvidia is estimated to have 70% or more market share in AI chips. AMD's market share is probably closer to 10%.
AMD is a safer bet than Nvidia stock as a long-term play, though both companies are poised for bright futures. AMD's valuations are more reasonable, and Nvidia may be closer to seeing its growth level off.
Both companies provide high-performance, AI-capable processors and related products and services including software for application development. Nvidia's platform CUDA is proprietary, while AMD's ROCm is open source. Open-source applications are more flexible but can also be more cumbersome.
Whether it’s mastering cutting-edge strategies, uncovering actionable investment opportunities from influential leaders, or breaking down complex topics, our in-depth journalism has you covered. Become a Forbes member and gain unlimited access to bold ideas shaking up industries, expert guides and practical investment advice that keeps you ahead of the market."
220,https://www.forbes.com/sites/antonyleather/2024/09/21/buy-an-nvidia-rtx-4090-before-its-too-late-shortages-predicted/,Buy A Nvidia RTX 4090 Before It’s Too Late: Shortages Predicted,"Sep 21, 2024, 01:29pm EDT",Antony Leather,"Nvidia looks set to discontinue production of its flagship GeForce RTX 4090 graphics card range next month to set the scene for an expected RTX 5000 launch including an RTX 5090 and likely RTX 5080 in late 2024 or early 2025. (Wccftech The move is expected to allow add in board (AIB) partners to clear stock of the now two year old model, but could result in shortages of the highly desirable high-end graphics card in the lead up to Christmas as supplies run dry.
Despite it's high price, which still sees most variations from board partners such as Asus, Gigabyte, MSI and Palit rarely dip much below $2,000, The RTX 4090 still sells well making up one percent of the Steam Hardware Survey's count of GPUs, with that count having risen over the last three months and eclipsing that of the RTX 4080. That is likely thanks to the RTX 4080 being discontinued after the RTX 4080 Super launched earlier this year.
In the past Nvidia has opted to launch is recent GPUs between summer and Christmas, though, so unless some other forces are at play that force the company's hand to going with an early 2025 launch instead, a pre Christmas launch would definitely fit with the usual launch windows as well as giving the PC hardware community something else to look forward to in the run up to end of the year.
This would also be a useful coalition of upgrade potential thanks to Intel and AMD launching new processors and motherboards starting from October. AMD will see X870 chipset motherboards hitting shelves that month with its highly anticipated Ryzen 9000X3D CPUs expected around January. Intel meanwhile, as well as releasing its new Lunar Lake mobile CPUs, which impressed at their launch in Germany recently, will also be fully announcing its desktop Arrow Lake-S processors.
These will use a new CPU socket and require DDR5 memory, but it's hoped that Intel can avoid AMD's lacklustre Ryzen 9000 launch, which was marred by delays and disappointing performance as well as Windows fixes. It's an important time of year for all three companies, then, although indications are that AMD will likely not be joining the race for the flagship gaming graphics card crown and will instead be focussing on the mid-range in a similar way to we've seen it do in the past, for example with the RX 5700 XT. This can help to lower costs and afterall, even the mighty RTX 4090 only accounts for one percent of the Steam Hardware Survey GPU count.
The RTX 5000 series will be based on Nvidia's Blackwell architecture using a rumored refined rather than entirely new manufacturing process called N4P. The Blackwell desktop GPU architecture should see a significant boost to transistor count, and could also make the move to PCIe 5 where motherboard support beckons, even if this will not likely see any significant benefits.
For more PC news and reviews including those for AMD, Intel and Nvidia's upcoming launches, don't forget to follow me here on Forbes using the blue button below, on Facebook or my YouTube channel."
221,https://www.forbes.com/sites/shivaramrajgopal/2024/09/17/nvidias-disclosure-needs-to-catch-up-with-its-valuation/,NVIDIA’s Disclosure Needs To Catch Up With Its Valuation,"Sep 17, 2024, 08:27pm EDT",Shivaram Rajgopal,"Without data about number of units of products sold and average price at which such products are sold, it is hard to have intelligent conversations about whether NVIDIA’s $2.9 trillion valuation is justified or not.
This semester, we discuss NVIDIA as our live case in my class. An important analytical tool that we usually ask students to think about is to decompose a firm’s revenue into quantity of product sold (Q), the product mix sold (M), the selling price per unit of the product (P) and any foreign currency impact (FX). The roots of this thinking go back to some of the Apples-to-Apples work conducted by Trevor Harris at Morgan Stanley in the late 90s and early 2000s.
Why decompose revenue this way? I often tell students that Q is among the best ways to ground the company’s valuation into a strategic plan for management.
Netflix Q
To illustrate this point, consider a simple example. Ignore M and FX for now.  As of 9/16/2024, Netflix trades at $702 a share. In the year just ended, Netflix reported an annual EPS (earnings per share) of $12.03. Assuming a cost of equity capital of roughly 9% (treasury bill yield of 3.65%, beta of 1.1, assuming a risk premium of 5%), Netflix’s earnings per share earned till perpetuity would amount to $133 ($12.03/0.09). This simple calculation suggests that Netflix needs to earn $58.2 more in earnings per share in the future [($702-133) = $569*.09]. This translates approximately to $23 billion more in annual income every year (450 million shares*$51). Now, bring this back to Q. Netflix reported 278 million paying subscribers as of July 2024. This suggests that the company will have to generate $83 in additional profit every year from every subscriber ($23 billion/278 million subscribers). That’s $83 in income not revenue or roughly $7 per month in profit. Is that feasible, given that the US and Canadian market (roughly 76 million subscribers) is likely saturated and likely the only ones most insensitive to price increases?
The bigger point is that one can at least have strategic conversations, grounded in data, regarding Netflix’s valuation. Most analyst and board room conversations tend to center around growth rate in earnings (g). I find abstract comments about growth rate (g) embedded in valuation somewhat useless. How do I calibrate g? What does that mean for Netflix’s strategy or the possibility that they will hit that growth rate? Conversations about Q are far more insightful to me.
So, what does this have to do with NVIDIA?
NVIDIA
Well, NVIDIA does not disclose Q and P. Consider the data. As of 9/16/2024, NVIDIA trades at $118 per share. I usually benchmark this to EPS from last year, but NVIDIA is a special case because their revenue in the first two quarters this year is as high as the revenue for all of last year. The cumulative EPS for the first half of this year is $1.29. Let’s simply extrapolate this to the year ($2.60). Assuming a cost of equity capital around 13% (treasury bill yield of 3.65%, beta of 1.8, assuming a risk premium of 5%), we are looking at $20 per share as the value of repeating this year’s forecasted EPS forever ($2.6/0.13). This suggests that NVIDIA will need to earn an additional $12.74 ($118-$20 = $98*.13) or essentially increase EPS by six times of this year’s income. Is that feasible?
I then look for Q and P and come back emptyhanded. NVIDIA has three fast growing segments (Data Center segment, Networking segment and the Automotive segment) and two steady segments (Gaming and Visualization). Here is NVIDIA’s disclosure in its latest 10-K that comes the closest to being useful.
Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago. Data Center revenue for fiscal year 2024 was up 217%. Strong demand was driven by enterprise software and consumer internet applications, and multiple industry verticals including automotive, financial services, and healthcare. Customers across industry verticals access NVIDIA AI infrastructure both through the cloud and on-premises. Data Center compute revenue was up 244% in the fiscal year. Networking revenue was up 133% in the fiscal year. Gaming revenue for fiscal year 2024 was up 15%. The increase reflects higher sell-in to partners following the normalization of channel inventory levels and growing demand. Professional Visualization revenue for fiscal year 2024 was up 1%. Automotive revenue for the fiscal year 2024 was up 21%. The increase primarily reflected growth in self-driving platforms.
Unfortunately, there is no conversation about how much Q and P does NVIDIA sell in even one major segment (data center). How many GPUs (graphical processing units) does NVIDIA sell? How many more does it need to sell to grow into its valuation? If your comeback is that it will do something else, please articulate what that something is and how much of P and Q of that something the company must sell? Without such data, it is hard to have intelligent conversations about whether NVIDIA’s $2.9 trillion valuation is justified or not.
Large sample evidence
In a recently completed paper, my co-authors (Sriniwas Mahapatro of the Rochester Institute of Technology, Prateek Manikpuri and Prasanna Tantri of the Indian School of Business) and I look at a natural experiment in India where manufacturing firms were required to disclose product-level revenues and quantities sold in audited financial statements from 1997-2011. We document that such mandated disclosure of quantities and product prices (i) reveal the persistence of sales growth of firms; (ii) reduce information asymmetry between the manager and the market; (iii) reduce analysts’ revenue forecast errors; and (iv) reduce the stock market reaction to earnings announcements.
The large sample evidence simply confirms the intuition that it is very difficult to bring back valuations to strategic conversations unless we know something about the firms’ P and Q. In particular, is the stock market’s assessment of NVIDIA’s future consistent with their plans on how much product and services to sell, how much to price the product and services, and what we might be assuming about competition or lack thereof or about the firm’s cost structure (e.g., fixed costs don’t change much as Q changes within a range). Concrete data on P and Q can move us from hypothetical guesswork to an evidence-based dialogue on the feasibility of NVIDIA growing into their valuation.
Comments welcome, as always."
222,https://www.forbes.com/sites/carolynschwaar/2024/09/17/nvidia-backs-software-start-up-enabling-complex-additive-manufacturing/,Nvidia Backs Software Start-Up Enabling Complex Additive Manufacturing,"Sep 17, 2024, 11:03am EDT",Carolyn Schwaar,"NVentures, Nvidia’s venture capital arm, is putting several millions of dollars behind a young company called nTop that’s become the leader in computational design software. This type of computer-aided design (CAD) software makes it easier and much faster for engineers and product developers to design very complex parts — everything from rocket engines to orthopedic implants — that are most often produced with additive manufacturing (3D printing).
Companies exploring new product design with nTop include Siemens Energy, NASA, Ford, Tesla, Honeywell, Adidas, and the U.S. Department of Energy. Although it is not exclusive to additive manufacturing, nTop software enables the type of complex features and part consolidation that is not manufacturable with traditional processes.
At its core, computational design uses algorithms and automation to generate solutions to engineering problems while helping engineers optimize part design and function, visualize and interact with designs in realistic virtual environments, and simulate a design’s real world performance, which takes a lot of computing power.
Nvidia’s graphics processing units (GPUs) are already the recommended power behind nTop’s software, nTop 5, but now both companies are collaborating deeper to integrate nTop 5 with the Nvidia OptiX rendering framework and Omniverse technologies to help engineers better visualize their designs.
Omniverse is a platform for developing applications for industrial digitalization and generative physical AI across a wide range of industries for everything from game development to simulating real-world environments for training self-driving cars.
nTop plans to integrate its digital design files into the Omniverse ecosystem to provide users with an immersive 3D collaboration environment to see and interact with live digital twins of their parts and assemblies. As changes are made to designs in nTop of, for example, a new electric motorcycle, they’re reflected on the Omniverse platform in real-time for evaluation and simulations.
“Product engineering and development teams working in every industry need powerful simulation capabilities to design their work in a physically accurate manner,” says Mohamed Siddeek, corporate vice president at Nvidia and head of NVentures.
The first proof of concept in the collaboration integrates the Nvidia OptiX — a rendering technology for creating ultra-realistic images and simulations in a virtual environments — into nTop 5 to provide more lifelike renderings.
“Through that integration, we’ve created a new feature called adaptive rendering, which is being rolled out to nTop customers this week,” says Bradley Rothenberg, co-founder and CEO, nTop. “This new feature will enable engineering teams to render extremely complex geometries into designs in seconds so they can iterate even faster through design options. In comparison, this would take other CAD systems hours, or may not even be possible.”
Nvidia and nTop’s technology teams are collaborating on a variety of new tools and features, Rothenberg says, which will launch later this year. Recently nTop announced new partnerships with software giants Autodesk and Materialise that enable engineering teams designing in nTop to get their parts manufactured on dozens of different kinds of metal and polymer 3D printing systems."
223,https://www.forbes.com/sites/patrickmoorhead/2024/09/12/antitrust-probes-into-nvidia-what-are-the-implications/,Antitrust Probes Into Nvidia: What Are The Implications?,"Sep 12, 2024, 10:35am EDT",Patrick Moorhead,"Does Nvidia have a monopoly in datacenter GPUs? Factually, yes, because it has more than 90% of that market. Mind you, it’s not illegal to hold a monopoly; it’s illegal to use monopoly power to harm customers, stifle competition or limit innovation. Is Nvidia doing any of that? Not as far as I know—but the U.S. Government seems to be embarked on a multiyear journey to find out.
Two things I want to point out before we go any further: (1) Nvidia is a client of my firm, as are many of its competitors and customers. (2) I am not a lawyer, though I have more than my share of experience with antitrust actions. I was a key witness in AMD v. Intel (2005), FTC v. Intel (2009) and EU v. Intel (2009). As part of that experience, I had the “fun” of being deposed on-camera by Intel lawyers for a total of 24 hours, the maximum allowed under Texas law. Later, I was also active in two different antitrust cases involving Qualcomm and Apple. So I’ve been around the block a few times when it comes to antitrust. Let’s dig in.
My level of surprise was zero when Bloomberg reported that the U.S. Department of Justice is ramping up an antitrust probe of Nvidia. Whenever any company holds such a dominant position in a vital sector—in this case, the most important part of the AI market—it’s bound to set off alarm bells with competitors, customers, regulators, you name it.
That’s why it’s not really relevant if Nvidia hasn’t received formal subpoenas yet. We’ve known for months that Nvidia is on the very short list of AI companies that the feds are looking into. In fact, I fully expect the DOJ’s action to be followed by similar probes in other jurisdictions, including the EU, South Korea and Taiwan (though likely not China).
More than that, I’ve watched very closely, and with more than a little skepticism, as FTC chief Lina Khan has spearheaded a more aggressive approach to antitrust behavior—or even the possibility of antitrust behavior—that has taken the idea of “pre-crime” to new levels in this area of the law. In other words, it’s not what you’ve done, but what you could do.
So what has Nvidia supposedly done in this instance? According to the Bloomberg report, the government thinks Nvidia may be making it hard for customers to switch to other vendors, and punishing customers that use other vendors’ chips—possibly by giving better pricing to those who use Nvidia’s chips exclusively.
Forcible vendor lock-in through tactics like these would be a smoking gun for the DOJ. Illegal tactics could include tying (You must buy X to get Y), retaliation (I won't ship you the volumes or get you the samples you want if you go with a competitor), exclusionary rebates (I’ll give you this price only if you don't buy from a competitor), or somehow technically blocking competitors’ fair access to the market. We saw that last one with both Apple and Microsoft; in this case, it could apply, for instance, if Nvidia is preventing competitors from interfacing with its market-dominant CUDA software platform.
The first thing to know is that these actions take time—easily a decade, in many cases—from investigation to judgment to appeal. You can take your pick of examples, including AT&T, IBM, Microsoft and recently Google. If there’s enough of a smoking gun, the courts or regulators may issue an injunction or other directive that forces the company to change its behavior, as has happened with Apple’s handling of its App Store in the EU this year.
So what about the big sell-off of Nvidia stock on the day the Bloomberg report came out? While I’m sure the antitrust news didn’t help the situation, Nvidia’s stock didn’t drop because of it. Investors are rightly worried about the economy, so the main pressures on Nvidia’s stock price are macroeconomic. When I offered my viewpoint on this story on Yahoo Finance, I joked that the biggest thing CEO Jensen Huang could do about the share price would be to calm people’s fears about a recession. (I also gave my thoughts on the matter on CNBC.) Meanwhile, institutional investors know very well that any serious DOJ investigation will take many years to resolve; they won’t care about it until way down the line when there’s a ruling.
In the closer term, the real impact of this investigation will likely be to slow Nvidia down a bit. If your company is involved in an antitrust investigation, it’s like putting sand in a gas tank: every conversation about pricing, strategy or allocation needs to happen with a lawyer in the room. You have to get legal approval for every pricing or bundling decision, and you have to put it all in writing—with lawyers copied on every piece of correspondence. (Tangent: no small part of AMD’s successful antitrust action against Intel was made possible by the staggering amount of work the legal team did combing through mountains of Intel’s correspondence. My lawyers told me that I had my own pile of evidence.)
I don’t think Nvidia will be slowed by this as much as Microsoft was when it came under this kind of scrutiny 20-plus years ago. Nvidia is moving so fast and executing its business so far ahead of its competitors that a little bit of sand in the gas tank likely won’t slow it down much.
That said, Nvidia of course needs to watch what it’s doing. At some point, we’re likely to know more about which companies have complained to the government about Nvidia’s practices. I wouldn’t be surprised if this includes not just its direct competitors, but also some of its largest hyperscaler customers. As I’ve noted recently, no one besides Nvidia is comfortable with the company having 90%-plus market share.
Meanwhile, which companies might capitalize on Nvidia’s antitrust challenges? AMD is the next best replacement for Nvidia GPUs, and then there’s Broadcom with its ASICs. Give it another year, and Intel will be more in the mix with its own GPU-based designs for datacenter chips.
Speaking of Intel brings us full circle. Believe it or not, Intel is still fighting the judgment in its EU case, 15 years later. That’s a good reminder of how long it could take for Nvidia’s antitrust story to unfold.
Moor Insights & Strategy provides or has provided paid services to technology companies, like all tech industry research and analyst firms. These services include research, analysis, advising, consulting, benchmarking, acquisition matchmaking and video and speaking sponsorships. Of the companies mentioned in this article, Moor Insights & Strategy currently has (or has had) a paid business relationship with AMD, AT&T, Broadcom, Google, IBM, Intel, Microsoft, Nvidia and Qualcomm."
224,https://www.forbes.com/sites/janakirammsv/2024/09/12/jfrog-brings-artifact-management-and-software-supply-chain-to-nvidia-nim/,JFrog Brings Artifact Management And Software Supply Chain To Nvidia NIM,"Sep 12, 2024, 12:55am EDT",Janakiram MSV,"JFrog, a leader in DevOps and DevSecOps solutions, recently announced a strategic partnership with Nvidia, marking a significant advancement in AI model deployment and security. This collaboration follows JFrog’s acquisition of Qwak AI, which has substantially bolstered its machine learning and artificial intelligence capabilities.
The partnership focuses on integrating Nvidia NIM microservices into the JFrog Platform. Part of the Nvidia AI Enterprise software suite, Nvidia NIM is designed to deliver GPU-optimized AI model services. This integration combines pre-approved AI models with centralized DevSecOps processes to create a smooth software supply chain workflow. The goal is to meet the growing demand for generative AI solutions that are ready for use in businesses.
Nvidia NIM is available as both API endpoints and container images, providing flexibility for developers to deploy AI models in different environments. These inference microservices can be integrated using industry-standard APIs, allowing for seamless inclusion into existing workflows. Additionally, NIM container images can be deployed on-premises, offering organizations the option to run GPU-accelerated models in their own data centers or managed environments. This self-hosted deployment ensures security and control over the data while leveraging NVIDIA’s optimized infrastructure for high performance and low latency AI inferencing.
JFrog Artifactory, the package repository, will be supported to store NIM container images that can be deployed within an enterprise datacenter.
A key challenge in the AI industry has been scaling machine learning model deployments in enterprise environments. Data scientists and ML engineers face issues such as fragmented asset management, security vulnerabilities and performance bottlenecks. The JFrog-Nvidia collaboration addresses these pain points by streamlining the deployment of secure ML models and large language models to production environments. It brings proven secure software supply chain management capabilities to generative AI models and artifacts.
The integration of Nvidia NIM microservices into the JFrog Platform is expected to offer multiple benefits. It provides centralized access control and management of NIM microservice containers alongside other assets, such as proprietary artifacts and open-source dependencies. This unified approach integrates seamlessly with existing DevSecOps workflows while delivering comprehensive security and integrity through continuous scanning at every development stage.
The collaboration also optimizes AI application performance using Nvidia’s accelerated computing infrastructure, offering low latency and high throughput for scalable AI model deployments. Additionally, the integration offers flexible deployment options, including self-hosted, multi-cloud and air-gap environments, ensuring adaptability for various enterprise needs.
This partnership is timely given the growing adoption of AI technologies across industries. As organizations rapidly embrace AI, implementing efficient and secure practices becomes crucial. The integration of DevOps, security and MLOps processes into a complete software supply chain workflow with Nvidia NIM microservices will allow customers to bring secure models to production efficiently while maintaining visibility and control throughout the pipeline.
This collaboration follows JFrog's acquisition of Qwak AI, which expanded the company's ML and AI capabilities. The acquisition, valued at $230 million, enabled JFrog to incorporate advanced MLOps functionality into its platform. Qwak’s technology allows data scientists and developers to focus on AI-powered application creation without being bogged down by infrastructure concerns.
With Qwak’s MLOps solution integrated into JFrog’s platform, customers can manage their AI models with JFrog Artifactory serving as the model registry and JFrog Xray ensuring the security of ML models. This unified platform supports DevOps, DevSecOps, MLOps and MLSecOps, providing full traceability and eliminating the need for separate tools and compliance efforts.
Through the Qwak acquisition, JFrog has expanded its solutions to offer a unified platform for managing both traditional models and large-scale AI initiatives. The platform simplifies the model development and deployment process, enhances model serving into production and offers model training and monitoring capabilities, including out-of-the-box dataset management and feature store support.
By treating ML models as packages, JFrog allows users to manage and secure models just as they would any software package. This approach ensures the security and provenance of AI throughout the development lifecycle.
JFrog’s partnership with Nvidia, combined with its acquisition of Qwak, addresses the growing need for secure and scalable AI model deployment in enterprise settings.
As AI adoption continues to expand, JFrog’s enhanced platform, supported by Nvidia’s GPU-optimized services and Qwak’s MLOps expertise, is well-positioned to meet the evolving needs of AI and machine learning."
225,https://www.forbes.com/sites/dereksaul/2024/09/11/nvidia-stock-pops-7-as-ceo-huang-touts-demand-for-chips/,Nvidia Stock Pops 8% As CEO Huang Touts Demand For Chips,"Sep 11, 2024, 03:28pm EDT",Derek Saul,"Demand for Nvidia’s high-tech products is “so great” its customers are “tense,” the artificial intelligence giant’s chief executive Jensen Huang said Wednesday, spurring a more than $200 billion jump in market value for the volatile Nvidia.
“Everyone wants to be first and everyone wants to be most,” Huang said at a conference hosted by Goldman Sachs, according to Bloomberg, referring to the “more emotional customers” of Nvidia as the semiconductor products it designs for generative AI are treated like nothing short of 21st-century gold.
“Demand is so great that delivery of our components, our technology, infrastructure, and software is really emotional for people,” explained Huang, reported Yahoo Finance, adding that Nvidia's technology “directly affects” the financial performance of its customers, which include Amazon, Facebook parent Meta, and Google parent Alphabet.
After mostly trading down in morning trading, Nvidia shares steadily shot up following Huang’s comments, booking a 8% gain by close, almost single handedly turning broad market losses earlier Wednesday into solid gains.
Huang’s touting of immense demand for his company’s technology comes after Wall Street was not quite swayed by Nvidia’s Aug. 28 earnings report, as shares fell more than 20% in the week after over concerns about Nvidia’s ability to sustain its near unprecedented revenue growth.
The Silicon Valley-based Nvidia most famously designs graphics processing units (GPUs), systems which handle the immense data needed to power AI applications. As interest in machine learning began to peak in late 2022 with the release of the ChatGPT generative AI chatbot, so did Nvidia’s financial results. Analysts project Nvidia will report $126 billion in sales and $67 billion in net profits in its fiscal year ending January. 2025, eye popping growth from the $$27 billion in sales and $4 billion profit it booked in the fiscal year ending January 2023. Nvidia stock has skyrocketed as demand for its AI systems soared, with shares up about 700% over the last two years.
Nvidia’s $216 billion jump in market capitalization Wednesday is the latest major swing for the volatile stock, as it is on pace for its 24th trading session of a more than 5% gain or loss of 2024. In comparison, Microsoft and Apple, the only two companies valued higher than Nvidia, have registered just a pair of 5% or more daily moves this year, combined.
Huang, Nvidia’s largest individual shareholder, enjoyed the largest bump to his net worth of any billionaire in the world Wednesday, gaining $6 billion, according to Forbes’ real-time tracker. The Nvidia cofounder and longtime CEO is the 14th-richest person in the world.
“The days of every line of code being written by software engineers, those are completely over,” Huang proclaimed Wednesday, referencing the upending of traditional workflow from generative AI."
226,https://www.forbes.com/sites/julianhayesii/2024/09/06/nvidia-ceo-jensen-huang-gives-contrary-advice-to-ceos-on-firing-people/,Nvidia CEO Jensen Huang Gives Contrary Advice To CEOs On Firing People,"Sep 06, 2024, 01:00pm EDT",Julian Hayes II,"The age of artificial intelligence (AI) is here, impacting business operations across various industries. Companies are rapidly integrating AI into their daily functions, while employees at all levels are quickly learning AI-related skills to stay competitive. Among the companies riding the AI wave, Nvidia stands out. Their recent second-quarter earnings more than doubled from a year ago, surpassing Wall Street estimates and further entrenching them as a dominant force in the tech sector. From humble beginnings—cleaning bathrooms—to leading one of the world's most valuable companies, Nvidia's CEO Jensen Huang's leadership approach is as bold as his company's success.
In a 60 Minutes interview, Huang was described as ""demanding, a perfectionist, and not easy to work for."" He agreed: ""If you want to do extraordinary things, it shouldn't be easy."" Currently valued at a little over $2.5 trillion, Nvidia ranks among the top five most valuable companies globally. According to a June poll of over 3,000 Nvidia employees, out of around 30,000, 76% are millionaires, and one in three has a net worth exceeding $20 million. Despite the company's intense ""pressure-cooker"" environment, Huang's leadership style defies expectations. Contrary to assumptions that a demanding CEO would quickly fire employees, Huang takes a more nuanced approach. In a fireside chat with Stripe CEO Patrick Collison, Huang revealed that he's slow to fire team members and prefers to ""torture them into greatness.""
This counterintuitive strategy offers key lessons for CEOs. Retaining employees and instead building them up provides two benefits for leaders and the overall organization.
While compensation remains an essential piece of the puzzle, salary alone isn't enough to engage today's top performers. As Millennials and Gen Z become the dominant segments in the labor market, they prioritize work-life integration over strict work-life separation where salary is the focal point. They also value mentorship and coaching more than previous generations, such as Gen X and Baby Boomers. Leaders can forge stronger loyalty by investing in the personal growth of their workforce through mentorship and coaching.
Team members who feel supported and valued are more likely to stay with the company, thus reducing turnover and other human capital expenses and bolstering the company's brand and culture. This investment in people leads to higher engagement, which extends beyond the organization's confines and positively impacts customer interactions. That said, rather than relying on perks to ""buy"" loyalty, companies should focus on gathering and acting on employee feedback, recognizing achievements, and providing greater autonomy in decision-making. When individuals are empowered to take creative risks without fear of repercussions, loyalty and better performance naturally follow.
In sports, the most talented team doesn't always win the championship—it's the team with the best chemistry. The same is true for business. Constantly replacing employees drains talent and, more importantly, weakens culture, stagnates performance, and impacts revenue as time is lost training new hires and rebuilding critical team dynamics. Instead, leaders can show patience with team members who display potential and work to unlock it through coaching and development.
A cohesive, long-standing team builds trust, loyalty, and alignment with the organization's mission. When team members are truly invested in the company's goals, their commitment goes beyond just earning a paycheck—they become mission-driven contributors. A unified, mission-driven team will consistently outperform a group of mercenaries with separate agendas.
The common adage ""hire slow, fire fast"" has been a staple of business culture. But in some cases, it's worth reconsidering. Hiring and firing slowly, particularly when employees show potential but need guidance, can lead to extraordinary results. By mentoring, coaching, leading by example, and, yes, ""torturing"" employees into greatness, leaders can create a recipe for building high-performing teams that set their organizations apart from the competition."
227,https://www.forbes.com/sites/zinnialee/2024/09/05/nvidia-joins-japanese-startup-sakana-ais-100-million-series-a-round/,Nvidia Joins Japanese Startup Sakana AI’s $100 Million Series A Round,"Sep 05, 2024, 02:00am EDT",Zinnia Lee,"Sakana AI, a Tokyo-based artificial intelligence startup cofounded by ex-Google researchers, has hit unicorn status after raising more than $100 million in a funding round that included billionaire Jensen Huang’s Nvidia.
The Series A round was led by Silicon Valley venture capital firm New Enterprise Associates, whose portfolio includes U.S. stock trading app Robinhood; Khosla Ventures, an early investor of AI poster child OpenAI; and Lux Capital, who had invested in leading AI startup Hugging Face.
It boosted Sakana AI’s valuation to more than $1 billion, according to a source with direct knowledge of the deal.
The one-year-old startup said in a statement Wednesday that it will team up with Nvidia on AI research, data center access and growing the AI talent pool in Japan. It will also use the fresh capital to expand its team and develop its technology for creating smaller and more cost effective AI models.
Nvidia’s Huang said in the statement: “The team at Sakana AI is helping spur the democratization of AI in Japan by developing cutting-edge foundation models to automate and speed up scientific discovery.”
Unlike many AI startups like OpenAI, which create large-scale models with a vast amount of training data, Sakana AI develops smaller models with less data and then have them work together to achieve higher efficiency. The startup’s name, which means “fish” in Japanese, draws inspiration from the way smaller animals like fish work together. It has released three AI models for applications including image generation.
Sakana AI previously raised a $30 million seed round in January, led by Lux Capital with participation from Khosla Ventures. Its other investors include Sony, Japanese telecom giants NTT Group and KDDI, as well as Google’s chief scientist Jeff Dean, Hugging Face cofounder Clem Delangue and U.S. billionaire Alexandr Wang, the cofounder of AI unicorn Scale AI.
Sakanaa AI was established by former Google employees David Ha and Llion Jones, as well as former Japanese diplomat Ren Ito. Prior to Sakana AI, Ha led Google Brain’s research in Tokyo while Jones co-authored a landmark research paper that underpins many of today’s popular AI products such as OpenAI’s chat bot ChatGPT. Ito was previously the COO of U.K. AI startup Stability AI and CEO of the European operations of Japanese tycoon Shintaro Yamada’s online used-goods marketplace Mercari. He had a brief stint at the Ministry of Foreign Affairs of Japan.
Update: September 6, 2024
This article has been updated to include Sakana AI’s post-money valuation.
"
228,https://www.forbes.com/sites/danielnewman/2024/09/05/the-state-of-the-ai-super-cyclenvidia-apple-and-the-overall-demand-for-ai/,"The State Of The AI Super Cycle - NVIDIA, Apple, And The Overall Demand For AI","Sep 05, 2024, 03:55pm EDT",Daniel Newman,"After a few tough days in the market to start off September, there is another wave of ramblings and musings as to whether the AI super cycle is over? And if not, what’s left and where do we go from here?
Despite its recent pull backs and the challenging news cycle it has faced, I would argue so far it has only been an NVIDIA super cycle in terms of money being made, with adjacent winners in the chipmaking and design space like TSMC, Synopsys, Micron Technology, Arm, and a few others.
Granted, we’ve seen some pops in companies like Dell Inc. and Super Micro Computer Inc. based on big upticks in AI optimized server sales, but those gains somewhat faded as margin pressure and the “buy up anything with AI” exuberance settled momentarily.
Meanwhile, SaaS companies have had some good days here and there, but few are convinced these companies are going to get incremental growth from AI. And while software will be the ultimate AI consumption layer, it hasn’t yet proven its economics to the market in terms of incremental revenue and customer adoption—time will tell.
We are seeing some promising numbers from consultants and systems integrators like IBM and Accenture on sprawling generative AI projects and some indicators from large financial services companies and healthcare/drug companies that AI is going to deliver efficiency and value to their businesses, but it hasn’t really shown in the numbers yet.
Cloud companies and hyperscalers are investing in the future, as are the largest enterprises. While NVIDIA has seen its profits and revenue soar, companies like Advanced Micro Devices Inc. are next in line to benefit yet are really only making a small dent at the moment.
Then there is a collection of companies just trying to find their place in the AI race. We know the AI device (handsets and PCs) cycle has been a bit slower and that incremental growth of SaaS from AI is showing signs of life from the likes of ServiceNow Inc., Salesforce Inc., and CrowdStrike Inc., who are proving AI can provide stickiness and drive growth on the top and bottom lines. Things have only just begun here.
It all begs the question: Did the markets get ahead of themselves with AI and chips makers? And are those stocks overvalued?
Clearly the market is coping with a digestion period for AI as we examine the bull thesis of cloud capex accelerating regardless of broader monetization. Contrast that against the background of AI proliferation and consumption being somewhat slower than some had hoped.
We have seen recent earnings reports where companies like Walmart Inc. and Amazon.com Inc. are beginning to explain how AI is improving supply chains and reducing developer costs considerably. Seeing these types of AI consumption for business efficiency and productivity gains will offset some of the concerns about long-term value.
Furthermore, chief executives of the largest NVIDIA customers, who represent more than 45% of its revenue, with a single customer allegedly at 19%, largely shrugged off concerns there is a bubble.
This is where the digestion period becomes an important debate. Meta Platforms Inc. CEO Mark Zuckerberg, Microsoft Corp. CEO Satya Nadella, and Alphabet Inc. CEO Sundar Pichai all pointed out they have no plan to slow investments. And while that may frustrate those looking for more conservative capex, these companies are thinking long-term sustainability.
For NVIDIA, this bodes well because competitiveness means moving from generation to generation, from a strong Hopper cycle to a stronger Blackwell cycle and Rubin after that.
Market reaction to NVIDIA’s results in late August suggests the company did what was expected after bullish investors considered the buy side whisper numbers, which were bigger than the ultimate guide.
So, let’s pivot a bit here and look at Apple. Is there any sort of “Super Wave” in tech when Apple doesn’t participate? At the very least, Apple will play a role as an accelerant for the trend toward AI and that may come in the form of products, partnerships, software and apps, and of course driving demand and sentiment from its massive user base. So, as we head into the big Apple event, what should investors and consumers expect from Apple’s AI offerings?
First and foremost, Apple Siri will become usable. While Apple has been more methodical in its AI strategy, one of the biggest opportunities for the company was to re-introduce Siri and make it something the iPhone install base genuinely wants to use.
Given the symbiotic relationship between people and their devices, delivering an OS level assistant that has high fidelity and robust ecosystem integration will be priceless in driving an upgrade cycle.
What is more, delivering native AI in core productivity apps (+Siri) will be invaluable. Calendar and email come to mind. Being able to write thoughtful emails that sound in our voices transcribed from Siri or having it know which items across our messaging apps require prioritization will be important to expediting the next big super cycle for handsets.
To be clear, it isn’t unique to Apple. These kinds of advanced functions being seamless from device to device should drive growth across the Apple portfolio and bode well for Qualcomm Inc., Android, and the broader OEMs making Copilot+PC, AI PC, and next-generation handsets.
Which company/companies will “win” the AI race? Will it be a private one like OpenAI or will a mega-cap tech name surge ahead?
There really isn’t one AI race being run.
We have the infrastructure build-out race, which is fueled by NVIDIA and its ecosystem. Then we have the cloud development and services build out and this right now is a toss-up as Microsoft, Google, and AWS battle with the likes of Oracle Corp. and several AI cloud providers (CoreWeave, Lambda).
There is a chip race that looks to capitalize on the inference pivot, but also vertical integration with Google, Microsoft, AWS, and Meta building their own chips. We have specialty AI chip companies as well.
And we still must solve a power issue. That will come from efficient chip designs, thermal management and cooling technologies, and on-device capabilities to reduce strain on networks.
So, while some may think that AI is overcooked, or that the very recent pull back on NVIDIA is a sign of AI fading or losing steam. And still others may think this alleged (and then not so much) antitrust probe from the DOJ is going to be a setback for not just NVIDIA but AI as a whole. To that, I would suggest it is really early for AI – for NVIDIA and even more so for AI opportunities beyond it."
229,https://www.forbes.com/sites/dereksaul/2024/09/05/nvidia-stock-attractive-after-15-selloff-bank-of-america-says/,"Nvidia Stock 'Attractive' After 15% Selloff, Bank Of America Says","Sep 05, 2024, 11:03am EDT",Derek Saul,"A post-earnings slide and regulatory fears pose an alluring point of entry for investors into Nvidia stock, argued Bank of America analysts, a vote of confidence in the artificial intelligence heavyweight as Nvidia navigates a rare rough patch on the stock market.
The Bank of America group led by Vivek Arya maintained their top rating for Nvidia, setting a $165 price target for the stock and arguing it’s an especially “attractive opportunity” for investors after shares fell more than 15% from last week’s high through Wednesday’s close.
The selloff came as the best-known architect of AI semiconductor technology faces “several headwinds” in the eyes of investors, according to Arya.
Concerns include delays for its next-generation Blackwell graphics processing unit for generative AI, a weak buyers market due to macroeconomic issues like the presidential election, and regulatory concerns which peaked Tuesday when Bloomberg reported the Justice Department subpoenaed Nvidia in an antitrust investigation, though the company later told Forbes it had not been subpoenaed.
Bank of America analysts said it assumes “no specific material impact” on Nvidia from the potential Justice Department probe, noting “the plethora” of recent government inquiries into big technology companies.
Shares of Nvidia rallied 2% to $108 in Thursday trading, but remained down 14% from its closing price last Wednesday ahead of its second-quarter earnings report.
$4 trillion. That’s Nvidia’s valuation implied by Bank of America’s $165 share price target. That is significantly larger than any public company ever, with the prior record set by Apple in July with a nearly $3.6 trillion market capitalization. Nvidia was the third most valuable company in the world Thursday with a $2.7 trillion valuation, trailing only Apple and Microsoft."
230,https://www.forbes.com/sites/jackkelly/2024/09/04/nvidia-employees-become-multi-millionaires-but-at-what-price/,Nvidia Employees Become Multi-Millionaires—But At What Price?,"Sep 04, 2024, 10:18am EDT",Jack Kelly,"Nvidia's meteoric rise in the artificial intelligence chip market has created a wave of millionaires among its workforce. This unprecedented wealth creation stems from the company's extraordinary stock performance, which has surged by 3,776% since early 2019.
Nvidia's employee stock purchase program that allows staff to buy shares at a 15% discount has transformed the lives of many employees, with the stock price soaring from $14 in October 2022 to nearly $107 at the time of publication.
However, despite their newfound wealth, Nvidia staff find themselves caught in a ""pressure-cooker"" atmosphere of long hours, intense meetings, relentless expectations and alleged micromanagement, according to a recent Bloomberg report.
Current and former employees at Nvidia describe their lucrative compensation packages as akin to ""golden handcuffs,"" indicating that while they enjoy substantial financial rewards, these benefits also trap them in a demanding work environment.
Nvidia CEO Jensen Huang has adopted a controversial leadership style that emphasizes pushing employees to their limits rather than resorting to layoffs.
During a fireside chat with Stripe cofounder Patrick Collison in April, Huang quipped that he prefers to ""torture employees into greatness,"" a statement that seems less hyperbolic in light of the Bloomberg report detailing Nvidia's intense work culture.
According to one employee account, they were expected to work seven days a week, with  their shift ending around 1 or 2 a.m. Another staff member described being regularly pulled into between seven to 10 meetings in a single day, which would escalate into heated arguments. Both individuals said that they would have departed from the company sooner if it weren't for the requirement to wait for their stock grants to fully vest.
Employee stock grants that vest over four years create a strong financial incentive for workers to remain at the company, even in a high-stress environment.
The chip giant becoming one of the world’s most valuable companies has created an unexpected and unique challenge for Nvidia: motivating long-term employees who have become multi-millionaires through stock appreciation.
Many long-tenured staff find themselves in a state of ""semi-retirement"" due to their substantial wealth from stock options. This phenomenon has led to internal tensions, as these employees’ engagement levels have reportedly diminished, Business Insider reported.
This issue came to a head at a December companywide meeting, where Huang addressed concerns about ""semi-retired"" workers. In response, he urged all employees to take ownership of their work, encouraging them to act as ""CEOs of their time.""
When managers observe declining performance or engagement from their team, they often feel compelled to increase oversight and control as a way to try to improve results. This can manifest as micromanagement behaviors.
Low productivity may cause supervisors to lose trust in employees' abilities to work independently, leading them to monitor work more closely and provide excessive direction. Additionally, disengaged employees who aren't communicating proactively could prompt managers to check in more frequently and request constant meetings and updates.
Despite reports of overwork and a stressful work environment at Nvidia, the company has a remarkably low turnover rate of 2.7%, compared to the semiconductor industry average of 17.7%. Moreover, it ranked No. 2 on Glassdoor’s annual “Best Places To Work” list in 2024, marking its fourth consecutive year among the top five companies.
A spokesperson for Nvidia declined to comment on the matter."
231,https://www.forbes.com/sites/siladityaray/2024/09/04/nvidia-slide-wipes-10-billion-from-ceo-jensen-huangs-fortune-as-selloff-continues-after-hours/,Nvidia Slide Wipes $10 Billion From CEO Jensen Huang’s Fortune—As Selloff Continues In Pre-Market,"Sep 04, 2024, 07:08am EDT",Siladitya Ray,"Nvidia CEO and co-founder Jensen Huang's fortune shrank by nearly $10 billion after the chipmaker’s share price fell amid a wider selloff of major tech stocks on Tuesday.
According to our estimates, Huang’s net worth was $94.6 billion as of early Wednesday, after falling by $9.8 billion on Tuesday.
The nearly $10 billion wipeout dropped the Nvidia CEO from 14th to 16th place on Forbes’ Real-Time Billionaires list, below Michael Dell, the founder and CEO of computer maker Dell, and Walmart heir Jim Walton.
On Tuesday, Nvidia’s share price fell by 9.5% ending the day at $108—as the company’s market cap was routed by a record $279 billion.
Nvidia’s stock fell a further 2.4% in after-hours trading following a Bloomberg report that the company had been subpoenaed by the DOJ as part of an antitrust probe.
In early trading on Wednesday, the stock was down to $105.80 almost 2% below Tuesday’s closing price.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
$90.6 billion. That is how much Huang’s fortune has risen since 2019, when Forbes estimated he was the 546th-richest person in the world. At the start of 2023, Huang’s net worth was $21.1 billion.
According to Bloomberg, Nvidia received subpoenas from the Justice Department as the agency expanded its probe into potential antitrust violations by the chipmaker. The subpoenas, which oblige Nvidia to respond, come after the agency sent out questionnaires on the matter. The DOJ’s investigation is reportedly focused on concerns about Nvidia allegedly making it harder for its customers to switch to chips made by other companies. The agency has not yet filed a formal complaint against Nvidia, but the subpoenas bring it one step closer.
Earlier this week, Politico reported that the DOJ was investigating Nvidia’s acquisition of Israeli start-up Run:ai, amid concerns about major AI technologies being cornered by a small group of dominant tech companies. Nvidia announced the acquisition in April this year, without disclosing a price tag.
Nvidia’s shares are down more than 14% since the company published its second-quarter earnings report last week. Nvidia reported a record $30 billion in sales for its second fiscal quarter ending in July—beating out analyst expectations of $28.6 billion. For its third fiscal quarter, Nvidia has forecast revenue of $32.5 billion, plus or minus 2%, which is slightly higher than average analyst estimates. The projection, however, was short of some of the loftier forecasts made by analysts, causing the company’s stock to fall. Reports of Nvidia’s next flagship AI chip, Blackwell, facing potential production delays have also raised concerns about the chipmaker sustaining its explosive growth.
Nvidia Stock Plunges 10% Amid Broader Stock Losses As Rocky September Kicks Off (Forbes)
Nvidia Gets DOJ Subpoena in Escalating Antitrust Probe (Bloomberg)"
232,https://www.forbes.com/sites/antoniopequenoiv/2024/09/04/nvidia-denies-it-was-subpoenaed-in-justice-department-antitrust-probe/,Nvidia Denies It Was Subpoenaed In Justice Department Antitrust Probe,"Sep 04, 2024, 05:09pm EDT",Antonio Pequeño IV,"An Nvidia spokesperson said Wednesday the company was not subpoenaed by the Justice Department, contrasting a report Tuesday that the company received a subpoena for information as part of an ongoing antitrust investigation into some of the technology sector’s biggest players.
Nvidia spokesperson John Rizzo told Forbes the company inquired with the Justice Department and has “not been subpoenaed,” noting Nvidia is “happy to answer any questions regulators may have about our business.”
Bloomberg reported Tuesday a subpoena was sent to Nvidia and other unnamed technology companies, citing unnamed people familiar with the investigation and reporting the Justice Department’s San Francisco office was spearheading the probe.
The outlet reported Wednesday, citing an unnamed source, that Nvidia received “a civil investigative demand, which is commonly referred to as a subpoena,” requesting details about its business and its RunAI acquisition.
Antitrust officials believe Nvidia may be making it more difficult for buyers to switch to other chip suppliers while penalizing those that do not exclusively purchase their AI chips, Bloomberg reported—a concern previously shared among those in the chipmaking industry, according to The New York Times.
The investigation into Nvidia has a focus on the company’s $700 million acquisition of AI management firm RunAI as regulators are concerned the deal makes finding alternatives to Nvidia chips difficult, according to Bloomberg.
The Justice Department did not immediately respond to Forbes’ request for comment.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
Nvidia shares closed down 9.5% on Tuesday at $108, later dropping more than 1.5% in after-hours trading. The chip designer has been in a slump since reporting record-setting earnings last week that failed to meet investors’ loftiest expectations. Shares dropped another 1.7% on Wednesday, and are now down more than 15% in the past week. However, the tech company’s stock is still well up on the year after starting January at $48.17 per share.
Nvidia is one of multiple tech giants involved in the Justice Department investigation, which is also looking into Microsoft’s partnership with OpenAI. The Nvidia probe comes as the company is estimated to control between 70% and 95% of the market for AI chips, according to CNBC. The company’s largest and most notable customers include Microsoft, Meta, Amazon and Google parent company Alphabet, with Microsoft and Meta allocating 40% of their budgets to Nvidia’s hardware, Bloomberg reported. Regulators have not filed a formal complaint against Nvidia, which also dealt with Justice Department subpoenas in 2006 over an antitrust investigation into its graphics chips.
U.S. Regulators Are Looking Into Microsoft, Nvidia And OpenAI (Forbes)
Nvidia’s French Offices Raided Over Antitrust Concerns, Report Says (Forbes)"
233,https://www.forbes.com/sites/dereksaul/2024/09/03/nvidia-stock-plunges-8-amid-broader-stock-losses-as-rocky-september-kicks-off/,Nvidia Stock Plunges 10% Amid Broader Stock Losses As Rocky September Kicks Off,"Sep 03, 2024, 01:33pm EDT",Derek Saul,"U.S. stock indexes staggered toward their worst day in weeks, with losses from trillion-dollar technology companies like Nvidia propelling the decline as lingering concerns about global economic growth weighed on commodity and equity prices alike.
All three major U.S. stock indicators suffered easily their steepest declines since Aug. 5’s crash sparked by global slowdown angst.
The benchmark S&P 500 declined 2.1% in September’s first day of trading, while the bluechip Dow Jones Industrial Average sank 1.5%, or 626 points, and the tech-concentrated Nasdaq tanked 3.3%.
Driving Wall Street’s negative attitude was a morning report from the Institute for Supply Management, which revealed lighter manufacturing activity in the U.S. than forecasted in July, accelerating investors’ growth worries as the report can forewarn a broader weakening in economic activity, and investors also broadly positioned for a month which has been poor for stocks this decade.
Worst hit by the risk-off selloff were big technology companies often considered more sensitive to economic slowdowns given their elevated valuations.
Shares of the U.S.’ six tech firms with market capitalizations over $1 trillion each declined more than 1%, led by artificial intelligence kingpin Nvidia’s 8% dive and iPhone maker Apple’s 3% drop, knocking out roughly $280 billion in market value for Nvidia and about $95 billion for Apple.
The stock slump, particularly modest considering the S&P has returned a blistering 19% year-to-date, evokes some less than stellar memories of Septembers past. The leading U.S. index fell 3.9% in Sept. 2020, 4.8% in Sept. 2021, 9.3% in Sept. 2022 and 4.9% in Sept. 2023, working out to an average decline of 5.7% for the S&P in the month over the last four years."
234,https://www.forbes.com/sites/gurufocus/2024/08/30/will-nvidia-continue-to-dominate/,Will Nvidia Continue To Dominate?,"Aug 30, 2024, 12:44pm EDT",GuruFocus,"By Yiannis Zourmpanos
Summary


Nvidia Corp. (NVDA, Financial) has been leading the headlines after its first-quarter 2025 results surpassed the Street's expectations. The performance includes a massive 262% year-over-year revenue boost and 18% sequential growth. With that, the company exceeded its guidance by $1.50 billion. This is alongside a 462% jump in non-GAAP earnings per share. These numbers indicate Nvidia's lead in artificial intelligence-driven markets like data centers, reflecting 87% of its top line.
There has been a surge in demand for Nvidia's HGX platform, which powers AI applications like large language models. While there is a supply constraint for the company's Hopper GPU platform, it has been shown that partnerships with manufacturers such as Taiwan Semiconductor (TSM, Financial) can push it to scale up production and continue beating analysts' expectations.
For the second quarter, management has guided for approximately $28 billion in revenue, continuing strong growth across all segments. This is most aggressively happening within AI and data centers. For Nvidia, market forecasts look conservative compared to its recent performance. This is due to the fast-paced adoption of AI technologies. Moreover, the operational leverage and improving margins hint at earnings per share increases and revenue growth boosts.
Lastly, Nvidia will benefit from increased AI spending. Research points to a 50% increase in data center capital expenditures for AI in 2025. Analysts have underappreciated the company's potential and remain surprised at its dominance in AI. One can expect it will beat earnings again in the second quarter, setting the stock up for further upside.
Oracle Cloud leverages Nvidia's powerful GPUs to accelerate AI, LLMs and digital twins for enterprise innovation
Moreover, Oracle Cloud Infrastructure—also known as OCI—recently expanded the availability of new compute instances accelerated by the newest Nvidia GPUs. These new compute instances include the latest Nvidia L40S GPU and a corresponding virtual machines that are set to be released soon. All these enhancements to Oracle's (ORCL, Financial) offerings indicate or prove Nvidia's leading position in the computing power market in demand for generative AI, LLMs and digital twins. Hence, these technologies have become crucial for enterprises to target improvements in operational edge and product innovations.
Further, Nvidia's GPUs represent the apex in performance for a wide range of workload applications, from AI training and inference to advanced graphics and video processing. The L40S GPU derived multi-workload acceleration from bump into breakthrough, making it perfect for enterprises looking to use generative AI and other advanced technologies. Hence, sharp capabilities coupled with the Oracle infrastructure and Nvidia's GPUs become a strong value proposition.
Nvidia-powered OCI instances revolutionize enterprise AI with L40S GPUs, driving demand and boosting growth
OCI's new instances accelerated by Nvidia GPUs are progressive within the enterprise market. These instances include the Nvidia L40S GPU, which excels in various workloads. The L40S offers significant improvements in AI training and inference that are vital for enterprises deploying large language models and generative AI. The L40S GPU also excels in graphics and media acceleration, which is ideal for digital twin applications.
Additionally, enterprises may leverage these capabilities to design, simulate and optimize products and processes before production. Further, OCI's bare-metal compute architecture, featuring the L40S, ensures high throughput and low latency for AI and machine learning workloads. The use of Nvidia BlueField-3 DPUs further enhances server efficiency. With that, OCI's supercluster delivers ultra-high performance with up to 3,840 GPUs.
Certainly, Nvidia's collaboration with OCI marks its strategic lead in the cloud computing space. This partnership will derive solid revenue growth from enterprises accessing powerful computing resources on demand. It also highlights Nvidia's focus on advancing AI and other transformative technologies.
Data Center revenue soars
In the first quarter, Nvidia's Data Center revenue soared 23% sequentially to $22.6 billion. This massive growth indicates high demand for AI and cloud services in the data center segment. Companies increasingly adopt AI solutions with Nvidia's GPUs, which are essential for AI workloads. Cloud providers considerably rely on Nvidia's tech to expand cloud infrastructures. As a result, Data Center revenue surged 427% year over year, marking its market dominance.
Moreover, the company leads in AI hardware with the CUDA platform. Nvidia is forging a competitive AI and deep learning software ecosystem, even as it collaborates deeply and sometimes opaquely with the major cloud providers, such as Amazon's (AMZN, Financial) AWS, Microsoft (MSFT, Financial) Azure and Alphabet's (GOOG, Financial) Google Cloud. These alliances enhance Nvidia's reach and further drive its data center revenue. The platform supports AI and deep learning, making the company's software ecosystem competitive.
According to tech expert Tharindu Fernando, Nvidia's leadership in AI hardware and strategic partnerships, particularly with major cloud providers like AWS and Azure, position it at the center of the AI revolution, with continuous innovation driving further growth. Nvidia's market share in data centers is also growing, outpacing competitors. Therefore, its technology is in high demand, and prospects for data centers are bright.
Massive growth potential despite future margin pressures
The potential of Nvidia remains unmatched in the AI space, with it commanding a top position in supplying the basic technology needed for powering AI infrastructure. Any sophisticated AI models' training and deployment—the OpenAI ChatGPT included—are now inseparable from the GPUs and APUs enhanced by CUDA software offered by the company.
This strategic lead is further backed by Nvidia's deep ecosystem, making it a large moat that will be hard to cross competitors. Its hold on the AI chip market—Nvidia commands an estimated 70% to 95% market share for AI training chips—demonstrates its products' importance to this fast-growing industry. In addition, Nvidia commands extreme pricing power, which is reflected in its impressive gross margins of about 78%.
While Nvidia's margins have been at all-time highs, there is evidence this degree of profitability cannot be sustained. Over time, competition could chip away at its pricing power and result in a steady downward trend for gross and net income margins. The well over 50% net income margin Nvidia has is extraordinary, but it may contract back down into the 40% to 45% range of more historical notes. While this reduction could hurt the stock price over the long term, this deflation is not likely to occur sharply.
Considering Nvidia's enormous lead and stronghold on the AI market, any decrease in profitability would likely happen gradually, allowing an investor to ride further gains for years to come. To that end, it does not demean the company's long-term potential, even with potential profitability erosion in the far future.
Conclusion
Nvidia's strategic partnership with Oracle Cloud Infrastructure extends its AI and advanced computing lead. With the GPU-accelerated instances being available on OCI, there is a growing demand for high-performance computing. Further, this collaboration points to the company's role in driving tech advancements across industries. With that, the stock represents a solid buy, given its strong market position and growth potential.
Finally, investors should watch for further developments in Nvidia's GPU offerings. The upcoming H200 and Blackwell GPUs will yield greater performance improvements. These advancements will likely derive continued demand for Nvidia's products within enterprises adopting AI, digital twins and advanced graphics.
I am/we currently own positions in the stocks mentioned, and have NO plans to sell some or all of the positions in the stocks mentioned over the next 72 hours."
235,https://www.forbes.com/sites/maryroeloffs/2024/08/29/nvidia-falls-more-than-4-on-opening-despite-beating-sales-profit-expectations/,"Nvidia Falls More Than 4% On Opening Despite Beating Sales, Profit Expectations","Aug 29, 2024, 10:01am EDT",Mary Whitfill Roeloffs,"Shares of Nvidia opened down more than 4% Thursday morning after reporting comparatively low—though still above expectations—sales and profit growth in the second quarter Wednesday, a potential sign that the rapid rise of artificial intelligence chip-making stocks could start to slow.
Nvidia's shares opened down 4.28% after also falling in after-market trading late Wednesday and pre-market Thursday, but quickly pared losses and were back up to $122.56 (down 2.43%) by 9:55 a.m.
The fall came despite the company beating Wall Street's earnings expectations in its second quarter report Wednesday—sales were up 122% from the same period last year to $30 billion and profits beat analyst projections at $16.6 billion—and the announcement of a new $50 billion share buyback.
But for a company whose shares have risen more than 150% this year to date, analysts warned ahead of time it was going to take more than the realized sales to see a rise in Nvidia's stock after the report.
Nvidia is forecasting $32.5 billion in revenue for next quarter, on the lower end of analyst expectations that range anywhere from $31.9 billion to $37.9 billion.
Nvidia's gross margins also dipped slightly, with the company predicting a “mid-70% range” for the year, potentially lower than an estimated full-year margin of 76.4%.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
""They beat but this was just one of those situations where expectations were so high,” JJ Kinahan, CEO of IG North America and president of online broker Tastytrade, told Reuters. “I don't know that they could have had a good enough number for people to be happy.”
Shares of other chip companies including Broadcom, ARM, Marvell and Intel were also trading down after-hours Wednesday, but largely recovered on opening Thursday morning. Broadcom was trading at $158.56 (up 0.24%), Arm at $125.59 (up 0.22%), Marvell at $69.04 (up 0.88%) and Intel at 19.94 (up 1.68%). U.S.-listed shares of TSMC, Nvidia's chip manufacturing partner, opened at $168, down a marginal 0.73%.
Artificial intelligence chipmaker Nvidia's stock has helped power the market larger rally this year and an AI craze on Wall Street. Nvidia is one of only three U.S. companies to ever achieve a $3 trillion valuation (joining Apple and Microsoft), and its shares have risen an impressive 154% so far this year. But concerns about sustainability and how long it will take AI to become profitable for tech giants has led to some skittishness from investors, impacting the overall chip stock landscape. Earlier this summer, the announcement of layoffs and disappointing earnings saw Intel stock drop 29%—its worst day in decades—and ignited a massive selloff of global semiconductor stocks. The selloff dragged down shares of TMSC, ARM and Nvidia, though some analysts at Bank of America and Jefferies later said the reaction was overblown."
236,https://www.forbes.com/sites/bethkindig/2024/08/29/nvidia-stock-is-selling-off-its-not-because-of-blackwell/,Nvidia Stock Is Selling Off: It’s Not Because Of Blackwell,"Aug 29, 2024, 09:08pm EDT",Beth Kindig,"My firm I/O Fund extrapolated supply chain data to conclude that Nvidia's new AI chip on Blackwell architecture is in production at Taiwan Semiconductor Manufacturing Company (TSMC) and Super Micro last week in the analysis “Nvidia Stock: Blackwell Suppliers Shrug Off Delay Ahead of Q2 Earnings.” The media was making much ado about nothing (and astonishingly, still is) despite crystal clear confirmation from Nvidia’s management team that all is well.
Given these delay rumors, it was widely expected that Nvidia’s management would provide some transparency in Q2 as to the status of Blackwell. I joined “Making Money with Charles Payne” on Fox Business Network shortly before Nvidia’s report, telling host Charles Payne that “we are getting bullish signals from the supply chain,” such as TSMC’s high-performance computing (HPC) growth and Super Micro’s liquid cooling growth, and that I “fully expect Nvidia’s management team to calm any concerns about the outlook for Blackwell.”
Direct liquid cooling doesn’t lie, as it’s intricately linked to the Blackwell launch, implying that Blackwell would indeed ship by Q4 – and Nvidia just confirmed that (multiple times) in Q2’s release:
“Blackwell production ramp is scheduled to begin in the fourth quarter and continue into fiscal 2026. In the fourth quarter, we expect to ship several billion dollars in Blackwell revenue.”
Later in the earnings call, Nvidia CEO Jensen Huang stated: “There were no functional changes necessary. And so we're sampling functional samples of Blackwell — Grace Blackwell in a variety of system configurations as we speak. There are something like 100 different types of Blackwell-based systems that are built that were shown at Computex. And we're enabling our ecosystem to start sampling those. The functionality of Blackwell is as it is, and we expect to start production in Q4.”
My firm I/O Fund had published for our premium members going into the print that the valuation was stretched, and it would require fiscal year revisions to create room in the valuation. As you’ll see below, we got a few revisions today, which is paramount for the stock price. Will these upward revisions be enough to sustain the price? I look at this and more below.
Q2’s revenue of $30.04 billion increased 122% YoY and 15% QoQ, with management pointing out that “customers continue to accelerate their Hopper architecture purchases while gearing up to adopt Blackwell.” This marked a $1.3 billion beat to the consensus estimate for $28.75 billion. It also was a deceleration from 262% YoY growth in Q1, as Nvidia is now facing tougher comps against the vertical ramp of Hopper last year. GAAP EPS of $0.67 beat estimates by $0.06, and represented YoY growth of 168% and QoQ growth of 12%.
Nvidia guided for Q3 revenue of $32.5 billion, once again above consensus estimates, though it was only $700 million higher than the $31.77 billion estimate at the midpoint. This represents growth of 79.4% YoY at midpoint, compared to the estimate for 75.3% growth next quarter. Despite this being one of the “smaller” beats in recent quarters, it’s a testament to the strength of Nvidia’s demand to guide for $2.5 billion sequential growth primarily based on Hopper demand with no contribution from Blackwell.
Data center revenue surpassed a $105 billion annualized run rate this quarter, up from $90 billion annualized last quarter, as Nvidia reported $26.27 billion in data center revenue, up 152% YoY and 16% QoQ. Nvidia said that “Hopper demand is strong, and shipments are expected to increase in the second half of fiscal 2025,” while Blackwell is on track to ramp in Q4 with “several” billions in revenue expected that quarter.
Notably, purchase commitments and obligations for inventory and capacity rose nearly 48% QoQ to $27.8 billion, including “new commitments for Blackwell capacity and components,” another signal that Nvidia is prepared to ramp in full-force come Q4.
In the segment, compute revenue was $22.6 billion, up 162% YoY, while networking revenue was $3.67 billion, up 114% YoY. In networking, Nvidia noted that InfiniBand and Ethernet drove growth in the quarter, and the 16% QoQ growth included “a doubling of Ethernet for AI revenue.”
Nvidia’s Q3 revenue guide implies data center revenue above $28 billion to $28.5 billion, which I had modeled in my firm's pre-earnings analysis earlier this week.
Nvidia cleared the delay concerns for Blackwell, saying that they “shipped customer samples of our Blackwell architecture in the second quarter. We executed a change to the Blackwell GPU mask to improve production yield. Blackwell production ramp is scheduled to begin in the fourth quarter and continue into fiscal 2026,” with several billion in Blackwell revenue expected in Q4. Purchase commitments reiterated that Nvidia is serious about launching on schedule, and lining up the capacity and components to launch in full-force by the end of the year.
I spoke with Yahoo Finance on Thursday morning following the report, reemphasizing that the delay concerns were “completely thrown off the table last night. … Wall Street obviously is very closely tied to estimates, and my firm never saw revisions downward based on the so-called delay. … Nvidia beat, and they’re saying Blackwell is basically on time,” which is “not a concern — if anything, it’s extremely bullish.”
However, I cautioned on the valuation: “When you have a high-flyer like Nvidia, you get stretched at times. Going into the print, we warned our members that this valuation is looking a little toppy. What we need is for the fiscal year estimates next year to go up, so we’re in a waiting game for analysts to revise their estimates upward, which eventually they will, but until then the valuation is stretched.”
This morning, while I was being interviewed by Yahoo, I’ve already seen analyst estimates for Nvidia’s revenue revised higher following the report:


However, the true impact of Blackwell is yet to be seen in these estimates, with the only clues right now being Q3’s $32.5 billion guide and expectations for several billion in Blackwell revenue in Q4.  From a long-term perspective, I explained on Yahoo Finance that the first “pathway for growth is to pay very close attention to Nvidia around the fiscal year guide,” while the “second-biggest moment of the year will be when Blackwell is shipping in volume. This will be the Q2 report, but we’ll get some signs in Q1 with that forward guide.” I believe that “early next year will be fireworks” for Nvidia, similar to Hopper’s moment in the fiscal Q1 report in May 2023.
Margins remained strong in Q2, with Nvidia reporting gross and operating margins at the high end and above its guided ranges. However, management guided for Q3 margins to contract slightly QoQ, suggesting that Q1 was the peak for both gross and operating margins with some pressure ahead as Blackwell gears up to launch in Q4.


The chart above shows Nvidia’s margins, with the slight sequential contraction this quarter and next quarter visible. It’s no small feat to maintain GAAP operating margin >60% for four consecutive quarters while simultaneously undergoing the semiconductor industry’s most advanced and most rapid product release cycle. However, with management guiding for full-year gross margins to be in the mid-70% range, my firm will be keeping a close eye on how margins trend in Q3 heading into Q4 as Blackwell ramps — where the market is a tad concerned is gross margins, which peaked at 78.4% and will exit the year in the mid-70% range.
My firm's pre-earnings writeup expressed concerns about the valuation going into the print, and I think the selling on Thursday reflects the valuation. My firm stuck our neck out over the past few weeks to bring quality information to my readers on how the supply chain for Blackwell is ramping. My firm was the first and only firm that I’m aware of to present actionable data that countered what other media outlets were reporting. To refresh your memory, media outlets stated Blackwell was delayed into Q1: “If the upcoming AI chips, known as the B100, B200 and GB200, are delayed three months or more, it may prevent some customers from operating large clusters of the chips in their data centers in the first quarter of 2025, as they had planned.”
In contrast, my analysis stated: “From the horse’s mouth, Nvidia’s own management team, it was stated during the GTC Financial Analyst Day in March that the very first systems will ship in Q4, but to expect constraints.”
Well, we have the answer – Blackwell is, in fact, shipping in Q4 and ramping in Q1. Purchase commitments up 48% QoQ help to reflect how serious the company is when it comes to the speed of ramping shipments.
Earnings reports are truly 50/50 – nobody can tell you what the market will do following a report. For example, I had high confidence Nvidia would beat, but there’s much more to consider than a beat. What’s important is to have a strategy. My firm champions actively managing tech positions rather than buy-and-hold. My firm's plan is to trim Nvidia at key levels and attempt to buy lower. This is due to valuation concerns, but also importantly, many AI stocks are trading at stretched valuations. I’ve stated publicly a few times that Nvidia is a buy on dips, implying investors who are patient will find entries at lower prices.
The I/O Fund uncovered clues in the supply chain regarding Blackwell’s launch, and outlined for readers why the delay rumors were no cause of concern prior to the report. My firm also bought Nvidia on July 31 at the recent low of $105. To get real-time trade alerts of our next buy and sell of the market leader Nvidia, take advantage of our special Labor Day sale and learn more here.
If you would like notifications when my new articles are published, please hit the button below to ""Follow"" me.
Please note: The I/O Fund conducts research and draws conclusions for the company’s portfolio. We then share that information with our readers and offer real-time trade notifications. This is not a guarantee of a stock’s performance and it is not financial advice. Please consult your personal financial advisor before buying any stock in the companies mentioned in this analysis. Beth Kindig and the I/O Fund own shares in NVDA at the time of writing and may own stocks pictured in the charts"
237,https://www.forbes.com/sites/petercohan/2024/08/28/nvidia-stock-down-after-hours-despite-q2-earnings-beat-and-raise/,Nvidia Stock Down On Slower Growth Despite Q2 Earnings Beat And Raise,"Aug 28, 2024, 04:37pm EDT",Peter Cohan,"Nvidia reported its latest quarterly results on August 28, exceeding Wall Street expectations and guidance as well as showing stronger-than-expected guidance for Q2, CNBC noted.
Here are the key numbers:


Demand for Nvidia’s Blackwell chips is “incredible,” CEO Jensen Huang said in a press release. “Global data centers are in full throttle to modernize the entire computing stack with accelerated computing and generative AI,” he added.
Nvidia said it shipped samples of Blackwell chips during the quarter, and changed the product to make it more efficient to manufacture. “In the fourth quarter, we expect to ship several billion dollars in Blackwell revenue,” Nvidia CFO Colette Kress wrote in a statement.
Nvidia said product issues were largely responsible for a decline in gross profit margins between the first and second quarters of 2024, noted the Wall Street Journal. More specifically, the company’s second quarter gross margin of 75.1% declined 3.3 percentage points from the previous period.
For the full fiscal year, Nvidia — which approved $50 billion in share buybacks — expects gross margins to be in the “mid-70% range” — slightly below the StreetAccount consensus of 76.4%, CNBC reported.
Nvidia also expects Hopper, the company’s current-generation chip, to increase total shipments for the next two quarters.
Customers are enjoying a fast return on investment on the company’s chips. “The people who are investing in Nvidia infrastructure are getting returns on it right away,” Huang said on a call with analysts. “It’s the best ROI infrastructure, computing infrastructure investment you can make today.”
What is the source of Nvidia’s return on investment? Despite charging a higher price than competitors do, the company’s chips perform better and cost less to run — more than offsetting their higher price due to their lowest total cost of ownership, according to my new book, Brain Rush.
While Nvidia’s results are impressive, the company’s growth is slowing down. For example, the AI chip designer’s earnings grew at an average of 500% while the company’s revenues grew in a range of 206% to 265%, during the previous three quarters, according to Investor’s Business Daily.
Nvidia’s forecast of 80% revenue growth in the third quarter represents a marked slowdown from the previous pace. “It appears the bar was just set a tad too high this earnings season,” Ryan Detrick, chief market strategist at Carson Group, told the Associated Press.
“Death, taxes, and NVDA beats on earnings are three things you can bank on. Here’s the issue. The size of the beat this time was much smaller than we’ve been seeing. Even future guidance was raised, but again not by the tune from previous quarters,” he added."
238,https://www.forbes.com/sites/antoniopequenoiv/2024/08/28/nvidia-earnings-ai-chip-designer-reports-record-revenues/,Nvidia Tops Expectations Again After Reporting Record Revenues,"Aug 28, 2024, 04:40pm EDT",Antonio Pequeño IV,"Nvidia continued its long run of topping Wall Street expectations in its latest quarterly earnings report published Wednesday, setting multiple earnings records amid significant year-over-year gains in sales and net income.
Nvidia reported $0.67 earnings per share and a record $30 billion in sales for its second fiscal quarter ending in July, beating analyst expectations of $0.64 and $28.6 billion, according to FactSet.
Nvidia’s $30 billion in sales marks a 122% increase from the same period last year.
Net income totaled $16.5 billion in its latest quarter, signaling another profit record for Nvidia after reporting $6.1 billion in the same period last year and $14.8 billion last quarter.
The tech giant’s datacenter division, the home of most of its artificial intelligence offerings, squashed analyst forecasts and raked in a record $26.3 billion in revenue, marking a 154% year-over-year increase.
Shares of Nvidia closed up/down 2.1% at $125.61 before lowering to $121.18 in after-hours trading following the publication of its earnings report.
This is a developing story. Check back for updates.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
Blackwell, Nvidia’s highly anticipated AI superchip, has reportedly experienced production delays, according to The Information, though two of Nvidia’s supply partners have indicated a version of the chip could ship out in the fourth quarter. Delays are not likely to impact Nvidia’s third fiscal quarter, though pushouts could put pressure on the company’s stock “amidst ongoing market uncertainty around rates/geopolitics,” according to Bank of America Securities.
Nvidia’s once-core gaming unit posted a small gain in the second quarter, with $2.9 billion in sales. However, the division continues to record sales below its 2021 second quarter record of $3.06 billion as Nvidia has increasingly focused on its AI offerings.
Nvidia designs the lion’s share of the semiconductor chips used to power AI technology, allowing it to secure tech giants like Microsoft, Amazon, Meta and Alphabet as its largest customers. Nvidia’s consistent and record profit gains have contributed to the company’s dazzling stock performance since the start of the year, when it traded at $48.17 per share. The tech company has cemented its place as the best stock of 2024 on the S&P 500 and is up more than 150% on the year and 500% over the last three years.
Nvidia Earnings: Stock Rallies As AI Giant Reports 600% Profit Explosion, 10-For-1 Stock Split (Forbes)
How Nvidia Stock Could Jump 2.5x To Reach $300 (Forbes)"
239,https://www.forbes.com/sites/craigsmith/2024/08/27/cerebras-speeds-ai-by-putting-entire-foundation-model-on-its-giant-chip/,Cerebras Takes On Nvidia With AI Model On Its Giant Chip,"Aug 27, 2024, 12:00pm EDT",Craig S. Smith,"AI is everywhere these days, and we’ve become accustomed to chatbots answering our questions like oracles or conjuring up magical images. Those responses are called inferences in the trade, and the colossal computer programs from which they rain are housed in massive data centers referred to as the cloud.
Now, brace for a downpour.
Cerebras Systems, known for its revolutionary wafer-scale computer chip, big as a dinner plate, is about to unleash one of the top AI models—Meta’s open-source LLaMA 3.1—on its chip. Not beside it or above it or below it, but on it—a configuration that could blow away traditional inference.
What’s more, Cerebras claims that its inference costs are one-third of those on Microsoft’s Azure cloud computing platform, while using one-sixth the power.
“With speeds that push the performance frontier and competitive pricing, Cerebras Inference is particularly compelling for developers of AI applications with real-time or high-volume requirements,” said Micah Hill-Smith, co-founder and CEO of Artificial Analysis Inc., which provides independent analysis of AI models.
This could create a ripple effect across the entire AI ecosystem. As inference becomes faster and more efficient, developers will be able to push the boundaries of what AI can do. Applications that were once bottlenecked by hardware limitations may now be able to flourish, leading to innovations that were previously thought impossible.
For example, in the realm of natural language processing, models could generate more accurate and coherent responses. This could revolutionize areas such as automated customer service, where understanding the full context of a conversation is crucial for providing helpful responses. Similarly, in fields like healthcare, AI models could process and analyze larger datasets more quickly, leading to faster diagnoses and more personalized treatment plans.
In the business world, the ability to run inference at unprecedented speeds opens new opportunities for real-time analytics and decision making. Companies could deploy AI systems that analyze market trends, customer behavior and operational data in real-time, allowing them to respond to changes in the market with agility and precision. This could lead to a new wave of AI-driven business strategies, with companies leveraging real-time insights to gain a competitive edge.
But whether this will be a cloudburst or a deluge remains to be seen.
As AI workloads move to inference and away from training operations, the need for more efficient processors becomes imperative. Many companies are working on this challenge.
“Wafer scale integration from Cerebras is a novel approach that eliminates some of the handicaps that generic GPUs have and shows much promise,” said Jack Gold, the founder of J. Gold Associates, a technology analyst firm. He cautions that Cerebras is still a startup in a room full of big players.
Cerebras’ AI inference service not only accelerates the pace of AI model execution but could also alter the way businesses think about deploying and interacting with AI in real-world applications.
In typical AI inference workflows, large language models such as Meta’s LLaMA or OpenAI’s GPT-4o are housed in data centers, where they are called upon by application programming interfaces, or APIs, to generate responses to user queries. These models are enormous and require immense computational resources to operate efficiently. Graphics processing units, or GPUs, the current workhorses of AI inference, are tasked with the heavy lifting, but they struggle under the weight of these models, particularly when it comes to moving data between the model’s memory and its compute cores.
But with Cerebras’ new inference service, all the layers of a model—currently the 8 billion parameter and 70 billion parameter versions of LLaMA 3.1—are stored right on the chip. When a prompt is sent to the model, the data can be processed almost instantaneously because it doesn’t have to travel long distances within the hardware.
The result? For example, while a state-of-the-art GPU might process about 260 tokens—pieces of data such as a word—per second for an 8-billion parameter LLaMA model, Cerebras claims it can handle 1,800 tokens per second. This level of performance, validated by Artificial Analysis, Inc., is unprecedented and sets a new standard for AI inference.
“Cerebras is delivering speeds an order of magnitude faster than GPU-based solutions for Meta’s Llama 3.1 8B and 70B AI models,” said Hill-Smith. “We are measuring speeds above 1,800 output tokens per second on Llama 3.1 8B, and above 446 output tokens per second on Llama 3.1 70B—a new record in these benchmarks.”
Cerebras is launching its inference service through an API to its own cloud, but it is already talking to major cloud providers about deploying its model-loaded chips elsewhere. This opens a massive new market for the company, which has struggled to get users to adopt its chip, called a Wafer Scale Engine.
The speed of inference today is limited by bottlenecks in the network connecting GPUs to memory and storage. The electrical pathways connecting memory to cores can only carry a finite amount of data per unit of time. While electrons move rapidly in conductors, the actual data transfer rate is constrained by the frequency at which signals can be reliably sent and received, affected by signal degradation, electromagnetic interference, material properties and the length of wires over which the data must travel.
In traditional GPU setups, the model weights are stored in memory separate from the processing units. This separation means that during inference, there's a constant need to transfer large amounts of data between the memory and the compute cores through tiny wires. Nvidia and others have tried all sorts of configurations to minimize the distance that this data needs to travel—stacking memory vertically on top of the compute cores in a GPU package, for example.
Cerebras' new approach fundamentally changes this paradigm. Rather than etching transistor cores onto a silicon wafer and slicing it up into chips, Cerebras etches as many as 900,000 cores on a single wafer, eliminating the need for external wiring between separate chips. Each core on the Wafer Scale Engine combines both computation (processing logic) and memory (static random access memory) to form a self-contained unit that can operate independently or in concert with other cores.
The model weights are distributed across these cores, with each core storing a portion of the total model. This means that no single core holds the entire model; instead, the model is split up and spread across the entire wafer.
""We actually load the model weights onto the wafer, so it's right there, next to the core,"" explains Andy Hock, Cerebras’ senior vice president of product and strategy.
This configuration allows for much faster data access and processing, as the system doesn't need to constantly shuttle data back and forth over relatively slow interfaces.
According to Cerebras, its architecture can deliver performance “10 times faster than anything else on the market” for inference on models like LLaMA 3.1, although this remains to be further validated. Importantly, Hock claims that due to the memory bandwidth limitations in GPU architectures, ""there's actually no number of GPUs that you could stack up to be as fast as we are"" for these inference tasks.
By optimizing for inference on large models, Cerebras is positioning itself to address a rapidly growing market need for fast, efficient AI inference capabilities.
One reason why Nvidia has had a virtual lock on the AI market is the dominance of Compute Unified Device Architecture, its parallel computing platform and programming system. CUDA provides a software layer that gives developers direct access to the GPU's virtual instruction set and parallel computational elements.
For years, Nvidia’s CUDA programming environment has been the de facto standard for AI development, with a vast ecosystem of tools and libraries built around it. This has created a situation where developers are often locked into the GPU ecosystem, even if alternative hardware solutions could offer better performance.
Cerebras' WSE is a fundamentally different architecture from traditional GPUs, requiring software to be adapted or rewritten to take full advantage of its capabilities. Developers and researchers need to learn new tools and potentially new programming paradigms to work with the WSE effectively.
“Nvidia’s monopolistic grip on the AI data center will be hard to break,” said Paul Schell, an analyst at global technology intelligence firm ABI Research. “Partnerships with independent software vendors for the creation of fine-tuned enterprise grade applications to run on their platform will go one step further in tempting potential customers from competitors like Nvidia.”
Cerebras has tried to address this by supporting high-level frameworks like PyTorch, making it easier for developers to use its WSE without learning a new low-level programming model. It has also developed its own software development kit to allow for lower-level programming, potentially offering an alternative to CUDA for certain applications.
But by offering an inference service that is not only faster but also easier to use—developers can interact with it via a simple API, much like they would with any other cloud-based service—Cerebras is making it possible for organizations just entering the fray to bypass the complexities of CUDA and still achieve top-tier performance.
This is in line with an industry shift to open standards, where developers are free to choose the best tool for the job, rather than being constrained by the limitations of their existing infrastructure.
The implications of Cerebras’ breakthrough, if its claims are borne out and it can ramp up production, are profound. First and foremost, consumers will benefit from significantly faster responses. Whether it’s a chatbot answering customer inquiries, a search engine retrieving information, or an AI-powered assistant generating content, the reduction in latency will lead to a smoother, more instantaneous user experience.
But the benefits could extend far beyond just faster responses. One of the biggest challenges in AI today is the so-called “context window”—the amount of text or data that a model can consider at once when generating an inference. Inference processes that require a large context, such as summarizing lengthy documents or analyzing complex datasets.
Larger context windows require more model parameters to be actively accessed, increasing memory bandwidth demands. As the model processes each token in the context, it needs to quickly retrieve and manipulate relevant parameters stored in memory.
In high-inference applications with many simultaneous users, the system needs to handle multiple inference requests concurrently. This multiplies the memory bandwidth requirements, as each user's request needs access to the model weights and intermediate computations.
Even the most advanced GPUs like Nvidia's H100 can move only around 3 terabytes of data per second between the high bandwidth memory and the compute cores. That's far below the 140 terabytes per second needed to efficiently run a large language model at high throughput without encountering significant bottlenecks.
""Our effective bandwidth between memory and compute isn't just 140 terabytes, it's 21 petabytes per second,"" Hock claims.
Of course, it’s hard to judge a company statement without industry benchmarks, and independent testing will be key to confirming this performance.
By eliminating the memory bottleneck, Cerebras’ system can handle much larger context windows and increase token throughput. If the performance claims hold true, this could be a game-changer for applications that require the analysis of extensive information, such as legal document review, medical research or large-scale data analytics. With the ability to process more data in less time, these applications can operate more effectively.
Hock said the company will soon offer the larger LLaMA 405 billion parameter model on its WSE, followed by Mistral’s models and Cohere's Command-R model. Companies with proprietary models (hello, OpenAI) can approach Cerebras to load their models onto the chips as well.
Moreover, the fact that Cerebras’ solution is delivered as an API-based service means that it can be easily integrated into existing workflows. Organizations that have already invested in AI development can simply switch to Cerebras’ service without having to overhaul their entire infrastructure. This ease of adoption, if paired with the promised performance gains, could make Cerebras a formidable competitor in the AI market.
“But until we have more concrete real-world benchmarks and operations at scale,” cautioned analyst Gold, “it’s premature to estimate just how superior it will be.”"
240,https://www.forbes.com/sites/investor-hub/article/where-will-nvidia-nvda-stock-be-5-years/,Where Will Nvidia Stock Be In 5 Years?,"Aug 26, 2024, 05:34pm EDT",Catherine Brock,"Nvidia has been a breakout stock since 2023, as demand runs hot for the company's enterprise AI solutions. After the chip designer reported triple-digit sales and earnings growth for the first quarter of fiscal 2025, investors are wondering how far NVDA stock can realistically grow in the next five years.
Let's dive in to answer that question, starting with a teaser: Even the conservative view might make your jaw drop.
Nvidia specializes in semiconductors, primarily microprocessors known as graphics processing units or GPUs. These are used in many applications, including Nvidia's own gaming PCs, autonomous vehicles and 5G RAN networks. But where Nvidia has become dominant recently is in AI-capable chips and related software.
Techspot reports that Nvidia owns 88% of the GPU market as of the first quarter 2024. That compares to 80% market share in the prior quarter. The jump in positioning came at the expense of competitors Advanced Micro Devices (AMD) and Intel (INTC), which both lost share in the first quarter.
In the first quarter of fiscal year 2025, Nvidia generated $26 billion in sales and non-GAAP diluted earnings per share of $6.12. Relative to the prior-year quarter, sales grew 262% and non-GAAP diluted EPS was up 461%. The company also increased its gross margin by 12.1 points.
Over the past 18 months, Nvidia has quadrupled its annual sales, added nearly 20 points to its gross margin and increased non-GAAP diluted EPS by more than five times. The stock price has gained nearly 500% as a result.
From a balance sheet perspective, Nvidia is strong. The chip designer's cash balance doubled in 2023, and the company now has more cash than debt.
In five years, the AI chip market is expected to be worth $311 billion, according to MarketsandMarkets. This translates to a compound annual growth rate of 20.4% from 2024. Applying that growth rate to Nvidia's data center sales, assuming the current annual run rate of about $90 billion, equates to more than $225 billion in AI-related sales by 2029. That alone could justify another doubling of Nvidia's stock price by 2029, assuming the margins don't degrade terribly.
While this line of thinking also assumes Nvidia won't lose market share in AI, which is unlikely, it also ignores the rest of Nvidia's business. Note, too, that other estimates predict a steeper trajectory for overall AI chip sales. As reported by CNBC, Nvidia's competitor AMD expects the AI chip market to reach $400 billion by 2027.
For those reasons, it doesn't feel farfetched to expect Nvidia's market cap to double in the next five years to $6 trillion. That estimate is actually conservative compared to other views. Beth Kindig of the I/O Fund, for example, expects Nvidia to reach $10 trillion in market cap by 2030.
Let's review some of the factors in play for Nvidia going forward.
Three growth drivers for Nvidia include its Blackwell platform launch, government demand for AI hardware and data center demand for hardware that supports multiple use cases, such as AI and 5G RAN.
Nvidia opened its doors in 1993 with a mission to revolutionize PC and video game graphics. Since then, the company has launched several industry firsts, including the GPU, the mobile GPU, the programmable GPU and the mobile workstation GPU.
In the early 2010s, CEO Jensen Huang identified AI as the company's next big opportunity—roughly a decade before the masses had heard of ChatGPT. Thanks to Huang's forethought and vision, Nvidia has established a dominant first-mover position in a technology that could revolutionize the way business is done.
With its forward-thinking culture and innovation mindset, Nvidia will likely continue to identify opportunities, reinvent and create new markets—at least as long as Huang is running the show.
Major market opportunities for Nvidia going forward include AI, gaming and digital-twin technology.
Nvidia is the AI chip leader now, but competition will surely increase over the next five years. The closest competitors today are chipmakers AMD and Intel. Cloud providers are also a threat as they work to develop their own AI solutions. Some of these providers, including Microsoft, Alphabet and Amazon (AMZN), are Nvidia customers today.
U.S. sanctions on high-performance chips sales to China have already affected Nvidia. These sanctions were first introduced in 2022 and then updated in 2023. Nvidia has developed lower-performance chips that are compliant, but Chinese demand for these so far has been weak.
Chinese sales contributed 17% to Nvidia's fiscal 2024 revenue. Chinese research firm CCID Consulting predicts that China will account for 30% of the global AI market by 2035, according to Reuters.
Three risks to monitor for Nvidia are market volatility, rising competition and the inherent uncertainty of innovation.
Right now, it's hard to imagine Nvidia doing anything but creating more value. The company has a lot going for it: dominance in one of investing's most popular industries, an enviable customer list, tons of cash, a knack for spotting opportunity and a proven process for innovating. These factors position the chip designer well for long-term growth.
It almost sounds too good to be true, which is a good reason to step lightly into a Nvidia position. Yes, the company stands to double or triple in value, again, in the next several years—but it won't be a smooth ride. Given that Nvidia's multiples are well above their five-year averages, you could use the volatility to your advantage. Waiting for a pullback will amplify your growth potential and take some of the stress out of the position.
Nvidia is a good company with loads of opportunity ahead. Unquestionably, there is long-term growth potential for investors who buy at the right price and can handle a wild ride.
What are the main drivers of Nvidia's growth?
Nvidia may also see rising demand for its products from multitenant data centers and government agencies.
How does Nvidia compare to its competitors?
Analysts say Nvidia's hardware and software are years ahead of the competition.
What are the risks associated with investing in Nvidia?
Risks of investing in Nvidia include market volatility, rising competition and the inherent uncertainty of innovation.
How will global expansion impact Nvidia's stock?
Global expansion of AI infrastructure will drive demand for Nvidia's products. Sanctions prevent Nvidia from selling its highest-performance chips into China, however, and that may allow international competitors to gain ground.
What should investors expect from Nvidia in the next five years?
If the AI market grows according to predictions and Nvidia retains margins and most of its market share, the company could double or triple in value in the next five years."
241,https://www.forbes.com/sites/janakirammsv/2024/08/23/google-brings-serverless-inference-to-cloud-run-based-on-nvidia-gpu/,Google Brings Serverless Inference To Cloud Run Based On Nvidia GPU,"Aug 23, 2024, 03:33am EDT",Janakiram MSV,"Google Cloud's recent enhancement to its serverless platform, Cloud Run, with the addition of NVIDIA L4 GPU support, is a significant advancement for AI developers. This move, which is still in preview, improves the platform's ability to handle complex AI workloads, allowing developers to deploy, scale and optimize AI-powered applications more efficiently.
Cloud Run has already established itself as a go-to serverless platform for developers due to its simplicity, fast autoscaling and pay-per-use pricing model. These features allow for rapid deployment and scaling of applications without the need to manage servers. With the integration of NVIDIA L4 GPUs, the platform’s capabilities are now extended to support real-time AI inference, a crucial requirement for many emerging generative AI applications. The NVIDIA L4 GPU is designed for inference at scale, offering up to 120 times the video performance compared to CPUs and 2.7 times the performance for generative AI tasks compared to previous generations of GPUs.
This integration is particularly beneficial for deploying lightweight generative AI models and small language models such as Google’s Gemma and Meta’s Llama. These models are popular for tasks like chatbot creation, real-time document summarization and various other AI-driven functions. For example, Google’s Gemma models (2B and 7B) and Meta’s Llama models (8B) can be used to develop highly responsive and scalable AI applications. The introduction of NVIDIA L4 GPUs in Cloud Run ensures that these models can run efficiently, even during peak traffic, without compromising performance.
Deploying AI models on Cloud Run with NVIDIA L4 GPUs is designed to be a seamless process. Developers can create container images that include the necessary dependencies, such as NVIDIA GPU drivers and the AI model itself. Once the container is built and pushed to a container registry, it can be deployed on Cloud Run with GPU support enabled. This process allows businesses to take full advantage of Cloud Run’s scalability and NVIDIA’s powerful GPUs without the need for specialized infrastructure management.
The platform’s flexibility extends to supporting various Google Cloud services, including Google Kubernetes Engine and Google Compute Engine, giving developers the choice of the level of abstraction they need for building and deploying AI-enabled applications. This flexibility is critical for businesses looking to tailor their AI deployments to specific needs while ensuring that they can scale efficiently as demand fluctuates.
The enhanced capabilities of Cloud Run with NVIDIA L4 GPUs extend beyond just AI inference. They also enable a variety of other compute-intensive tasks such as on-demand image recognition, video transcoding, streaming and 3D rendering. This makes Cloud Run a versatile platform that can cater to a wide range of applications, from AI-driven chatbots to media processing services. The flexibility offered by the platform, coupled with its ability to scale down to zero during inactivity, ensures that businesses can optimize costs while maintaining high performance during active usage.
A key aspect of this development is the NVIDIA-Google Cloud partnership, which aims to provide advanced AI capabilities across various layers of the AI stack. This partnership includes the provision of Google Cloud A3 VMs powered by NVIDIA H100 GPUs, which offer significantly faster training times and improved networking bandwidth compared to previous generations. Additionally, NVIDIA DGX Cloud, a software and AI supercomputing solution, is available to customers directly through their web browsers. This allows businesses to run large-scale AI training workloads with ease.
NVIDIA AI Enterprise, which is available on Google Cloud Marketplace, provides a secure, cloud-native platform for developing and deploying enterprise-ready AI applications. This platform simplifies the process of integrating AI into business operations, making it easier for companies to harness the power of AI without needing extensive in-house expertise.
Several companies are already benefiting from the integration of NVIDIA GPUs into Google Cloud Run. For instance, L’Oréal, a leader in the beauty industry, is using this technology to power its real-time AI inference applications. The company has reported that Cloud Run’s GPU support has significantly enhanced its ability to provide fast, accurate and efficient results to its customers, particularly in time-sensitive applications.
Another example is Writer, an AI writing platform that has seen substantial improvements in its model inference performance while reducing hosting costs by 15%. This has been made possible through Google Cloud’s AI Hypercomputer architecture, which leverages NVIDIA GPUs to optimize performance and cost-efficiency.
The addition of NVIDIA L4 GPU support to Google Cloud Run represents a major milestone in cloud-based AI development and serverless AI inference. By combining the ease of use and scalability of Cloud Run with the powerful performance of NVIDIA GPUs, Google Cloud is offering developers and businesses the tools they need to build, deploy and scale AI applications."
242,https://www.forbes.com/sites/bethkindig/2024/08/22/nvidia-stock-blackwell-suppliers-shrug-off-delay-ahead-of-q2-earnings/,Nvidia Stock: Blackwell Suppliers Shrug Off Delay Ahead Of Q2 Earnings,"Aug 22, 2024, 11:24pm EDT",Beth Kindig,"Bulletproof Nvidia showed an unusual bout of weakness this past month following a report from The Information that Nvidia’s new AI chips are delayed. The report asserts that Nvidia’s upcoming artificial intelligence chips will be “delayed by three months or more due to design flaws,” resulting in a final flush of selling where the stock was down (-15%) in 7 days.
According to the report that was based on two anonymous sources, “if the upcoming AI chips, known as the B100, B200 and GB200, are delayed three months or more, it may prevent some customers from operating large clusters of the chips in their data centers in the first quarter of 2025, as they had planned.” This statement sent the market into a panic as it implies all three Blackwell SKUs will be delayed into the June quarter given the statement a three-month delay may prevent large clusters of Blackwell from not being operable in the first quarter.
It's strange then, to say the least, that according to two of Nvidia’s closest supply partners, there is evidence the GB200s will initially ship in Q4, and are expected to see an increase of production volume in Q1.
The third supplier provides a read-through that the fab producing the chips is not seeing any material impact. This is important as the The Information also asserts the machines fabricating Blackwell GPUs are sitting idle. Per the report: “it is highly unusual to uncover significant design flaws right before mass production. Chip designers typically work with chip makers like TSMC to conduct multiple production test runs and simulations to ensure the viability of the product and a smooth manufacturing process before taking large orders from customers. It’s also uncommon for TSMC, the world’s largest chipmaker, to halt its production lines and go back to the drawing board with a high-profile product that’s so close to mass production, according to two TSMC employees. TSMC has freed up machine capacity in anticipation of the mass production of GB200s but will have to let its machinery sit idle until the snags are fixed.”
The quote above implies the issues were entirely unforeseen, which might not be the case. My firm covers Nvidia’s management team statements quite closely since I first covered the AI thesis in 2018, and management has been quite clear that CoWoS-L packaging for Blackwell will require more time for testing than previous generations. I’ve dug up some of this commentary for you below.
Nvidia is delivering the history’s most aggressive product road map on new fab processes. This is a “move fast, break things” problem, which contrasted to strictly a design flaw, does not mean the architecture inherently has issues. Rather to contrast, the progression of this generation is testing the upper limits of manufacturing complexities. Blackwell with CoWoS-L packaging seeks to increase yields by circumventing a silicon monolithic interposer, and instead, will use an interposer with higher yields to help package the processing and memory components seamlessly together. The result will be to break ground on unprecedented performance gains for memory-intensive tasks.
These nuances matter for tech investors. Around this time, on August 2nd, my firm took the opportunity to buy our last Nvidia tranche at $105.73 in an effort to catch what we believe will be about 25% - 50% upside before price tops out.
We also look more closely at supply chain commentary, as there is one supply chain partner in particular that has reported a mysteriously high level of growth in a segment that is tied to Blackwell. We covered this for our premium members the evening of the supplier’s earnings report on August 6th when Nvidia stock was bottoming at $105.
As a reminder, we don’t make earnings calls, as many factors can affect stock price. Instead, we present quality research so that investors are fully informed to make their own decisions. From there, we take this a step further and publish every single trade we make on our research site. In finance, full transparency is rare, yet through never-ending tenacity, my firm has offered up to 3900% gains on Nvidia alone.
We continue this long-standing dedication to our readers in the analysis below.
TSMC releases monthly numbers which would reflect quickly if a highly anticipated release was causing idle machines. Instead, July monthly revenue showed a sharp acceleration from a decline in May and June to a MoM growth of 23.6% to NT$256.95 billion.
On a MoM/YoY basis, July reported the second largest growth this year:
TSMC’s MoM growth can be lumpy, yet July month’s 38.3% YoY growth points to a positive start to the September quarter. The company guided for revenue of $22.4 billion to $23.2 billion, representing YoY growth of 31.9% at the midpoint.
The analyst consensus estimates are trending higher, which typically, you’d see a decline in the analyst estimates on the news of a material delay. Analysts expect Q3 revenue to grow 38.1% YoY to $23.32 billion from the earlier 32.5% growth expected in mid-June and 32.1% growth expected in mid-May.
Note: The analyst estimates below differ slightly from reported figures in the company IR due to the currency conversion. However, we use the estimates below to understand the expected growth rate trend.
TSMC offered positive commentary on its business and raised the outlook when it reported its Q2 report last month. The company’s revenue grew by 32.8% YoY to $20.82 billion and beat the midpoint guide of 27.6% growth, helped by strong AI demand.
On a QoQ basis, the chipmaker’s high-performance computing (HPC) revenues rose 28% QoQ to $10.8 billion and accounted for 52% of Q2 revenue, up from 46% of revenue in Q1. HPC is above the 50% mark for the first time.
C.C. Wei, Chairman and CEO of the company, said in the Q2 earnings call, “Our business in the second quarter was supported by strong demand for our industry-leading 3-nanometer and 5-nanometer technologies, particularly offset by continuous smartphone seasonality.” There was a similar trend in Q1 as revenues were impacted by smartphone seasonality and offset by HPC revenue.
Wei also said that “over the past three months, we have observed strong AI and high-end smartphone related demand from our customers, as compared to three months ago, leading to increasing overall capacity utilization rate for our leading-edge 3-nanometer and 5-nanometer process technologies in the second half of 2024. Thus, we continue to expect 2024 to be a strong growth year for TSMC.”
They raised the full-year guidance to “slightly above mid-20s percent in US dollar terms” from the earlier “increase by low to mid-20% in U.S. dollar terms.” He further added, “we have such high forecasted demand from AI related business.” Given TSMC has many high-profile customers, the HPC segment alongside the CEO commentary help to differentiate the impact is coming from AI, rather than being mobile-related.
The I/O Fund built a leading AI portfolio beginning with Nvidia’s AI thesis in 2018, with up to 2,600% gains on Nvidia alone provided to our free readers. Premium members receive real-time trade alerts on NVDA and our entire portfolio, including two AI semiconductors we believe are poised for growth with allocations rivaling our NVDA holding.
TSMC has limited CoWoS-L capacity to produce Blackwell chips. This is a problem all investors should get comfortable with as we head into 2025.
TSMC’s chip-on-wafer-on-substrate (CoWoS) architecture refers to the 3D stacking of memory and processors modules layer by layer to create chiplets. The architecture leverages through-silicon vias (TSVs) and micro-bumps for shorter interconnect length and reduced power consumption compared to 2D packaging.
There are three types of CoWoS architectures, which replaced multi-chip modules by scaling up the interposer area to fit multiple dies. Current CoWoS interposers are up to TSMC’s 3.3X reticle limit, with the goal of building interposers that can reach 8X the reticle limit by 2027. At the North American Technology Symposium earlier this year, TSMC stated they will reach 5.5X reticle limit by 2025 for more than a 3.5X increase in compute power.
As transistor density increases, advanced packaging solutions help to alleviate bottlenecks by increasing interconnect density, which results in higher signal speed and processing power.


Nvidia designs offer pure ingenuity, for example, the A100s offered sparsity and the H100s offered a transformer engine. We covered the importance of the Transformer Engine for our premium site six months prior to Hopper shipping, which led to entries as low as $10.85 when factoring in the stock split. Ultimately, Nvidia’s design ingenuity combined with TSMC’s process improvements defy Moore’s Law.
Due to TSMC’s CoWoS-L requiring more complexity and precision, it was already expected the validation and testing process would be time consuming. We had stated in the analysis Nvidia Q1 Earnings Preview: Blackwell and The $200B Data Center that “the advanced CoWoS packaging that is needed to combine logic system-on-chip (SoC) with high bandwidth will take longer, and thus, it’s expected that Blackwell will be able to fully ship by Q4 this year or Q1 next year. How management guides for this will be up to them, but commentary should be fairly informative by Q3 time frame.”
Per another source, Trend Force last April: “Although NVIDIA plans to launch products such as the GB200 and B100 in the second half of this year, upstream wafer packaging will need to adopt more complex and high-precision CoWoS-L technology, making the validation and testing process time-consuming. Additionally, more time will be required to optimize the B-series for AI server systems in aspects such as network communication and cooling performance. It is anticipated that the GB200 and B100 products will not see significant production volumes until 4Q24 or 1Q25.”
From the horse’s mouth, Nvidia’s own management team, it was stated during the GTC Financial Analyst Day in March that the very first systems will ship in Q4, but to expect constraints. In a roundabout way, the CEO tells investors what to expect should this happen, which is that customers will continue to build with H100s, H200s and any other supply they can get their hands on.
Atif Malik, Citigroup:
Hi. I am Atif Malik from Citigroup. I have a question for Colette. Colette in your slides, you talked about availability for the Blackwell platform later this year. Can you be more specific? Is that the October quarter or the January quarter? And then on the supply chain, readiness for the new products is the packaging, particularly on the B200 CoWoS-L and how you are getting your supply chain ready for the new products?
Colette Kress:
Yeah, so let me let me start with your second part of the question, talking about the supply-chain readiness. That's something that we've been working well over a year getting ready for these new products coming to market. We feel so privileged to have the partners that work with us in developing out our supply chain. We've continued to work on resiliency and redundancy. But also, you're right, moving into new areas, new areas of CoWoS, new areas of memory, and just a sheer volume of components and complexity of what we're building. So that's well on its way and will be here for when we are ready to launch our products. So there is also a part of our supply chain as we talked earlier today, talking about the partners that will help us with the liquid cooling and the additional partners that will be ready in terms of building out the full of the data center. So this work is a very important part to ease the planning and the processing to put in all of our Blackwell different configurations. Going back to your first part of the question, which is when do we think we're going to come to market? Later this year, late this year, you will start to see our products come to market. Many of our customers that we have already spoken with talked about the designs, talked about the specs, have provided us their demand desires. And that has been very helpful for us to begin our supply chain work, to begin our volumes and what we're going to do. It's very true though that on the onset of the very first one coming to market, there might be constraints until we can meet some of the demand that's put in front of us. Hope that answers the question.
Jensen Huang:
Yeah, That's right. And just remember that Hopper and Blackwell, they're used for people's operations and people need to operate today. And the demand is so great for Hoppers. They — most of our customers have known about Blackwell now for some time, just so you know. Okay, so they've known about Blackwell. They've known about the schedule. They've known about the capabilities for some time. As soon as possible, we try to let people know so they can plan their data centers and notice the Hopper demand doesn't change. And the reason for that is they have an operations they have to serve. They have customers today and they have to run the business today, not next year.
—End Quote
Recently, Nvidia’s VP Ian Buck stated at BofA Financial Conf in June 2024; “So we stated recently in our earnings that Blackwell has now entered into production builds. We started our production.
The samples are now going — will go out this quarter, and we're ramping for production outs later this year. And then everything — that always looks like a hockey stick, you start small and you go pretty quick to the right. And the challenge, of course, is with every new technology transition comes — the value is so high, there's always a mix of a challenge of supply and demand. We experienced that certainly with Hopper. And there'll be similar kinds of supply/demand constraints in the on-ramp of Blackwell certainly at the end of this year and going into next year.”
Taking this full circle, let’s go back to what TSMC said in the most recent earnings call about CoWoS capacity:
Management stated in the earnings call Q&A that the supply is expected to continue to be tight next year, and they are working with OSAT (Outsourced Semiconductor Assembly and Test) partners to increase production capacity.
Gokul Hariharan:
“How do you think about supply demand balance for AI accelerator and CoWoS advanced packaging capacity? And I think in your symposium you talked about 60% CAGR, component growth for CoWoS capacity in the next four, five years. So, could you talk a little bit about how much capacity for CoWoS would you be planning to build next year as well?”
C. C. Wei:
“Gokul, I also try to reach the supply and demand balance, but I cannot today. The demand is so high, I have to work very hard to meet my customers' demand. We continue to increase, I hope sometime in 2025 or 2026 I can reach the balance. You're talking about the CAGR or those kind of increase of the CoWoS capacity. Now it's out of my mind. We continue to increase whatever, wherever, whenever I can. Okay. The supply continues to be very tight, all the way through probably 2025 and I hope it can be eased in 2026. That's today's situation.”
Gokul Hariharan:
“Any thoughts on next year capacity? Are you going to double your capacity again next year for CoWoS?”
C. C. Wei:
“The last time I said that, this year I doubled it, right? More than double. Okay. So next year, if I say double, probably I will answer your question again next year and say more than double. We are working very hard, as I said. Wherever we can, whenever we can.”
—End Quote
My notes: There were many opportunities for TSMC to report a material impact from idle machines – quarterly numbers ending in June, July monthly numbers, commentary during the earnings call from the CEO that establishes the opposite, which is that capacity is primarily the issue (rather than a dire flaw that is halting production) and the company is working hard to increase this capacity.
Earlier this month, TrendForce citing Money DJ’s report, estimated that CoWoS capacity is in short supply at 35,000 to 40,000 wafers this year. With outsourced capacity, 2025 production could be over 65,000 wafers per month.
According to the report, TSMC will assign the orders of the initial stage of CoWoS packaging, Chip on Wafer (CoW) to OSAT partner SPIL. This is the first time the company is outsourcing this process since the demand is high and previously WoS (Wafer-on-Subtrate) process was outsourced while keeping the higher margin CoW process in-house.
According to DigiTimes, the company is expected to have CoWoS production of 60,000 wafers per month in 2025 and a further increase to 70,000 to 80,000 in 2026 after the company recently acquired Innolux Fab. The 2025 production capacity would suggest a 300% increase from 15,000 at the end of 2023.
Super Micro stock surged alongside Nvidia over the past year and a half with returns of 659% compared to Nvidia’s returns of 787.8%. Supermicro is a leading partner on building AI systems with Hopper GPUs by leveraging air cooled and liquid thermal designs for AI accelerators to grow upwards of 5X faster than the industry average for subsystems and server systems.
The Hopper generation is primarily air cooled. However, the percentage of air-cooled systems shipped versus liquid cooled systems will change (dramatically) with Blackwell.
In June, we wrote an analysis on AI Power Consumption: Rapidly Becoming Mission Critical which stated that as the industry progresses towards a million-GPU scale, this puts more emphasis on future generations of AI accelerators to focus on power consumption and efficiency while delivering increasing levels of compute. Data centers are expected to adopt liquid cooling technologies to meet the cooling requirements to house these increasingly large GPU clusters.
Specifically, it’s the Blackwell architecture that kicks off the need for liquid cooling. Most servers today are air-cooled yet AI necessitates a shift to liquid cooling as the H100 GPUs are already at 700W of power and Blackwell GPUs will see a 40% increase to 1,000W or higher. The B200 doubles the transistor count compared to the H100 and provides 20 petaflops of AI performance compared to the H100s 4 petaflops. The resulting 3X leap in training performance and 15X leap in inference performance is shifting the focus to liquid cooling as 1,000 watts is too hot to be air cooled.
The B200 systems and chipsets will be the first release to be primarily liquid cooled, according to Dell, who competes with Super Micro on building AI servers. Note that per the statement from Dell in March, the B200s are due in early 2025.
Tom’s Hardware has also stated that direct liquid cooling will start with Blackwell: “Even Nvidia's high-end H100 and H200 graphics cards work well enough under air cooling, so the impetus to switch to liquid hasn't been that great. However, as Nvidia's upcoming Blackwell GPUs are said by Dell to consume up to 1,000 watts, liquid cooling may be required.”
VP Ian Buck of BofA GTC Conference in June of 2024 also stated: “The opportunity here is to help [customers] get the maximum performance through a fixed megawatt data center and at the best possible cost and optimized for cost. By doing 72 GPUs in a single rack, we need to move to liquid cooling. We want to make sure we had the higher density, higher power rack, but the benefit is that we can do all 72 in one NVLink domain.”
Super Micro is a proxy for Nvidia as its growth has been in lock-step with the AI GPU juggernaut since the launch of the H100 nearly two years ago. Most importantly, we have a key metric from Super Micro that is specifically tied to Nvidia’s Blackwell launch, which is the ramp of liquid cooling.
Liquid cooling has been around for 30 years, yet the H100s and H200s launched with air cooled systems. Today, Super Micro builds HGX AI supercomputers with racks that support 64 H100s, H200s or B200s with direct liquid cooling (DLC), saving up to 40% of energy costs. Although H100s and H200s have the option for DLC, the CFO of Super Micro has stated that as GPUs and CPUs run over 1,000 watts, the benefits of liquid cooling are “going to start to become painfully obvious.”
Per the CEO in last month’s earnings call, it was the months of June and July specifically when DLC started to ramp: “I mean as you know liquid cooling have been in the market for 30 years and market share compared with overall datacenter size always small, less than 1% or close to 1%, I would have to say. But just June and July two months alone, we shipped more than 1,000 racks to the market. And if you calculate 1,000 racks, AI rack is about more than 15% on a global datacenter new deployment.”
Next quarter will mark the highest quarter growth in Supermicro’s history with guidance for 206.6%, an acceleration from the previous quarter’s growth of 144%. This is 590 bps higher than Super Micro’s previous record quarter for 200.7% growth.
Considering that Blackwell is a clear catalyst for direct liquid cooling, it is odd to say the least that Supermicro reported on August 6th that demand for direct liquid cooling is surging, a mere four days after The Information’s dire report.
According to Super Micro’s earnings report, the company’s direct liquid cooling capacity grew 50% month-over-month from 1,000 racks per month to 1,500 racks per month. By year end, the company will grow to 3,000 racks per month, resulting in 200% growth in six months.
This represents an increase from Super Micro’s original estimate the company would end the year with 1,500 racks. The CFO stated: “But even we were surprised by the acceleration that we saw in the liquid-cooled rack market.”
SuperMicro offers liquid cooled H200 HGX systems, yet the H200s run up to 700W; not the 1000W that necessitates DLC. I have yet to see where the H200 was expected to drive overnight demand for DLC, rather, it’s been expected for some time that Blackwell would be the catalyst for the DLC market.
To put the sudden surge in context, Super Micro stated: “I believe for June and July in last next two months we may ship at least 70% to 80% of liquid cooling compared with all the liquid cooling in the world. So for liquid cooling, we have at least 70% to 80% market share” – the readthrough is the DLC market skyrocketed very suddenly in the last two months.
Supermicro’s report is communicating that servers that require direct liquid cooling are soaring (suddenly) as of June and July from 1% of all new servers shipped to 15% at 1,000 racks. Management is also communicating that it’s expected to continue to soar to 3,000 racks by the end of this year, reaching up to 30% of servers shipped.
Yet if Blackwell is materially delayed, how can liquid cooling be skyrocketing?
Theory #1: The Delay Was Accounted For:
Per the GTC commentary from management, the very first GB200 systems will ship in Q4 and will ramp from there, with the understanding Blackwell will be capacity constrained. Financial analysts knew CoWoS-L could present delays, and the April press release from Trend Force clearly describes this, stating CoWoS-L is “making the validation and testing process time-consuming.”
Nvidia reiterated that Q4 is when the first systems would ship after The Information’s report with an Nvidia spokesperson stating to The Verge: “Nvidia expects production of the [B200] chip “to ramp in 2H,” according to a statement that Nvidia spokesperson John Rizzo shared with The Verge. “Beyond that, we don’t comment on rumors.”
The delay may have already been accounted for, as discussed, it’s a new packaging process and a more complex chip, with many statements on record it would require additional testing. This would help explain why TSMC and Super Micro are raising/beating estimates driven by their AI segment as it implies their guidance was aligned with the delay.
Theory #2: The GB200s NVL36 and NVL72s are Hogging CoWoS-L Capacity
My firm has been reporting on X for months that GB200 demand is surging. For example, UBS said that it believes “demand momentum for $NVDA Blackwell rack-scale systems remains exceedingly robust” and that the “order pipeline for (Nvidia's) NVL72/36 systems is materially larger than just two months ago.”
Source: Beth Kindig’s X Account
According to reports from Wccftech: “Team Green is expected to ship 60,000 to 70,000 units of NVIDIA's GB200 AI servers, and given that one server is reported to cost around $2 million to $3 million per unit, this means that Team Green will bag in around a whopping $210 billion from just Blackwell servers along, that too in a year.”
The weight of that report cannot be overstated as it implies 26% upside to 2025’s estimates based on one SKU alone. In fact, this one SKU is expected to drive 9% more revenue than analysts currently have estimated two years out for FY2027.
Theoretically, if the GB200 systems are seeing enough demand to exceed FY2027 estimates (per the preliminary data), Nvidia would be wise to cancel the B100s and B200s built on CoWoS-L capacity entirely, and switch these SKUs back to CoWoS-S. There’s a write-up on new SKUs based on CoWoS-S capacity and air-cooling from Semi Analysis here.
Here’s why the GB200 can drive this kind of revenue so quickly:


In this case, Nvidia would theoretically prioritize the GB200 NVL36 and NVL72 as the price points are quite high. The two NVL36 and NVL72 rack configurations carry a ~27% to ~54% higher selling price per GB200, making it understandable why Nvidia would focus on the racks given production constraints from CoWoS capacity.
Ultimately, reconfiguring lower priced SKUs will not matter to Wall Street if it’s based on outsized demand for GB200s. This theory hinges on Super Micro’s report, as it’s the sudden surge in direct liquid cooling sales that is truly mysterious. From my vantage point today, it feels nearly impossible that Super Micro could report this level of surge in direct liquid cooling from 1% of systems in May, to 15% of systems today, to 30% of systems by the end of the year and for there to be a material, unforeseen delay in every Blackwell SKU.
If the B100s and B200s are pushed out in favor of the GB200NVLs, then next year will be game-on for Nvidia investors as these systems sell at a high multiple. Keep an eye out for where bad news now (some GPUs are canceled) eventually becomes good news over the next four quarters (in favor of systems priced 36X to 72X higher).
Briefly, I’d like to mention Foxconn has recently stated in an earnings call: ""We are on track to develop and prepare the manufacturing of the new [Nvidia] AI server to start shipping in small volumes in the last quarter of 2024, and increase the production volume in the first quarter of next year.”
The company also indirectly debunked The Information’s assertion that “it is highly unusual to uncover significant design flaws right before mass production” with Foxconn stating the opposite ""It is normal to dynamically adjust [shipment schedules] when the specs and technologies of a new product are largely upgraded. Whether the shipping schedule changes or not, Foxconn will be the first supplier to ship the first batch of GB200,"" Wu said.
Note Foxconn specifically calls out shipping the GB200, rather than the B100, which was due to ship first. Hopefully, by now it’s clear to our readers should the B100 be bumped, this could have a bullish readthrough if Nvidia re-allocates CoWoS-L capacity to the higher priced GB200 systems.
To further the conversation on why a delay in the B100s and B200s can be absorbed, it’s worth taking a moment to discuss the H200.
The H200 is shipping now and is a force of its own with 141 GB of HBM3e memory, up from 80 GB of HBM3 memory in the H100. The GH200 superchip is also equipped with HBM3e and is shipping this quarter.
By significantly boosting memory per GPU – up ~75% from 80 GB of HBM3 in the H100s – the H200 allows Nvidia’s customers to address memory-constrained workloads, such as workloads requiring the largest LLMs, which were built and trained on the H100s. This will fill the gap between shipments of the H100 and Blackwell by easing one critical bottleneck to AI training – memory bandwidth.
The question that I’ve seen raised time and again by investors is why is Nvidia’s GPU demand is this durable in a typically cyclical industry? The answer lies within the H200 and Blackwell. As VP Ian Buck explained at BofA’s GTC Conference in June, “From the end of '22 to today, I think we've improved Hopper's inference performance by 3x. So we're continuously making the infrastructure more efficient, faster and more usable. And that gives the customers who have to now buy at a faster clip, confidence that the infrastructure that they've invested in is going to continue to return on value and does so.”
More importantly, Buck emphasized that hyperscalers “can retire their old legacy systems that maybe they've just left, not upgraded. They can accelerate the decommission of the older CPU infrastructure.” Essentially, Nvidia’s customers can free up megawatts of power and hundreds of racks (and save millions with performance and efficiency gains providing lower TCOs) by decommissioning prior generations of GPUs or CPU-based servers, and this goes for the H200 and Blackwell. Customers can retire older GPU generations such as Volta and Ampere and refit it with H200s, while waiting for Blackwell chips to build new infrastructure, allowing them to benefit from the memory upgrades while in mid-cycle for the Blackwell upgrade.
On the HBM3 side, Micron, SK Hynix and Samsung are intertwined in a deep competition for supply, with SK Hynix serving as the primary supplier for the H100 and Micron being the first to announce itself as the supplier for the H200. Micron has said it is sold out of HBM3e supply through 2025, with preparations and discussions already being made for HBM4 and HBM4e in 2026 and beyond. SK Hynix also revealed earlier in May this year that it was nearly entirely sold out of HBM through 2025. On the other hand, Samsung has reportedly struggled for some time to validate its HBM3e chips with Nvidia due to power consumption and heat issues.
We’re still seeing no signs of slowing for H100 and H200 demand, with DigiTimes reporting last week that H100 and H200 production volumes have been “increasing monthly.” There are also signs in the broader DRAM market that point to HBM demand remaining robust, another signal pointing to lasting Hopper demand. DRAM revenue in the June quarter surged nearly 25% QoQ to $22.9 billion, driven primarily by HBM demand and rising prices due to “aggressive procurement strategies” from buyers.
As of now, there’s a disconnect between next fiscal year’s revenue estimates of $167 billion and the $210 billion in GB200s alone expected to ship next year. Perhaps analysts are waiting for signals the supply chain can produce these outsized orders. So far, so good with the signals we see from TSMC and SMCI’s most recent earnings reports. Foxconn commentary helps, as well.
Where Nvidia investors run a risk is the valuation of 25X forward PS and 45X PE Ratio as it’s the highest the stock has traded since the market has priced in the AI accelerator boom. My firm believes in an active approach to managing risk. For example, if you had bought the 2022 top in Nvidia, you’d currently be up over 275%. If you bought the October 2022 low, you’d be up over 1100%. It is unlikely many bought the top and bottom in any stock (we actually did buy Nvidia at the very low on October 18th, 2022, but it’s rare). Yet, being cognizant of the larger trend and pattern in play has allowed us to increase our return while decreasing the risk with Nvidia.
Point being, we actively seek to buy quality companies at lower prices. Let the market (with help from the media) doubt the AI juggernaut in its first inning, let them drag the price down, and then our plan is to pounce … because Blackwell is on its way, the GB200s are going to crush expectations in FY2026, we are getting the green light from suppliers the delay is immaterial at this time, demand/big tech capex remains high, and let’s be real, nothing can stop what’s coming.
Our premium members will receive our post-earnings analysis right after the report. If you own Nvidia stock, or are looking to own NVDA, we encourage you to attend our weekly premium webinars, held every Thursday at 4:30 pm EST. Next week, we will discuss our plan following NVDA’s earnings, as well as a handful of lesser-known AI plays for 2024 – what our targets are, where we plan to buy as well as take gains.
If you would like notifications when my new articles are published, please hit the button below to ""Follow"" me.
Please note: The I/O Fund conducts research and draws conclusions for the company’s portfolio. We then share that information with our readers and offer real-time trade notifications. This is not a guarantee of a stock’s performance and it is not financial advice. Please consult your personal financial advisor before buying any stock in the companies mentioned in this analysis. Beth Kindig and the I/O Fund own shares in NVDA at the time of writing and may own stocks pictured in the charts"
243,https://www.forbes.com/sites/investor-hub/article/nvidia-stock-earnings-preview-what-to-know/,Nvidia Second Quarter Earnings Preview: What To Know Before The AI Leader  Reports,"Aug 20, 2024, 04:29pm EDT",Sasirekha Subramanian,"Nvidia shares fell nearly 23% from mid-July through early August, closing on August 7 at $98.91. The selloff was triggered by fears that Nvidia’s big tech customers may trim their artificial intelligence spending budgets, as well as rumored delays in the rollout of Nvidia’s Blackwell GPUs slated for launch later this year. The stock drawdown was further exacerbated by some high profile investment funds exiting or cutting stake in Nvidia, and doomsday predictions Nvidia’s shares were running rampant on Wall Street. However, the stock staged a strong recovery of more than 30% from these low levels, closing August 19 at $130. Even as the AI bellwether braces to release its second-quarter earnings report on August 28, what should investors expect from Nvidia’s upcoming earnings print? Is it time to sell Nvidia after its nearly 3,000% rally in the past five years?
When is a good time to sell Nvidia or any other stock? When the stock fundamentals have deteriorated and do not support the current price levels at which the stock trades. Is that true of Nvidia?
After a meteoric rise in the past years, Nvidia stock lost its sheen somewhat after its recent slump of more than 20%. The selloff was triggered by concerns related to a possible capex slowdown by Nvidia’s top customers and hyperscalers Microsoft (MSFT), Meta Platforms (META), Amazon (AMZN) and Google (GOOG).
However, all these companies have pledged to boost their spending on AI-related infrastructure even higher. Google parent Alphabet said its future capex will be “at or above the first-quarter level of $12 billion.”
Microsoft, which is Nvidia’s largest customer, reported capital spending of $55.7 billion for the fiscal year (ended in June) up 75% from last year, and noted that capital spending in fiscal 2025 will top that level to meet the growing demand signal for its AI and cloud products.
Meta stated that it currently expects significant capex growth in 2025 as it invests to support its AI research and product development efforts. The elevated AI capital spending appears to be paying off for Meta, which raised its third-quarter revenue guidance to $38.5 billion to $41 billion vs. the $39 billion consensus estimate, thanks to a robust growth in advertising. Meta has been leveraging AI to optimize ad targeting, which has significantly improved its advertising performance and business efficiencies. Amazon expects capital investments to be higher in the second half of the year and the majority of the spend will be to support the growing need for AWS infrastructure as the company continues to see strong demand in both generative AI and non-generative AI workloads. JPMorgan sees the four hyperscalers–Google, Microsoft, Meta and Amazon–spending a collective $200 billion in capex this year with AI buildout dominating the capital spending, which in turn bodes well for Nvidia.
Another cause for concern centers around the rumored delay in the rollout of Nvidia’s next generation Blackwell AI chips, which are roughly two times faster than Nvidia’s current Hopper models, but with notable improvements in energy efficiency. The delay is reportedly attributed to the complexity of the chip-on-wafer-on-substrate (CoWoS) packaging technology used by Taiwan Semiconductor Manufacturing Company (TSM) that manufactures these chips for Nvidia. The Blackwell launch is scheduled for later this year.
However, Nvidia commented that Blackwell sampling has started, and production is on track to ramp in the second half. Nvidia's hardware partners, like Foxconn, Quanta, Wistron, Pegatron, and Asus, have already demonstrated their Blackwell-based servers at Computex, a computer expo held annually in Taipei, Taiwan. Investors are hoping that Nvidia may provide a more specific timeline for the Blackwell ramp in its upcoming earnings call, beyond an ambiguous reference to the second half of the year.
Even assuming there is a delay in a worst case scenario, it is good to know that Nvidia is working on fixing the design complexities rather than bringing out a flawed model that is bound to fail. The delay (if there is one) is unlikely to hamper Nvidia or its prospects in the longer run. Don’t forget, Nvidia is still the undisputed leader in the data center GPU market, and even if AMD brings out a rival product comparable to Blackwell, it will still have to catch up with the yawning chasm in market share.
Besides, there’s the alluring revenue dynamics at play. According to news reports that reference a Morgan Stanley analysis, Nvidia and its partners are expected to price an AI server cabinet, featuring the upcoming Blackwell GPUs, between $2 million and $3 million. This pricing could potentially lead to an estimated annual revenue exceeding $200 billion in 2025 based on the requirement for tens of thousands of AI servers. If reports were to be trusted, Nvidia may have boosted its Blackwell orders with TSMC by 25% and that could be favorable for TSMC, which is manufacturing the most powerful AI chip in the world based on Blackwell architecture.
The much-touted exodus of top money managers from Nvidia shares during the second quarter is reflected in some 13F filings. The SEC's Form 13F must be filed every quarter by institutional investment managers with at least $100 million in assets under management (AUM).
According to the latest 13F filings,


While it may seem like a great strategy to follow “smart money,” it should be noted that the 13F is just one among several data points for investors to make informed investment decisions. Besides, the 13-F can be filed up to 45 days after the end of a quarter, meaning that the filing only gives a peek into past strategies of institutional investment managers and may not hold much relevance at the time when it comes into public knowledge. Also, institutional investment managers have deep pockets, and specific investment strategies which they are not required to reveal to the public fully, like their short positions for example. So attempting to reverse-engineer their success without understanding the underlying strategy can often burn the investor.
At this point, does it look like there is any compelling reason to sell Nvidia shares?
As a key enabler of AI with its GPUs powering even supercomputers used by Meta and Tesla, Nvidia continues to dominate the AI chip market with an estimated market share ranging from 70% to 95%. For a hardware company, Nvidia’s gross margins are quite high at 73.8% for fiscal 2024. Nvidia’s competitive moat lies in CUDA, its proprietary software stack that allows developers to leverage the parallel processing capabilities of Nvidia GPUs to accelerate machine learning workloads. Attempts to migrate from the CUDA have gone south, but Nvidia is never negligent. It constantly evolves CUDA’s capabilities, to retain its market leadership. But that hasn’t deterred AMD, Intel and Google from persevering in valiant attempts to unseat the CUDA from its pedestal.
Nvidia’s upcoming Blackwell platform has an inference capability that is 30x of the incumbent Hopper’s, while consuming 25x less cost and energy. Nvidia has dismissed any concerns of customers holding off on Hopper orders because of the upcoming Blackwell launch. The demand for both Hopper and Blackwell platforms is well ahead of supply and this is expected to continue well into the next year. The upcoming earnings report will provide more clues on the Blackwell ramp.
Nvidia sees a long-term market opportunity of $1 trillion with $300 billion from datacenter, $100 billion from gaming, $300 billion from autonomous vehicles and robotics, $150 billion from industrial meta verse via its Omniverse ecosystem and $150 billion from Enterprise via Nvidia AI Enterprise–its operating system for enterprise AI, as well as DGX Cloud that gives customers instant access to Nvidia AI supercomputing in global-scale clouds.
Nvidia’s growth has been strong and profitable. In the last five years, revenue has grown from $10.9 billion in fiscal 2020 to $60.9 billion in fiscal 2024 with revenue reaching $26 billion for the first quarter of 2025, while operating margins have risen from 34% to 61%, and at 69% for the first quarter.
Data Center revenues constitute a major chunk of its top line, and have grown at a 75% CAGR in the past five years to $47.5 billion in fiscal 2024. In the first quarter, data center revenues were $22.6 billion.
Gaming revenue, which represented 17% of Nvidia’s fiscal 2024 top line, has grown at a five-year CAGR of 11% from $5.5 billion in fiscal 2020 to $10.4 billion in fiscal 2024.
Nvidia’s Professional Visualization segment represented 3% of 2024 revenues, but is seen as a significant future growth driver. The business offers Omniverse as a development platform for enhancing productivity and introducing new capabilities in design, manufacturing and digital content creation.
Nvidia’s automotive revenues stem from its platform solutions for automated driving and in-vehicle cockpit computing. The Nvidia DRIVE
iShares S&P 500 Value ETF
 is an end-to-end Autonomous Vehicle (AV) platform with a full software stack powered by systems-on-a-chip (SoCs) in the vehicle. Xiaomi’s first electric vehicle, the SU7 sedan is built on the Nvidia Drive Orin, which is Nvidia’s AI car computer for software-defined autonomous vehicle fleets. Nvidia Drive Thor, the successor to Orin, has already procured design wins with EV makers, and slated for production in vehicles in 2025. Although automotive revenues represented only 2% of the top line in fiscal 2024, this is another key area of growth for Nvidia in the future, amid the rising momentum of self-driving vehicles. The global autonomous vehicle market is estimated to grow to $448.6 billion by 2035, according to Allied Market Research.
Financial Health Indicators (H2)Nvidia’s free cash flow (FCF) has grown from $4.3 billion in fiscal 2020 to an impressive $26.9 billion in fiscal 2024, with first quarter FCF at a stellar $14.9 billion. In fiscal 2024, the company spent $395 million in dividends, which it plans to maintain, and $9.5 billion on stock buybacks, reflecting its strong alignment with shareholders. It is not surprising considering that Nvidia’s Founder CEO Jensen Huang owns more than a 3% stake in Nvidia. There was some controversy about the CEO offloading some shares and making good profits before Nvidia shares dropped. Although the timing of the sale turned out to be unusual, the shares were sold under a 10b5-1 trading arrangement that allows company insiders to sell company stock on a predetermined schedule without breaching insider trading laws. So, there’s no cause for concern. Like other shareholders, Jensen Huang experienced a significant paper loss when Nvidia’s stock value fell in the recent period.
AI is a secular growth trend, not a one-off pattern to fizzle out. According to management consulting firm MarketsandMarkets, AI is estimated to grow from a value of $214.6 billion in 2024 to $1,339.1 billion by 2030, and Nvidia’s strong positioning makes it a key beneficiary of these AI tailwinds. However, Nvidia’s growth is expected to normalize from the fast, furious and feverish pace to stable and more sustainable levels.
A low interest rate environment is typically better for tech companies, as it spurs R&D spending, deal making (M&A) and fundraising in the tech sector. However, AI has been the winning theme so far, defying the higher-for-longer interest environment, by shaping capital spending plans of large tech enterprises.
Capex of mega cap techs are at new highs, and the budget is expected to expand further in 2025. Tech spending intent by corporations remained positive in the second quarter of 2024, as reflected by the U.S. Technology Demand Indicator (TDI) score of 51.71 in the second quarter (as measured by 451 Research). The score is slightly lower than the first-quarter number of 52.11, but still positive as a score above 50 typically indicates expansion. This bodes well for Nvidia, which is a key beneficiary of AI tech spending. With the wide expectation of Fed interest rate cuts starting in September, the capital spending environment in the tech sector is likely to get even more conducive.
The global gaming market is estimated to generate $187.7 billion in revenue in 2024 with nearly half of it originating from mobile games, according to gaming market data and research firm Newzoo. Nvidia’s flagship gaming GPU product line GeForce, supports more than 200 million gamers. The latest RTX 40-series graphics cards of GeForce based on Ada Lovelace architecture continue to stay on top of the game. The gaming industry is looking forward to Nvidia’s next-generation GeForce RTX 50-series graphics cards based on the Blackwell architecture. Significant growth in e-sports–the gaming competitions for professional players and teams–also highlights the demand for high speed and performance packed Nvidia GPUs.
Industrial Metaverse is a virtual environment that will apply metaverse elements like digital twins, virtual reality (VR) and augmented reality (AR
Arweave
) to industrial applications, to enhance the efficiencies of industrial operations and processes. A simple example would be the maintenance team in a manufacturing company applying VR to simulate repairs on a digital twin of a machine and optimizing the procedure before actually applying it to a physical machine. Allowing businesses to model prototypes and test in a digital environment before committing physical and human resources to a project, will enhance operational efficiency, reduce downtime, and improve overall productivity, while unlocking immense value for enterprises. Nvidia’s Omniverse is a scalable, multi-GPU real-time development platform for building and operating metaverse applications. Nvidia is collaborating with Siemens to build the industrial Metaverse and to increase use of AI-driven digital twin technology for enhancing industrial automation. MarketsandMarkets estimates the global industrial metaverse market to reach $228.6 billion by 2029 from an estimated $28.7 billion in 2024, with an estimated CAGR of 51.5%, thanks to the rising adoption of digital twins, advancements in AR, VR, AI, IoT, and rising demand for efficiency and optimization in the industrial sector. As a key player, Nvidia is well positioned to benefit from this growth.
A report citing Jeffries analysts noted that the U.S. may implement new trade restrictions that could ban Nvidia from selling its H20 AI chips to China. H20 is Nvidia’s specially designed chip for China, after regulators tightened restrictions on selling high-end AI chips to China, including Nvidia’s H100, citing national security concerns. However, H20’s computing power is significantly lesser compared to H100, to achieve compliance with the U.S. sanctions. But, now even the H20 may possibly be banned for sale in China when the U.S. reviews its semiconductor export controls in October, says the report and if the ban occurs, Nvidia stands to lose an estimated $12 billion in revenue. Analysts had previously projected that Nvidia will deliver more than 1 million new H20 chips to China, and generate more than $12 billion in sales as each H20 chip is priced between $12,000 and $13,000. In the prior financial year, Nvidia’s China revenues were estimated at $10.3 billion.
According to a WSJ report, Huawei is preparing to challenge Nvidia’s H100 AI chips with its Ascend 910C, and expects to begin shipping these chips as early as October. The Ascend 910C chip is reportedly being tested by Chinese companies, while TikTok parent ByteDance, Baidu and China Mobile may be engaged in early discussions to buy the chip. If the news reports were to be believed, this could pose a serious threat to Nvidia’s market share in China, which is already very limited because of U.S. trade restrictions.
Nvidia is scheduled to release second-quarter 2025 earnings on August 28. Here’s a rundown of what investors can expect…


In the past five quarters, Nvidia shares have continued to set new highs in the days and weeks following a strong earnings report and guidance. However, after Nvidia reported spectacular results for the second quarter of 2023, the stock rose by a lackluster 6% although that was still a new high. Was it because the Nvidia stock had run up significantly ahead of its second quarter announcement? Will that pattern repeat this year?
Source: Yahoo Finance
Nvidia is firing on all cylinders. Fundamentally nothing has changed much for Nvidia. It is still the market leader for AI chips and demand for its GPUs appears as strong as ever with its top customers having pledged to keep spending heavily on AI infrastructure. Any delay in the Blackwell ramp may cause near-term volatilities, but the true risk lies in the likelihood of a trade ban that would stop it from selling its H20 AI chips to China, the second largest economy in the world. But if the company loses an estimated $12 billion in China revenues, there is still the $200 billion in estimated revenue potential for Blackwell. The true competitive edge of Nvidia lies in the strong moat it builds around its products, like the CUDA for instance.
Nvidia could report a stellar second quarter, but will the Nvidia stock rally in the near-term? That depends. If the stock runs up ahead of its second-quarter earnings, then the post-earnings rally may likely be lackadaisical. But, Nvidia stock is for the long haul, as AI is here to stay. The strategy would be to buy on dips.
Please note that I am not a registered investment advisor and readers should do their own due diligence before investing in this or any other stock. I am not responsible for the investment decisions made by individuals after reading this article. Readers are asked not to rely on the opinions and analysis expressed in the article and encouraged to do their own research before investing.
Revenue, earnings per share, operating/gross margins, data center revenue, gaming revenue, automotive revenue, professional visualization revenue, and guidance. On the business side, updates on Blackwell ramp and China sales updates.
How has Nvidia’s stock performed recently?
Nvidia stock has been quite volatile in the recent past. Nvidia shares took a nearly 23% hit from mid July through early August, closing trade on August 7 at $98.91. However, the stock has staged a strong recovery of more than 30% from these low levels, closing on August 19 at $130.
When the U.S. reviews its semiconductor export controls in October, if it bans Nvidia from selling the H20 AI chips to China, the AI bellwether stands to lose an estimated $12 billion in revenue.
What are the main risks facing Nvidia?
A potential trade ban to sell to China, and growing competition to unseat its CUDA dominance.
Nvidia sees a long-term market opportunity of $1 trillion from datacenter, gaming, autonomous vehicles and robotics, and industrial meta verse. Sovereign AI is also perceived as a key growth driver amid the ever increasing data privacy issues.
Which are good AI stocks to watch other than Nvidia?
Arm Holdings (ARM) and Taiwan Semiconductor Manufacturing Company (TSM) are good AI stocks to watch.

"
244,https://www.forbes.com/sites/karlfreund/2024/08/20/amd-acquires-zt-addressing-a-key-nvidia-advantage-still-two-to-go/,"AMD Acquires ZT, Addressing A Key Nvidia Advantage. Still, Two To Go","Aug 20, 2024, 12:35pm EDT",Karl Freund,"By acquiring ZT Systems, a favorite Hyperscale systems designer, AMD addresses a fundamental weakness it faces versus Nvidia: AI requires a complete system design, not just a fast chip. What else must it do to grab a 20% share?
At a recent investment conference hosted by KeyBanc, I presented my thesis on whether AMD could catch up with Nvidia for AI. Looking only at specs, AMD has already caught up with Nvidia on the chip front. However, these performance claims are not AI performance; they are raw math benchmarks, not AI benchmarks. And these benchmarks are at a chip level, not at the rack level Nvidia touts with the Blackwell-based NVL-72. AMD CTO Mark Papermaster committed that AMD would publish MLPerf last year, so I expect this issue will be addressed soon. So, let’s talk about the other three areas AMD needs to invest and innovate in to become a serious AI contender (>10% share).
I noted three areas AMD needs to invest in to close the gap with the AI leader: Systems, Software, and Networking.  On the systems side, buying New Jersey-based ZT Systems will help, primarily with cooling solutions.  ZT’s customers are some of the largest hyperscalers, and they know what is needed to compete at the rack level. Selling off the actual hardware business and retaining the critical engineers is an intelligent way to build up AMD systems mojo. However, the transition could easily take a couple of years. While ZT helps AMD on the Systems side, Networking and Software remain a work in progress:
However, the systems gap AMD must address is a networking problem, which ZT does not address. Nvidia has three networking technologies: NVLink, Ethernet, and InfiniBand. NVLink latency is about 500-1000 times lower than Ethernet, which AMD uses for multi-system solutions, and it is roughly 18 times faster in terms of bandwidth. When training very large language models, that’s a show-stopper for AMD (and Intel, by the way).
So, the most critical missing element here is the GPU-to-GPU NVLink interconnect that can scale beyond an 8 GPU server.  Above is pictured an NVidia NVL72 Blackwell system, with direct connection between all 72 GPUs for minimum latency needed for very large language model training and inference. AMD, Intel, and other major tech companies are collaborating on an open alternative to NVLink technology called Ultra Accelerator Link (UALink).
UALink specification will support up to 1,024 accelerators in a cluster, more than NVLink/NVSwitch 5.0, but its going to take a couple years to get here. The spec is nearly finished, and will be updated later this year. And then it will take a couple more years to produce silicon. So call it a 2027 thing.  NVLink 6.0 will be shipping by then, probably at twice the speed of 5.0.  But at least the rest of the crowd (AMD, Intel, etc) will have something that more closely aligns with customer demand to interconnect thousands of accelerators.
AMD realizes it is far behind Nvidia on the software front, and has recently acquired Silo.ai to help shore up this vital area.  Silo is a European based software company that provides AI software for financial services, aviation, healthcare, manufacturing, consumer goods, and telecommunications. Silo is (or was) Europe’s largest privately-held AI software company, and will certainly help AMD begin to close this gap. However, the biggest software problem for AMD remains CUDA, the low-level libraries that makes Nvidia GPUs sing, as well as performance-enhancing software like Triton inference server and TensorRT-LLM, which can more than quadruple performance of Nvidia GPUs.  We suspect addressing this gap is a high priority for AMD.
AMD is on an arduous path to address the competitive gaps for large-scale AI in the three areas we have identified. Yes, Nvidia has a lot more software beyond that we have noted here (e.g., Omniverse and the slew of technologies for Physical AI), but the rest of the story will flesh out over time as AMD builds an ecosystem and fixes its networking deficiencies. (We note that the ZLUDA project meant to provide a CUDA remedy has been cancelled.) It is going to take time, and perhaps another acquisition in the networking space, unless it wants to cede that revenue to Broadcomm.
Disclosures: This article expresses the opinions of the author and is not to be taken as advice to purchase from or invest in the companies mentioned. My firm, Cambrian-AI Research, is fortunate to have many semiconductor firms as our clients, including BrainChip, Cadence, Cerebras Systems, Esperanto, IBM, Intel, NVIDIA, Qualcomm, Graphcore, Groq, SImA,ai, Synopsys, Tenstorrent, Ventana Microsystems, and scores of investors. We have no investment positions in any of the companies mentioned in this article. For more information, please visit our website at https://cambrian-AI.com."
245,https://www.forbes.com/sites/davidprosser/2024/08/19/how-fabric-cryptography-plans-to-become-the-nvidia-of-privacy-tech/,How Fabric Cryptography Plans To Become The Nvidia Of Privacy Tech,"Aug 19, 2024, 02:36pm EDT",David Prosser,"Michael Gao, the co-founder of California-based start-up Fabric Cryptography, makes a bold claim. “We’re going to do for cryptography what Nvidia has done for artificial intelligence (AI),” he says. Given the stellar performance of Nvidia, largely related to its market leadership in AI chip technology, that’s some promise. But investors are ready to back the ambition of Gao and his colleagues – the company is today announcing a $33 million fund raising.
Founded only last year, Fabric has developed a new piece of hardware aimed specifically at the cryptography sector, where concerns over data privacy and security have seen a surge in investment in new types of technology. “We’re the first company in the world to create a chip that is focused directly at the needs of cryptographers,” says Tina Ju, Gao’s co-founder. “The latest cryptography tools represent a whole new generation of technology and they require a whole new architecture to support them.”
The architecture in question is Fabric’s “verifiable processing unit (VPU)”. It’s a chip designed specifically with an instruction set that is bespoke to cryptography, enabling users to break down the algorithms used in the discipline into their mathematical building blocks, which are accelerated and supported by the chip.
Fabric points to developments in cryptography such as zero-knowledge proofs as necessitating special attention from hardware designers and manufacturers. ZKPs effectively enable one party to use its data to establish that something is true without actually providing access to the data. Such concepts promise to revolutionise data privacy, but could also have widespread applications in multiple disciplines as organisations and individuals work to establish trust with one another. Gao points out, for example, that this type of cryptography could be used by generative AI developers to prove that the data in their models is not biased or inaccurate, without the data and the model actually having to be shared.
Despite the potential of advanced cryptography to deliver on such ambitions, the hardware sector has not invested in its needs, Fabric argues.
“There exists a whole world of advanced cryptographic algorithms that go beyond protecting our data, and can actually begin to guarantee trust, if we can run them efficiently,” Gao says. “Billions of dollars have been poured into better AI chips of all kinds, but researchers and industry projects in cryptography have had to settle with CPUs or GPUs, which were never made for the kind of intensive mathematics that advanced cryptography uses.”
Fabric’s technology, aimed at closing this gap in the market, is ready to go into mass production early next year and the company says it already has substantial pre-orders for its VPU. Today’s Series A investment will give the business the headroom it requires to reach mass production – and provide funding for further research and  development.
The funding round is co-led by Blockchain Capital and 1kx, with participation from specialists in the sector, such as Offchain Labs, Polygon, and Matter Labs. It follows an earlier $6 million seed round led by Metaplanet with participation from investors including Inflection and Liquid2 Ventures. Today’s announcement takes the total amount of funds raised by the company since its launch last year to $39 million.
""What sets the VPU apart is its unique combination of programmability, flexibility and performance,” argues Dr Wei Dai, cryptographer and research partner at 1kx. “The VPU can be programmed to run virtually any cryptographic workload efficiently using its innovative instruction set. Unlike other fixed-function chips, which are common in cryptography, the VPU is future-proof – it can adapt to new cryptography algorithms as they are developed and productionised.”
Yuan Han Li, an investor at Blockchain Capital, also believes the technology could prove transformative. He describes it as “the world’s first VPU to help bring about a world where privacy and verifiability are non-negotiable components of all digital systems.”"
246,https://www.forbes.com/sites/dereksaul/2024/08/19/nvidia-led-rally-boosts-sp-500-toward-best-stretch-of-2024/,Nvidia-Led Rally Boosts S&P 500 To Best Stretch Of 2024,"Aug 19, 2024, 03:04pm EDT",Derek Saul,"The stock market recovery advanced further Monday, with two of the U.S.’ major indexes amidst their longest winning streaks this year, as a familiar name leads the latest bounce.
Up 1% by late afternoon, the bellwether S&P 500 comfortably gained for its eighth consecutive trading session, while the tech-heavy Nasdaq Composite’s 1.4% rally also was also its eighth consecutive day in the green.
That is the S&P’s longest winning streak since Oct. 30-Nov. 8, 2023 and the Nasdaq’s longest since Dec. 7-19, 2023.
The S&P registered its highest intraday price since July 18 and the Nasdaq its highest since July 23.
Buoying the rally was a 4% rally from artificial intelligence maven Nvidia as the semiconductor chip designer rose to its highest share price since July 15, and a 1% gain from Class B shares of Berkshire Hathaway, the Warren Buffett-run conglomerate which hit its most expensive price ever Monday.
The 30-constituent Dow Jones Industrial Average hit its top level since Aug. 1 after rising 0.6% Monday; the Dow is only amid a five-day winning streak due in large part to its exclusion of Nvidia, whose 4% gain last Monday lifted the Nasdaq and S&P to mild gains as the Dow fell 0.4%.
Nvidia surpassed Microsoft as the world’s second-largest company by market capitalization Monday for the first time since June, according to YCharts data. Nvidia stock is up almost 30% over the last three weeks, adding some $700 billion in market value.
Monday’s rally extended the stock bounce from multi month lows set earlier in August: The Dow is up 6% from its August 5 bottom, the S&P 8% and the Nasdaq more than 10%. The Dow and S&P are both within about 1% of their all-time closing highs set last month, while the Nasdaq is within 5% of its peak. The recovery comes after the wipeout during the first week of August coinciding with a weaker-than-expected U.S. monthly jobs report and concerns about the potential for unusually aggressive monetary policy from the Bank of Japan.
The CBOE Volatility Index (VIX), known as Wall Street’s fear gauge, fell a further 1% Monday to its lightest level in four weeks, now almost 80% below its Aug. 5 high, indicating restored confidence in the market."
247,https://www.forbes.com/sites/petercohan/2024/08/09/nvidia-stock-could-pop-to-record-if-q2-earnings-report-is-boffo/,Nvidia Stock Could Pop To Record If Q2 Earnings Report Is Boffo,"Aug 09, 2024, 08:32am EDT",Peter Cohan,"Nvidia’s shares have lost 26% of their value since peaking at nearly $141 in June.
Why has the stock fallen so far in the last two months? Are Nvidia’s shares a bargain? My thoughts: I am not sure why the stock fell and the shares could soar August 28 if Nvidia reports expectations-beating second quarter sales and earnings growth and raises its guidance.
Nvidia stock could be getting dragged down by news reports including:


It is even possible hedge funds — seeking to raise capital on August 5 to cover margin calls on their underwater carry trades  — might have taken gains on their Nvidia holdings, The Economist suggested.
Here are the reasons Nvidia stock could rise:


Companies generally enjoy an increase in their stock prices if they report better than expected growth in revenue and earnings and raise their forecast each quarter.
Nvidia has blown through analysts’ expectations since the second quarter of 2023. That trend is likely to continue when the company releases its second quarter report for fiscal 2025 — for the July 2024-ending quarter.
Since the July 2023-ending quarter, here are the percentages by which Nvidia exceeded analysts’ earnings per share expectations, according to Nasdaq:


For the July 2024-ending quarter, expectations are high.
Analysts estimate Nvidia will report earnings per share of $0.59 — a 136% increase from the previous year, according to Barchart. On May 22, Nvidia forecast 197% revenue growth for fiscal 2025’s Q2 — exceeding analysts’ expectations, noted the Wall Street Journal.
Strong AI spending should help Nvidia make its own ambitious targets for data center revenue. Analysts expect Nvidia to report $25 billion in data center revenue for the July quarter — roughly unchanged from a year ago, the Journal reported.
The performance of Nvidia’s stock could hinge on whether the company exceeds that target and raises guidance. Since the magnitude of Nvidia’s earnings surprises have steadily declined in the previous four quarters, the AI chip designer’s Q2 surprise could be lower than the first quarter 2024 result was.
Alphabet, Microsoft, Meta, and Amazon —  which together account for 40% of Nvidia’s revenue, according to Bloomberg — plan to continue spending heavily on AI-related infrastructure.
Therefore, growth in demand for Nvidia’s graphics processing units — widely used to train and operate the large language models that power generative AI — could remain strong.
These cloud services providers — that boosted by 64% their second quarter capital expenditures to $58.5 billion, according to the Journal — say they would rather invest too much in generative AI than too little.
Here is how that breaks down by company:


While this should be great news for Nvidia bulls, macroeconomic worries may be motivating some investors to sell the chip designer’s stock. “Nobody reduced numbers and said things are not working with AI or we’re taking a pause on AI,” Rhys Williams, chief strategist at Wayve Capital Management LLC, told Bloomberg. “It’s just that people are very nervous” about macroeconomic uncertainties, he added.
To be sure, investors are also impatient for companies to quantify the extent to which their generative AI investments are driving revenue and profit growth. “We haven’t yet seen a way to monetize AI, so the return on that spending is unclear,” said Srini Pajjuri, managing director and senior research analyst at Raymond James, according to Bloomberg. “The question is, how long can this continue?”
Contrary to Pajjuri’s contention, some companies are monetizing AI. For example, Meta reported faster-than-expected revenue growth in its digital advertising business for the second quarter. The reason? Meta’s generative AI ad platform matches ads more effectively to potential buyers of advertisers’ products, according to an August Forbes post.
Moreover, ServiceNow reported a bigger than expected backlog due to strong demand for the company’s generative AI-powered digital workflow services, noted a July Forbes post.
To be fair, given these companies’ failure to quantify how much revenue is attributable to generative AI, it is unclear whether the resulting profits are sufficient to offset the high cost of building and operating large language models.
However, two things are clear. First, unless companies invest, there is little chance they will be able to profit from generative AI before rivals do.
Second, there will be a time lag — potentially years — between when companies begin experimenting with generative AI and when a fraction of those experiments turn into new growth vectors, I argue in my new book, Brain Rush.
Nvidia dominates the GPU market — with 88% share in the first quarter of 2024, according to Jon Peddie Research.
Many competitors are trying to win some of that GPU market share. Rivals include AI chips provided by Nvidia customers. These include Google’s Trillium custom AI accelerator, Microsoft’s Azure Maia, Meta’s custom AI chip  — the Meta Training and Inference Accelerator — and Amazon’s Trainium and Inferentia chips, the companies said in their second quarter earnings calls.
Yet thanks to Nvidia’s CUDA software — that developers have been using for well over a decade — and a raft of partnerships about which I wrote in Brain Rush, the chip designer’s lead is difficult for these companies — and other rivals such as AMD and Intel — to surmount.
While Nvidia launched CUDA in 2007, the company has continuously adapted the library. More than five million developers at about 40,000 companies now use CUDA — which includes “more than 300 code libraries and 600 AI models, and supports 3,700 GPU-accelerated applications,” noted the Journal.
What’s more, Nvidia keeps creating the future of AI chips. Google and Nvidia’s other data center customers are lining up to take delivery of Nvidia’s state-of-the art Blackwell chips when they are available. CEO Jensen Huang said Blackwell would generate “a lot of revenue” for Nvidia in 2024 during a May investor call.
Given all the attention being paid to Nvidia, I expect its shares to move sharply on August 28. They will go up far should the company beat and raise. Otherwise, the chip designer’s stock is sure to plunge."
248,https://www.forbes.com/sites/karlfreund/2024/08/09/can-groq-really-take-on-nvidia/,Can Groq Really Take On Nvidia?,"Aug 09, 2024, 04:24pm EDT",Karl Freund,"The Silicon Valley startup has just raised an astonishing $640M in additional funding, and is valued at $2.8B.  Have they captured AI Lightning in a bottle?
Groq has announced a $640M Series D round at a valuation of $2.8B, led by BlackRock Private Equity Partners. Founded by one of the developers of the original Google TPU, Jonathan Ross, Groq has shifted towards extremely fast and large-scale inference processing based on the Language Processing Unit which Ross pioneered. The additional investment will enable Groq to accelerate the next two generations of LPUs.
“You can’t power AI without inference compute,” said Jonathan Ross, CEO and Founder of Groq. “We intend to make the resources available so that anyone can create cutting-edge AI products, not just the largest tech companies. This funding will enable us to deploy more than 100,000 additional LPUs into GroqCloud.”
The Groq LPU is a single core processor designed for LLMs, interconnected with a fast switchless routing fabric using 288 QSFP28 optical cables.  A rack is built from 9 GroqNode 1 servers (1 server acts as a redundant resource) with a fully connected internal RealScale network delivering accelerated compute performance up to 48 Peta OPs (INT8), or 12 PFLOPs (FP16). Groq has championed its open-source approach, fully embracing models such as Meta’s Llama 3.1.  By providing access to LPUs on the GroqCloud demonstrating its amazing performance,  the company has built a loyal following, with over 70,000 developers using the GroqCloud to create applications.
What’s Groq’s claim to fame? Extremely fast inference. How do they do it?  SRAM.
Unlike AI GPU’s from Nvidia and AMD, Groq uses on-chip SRAM, with 14GB of high bandwidth shared memory for weights across the rack.  SRAM is some 100x faster than the HBM memory used by GPUs. While 14GB of SRAM in a rack is dwarfed by the HBM in a rack of GPUs, the Groq LPU's SRAM is particularly effective in inference tasks where speed and efficiency are paramount.  For appropriately-sized models, the faster SRAM and optical switchless fabric can generate some 500-750 tokens per second.  Thats amazingly fast, and you just have to go to their website to see it live (its free!).  To put things into perspective, ChatGPT with GPT-3.5 can only generate around 40 tokens/s. Now you can see why they call their cloud access “Tokens as a service”.
Unfortunately, there is no free lunch. SRAM is far more expensive than DRAM or even HBM, contributing to the high cost of Groq LPU cards, which are priced at  $20,000 each.  More importantly, SRAM is 3 orders of magnitude smaller than a GPU’s HBM3e.  So, this platform can’t do trainng.
But smaller models are in fashion of late. And if your model is, say, 70B parameters or less, you would do well to check it out on Groq. Groq has also run larger models, such as Llama 3.1 405B.  Can you read 500 words (tokens) per minute? No. But inference processing is becoming inter-connected, with the output of one query being used as the input to the next. In this world, Groq brings a lot to the table.
If you want to compete against Nvidia, you better have something very different than just another GPU. Cerebras has a completely different approach in Training. Groq has amazingly low latency in inference, albeit for smaller models. With the new funding in place, Groq can move to 4nm manufacturing to support larger models, probably in the next year.  Note that Groq also doesn’t have the same supply issues  Nvidia faces; they use Global Foundries, not TSMC.
LLM Inference processing is starting to take off. Most recently, Nvidia said that 40% of their data center GPU revenue last quarter came from inference processing.
Disclosures: This article expresses the author's opinions and
should not be taken as advice to purchase from or invest in the companies mentioned. Cambrian-AI Research is fortunate to have many, if not most, semiconductor firms as our clients, including Blaize, BrainChip, Cadence Design, Cerebras, D-Matrix, Eliyan, Esperanto, GML, Groq, IBM, Intel, NVIDIA, Qualcomm Technologies, Si-Five, SiMa.ai, Synopsys, Ventana Microsystems, Tenstorrent and scores of investment clients. Of the companies mentioned in this article, Cambrian-AI has had or currently has paid business relationships with Groq, Cerebras, and Nvidia. For more information, please visit our website at https://cambrian-AI.com."
249,https://www.forbes.com/sites/dereksaul/2024/07/30/nvidia-stock-falls-to-2-month-low-down-25-from-peak-amid-near-800-billion-swoon/,Nvidia Stock Falls To 2-Month Low—Down 25% From Peak Amid $800 Billion Swoon,"Jul 30, 2024, 11:59am EDT",Derek Saul,"Nvidia stock faltered again Tuesday, continuing the artificial intelligence king’s summertime blues as the stock delves further into correction territory.
Nvidia stock fell 7% to $104, outpacing the Nasdaq Composite index’ s1.3% decline as the tech stock slump extended.
Notching its lowest closing share price since May 23, Nvidia stock is now down 26% from its June intraday peak of over $140 per share, well beyond a 10% correction for the semiconductor chip designer which dominates the market for the advanced technology powering generative artificial intelligence programs.
The 16% July decline would be Nvidia’s worst monthly performance since September 2022, when its stock traded at a split-adjusted price of below $15.
At about $2.55 trillion, Nvidia’s market capitalization is down about $785 billion from its record $3.34 trillion valuation as of June 18’s market close, erasing more than the equivalent of Tesla in a span of six weeks.
There isn’t a singular event that caused Nvidia’s Wall Street woes this month, but rather several smaller trends that added up. It’s routine for a stock to decline after a period of extended gains as investors and fund managers cash in on their profits and adjust their portfolios—Nvidia stock is still up 110% year-to-date and 610% dating back to the end of 2022, delivering truly eye-popping returns for a company of its size. And stock prices of course go down if more people are selling than buying. Nvidia also was hurt by a slump across semiconductor stocks as investors digested former President Donald Trump’s hawkish commentary on trade and defense policies in East Asia, a region crucial for American chip makers for manufacturing and sales. And Nvidia is perhaps the clearest victim of the broader market’s rotation out of big technology companies and into lagging smaller firms as investors prepare for the first interest rate cuts since 2020. Shares of Apple, Microsoft and Google parent Alphabet, the three other companies valued at over $2 trillion, are each down more than 7% apiece from their respective all-time highs set earlier this month.
Nvidia’s slump is not due to any sort of expectations for a lull in its financial performance, as it is expected to be by far the largest contributor to second-quarter earnings growth across the S&P 500, according to FactSet data. Analysts project Nvidia to grow its net income by 140% year-over-year to $15 billion, and their average target price of $134 per share indicates strong belief in a bounceback for Nvidia stock, which was the best performer of any S&P component in 2023 and the first half of 2024. Nvidia will report earnings at the end of next month.
"
250,https://www.forbes.com/sites/karlfreund/2024/07/23/nvidia-ai-foundry-and-nims-a-huge-competitive-advantage/,Nvidia AI Foundry And NIMs: A Huge Competitive Advantage,"Jul 23, 2024, 07:00pm EDT",Karl Freund,"Nvidia has fleshed out a complete software stack to ease custom model development and deployment for enterprises. Is this AI Nervana? And can AMD and Intel compete with this?
In order for enterprises to adopt AI, its got to become a lot easier and more affordable. Nvidia has (re-) launched AI Foundry to help Enterprises adapt and adopt AI to meet their business needs without having to start from scratch. And without having to spend gazillions of dollars.
The timing is spot-on as investors grow nervous that it may be hard  for enterprises to make a good return on their AI investments. And without Enterprise adoption, AI will fail and we will be back to an AI Winter. To counter that narrative, Nvidia is expected to share Enterprise ROI stories during its next earnings call. And the new AI Foundry coupled with NIMs could become the standard path forward for most companies. While many components of this story are indeed open source, they only run on Nvidia GPUs. I know of no other chip company with anything even close to NIMs or the AI Foundry.
The Nvidia AI Foundry is a combination of software, models, and expert services to help Enterprises not only get started, but complete their AI journey.  Will this put Nvidia on a collision course with its ecosystem consulting partners such as IBM and Accenture? Accenture has been using the Nvidia AI Foundry to revamp its internal enterprise functions, and has now taken what they have learned and created the Accenture AI Refinery to help its clients do the same. Deloitte is on a similar path.
According to Nvidia’s blog on the Foundry, “Just as TSMC manufactures chips designed by other companies, NVIDIA AI Foundry provides the infrastructure and tools for other companies to develop and customize AI models — using DGX Cloud, foundation models, NVIDIA NeMo software, NVIDIA expertise, as well as ecosystem tools and support.”
When initially rolled out back in late 2023, Nvidia Foundry was focussed on Microsoft Azure hosted AI.  Since then, Nvidia has recruited dozens of partners to help deliver the platform, including AWS, Google Cloud, and Oracle Cloud as well as scores of generative AI companies, model builders, integrators and OEMs.
The NVIDIA AI foundry service pulls together three elements needed to customize a model for a specific data set or company — a collection of NVIDIA AI Foundation Models, NVIDIA NeMo framework and tools, and NVIDIA DGX Cloud AI supercomputing services — giving enterprises an end-to-end solution for creating custom generative AI models.
But you thought thats what RAG was for, right?  Yes, Retrieval Augmented Generation can do a great job of adding company-specific data to an LLM. But Nvidia said that the Foundry can produce a customized model that is fully ten points more accurate than a simple RAG augmentation.  Ten points can make the difference between a great model and one that may be thrown on the trash heap.
NIMs provide the building blocks needed to greatly simplify and expand the domains that the Foundry can build on.  Nvidia shared over 50 NIMs they have already created for various domains. Recall that a NIM is a containerized inference processing micro-service that the Nvidia NIM Factory has built, and that an Enterprise AI License provides access to the ever-growing NIM Library on ai.nvidia.com.
The Foundry launch was timed to coincide with Meta’s release of Llama 3.1 405B, which is the first open model that can rival the top AI models from OpenAI, Google, and others, when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and now with multilingual translation. Meta believes the latest generation of Llama will ignite new applications and modeling paradigms, including synthetic data generation to enable the improvement and training of smaller models, as well as model distillation. Nvidia Foundry also supports the NVIDIA Nemotron, CodeGemma by Google DeepMind, CodeLlama, Gemma by Google DeepMind, Mistral, Mixtral, Phi-3, StarCoder2 and others.
And true to form, Nvidia shows that it can increased performance of models like Llama 3.1 with optimized NIMs.  Inferencing solutions like NVIDIA TensorRT-LLM improve efficiency for Llama 3.1 models to minimize latency and maximize throughput,  enabling enterprises to generate tokens faster while reducing total cost of running the models in production.
Nvidia also released today four new NeMo Retriever NIM microservices to enable enterprises to scale to “agentic AI” workflows — where AI applications operate accurately with minimal intervention or supervision — while delivering the highest accuracy retrieval-augmented generation, or RAG. These new NeMo Retriever embedding and reranking NIM microservices are now generally available:


“NeMo Retriever provides the best of both worlds. By casting a wide net of data to be retrieved with an embedding NIM, then using a reranking NIM to trim the results for relevancy, developers tapping NeMo Retriever can build a pipeline that ensures the most helpful, accurate results for their enterprise,” Nvidia explained in their blog.
Perhaps an example would help. Suppose you want to build a digital assistant to help patients with personalized information. Nvidia showed how they can combine 3 agents and 9 NIMs to build an assistant application. This is pretty close to Nervana and way beyond anything that the competition can offer.
While the competition continues to improve the performance and connectivity of their accelerators, Nvidia is building the software that enables AI adoption. I know of no competitor to NIMs, nor a competitor to Foundry.  And of course, nobody has introduced a competitor to Transformer Engine nor TensorRT-LLM, both of which can deliver 2-4 times the performance of a GPU without these features.
As enterprises work to adapt and adopt custom models for their business and applications, Nvidia is providing  an easy on ramp to become an AI-enabled enterprise.
As for pricing, while NIM is included in the Enterprise AI license for each GPU, Foundry is priced based on a specific customer situation and is not included in Enterprise AI.
Here’s more detail on the Foundry:
Disclosures: This article expresses the author's opinions and
should not be taken as advice to purchase from or invest in the companies mentioned. Cambrian-AI Research is fortunate to have many, if not most, semiconductor firms as our clients, including Blaize, BrainChip, Cadence Design, Cerebras, D-Matrix, Eliyan, Esperanto, GML, Groq, IBM, Intel, NVIDIA, Qualcomm Technologies, Si-Five, SiMa.ai, Synopsys, Ventana Microsystems, Tenstorrent and scores of investment clients. We have no investment positions in any of the companies mentioned in this article and do not plan to initiate any in the near future. For more information, please visit our website at https://cambrian-AI.com.
"
251,https://www.forbes.com/sites/stevemcdowell/2024/07/18/mistral-ai-and-nvidia-unveil-new-language-model-mistral-nemo-12b/,Mistral AI And Nvidia Unveil New Language Model: Mistral NeMo 12B,"Jul 18, 2024, 03:22pm EDT",Steve McDowell,"Mistral AI and NVIDIA launched Mistral NeMo 12B, a state-of-the-art language model for enterprise applications such as chatbots, multilingual tasks, coding, and summarization. The collaboration combines Mistral AI's training data expertise with NVIDIA's optimized hardware and software ecosystem, offering high performance across diverse applications.
This state-of-the-art language model promises to enhance the development of enterprise applications with its exceptional capabilities in chatbots, multilingual tasks, coding, and summarization. The collaborative effort between Mistral AI’s deep expertise in training data and NVIDIA’s optimized hardware and software ecosystem has resulted in a model that sets new benchmarks for performance and efficiency.
The partnership between Mistral AI and Nvidia was pivotal in bringing the Mistral NeMo 12B to life. Leveraging NVIDIA's top-tier hardware and software, Mistral trained the model on the Nvidia DGX Cloud AI platform, which provides dedicated, scalable access to the latest Nvidia architecture. This synergy has enabled the development of a model with unprecedented accuracy, flexibility, and efficiency.
Mistral NeMo 12B handles large context windows of up to 128,000 tokens, offering unprecedented accuracy in reasoning, world knowledge, and coding within its size category. Built on a standard architecture, it ensures seamless integration, serving as a drop-in replacement for systems currently using the Mistral 7B model.
The Mistral NeMo 12B model excels in various complex tasks:


The model's open-source nature, released under the Apache 2.0 license, encourages widespread adoption, making advanced AI accessible to researchers and enterprises.
The model is packaged as an Nvidia NIM inference microservice, offering performance-optimized inference with TensorRT-LLM engines, allowing for deployment anywhere in minutes. This containerized format ensures enhanced flexibility and ease of use for various applications.
Available as part of Nvidia AI Enterprise, Mistral NeMo 12B also includes comprehensive support features from Nvidia:


This allows direct access to Nvidia AI experts and defined service-level agreements, delivering consistent and reliable performance for enterprise users.
While the new model is available as part of Nvidia AI Enterprise, its availability is much broader, including availability on Hugging Face. Mistral released NeMo under the Apache 2.0 license, where anyone interested can use the technology.
As a small language model, Mistral NeMo is designed to fit on the memory of affordable accelerators like Nvidias L40S, GeForce RTX 4090, or RTX 4500 GPUs, offering high efficiency, low compute cost, and enhanced security and privacy.
The AI market is one of the most competitive markets in technology, with giants like OpenAI, IBM, Anthropic, Cohere, and nearly every public cloud provider all working to find the right solutions to bring the value of generative AI into the enterprise. The line between competitor and partner is often blurry, such as Mistral's relationship with Microsoft, which has its internal efforts and a deep relationship with OpenAI. This is a world that continues to evolve.
Mistral AI is a strong, and growing, competitor in the AI model space, showing the necessary blend of technical competence and execution. Its execution is impressive. In August alone, Mistral released its NeMo model with Nvidia, its new Codestral Mamba model for code generation, and Mathstral for math reasoning and scientific discovery. It has strong relationships with companies like Nvidia, Microsoft, Google Cloud, and Hugging Face, yet faces equally fierce competition.
Mistral AI was founded with the mission to push the boundaries of AI capabilities. Thanks to its innovative approaches and strategic partnerships, the company has no difficultly adhering to that mission while growing its importance in the field. The release of Mistral NeMo 12B continues that momentum forward. We can’t wait to see what’s next."
252,https://www.forbes.com/sites/dereksaul/2024/07/17/apple-nvidia-lead-tech-stock-slump-as-lower-rates-and-trumps-china-stance-come-into-focus/,"Apple, Nvidia Lead Big Tech’s $500 Billion Slump As Nasdaq Limps To Worst Day Since 2022","Jul 17, 2024, 10:09am EDT",Derek Saul,"Large technology stocks flailed Wednesday as the tech-heavy Nasdaq Composite index paced toward its worst day since Dec. 2022, highlighting the last week’s shifting market regime as lower interest rates appear on deck and investors view a Trump presidency as increasingly likely.
The Nasdaq fell 2.8% by market close, a wider loss than the S&P 500’s 1.4% decline and the Dow Jones Industrial Average 0.6% gain, sinking to its lowest level since July 1.
Among the most notable losers Wednesday were Nvidia (shares down 6.6%) and Apple (-2.5%)—both of which faced downward pressure due to concerns about their key China units after President Joe Biden reportedly moved to curb China’s access to high-end semiconductor chips and former President Donald Trump touted the “phenomenal” prospect of tariffs on Chinese goods in an interview with Bloomberg Businessweek published late Tuesday.
All of the “magnificent seven” mega-cap tech stocks in Alphabet, Amazon, Apple, Meta, Microsoft, Nvidia and Tesla declined more than 1%, shedding more than $500 billion of aggregate market value.
The magnificent seven’s rare slump, with the septet down an average of 5.8% over the last week, is the latest evidence of a major market repositioning since Thursday’s better-than-anticipated inflation data reignited bets of significantly lower interest rates and betting odds began to heavily favor Trump in November’s presidential election.
Since last Wednesday, the wider spanning Dow is up 3.6% compared to the tech-concentrated Nasdaq’s 3.4% decline, both trumped by the small-cap Russell 2000’s 9.1% rally, evidence, signifying investors’ belief that lower rates can help stimulate earnings growth across corporate America, rather than just in the handful of artificial intelligence leaders as has been the case in recent quarters.
The S&P’s energy and financial sectors are up 4% apiece over the past week, trouncing the information technology and communication services’ more than 5% respective drops, with bank and oil companies’ rallies likely tracing back to Trump’s promises for lighter regulation in the industries, opening the door for higher profits.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
It’s a renaissance for the long forsaken small caps: The Russell 2000 tallied its best five-day stretch relative to the S&P ever from last Wednesday to Tuesday, according to Charles Schwab strategist Kevin Gordon.
The Russell 2000, which tracks 2,000 public American companies with a median market value of about $1 billion, is up 11% year-to-date, up from a flat return through last Wednesday. Traders now expect the Federal Reserve to lower interest rates from a 23-year high beginning in September, which will help companies’ earnings as loans grow less expensive. Even after the recent breather, big tech remains by far the biggest winner of the recent stock market, with the S&P’s information technology and communications sectors the top performing components on a year-to-date, 1-year and 5-year basis.
Nvidia stock just officially entered correction territory, with shares down more than 10% from its close last Wednesday, now at a 12.5% week-over-week loss. Nvidia stock is still up about 140% this year, the best return of any company listed on the S&P for the whole year. Meta stock (down 5% Wednesday) also hit a correction, falling 13% over the last week, but it’s still up a robust 34% this year.
“It is a bad idea to make meaningful portfolio adjustments based on who you think is going to win the upcoming election,” cautioned Wells Fargo Investment Institute strategist Scott Wren in a Wednesday note to clients, noting polling errors and differences in campaign promises versus actual policy."
253,https://www.forbes.com/sites/moorinsights/2024/07/15/nvidia-doubles-down-on-ai-and-taiwan-at-computex-2024/,Nvidia Doubles Down On AI And Taiwan At Computex 2024,"Jul 15, 2024, 10:01am EDT",Anshel Sag,"At Computex 2024 last month, Nvidia made a big splash with the first keynote of the show, held offsite at the athletics stadium of National Taiwan University. Nvidia, like many other vendors at the show, doubled down on its position in AI, both in the datacenter and inside AI PCs. While much of what CEO Jensen Huang talked about during the opening keynote was focused on datacenter and cloud AI, there were still a plethora of other announcements from the company focused on its PC gaming business and on injecting more AI into gaming. These new AI PC use cases also seemed to focus heavily on utilizing a hybrid AI approach leveraging both cloud and local AI compute to deliver a more advanced gaming experience.
During the keynote, Nvidia didn’t introduce many new concepts that it hadn’t already covered during its presentations at CES or GTC earlier this year. It did update some of its power targets for GB200 NVL72 racks, now claiming a more efficient 100 kilowatts per rack instead of the previously quoted 120 kilowatts. Huang also showed what a NVLink spine looked like and struggled to carry it on stage to demonstrate the size and weight of the interconnect.
Nvidia did give more visibility into the company’s future roadmap all the way out to 2027. The company has updated its cadence of GPU launches to an annual schedule, which is an acceleration from its old 18- to 24-month cadence. This means that we should get Blackwell Ultra in 2025, Rubin GPU and Vera CPU in 2026 and Rubin Ultra in 2027. Do keep in mind that Vera will likely be paired with Rubin, much like Grace is paired with Blackwell and Hopper. (Fun fact: just as the Grace and Hopper components were named for the computer scientist Admiral Grace Hopper, the Vera and Rubin components are named for the astronomer Dr. Vera Rubin, who did pioneering studies of galactic rotation.) So, we can expect VR to be the future nomenclature for those platforms, likely in a VR100 and VR200 configuration.
While Nvidia has been dominant in both enterprise and cloud AI environments, it has struggled to communicate its capabilities on the PC. This is especially true with the advent of the AI PC. Ironically, Nvidia was one of the first companies to bring AI capabilities to the PC via its GPUs with technologies such as DLSS, which has been pivotal in enabling real-time ray tracing. That aside, Nvidia has been making lots of product announcements this year to beef up its AI PC story, including with the launch of ChatRTX.
To further improve its on-device capabilities, Nvidia took what was once an April Fool’s Joke and turned it into a real beta with Project G-Assist, which is a GeForce AI Assistant. I got to experience this firsthand in the Nvidia suites at Computex and was really impressed with what it could do and how much it enhanced the gaming experience. Capabilities ranged from helping adjust graphics settings to walking the user through certain game intricacies and answering questions about objects in the in-game line of sight.
Nvidia also demonstrated the next generation of its ACE digital human platform, which is powered for on-device use by Nvidia NIMs that add more intelligence and interactivity to NPCs in open-world and RPG titles. Nvidia ACE’s demos were a great way for the company to demonstrate its hybrid AI capabilities to improve performance and latency.
At Computex, the company also announced that it is working with Microsoft to deliver Copilot+ PC specs in a new category of laptops using RTX 4070 GPUs paired with other vendors’ SoCs. My understanding is that most of those will be SoCs that have dedicated NPUs, such as the AMD Ryzen AI 300 series or perhaps Intel’s Lunar Lake, but it remains unclear when these configurations will debut.
To further back up Nvidia’s AI PC story, the company also noted that Windows Copilot Runtime would be adding GPU acceleration for local PC SLMs. This, paired with Nvidia’s RTX AI Toolkit, should make access to Nvidia’s GPUs much easier for third-party developers building Windows AI applications, further strengthening the company’s AI PC story. AI PC darlings Adobe, Blackmagic Design and Topaz are already onboard to take advantage of the RTX AI Toolkit for their apps, and I’m excited to see how all these apps will leverage both GPU and NPU optimizations to maximize performance.
Nvidia has been working hard this year to strengthen its position on the client side of the AI equation, especially on the AI PC. Meanwhile, Nvidia’s position in AI for the cloud and the datacenter is dominant, and with an accelerated annual cadence it’s quite clear that the company will be difficult to catch up with.
What I want to see from Nvidia in the future is a better end-to-end AI story that showcases its strengths as hybrid AI continues to become a more prevalent model for AI consumption. Nvidia has already released its latest RTX 4000 Super family of GPUs this year, but even now there are rumors about the next-generation 5000 series, which will likely lean even further into AI capabilities such as frame generation and other rendering techniques. As touched on above, Nvidia’s role in Copilot+ PCs could also evolve with time if current rumors are any indication; I’ll be looking especially closely for products in that vein early next year.
Moor Insights & Strategy provides or has provided paid services to technology companies, like all tech industry research and analyst firms. These services include research, analysis, advising, consulting, benchmarking, acquisition matchmaking and video and speaking sponsorships. Of the companies mentioned in this article, Moor Insights & Strategy currently has (or has had) a paid business relationship with AMD, Intel, Microsoft and Nvidia."
254,https://www.forbes.com/sites/zoyahasan/2024/07/11/how-this-first-time-founder-got-softbank-nvidia-and-microsoft-to-write-him-a-billion-dollar-check/,"How This First-Time Founder Got SoftBank, Nvidia And Microsoft To Write Him A Billion-Dollar Check","Jul 11, 2024, 07:00am EDT",Zoya Hasan,"In the summer of 2018, 25-year-old Alex Kendall followed Jensen Huang into an elevator after the Nvidia CEO had finished his talk at an AI conference in Salt Lake City. With only 20 seconds alone with Huang, Kendall pitched him Wayve: then a year-old startup in London building AI to let cars drive themselves, claiming it could do it safer and cheaper than anyone else on the road by giving the car “its own brain.”
The literal elevator pitch planted the seeds for what, six years later, would become a $1.05 billion Series C funding round. Closed in May, the raise attracted investment not only from Nvidia, but tech titans SoftBank and Microsoft, a company whose AI bets have propelled it to become one of the world's most valuable enterprises with a $3 trillion market cap.
Neither Kendall nor his investors would disclose Wayve’s current valuation. The deal, however, was the largest cash raise for a tech startup in the U.K. And Kendall pulled it off with no revenue, no customers and no commercially available product.
Investor’s aren’t worried. “Every one or two months I've been in the vehicle, the AI has become cleverer every time,” says SoftBank investor Kentaro Matsui. “We’re quite confident there’ll be multiple automakers who will very much be willing to partner with them.”
Matsui also likes how Wayve is a pure software play, and how Kendall, now 31, has secured industry vets—like Mobileye’s Erez Dagan and Waymo’s Dan McCloskey—for its leadership team. He was first pitched by the founder three years ago, but says late last year is when car manufacturers began taking serious notice of the startup.
Wayve has attracted some of tech’s biggest names with the promise of making any car a self-driving car using cameras and code alone. Based on computer vision and the machine learning power of AI, Wayve’s software processes live images to train itself to drive via visual observation, like a 15-year-old watching from the passenger seat.
“Robots can be told how to behave in a set environment, but they need to have the intelligence and common sense reasoning to be able to do this in a way that we trust,” says Kendall, Forbes Under 30 alum. “Everyone is focusing on building more sensors and infrastructure and hardware to make this possible, but for me, it’s an intelligence gap.”
Although it’s the same hardware-agnostic approach used in Tesla vehicles—a theory criticized for its lack of safety measures—Kendall sees an open lane by selling his technology to third-party car makers. His pitch: Wayve’s software can add self-driving powers to your vehicles, and on the cheap.
Other startups have promised to perform a similar feat, and failed. Drive.ai, backed by renowned AI scientist Andrew Ng, made headlines for this capital-intensive undertaking back in 2016. After raising $77 million, it was bought out by Apple in 2019 on the brink of bankruptcy.
Like any good car salesman, Kendall bet big on test drives. Armed with driving data collected from partnerships with U.K delivery companies DPD, Asda and the Ocado Group, he trained Wayve to navigate London’s notoriously tricky roads. By last spring, Kendall was taking SoftBank’s Matsui, Microsoft CEO Satya Nadella–and even Bill Gates–for an autonomous spin through the city’s bustling streets. The demo, where the software navigated swerving bikes, buses and pedestrian traffic, helped seal the $1 billion deal.
Growing up in the South Island of New Zealand, Kendall had a knack for building things since childhood. In 2014, he landed a scholarship at Cambridge University to pursue a PhD in robotics and computer vision, where he came up with Wayve. He banded together with cofounder Amar Shah (who’s no longer with the company), raised $2 million and hired ten other students to bring the idea to life.
A software prototype in hand, he set his sights on finding investors. Kendall attended as many tech conferences as he could, often flying to the U.S. His technique was straight forward–have a demo ready on his phone and show as many people as possible. “Entrepreneurship is about relationships,” he says. “You always have to be pitching.”
His many bylines on published research coming out of his PhD were another asset. In 2019, Meta’s chief AI scientist Yann LeCun read Kendall’s academic work and invested in Wayve’s $20 million Series A. VC firm Balderton Capital, who co-led the Series A, says it found Kendall through one of his papers that went viral in the Cambridge area. Soon, Dr. Jamie Shotton, who led Microsoft’s Mixed Reality and AI Lab out of Cambridge, joined Wayve as its chief scientist. By 2022, Microsoft put cash into Wayve’s $200 million Series B.
Now, Kendall must take Wayve from academic papers to real-world products. It’s a treacherous road.“This space is difficult. It's very hard to commercialize. The Vision Fund has invested into multiple AV companies in the past, and not many are successful,” Matsui told Forbes. “Wayve is very different from what we’ve invested in, and that comes from experience that this is a better way.”
It’s still up for debate whether Wayve’s camera-first technology is the key to unlocking everyday self-driving. Edward Niedermeyer, an industry analyst who’s authored several books on Tesla, says Elon Musk’s camera-based tech has hit pot holes. It took Tesla eight years to roll out a version similar to Wayve, autonomous driving with hands by the wheel, and that’s been linked to multiple fatalities. Wayve’s saving grace, Niedermeyer says, might just be its business model.
Other companies focusing on hardware are far ahead. Alphabet’s Waymo has partners like Mercedes-Benz and cars that don’t need drivers at all. Forbes estimates Waymo will generate some $50 million in revenue this year, even though it only operates in four cities so far.
“Judge us by what we do over the next five years,” Kendall says. “We can deliver this and deliver it robustly.”"
255,https://www.forbes.com/sites/digital-assets/2024/07/05/tokenization-and-onchain-transform-nvidia-stock-real-estate-with-propy/,"Tokenization And Onchain Transform Nvidia Stock, Real Estate With Propy","Jul 05, 2024, 08:00am EDT",Sandy Carter,"I just bought a Pudgy Plushie with a digital key to enter the new Pudgy World. Last year, in Tampa, I had dinner with Leslie Alessandra, a Florida real estate investor and founder of blockchain company DeFi Unlimited. Her company sold the first piece of real estate as an NFTAPENFT.
Some may dismiss me as someone who’s fallen for the latest fad. They’re wrong. Tokenization – the representation of real-world assets on a blockchain – is long past the hype phase. It continues to transform entire industries and business models while updating our concept of ownership and value in the digital age.
During my recent trip to the World Economic Forum in Davos, tokenization was a hot topic among global leaders and business innovators. Fortunately, you don’t need to be a head of state to care about tokenization. Before long, much of what you value and what gives you joy, whether online or in the real world, whether professionally or personally, will eventually be represented on a blockchain.
Tokenization changes everything. Let’s explore.
Let’s start with the basics. Picture this—you have a plushie that you absolutely adore. It’s valuable to you in several ways: first, it’s always there on the couch when you’re down and need a cuddle…but it’s also a limited edition piece with market value. While you can’t tokenize affection, you can represent its monetary value. By tokenizing this plushie, you create a digital record of its ownership on a blockchain. This digital token is a unique identifier that proves the plushie is yours. Now, you can transfer, trade, sell, or even borrow against the value recorded on the blockchain, just like the physical item.
Tokenization is the process of converting ownership into a digital store of value, represented by a token on a blockchain. These tokens can represent physical or digital assets, from real estate properties to digital art. By embedding the value and ownership of these assets into a secure and immutable blockchain record, tokenization transforms how we manage and exchange assets, while providing us with new ways to unlock the value that resides within them.
It was best said in a space hosted by “Crypto Megan”, or Megan Nilsson, ""Tokenization is revolutionizing the way we interact with assets, offering unprecedented accessibility, liquidity, and security. It's not just the future of finance; it's the future of how we value and exchange everything from real estate to art, and even space exploration.""
Tokenization turns physical and digital assets into blockchain-based tokens, moving them on-chain and providing a precise and tamper-proof digital footprint. This secure record ensures that ownership details can’t be altered, preventing fraud and building trust. ""On-chain"", a term that goes hand-in-hand with tokenization, refers to activities or transactions recorded on a blockchain.
Transparency is another key benefit, as every transaction is meticulously recorded, creating a clear, traceable asset history. It’s exactly the sort of record-keeping you see in the highest echelons of business and financial services – except with tokenization, this same level of robustness, reliability, and ‘certitude of ownership’ is available to everyone and, potentially, everything.
Another reason tokens represent a genuine revolution in financial affairs is their sheer efficiency. Tokenization reduces the need for intermediaries, speeding up transactions and cutting costs. It also democratizes investment opportunities by breaking down traditionally illiquid assets into smaller, tradable tokens, making high-value assets accessible to more people.
One of my favorite illustrations of the power of tokenization is to imagine a billionaire’s wine cellar, full of bottles so rare and expensive that few of us could afford even a single one. If you tokenize that cellar, you’re not splitting it into individual bottles, but instead, you can sell ‘shares’ of its whole value. You could even own a share of a bottle of Chateau Margeaux ‘53 – without even drawing the cork.
There’s much more, of course. Tokenization supports smart contracts, which add functionality by automatically enforcing agreements and executing transactions when conditions are met, enhancing operational efficiency and reducing errors. As innovators delve into the allure of tokenization, more benefits and utilities will present themselves in the years ahead. However, the transformative implications of tokenization are already being felt today.
Tokenization is a radical shift from traditional business practices that rely on manual processes, intermediaries, and physical records. In real estate, tokenizing property deeds simplifies transactions and reduces the need for intermediaries. In the art world, tokenization provides a tamper-proof certificate of authenticity and ownership, making art sales simpler, more secure, and accessible to all – not just millionaire collections or heavily-endowed galleries. Tokenization also allows for fractional ownership, making high-value assets accessible to more investors by breaking them into affordable pieces.
Not quite. Tokenization can benefit anyone, regardless of whether they’re interested in investment opportunities or not. Everyone gains from tokenization - it drives significant economic benefits by enhancing operations, reducing costs, and boosting productivity. In real estate, tokenization makes transactions faster and more affordable.
Propy, Inc. is a Palo Alto-based company that uses blockchain technology and AI to streamline real estate transactions. Their platform integrates all parties and documents into one place, and aims to provide self-driving transactions for buyers, sellers, brokers, title agents, and notaries.  The CEO of Propy Inc, Natalia Karayanevarecently said in a twitter space that “Tokenization isn't just about value; it's about new utility like airdrops, rewards, and discounts!. Tokenization transforms ownership, integrating it with community activities. “
In finance, tokenization unlocks liquidity for traditionally illiquid assets, creating new investment opportunities. In fashion, tokenization ensures the authenticity of luxury goods, protecting consumers from counterfeits. In digital identity, tokenization securely stores and manages credentials, simplifying verification processes. Tokenization changes how we understand ownership so that even a Pudgy Penguin is more than just a toy.
Just like ""online"", which, believe it or not, used to sound exotic and mysterious, ""onchain"" is rapidly becoming a commonplace term, and is well on the way to entering our lexicon. You’ll be hearing it a lot more in the years to come, even if you don’t read the financial pages.
In fact, announced recently was an interesting development.  INX and Backed have announced a pioneering partnership to launch on-chain tokenized NVIDIASPDR Dow Jones Industrial Average ETF Trust stock trading for eligible non-US users. This initiative introduces bNVDA, a tokenized security backed one-to-one by NVIDIA Corp stock, issued on the EthereumEthereum network. The blockchain ledger will convey ownership, combining traditional financial assets with the liquidity and accessibility of digital assets. This partnership aims to democratize access to financial assets, allowing non-US investors to trade bNVDA with the speed and ease of cryptocurrencies, even outside traditional stock market hours. This launch marks the beginning, with plans to include ETFs, bonds, and commodities in the future.
Yes, tokenization is already reshaping how we interact with and manage various assets across different sectors. NASA and other space agencies are exploring tokenization to manage space debris, assigning each piece a unique digital token on the blockchain. This system records the ownership and responsible party for removal, potentially transforming space debris into valuable, tradable assets and creating a new market for recycled materials.
Similarly, in Japan's thriving secondary market for luxury baby clothes, tokenization is used to ensure authenticity. Startups in Japan are starting assign a digital token to pieces of Burberry baby clothing, creating a verifiable record of authenticity and ownership on the blockchain. This digital proof protects buyers from counterfeits and maintains the value of genuine products.
Tokenization also plays a significant role in validating sustainability claims in retail. For instance, a jacket marketed as sustainable can have its entire lifecycle tracked on the blockchain, capturing every detail from raw material sourcing to final delivery. This transparency builds trust with consumers by providing verifiable information about the product's adherence to ethical standards.
Additionally, event ticketing is enhanced through tokenization, as demonstrated by the NFL's implementation of tokenized tickets for the Super Bowl. Each ticket, represented by a digital token on the blockchain, served as proof of attendance, drastically reducing fraud and providing a verifiable record of participation.
I envisage tokenization integrating more deeply with emerging technologies like ARArweave, VR, and IoT, creating immersive experiences. Digital twins of tokenized assets will provide real-time insights, enhancing their value. AI will play a crucial role in this integration, leveraging machine learning algorithms to analyze data from digital twins and predict future trends. AI-driven analysis will optimize asset management and decision-making processes.
As Yat SuiSui, c0-founder of Animoca Brands, recently told me in a Twitter Space, ‘Tokenizing opens up so many more prospects beyond the value aspect, the utility and the services that come from tokenizing open up a myraid of opportunities and businesses.”
Tokenization may also become more autonomous, with AI-enhanced smart contracts automating complex transactions and adaptive responses, revolutionizing industries like finance and supply chain management.
Tokenization reshapes how we view and interact with assets, offering unprecedented opportunities for security, efficiency, and innovation. As you explore the potential of tokenization for your brand, consider how it can enhance your offerings, streamline operations, and open new markets.
By embracing tokenization, brands can position themselves at the cutting edge of the digital economy, ready to leverage the full potential of this revolutionary technology. So, are you ready to dive into the world of tokenization and explore its endless possibilities? Let's get on-chain and discover a whole new way of doing business!"
256,https://www.forbes.com/sites/dereksaul/2024/07/04/nvidia-and-these-surprise-ai-darlings-are-2024s-best-stocks/,Nvidia And These Surprise AI Darlings Are 2024’s Best Stocks,"Jul 04, 2024, 04:15am EDT",Derek Saul,"Belle of the ball Nvidia was the S&P 500’s best-performing stock during the first half of 2024, but there were several perhaps less predictable winners as the S&P rose to a record high, including a sector typically viewed as an underperformer during macroeconomic periods similar to today’s.
Nvidia’s 150% return since the end of last year is the best of the roughly 490 S&P components listed on the benchmark American equity index for the entirety of 2024 (data storage leader Super Micro Computer, General Electric spinoff GE Vernova and private equity giant KKR were among the about 10 midyear additions).
Growing its market value from $1.2 trillion to over $3 trillion, Nvidia rode its 600% annual profit growth and the potential to further capitalize on its graphics processing units (GPUs) which power mush of the generative AI revolution, repeating its success as the S&P’s biggest first-half and full-year winner in 2023.
Super Micro is the S&P’s top year-to-date returner among today’s constituents at 188%, but all of its gains came before it joined the index March 18, as its stock is down more than 15% since its inclusion.
After Super Micro and Nvidia, the S&P’s top 10 also include fellow AI tech names Micron (No. 7, 55% return) and CrowdStrike (No. 9, 51% return), and GLP-1 weight loss drug heavyweight Eli Lilly (No. 5, 57% return).
It’s also been a good year for a trio of lesser-known power providers: Vistra (No. 3, 119% return), Constellation Energy (No. 4, 73% return) and NRG Energy (No. 8, 52% return).
Constellation, NRG and Vistra are all classified as utilities, providing remarkable returns in a sector which conventional wisdom pegs as a poor bet during high interest-rate environments due to utility companies’ reliance on debt financing to fund their capital-draining operations.
Management at each of Constellation, NRG, and Vistra alluded to demand for their services among power-hungry AI data centers as a boon for their businesses, showcasing the offshoot effects of the AI boom starting to seep into the stock market.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.


The S&P’s utilities sector is the third-best performer this year of the index’s 13 sectors, returning nearly 11%. That trails only the information technology sector, which includes Nvidia and Super Micro, and the communication services sector, which covers stocks like Netflix and Meta, both groups comprised of the stocks most heavily exposed to AI.
Utilities’ renaissance comes after it was by far the biggest loser of 2023, losing 10.6% as the S&P gained 26%. Interest rates, which are at their highest level in the U.S. since 2001, dampened earnings across the sector, with titans like NextEra Energy’s first-quarter earnings before interest, taxes, depreciation and amortization (EBITDA) down some 30% compared to 2022’s first quarter. Higher rates also lure investors away from the high dividend payouts often offered by utility companies, as higher-yielding government bonds offer a higher sense of security. The stock market broadly has defied conventional wisdom that higher interest rates bring lagging returns, as the S&P, tech-heavy Nasdaq and Dow Jones Industrial Average all notched all-time highs in 2024.
The Wells Fargo Investment Institute downgraded its rating on the utilities sector to unfavorable in its midyear outlook published in June. Sameer Samana, the Wells Fargo wealth management division’s senior global market strategist, explained to Forbes that the utilities sector is “toward the bottom” of industries that would benefit from the second-order effects of the AI boom. It would take several years for utility companies to actually provide more power to account for any new demand tied to AI applications, Samana added, concluding there are ”more direct and more immediate ways to play” AI."
257,https://www.forbes.com/sites/digital-assets/2024/06/28/the-future-of-ai-in-schools-and-small-businesses-through-nvidia-aws-and-universities/,"The Future Of AI In Schools, And Small Businesses Through Nvidia, AWS, And Universities","Jun 28, 2024, 07:00am EDT",Sandy Carter,"I’m going to tell you the biggest problem with Artificial intelligence. It has nothing to do with the tech, which becomes more unbelievably capable with each passing month. No – for me, the real problem is that so many people can’t see the point of AI; they can’t understand how it benefits their lives.
In a way, this image problem is hardly surprising. For all the buzz about AI, only about 7% of organizations are leveraging it to its full potential. This statistic underscores a critical challenge - how do we prepare the next generation for an AI-dominated world?
This obstacle is amplified when so many people are indifferent at best, and at times outright skeptical about its value. Moreover, how do we make AI accessible and applicable to everyone, from small business owners to college students?
That’s why I believe one of the biggest challenges for the AI industry is to address the significant gap between its potential and its current application in everyone’s lives. And what better place to start than in our local communities and schools?
When I think about the world our young people will inherit, it’s clear AI will play a pivotal role. In 10 to 15 years, the students of today will be the professionals and innovators of tomorrow. It’s like children of the 1990s who watched the Internet transform as a curiosity and a plaything into something ubiquitous and truly transformative; something they couldn’t imagine living without.
Like those guinea pigs of the early Web, the next generation will soon live and work in a world where AI is integrated into almost every aspect of life and business. Our job now is to ensure that they do not become passive consumers of AI technology, and instead become active and informed participants who care about its development and applications.
I believe the first step is integrating AI education into our school curricula. This means more than just teaching students how to use AI tools; we must help them understand the underlying principles and ethical considerations, too. AI should be taught as a ‘thought partner’ and strategist.
For example, Nvidia has been making significant strides in advancing AI education by collaborating with high schools and colleges. Through programs such as the Nvidia Deep Learning Institute (DLI), students gain access to cutting-edge resources and hands-on training in AI, machine learning, and GPU-accelerated computing. Nvidia provides educational institutions with powerful hardware, such as GPUs, and comprehensive curriculum materials to integrate into their courses. This partnership not only equips students with critical skills needed for the future tech landscape but also fosters innovation and research. By engaging directly with educational institutions, Nvidia helps to bridge the gap between academic learning and real-world applications, preparing the next generation of tech leaders and innovators.
To get the most out of AI, one must learn to treat it as a tool, not a guru. As we’ve seen when AI is left to its own devices, it needs human guidance to operate effectively and safely. This requires clear context, asking the right questions, and iterating based on AI’s responses.
As Dr Craig Hansen, Founder of Summit Institute & New Zealand's leading voice for AI in education, told me recently, “AI provides the biggest equity opportunity this decade for disadvantaged communities and learners. It is critical that educators are supported to be both confident and competent in the skills, ethics, privacy and security of AI use. This will pave the way for students to safely use AI to accelerate their learning through accessing personalized learning, and building their own AI applications. If schools ignore these opportunities, the digital divide will become too big to close and the opportunity will be lost due to the exponential pace of change AI is bringing.”
A practical example from my experience involves using AI to uncover brand insights based on customer reviews. This process involved my team and I carefully crafting prompts, analyzing responses, and iterating until we achieved valuable insights.
Like anything worthwhile, success at AI doesn’t happen by accident: it requires hard work. Exercises like the one I’ve described demonstrate AI’s capabilities and highlight the critical importance of teaching the skills needed to harness it effectively.
In the educational sphere, high schools are beginning to embrace AI as a critical component of their curricula. Take, for instance, Thomas Jefferson High School for Science and Technology (TJHSST), located in Alexandria, Virginia. This school is renowned for its focus on STEM education and is consistently ranked as one of the top high schools in the United States. The school has integrated AI into its science and technology courses, allowing students to work on projects that utilize machine learning algorithms to analyze environmental data or develop predictive models for various scientific phenomena.
One notable project at TJHSST involved students using AI to monitor and predict water quality in local streams, combining data analysis with hands-on environmental science. This project provided practical experience with AI and underscored the importance of environmental stewardship, making it a compelling example of how AI can be applied to real-world problems.
And big tech is trying to help.  For example, the AWS Machine Learning University educator enablement program provides faculty at community colleges and universities with the necessary skills and resources to teach Artificial Intelligence and Machine Learning (AI/ML).
By engaging students in real-world applications of AI, schools are preparing them for future careers while delivering a deeper understanding of the technology that will shape their lives. Above all, it’s a fun and engaging way of showing how to apply the analytical capabilities of AI to practical fieldwork, and how it results in better insight into problems, and more effective solutions. This forward-thinking education is crucial for developing the next generation of leaders equipped to navigate and innovate in an AI-driven world.
In talking with Margo Bastow, Director of Community Relations, Santa Maria College, she told me “As educators, we recognize that preparing students for the future job market is a collaborative effort that extends beyond the classroom. Parents play a crucial role in supporting their children’s skill development and guiding their career aspirations. In the era of AI, it is essential for parents to take the time to understand and embrace this transformative technology. By familiarizing themselves with AI and its potential impact on various industries, parents can better equip their children with the skills that will be most valuable in the coming years.”
Moreover, we encourage parents to actively engage in conversations with their children about the exciting opportunities that exist in the field of technology and to inspire them to consider pursuing careers in AI and related disciplines. Together, as a community, we can create a supportive environment that nurtures our students’ passions and prepares them for a future where AI will undoubtedly play a significant role.”
While high schools are laying the groundwork for AI education, colleges are taking it a step further by integrating AI into their advanced curricula and research projects.
A standout example is Texas State University's McCoy College of Business Administration. Their mission is to demystify AI and make it accessible and practical for students, especially aspiring business leaders.
One of the innovative initiatives at McCoy is the development of AI-driven decision trees for sales development representatives (SDRs). This tool helps small businesses streamline their sales processes by using AI to predict and guide customer interactions. It's a tangible example of how AI can be integrated into everyday business operations, providing immediate, practical benefits. By applying AI to solve real-world challenges, students gain valuable insights and hands-on experience that will serve them well in their careers.
The college also emphasizes a hands-on approach to AI and data engineering. Students are encouraged to engage in projects that require them to build new systems and products using AI technologies. This approach thus equips students with technical skills while developing key critical thinking and problem-solving abilities. They quickly learn to navigate the complexities of AI, from understanding algorithms to addressing ethical considerations in AI applications.
Tim Hayden is the CEO at BrainTrust and also sits on the McCoy Task Force 4.0 which aims to guide AI initiatives. At BrainTrust, he also hires data engineer interns and entry-level leadership from Texas State due to their “global” cross-discpline efforts to be ready for AI .
In speaking with Tim, he told me that “It is exciting to see McCoy acutely applying AI to course work and projects. This ensures students are future-ready for a brave new world beyond the classroom. When you consider how generative and applied machine intelligence may reshape marketing and customer experiences, and larger business, logistics, and product development initiatives, having a holistic respect for iterative change and optimization is critical.”
McCoy College of Business Administration is an example of how educational institutions can play a pivotal role in preparing the next generation for a future where AI is ubiquitous. Integrating AI into their curricula with a hands-on learning environment is proving to equip students with the skills and knowledge needed to thrive in an AI-driven world.
One of AI’s most persistent image problems is that it’s solely for large enterprises, being built and used behind the plate glass windows of multinational megacorps. The best way to make people realize that the benefits apply equally to everyone is to put it to use at the local level, in our communities.
What better place to start than in small businesses?
Integrating AI can seem daunting for small businesses that think they lack the skills or financial resources to implement it effectively. The reality is that AI technology has become increasingly accessible, offering straightforward, manageable entry points for small businesses to test its potential. I’ve seen from experience that most small businesses can explore AI through easy trials and simple implementations.
Starting with AI-powered tools that are already in use can be an effective first step. Many small businesses might not realize they already have access to AI features in their everyday tools. For example, email marketing platforms like Mailchimp and Constant Contact use AI to suggest the best times to send emails and provide guides on how to personalize content for different audience segments.
Customer Relationship Management (CRM) tools like HubSpot and Salesforce provide AI-driven insights that help businesses manage customer relationships and forecast sales. By exploring and utilizing these built-in AI features, small businesses can begin to see the benefits of AI without any additional investment or learning curve.
Creating engaging content is crucial for marketing, and AI can simplify this process. AI-based content creation tools like Copy.ai and Jasper can generate marketing copy, social media posts, and blog articles based on simple prompts. Graphic design tools like Canva and Adobe Spark use AI to help businesses create professional-quality graphics and videos with minimal effort.
For example, Augie is an AI video editing assistant that transforms text scripts, narration audio, or webcam/phone recordings into custom videos, enabling small businesses to create professional-looking videos in minutes without any editing skills required.
As Jeremy Toeman, CEO and Founder at Augie Labs, said “We are living in a video-first marketing world, yet most businesses have neither the skills nor budgets to participate. Augie was created to bridge this gap. Our platform empowers small businesses to effortlessly create professional, engaging videos, leveling the playing field and allowing them to compete and thrive without needing specialized skills or substantial budgets.”
By exploring these AI tools and approaches, small businesses can begin to harness the power of AI to enhance their operations, improve customer service, and create high-quality content, all while staying competitive in a rapidly developing digital landscape.
Many small businesses are looking for certifications to help them learn AI.  Tonya Colbert, the University of San Francisco’s Program Manager at the Zschool for the Strategic AI Programs, says, “One of the most significant challenges in the AI space is the speed at which technology evolves. Staying current requires a willingness to learn and adapt continuously. This is where programs like the Strategic AI Program at the University of San Francisco’s School of Management come into play. These programs provide certifications that are essential for staying ahead in the AI field.”
“I’ve found that being part of such programs keeps you updated with the latest developments and provides a framework for understanding how to implement these advancements practically. For instance, analyzing competitors and positioning with AI tools is not just about the technology itself but also how you apply it strategically to gain a competitive edge.”
In the years to come, AI will play a central role in our lives. Knowing this, we must make AI work for us, rather than the other way around. We can start by integrating AI education into our schools and making it accessible to everyone in our communities. By paying attention to AI principles and practical applications, we can ensure that the next generation is not just prepared for an AI-driven world but is actively shaping it.
In my own work, I’ve seen firsthand the transformative potential of AI. From uncovering brand insights to developing AI-driven decision trees, the possibilities are endless. But to fully realize these possibilities, we need to embrace a mindset of continuous learning and adaptability. Whether through formal education programs or hands-on experience, the key is to keep learning, iterating, and refining our approach.
AI is not just a tool; it’s a partner in our journey towards a more innovative and efficient future. By embracing this partnership, we can unlock new opportunities and drive meaningful change in our communities and beyond."
258,https://www.forbes.com/sites/petercohan/2024/06/28/nvidia-stock-is-up-150-and-3-key-advantages-could-keep-it-rising/,Nvidia Stock Is Up 150% And 3 Key Advantages Could Keep It Rising,"Jun 28, 2024, 01:59pm EDT",Peter Cohan,"Nvidia stock shares rose 150% in the first half of 2024 — outpacing the S&P 500’s 15% increase by a factor of 10, according to the Wall Street Journal.
Can Nvidia stock keep rising? The answer depends on whether the AI chip designer can keep beating investors’ high expectations and forecast faster-than-anticipated growth.
To accomplish that, overall demand for AI chips must remain torrid and Nvidia must maintain or grow its dominant market share. The balance in favor of both trends persisting is strong. Here is why:
Although companies are still seeking high returns on their investment in generative AI, analysts expect companies’ urge to build AI chatbots to drive rapid growth in demand for GPUs for the next several years.
Despite challenges from rivals, according to CNBC, Nvidia’s market share is growing even larger due to three competitive advantages:


The S&P 500 has rewarded investors in companies that have capitalized on generative AI through unexpectedly high revenue growth. For instance, Nvidia — which enjoyed 262% revenue growth and a 57.1% net margin in the April 2024-ending quarter — accounted for 30% of the S&P 500’s total return in the first half, noted the Journal.
Those revenues are coming from a huge market for AI chips. The business is expected to double from $200 billion in 2023, according to Grand View Research, to “$400 billion per year by 2027,” according to The Economist.
By including Alphabet, Microsoft, Meta Platforms and Amazon, these five AI beneficiaries, accounted for “well over half of the broad U.S. stock index’s return,” the Journal wrote.
“Clearly, artificial intelligence has been a big boost to a number of the larger tech companies,” Holly MacDonald, chief investment officer at Bessemer Trust, told the Journal. “What’s been occurring there is not just buzz around AI, but we’re actually seeing it affecting results.”
The other four companies are benefiting from their positions in the generative AI value network, which I describe in my new book, Brain Rush. Microsoft beat analysts’ sales expectations — as AI drove demand for the company’s software and cloud services, the Journal reported.
Amazon’s market capitalization topped $2 trillion after CEO Andy Jassy focused the company on AI innovations; Meta launched AI tools for advertisers; and AI is powering Google’s search engine responses, noted the Journal.
Many investors see the promise of a transformative technology that could fuel leaps in productivity and growth. “It’s early innings, we think, of a multiyear secular bull phase in AI,” Mona Mahajan, senior investment strategist at Edward Jones, told the Journal.
Nevertheless, companies buying the cloud services and software to build AI chatbots are looking for the high payoff uses of the technology. Based on my interviews with dozens of business leaders, generative AI in companies is caught in a bipolar battle.
Peer pressure can force CEOs to tell Wall Street how generative AI will transform their business. That pressure is reflected in a record level of mentions of the term AI in investor conference calls.
At the same time, CEOs may fear that generative AI hallucinations could threaten their company’s reputation.
This inconsistent battle has significant implications for business. Of 200 to 300 generative AI experiments the typical large company is undertaking, a mere 10 to 15 have been rolled out internally, and perhaps one or two have been released to customers.
That’s according to Liran Hason, CEO of Aporia, a startup offering guardrails to protect companies from AI hallucinations who spoke with me in a June 3 interview.
Despite the risk companies may struggle to earn high payoffs from their generative AI-powered chatbots, I think Mahajan’s “early innings” comment is right.
Nvidia faces competition from large rivals including AMD and Intel; a variety of startups and operators of data centers — notably Amazon and Meta — who are trying to develop their own GPUs, according to the New York Times.
Despite the competition, Nvidia’s share of the GPU market has increased substantially in the most recent quarter. In order to win share from Nvidia, rivals must overcome three of the company’s powerful competitive advantages.
Nvidia is aware of its rivals and expressed confidence in its ability to maintain its market position. At a June 26 shareholder meeting, Nvidia CEO Jensen Huang responded to a question about the competition by outlining how a decade ago, the company made investments to change focus from gaming to data centers, reported CNBC.
Nvidia is also investing in applications for other markets. For example, the company is partnering “with every computer maker and cloud provider” to build market share in industrial robotics, CNBC wrote.
The wide availability of Nvidia’s platforms  — through every cloud provider and computer maker — sustains a flywheel that leads to greater market share. How so? The company’s large installed base creates an attractive market opportunity for developers to “make the improvements needed to attract even more users,” Huang said.
Nvidia’s share of the GPU market increased considerably in the first quarter of 2024.
Specifically, Nvidia’s GPU market share rose from 80% in the fourth quarter of 2023 to 88% in Q1 2024, according to a Jon Peddie Research report featured by Techradar. Meanwhile, AMD’s share of the GPU market fell from 19% to 12% while Intel’s share declined from 1% to “negligible,” noted Techradar.
Companies win or lose market share based on how well they perform on the criteria customers use to choose among alternative suppliers. Delivering what customers perceive as more value, enables the winning supplier to charge a higher price.
Such a differentiation strategy contributes to Nvidia’s high profitability. Due to their better performance and lower cost to run, Huang said the company’s AI chips more than offset their higher price by providing customers the “lowest total cost of ownership,” CNBC reported.
For an objective perspective, I interviewed an industry expert who explained how Nvidia prevails over rivals. GPU buyers evaluate vendors based on price, overall cost, product quality, software, economic payoff, and other criteria, according to a June 27 email from researcher Jon Peddie.
GPU purchase criteria include “all the above, as well as size of the vendor,” Peddie wrote. “Big companies like dealing with big companies that have staying power. Software compatibility is a very big decision point. Software is fragile and tricky to get right, and the old saying — if it ain’t broke – don’t fix it, applies,” he added.
Nvidia is way ahead of its rivals when it comes to satisfying customers’ purchase criteria for GPUs. Here are the three sources of competitive advantage that enable Nvidia to prevail on these CPC:


If demand for AI continues to grow and rivals cannot match Nvidia’s competitive advantages, the company’s stock is likely to continue its rapid rise."
259,https://www.forbes.com/sites/antoniopequenoiv/2024/06/25/nvidia-jumps-back-above-3-trillion-valuation-as-stock-gains-following-massive-selloff/,Nvidia Jumps Back Above $3 Trillion Valuation As Stock Gains Following Massive Selloff,"Jun 25, 2024, 04:29pm EDT",Antonio Pequeño IV,"Nvidia’s stock slightly recovered Tuesday following a massive three-day selloff, closing up more than 6% and resecuring a more than $3 trillion market cap following the slide that ended its brief standing as the world’s most valuable public company.
Nvidia’s stock closed up more than 6% at $126.09, recovering from a Monday share dip that brought its price down to $118.11.
Nvidia’s market cap reached $3.1 trillion after closing at $2.9 trillion Monday—the only day it traded below $3 trillion since June 11.
Nvidia shares still have a ways to go before returning to its pre-selloff price ($135.58), which briefly made the chipmaker the most valuable company in the world, ahead of Microsoft and Apple.
The chipmaker’s market cap still trails behind Apple ($3.2 trillion) and Microsoft ($3.3 trillion), shares of which closed up a fraction of a percent Tuesday.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
Nvidia’s recent drop in share price did not have a clear catalyst, though it did materialize after a wildly strong year-to-date rally. Despite the decline, Nvidia shares are up more than 150% since the start of the year, when they traded for around $48. The company’s stock traded at $1,210 per share before a 10-for-1 stock split brought it down to $120 per share. Nvidia’s rise has been fueled by an artificial intelligence craze that has triggered wild demand for the company’s chips and graphics processing units, namely from fellow juggernaut tech companies like Microsoft, Meta and Amazon.
Nvidia Stock Sinks Another 5%—Market Cap Down $500 Billion From Last Week’s Peak (Forbes)
Nvidia Stock Endures $277 Billion Midday Selloff As AI Giant Falls Behind Microsoft (Forbes)"
260,https://www.forbes.com/sites/danirvine/2024/06/25/nvidias-explosive-growth-mirrors-tech-giants-of-the-past/,NVIDIA's Explosive Growth Mirrors Tech Giants Of The Past,"Jun 25, 2024, 02:50pm EDT",Dan Irvine,"In recent years, artificial intelligence has moved from the realm of imagination to possibility and is poised to be one of the most significant innovations over the coming half-century. While the internet will likely remain the most critical innovation for generations, AI is still, in all probability, a game changer. Like the early days of the internet, the road to maturing technology and best use cases will be longer and bumpier than many investors may believe today.
NVIDIA has been the perfectly positioned company powering AI computing and has been picked by the investment world as the primary beneficiary of a new world controlled by computers. In just two years, NVIDIA’s market cap has grown massively, putting the company among the most valuable companies in the world, including Microsoft and Apple. At first, NVIDIA was adding a trillion dollars in market capitalization over months, but the last trillion of growth came in just a matter of weeks.
In the early days of the internet, Sun Microsystems and Cisco exploded in value and size as the internet entered the mass consciousness of the population. These companies' technologies and products, like NVIDIA today, were literally providing the technological plumbing of the internet, as NVIDIA drives AI infrastructure today. Investors' cash is piling into the darlings of this developing technology.
The internet and AI revolutions share so much in common; the parallels are clear if you have studied market history or, better yet, were investing through the 1990s and early 2000s. Over the last 100-plus years, there have been many game-changing innovations, and with each one, distinct phases to the investment cycle. AI seems to be following the same familiar investing cycle of all major innovations of the past.
Phase 1: Early investors see potential for a new technology and invest in companies most likely to benefit. These investors know that years, potentially a decade, may pass before their investment thesis comes to fruition. These investors are insiders or investors with a very long-term outlook. Investors at this early phase will not become sellers based on price; they hold until the investment thesis is either realized or they are certain they were wrong.
Phase 2: The technological potential Phase 1 investors identified early make significant progress, drawing in additional sophisticated investors who see imminent potential and begin taking positions in companies that are beneficiaries of the new technology. In this phase, the technology has been developed but is not widely used or available yet. This phase also introduces significant volatility in the stock price as there are always substantial delays, unforeseen challenges, and higher costs. Often, issues outside the control of operators of companies working to bring innovation forward include insufficient supply chains for scaling, a regulatory environment, or inadequate public and private infrastructure. Challenges in Phase 2 usually can only be resolved with time and significant investment.
Phase 3: Recognition of the masses happens when the public widely recognizes the importance of the new technology, accelerating development and adoption and attracting a large-scale inflow of retail investors into companies associated with the new technology, resulting in exploding market caps. This is the perfect phase to follow the trend and participate in what can be multiple years of explosive share price growth.
Phase 4:  The transition from a new technology with limited adoption and use cases to mature companies with business models built around profiting from and delivering the now widely used technology. This phase is usually a tumultuous period for the companies that have been the leaders in the new technology, particularly early in the transition from Phase 3. Early Phase 4 is where crashes in high-flying companies usually happen. New competition and slowing growth usually lead investors to exit the original darlings because of increasingly unrealistic valuations and diversify out to product and service providers, building rapidly growing businesses using the new technology. This can result in share price losses for the high fliers of 50% to more than 90% over a very short period of time. Phase 4 can last years to many decades. It could be argued that the introduction of the internet is still in Phase 4 as new use cases and successful companies providing and relying on the internet adjacent products and services continue to sprout up regularly.
Looking at these often-repeated phases of the introduction of important innovation into the world, AI seems to be following the script. The script says we might be near the end of Phase 3, and NVIDIA and numerous names providing AI-related products and services have exploded in value, but eventually, investors will adjust expectations down, followed by their share prices.
Cisco Systems' share price went parabolic in the late 1990s, like NVIDIA's over the last year and a half, as AI entered the mainstream lexicon. However, the extent of NVIDIA’s price increase makes Cisco Systems look like the minor leagues in comparison.
Furthermore, specific AI use cases and companies dedicated to delivering on those for niche segments of consumers are just beginning to get started in AI technologies. AI is still very early in development. The technology will almost certainly prove invaluable to most businesses in the future, but this will only be possible with specialized AI tools specific to their businesses. So, the maturation of the technology seen during Phase 4 is still a long way off.
AI likely continues to present one of the best investment opportunities in the market for long-term holders. Still, the fate of the early beneficiaries like NVIDIA is far from certain. Cisco Systems is below its early 2000 high twenty-four years after dropping around 90% during the dot com crash as the internet moved from Phase 3 to Phase 4. Sun Microsystems crashed and no longer exists as a company, an outcome thought unthinkable by investors in the late 1990s. There could just as easily be an AI crash, and investors should plan for this potential and make sure to realize profits when the getting is good in these market manias and begin to look for companies innovating to serve specific industries with AI technologies as well as businesses that are improving or adding to the infrastructure and supply chains that will be needed as AI technology matures and adoption becomes more widespread.
We live in exciting times, and we are just scratching the surface of how AI technology will change our lives. From an investing perspective, there are many opportunities, but investors should use market history as a guide to set expectations and find the best opportunities where AI may be in the innovation cycle."
261,https://www.forbes.com/sites/dereksaul/2024/06/24/nvidia-stock-sinks-another-5-market-cap-down-500-billion-from-last-weeks-peak/,Nvidia Stock Sinks Another 5%—Market Cap Down $500 Billion From Last Week’s Peak,"Jun 24, 2024, 11:40am EDT",Derek Saul,"Nvidia stock’s historic rally was doused further Monday, as shares of the artificial intelligence technology dominator are mired in one of their worst stretches in years, cutting into their still impressive gains.
Nvidia’s stock fell more than 5% to about $120 by late morning Monday, hitting its lowest intraday share price since June 10, the day of its 10-for-1 stock split.
The selloff follows Thursday’s and Friday’s respective 3.5% and 3.2% drops, extending its losses from Tuesday’s record close to 12%.
Nvidia’s market capitalization is down an astonishing $400 billion during that stretch, which did not coincide with any major negative catalysts but rather was likely a result of a centibillion-dollar breather.
From Thursday’s intraday peak, Nvidia’s market value is down more than $500 billion, more than the total valuation of Exxon Mobil, the S&P 500’s 14th-largest company by market cap.
Should Monday’s losses hold, it will be the first time that Nvidia shares fell 3% or more in three consecutive trading sessions since June 2022, when the stock traded below $20 per share.
Nvidia’s market value fell by more than$150 billion to $2.9 trillion Monday, on pace to close below $3 trillion for the first time since June 11, but remains the world’s third-largest company, behind only Apple and Microsoft.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
Monday was largely a good day for the broader market despite Nvidia’s slip, as the median S&P stock rose 1% and the Dow Jones Industrial Average gained 380 points (1%) to a four-week high. Still, the S&P only rose 0.3% Monday and the tech-heavy Nasdaq sank 0.2%, evidence of Nvidia’s significant impact on the market cap-weighted S&P and Nasdaq. Nvidia is not one of the 30 stocks comprising the Dow, which largely caused the index to underperform its peers over the last 18 months.
Even after the recent slump, Nvidia has still delivered a robust 140% return to investors this year and a remarkable 1,000% return from its Oct. 2022 bottom. Nvidia’s rise to prominence coincided with Wall Street’s focus on generative AI, as Nvidia architects much of the Silicon technology powering AI applications. The company reported a 628% year-over-year increase in earnings for its quarter ending April 30 as its earnings power took hold, though it likely needs to rapidly grow its financials even further to justify its ballooning valuation.
Jensen Huang, Nvidia’s chief executive since its inception, became more than $5 billion poorer Monday due to his company’s stock market tumble, according to Forbes’ calculations. Huang is about $15 billion less rich than he was at Tuesday’s market close, though he remains the 15th-richest person on the planet with a net worth of over $105 billion.
"
262,https://www.forbes.com/sites/garthfriesen/2024/06/23/is-nvidia-stock-in-a-bubble-or-justified-by-ai-growth/,Is Nvidia Stock In A Bubble Or Is It Justified By AI Growth?,"Jun 23, 2024, 04:06pm EDT",Garth Friesen,"Nvidia shares dropped nearly 7% over a two-day span, sparking concerns that the meteoric rise of the artificial intelligence market leader's stock price may have peaked. Nvidia briefly surpassed Microsoft and Apple midweek to become the world's most valuable company but slipped back to third place by Friday's close. Investors are now debating whether to take profits on NVDA and the broader AI sector or stick with the theme that has driven most of this year's S&P 500 gains.
First, the recent 7% dip should be taken into context. Nvidia shares are still up 156% year to date, and last week's dip is relatively minor and barely shows up on a long-term chart. Investors should expect dramatic and painful drawdowns from companies that have moved so far so fast. Nvidia is no exception. Its shares fell by 66% at one point in 2022 and have had four drawdowns of approximately 15% or more in the last year alone.
Still, several concerns are causing investors to reevaluate their faith in the Nvidia rally. Gaining the status as the world's most valuable company when the overall economy appears to be losing momentum is one obvious worry. The extreme domination of large-cap technology stocks at the index level is another. Also, the significant outperformance of AI-related stocks in the last month relative to the rest of the market has once again prompted talk of bubble-like price action. These concerns provide an opportunity to pause and look at Nvidia from a larger perspective. Is a bubble forming, or is the share price appreciation warranted?
There are some standard definitions of a stock or stock market bubble, primarily using history is a guide. Typical characteristics are rapid and substantial price increases that outpace reasonable valuations; widespread investor enthusiasm and speculation, often accompanied by a fear of missing out and speculative behavior; and a belief in new paradigms, technologies, or economic conditions that create excitement about future growth prospects.
Nvidia and other AI stocks have rocketed higher in the last 18 months. Nvidia has soared nearly 800% since the start of 2023 and other tech giants like Meta (+297%), Amazon (+120%) and Microsoft (+88%) have done the same. However, the near-parabolic rise is not without fundamental support. Nvidia continues to exceed analysts' expectations of earnings and profit growth and boasts an estimated market share of 80% in specialized chips needed for most AI applications. The other technology leaders are also delivering earnings growth above what many thought possible.
Specifically, Nvidia’s financial results for the first quarter of fiscal year 2025 showed revenue growth of 262% compared to a year ago. Data center growth was even more impressive, climbing 427% from last year. With gross margins of 78%, profits are following suit. Clearly, there is more to Nvidia than pure speculation.
From a valuation perspective, Nvidia is not cheap. According to Bloomberg data, NVDA’s one-year forward price-to-earnings ratio is 47x, compared to 35.9x for the Philadelphia Stock Exchange Semiconductor Index, 29x for the NASDAQ 100 Index, 22.6x for the S&P 500, and 17.8x for the equal-weight S&P 500 Index. Yes, Nvidia is expensive relative to the broader market, but that is to be expected given the rapid growth in revenue and earnings. Investors are willing to pay up for growth, and Nvidia is delivering. Applying traditional valuation metrics to fast-growing companies in relatively new industries is often futile. But are the current multiples too extreme?
By comparison, during the peak of the dot-com bubble in 2000, Nasdaq 100 stocks traded at an absurd trailing P/E of 200, more than six times today's level. Cisco, a poster child of the dot-com era, was valued at a multiple of more than 150 times forward earnings when the stock hit its peak in March 2000. In this context, neither Nvidia nor the broader market are near the valuation levels seen during the internet bubble. However, Nvidia’s recent earnings and sales multiple expansion suggests a disconnect between the share price and performance of the underlying business. AI stocks have spiked, and valuations have stretched, but at least some fundamental support is behind the craze.
If rating its rapid price appreciation bubble on a scale of 1 to 10, the score is likely a seven.
Another potential sign of a bubble is extreme euphoria and speculation surrounding a stock or a theme.
The options market is one place to look for excessive sentiment. Stocks or sectors where call option volatility is trading higher than put option volatility is often a sign that investors expect significant upside from existing levels. Currently, 1-month and 3-month call options have roughly the same level of implied volatility as put options. This is consistent with market pricing over the last year. Call options were more expensive last March around the earnings report. Today, there does not appear to be extreme bullishness in the options market.
Still, everyone is watching the AI boom. Public company mentions of AI in earnings calls are at a record high. Roughly 40% of S&P 500 discussed AI on their earnings calls this past quarter — up from 1% five years ago, according to data from FactSet. Most of these companies cite AI as a potential productivity booster rather than an additional source of revenue, and many are making substantial investments in the technology. There is widespread enthusiasm about AI, but it should not be considered speculation since productivity improvements have already been established.
A potentially worrisome sign would be a rush of new companies tapping the public markets to raise money for anything related to AI, similar to the internet bubble where investors scrambled to buy anything with "".com"" in the name. According to Bloomberg, total initial public offerings are running at about $16 billion in proceeds raised year-to-date versus about $20 billion for all of 2023. In terms of deal count, there have been 88 IPOs through mid-June 2024, compared to 154 in 2023. The record was set in 2021 with 1035 IPOs. If there was an AI startup bubble, it peaked in 2021.
If rating the level of speculation in Nvidia to past bubbles on a scale of 1 to 10, the score is probably a six.
AI is not the first technology to induce predictions of a materially altered future. Crypto was supposed to be the future of finance, and 3-D printing was expected to revolutionize manufacturing. Neither crypto nor 3-D printing has lived up to the initial hype. AI may fall into the same category, but there are reasons to believe it will indeed have a material impact on economics and society.
Big technology firms and dozens of large corporations are making huge investments in the space. Hundreds of billions of dollars are flowing into Nvidia's chips and AI-related infrastructure, such as data centers and the electrical grid. These investments are already paying off in direct AI revenue and enhanced productivity.
According to a recent PwC report, AI is predicted to increase productivity, raise growth domestic product growth, contribute to scientific discovery, and improve the healthcare system. The study predicts AI has the potential to contribute 14%, or $15.7 trillion, to the global economy by 2030. These are lofty predictions and very well may materialize. However, the fact that these studies are becoming more prevalent indicates belief that a new paradigm has formed.
A number of changing circumstances could quickly alter the impact of this new paradigm. Shifts in global trade policy, emerging new technologies and competitors, a markdown in global growth expectations, or geopolitical shocks are just a few examples. AI could have a significant impact on many aspects of society, but it is possible that the return on investment will fail to materialize. Regardless of the outcome, investors are betting on, and talking about, a “new future.”
When comparing the level of belief in a new paradigm to past bubbles on a scale of 1 to 10, the score would likely be an eight.
Nvidia has become larger than the respective stock markets of Germany, France, and the U.K. Nvidia and other companies directly benefiting from AI are now at valuation levels that offer little room for error, and they must continue surpassing expectations and maintaining their market leadership to justify their current price.
Is there a risk of another drawdown similar in magnitude to the ones we have witnessed over the last year? Absolutely. But is there a bubble-bursting crash in all AI-related stocks around the corner? Likely not. According to standard definitions of a bubble, Nvidia does not appear to be in the late stages of a one: valuations are high, but not at extreme levels; enthusiasm about AI is widespread, but not without fundamental support; and AI has already started to deliver on some of the lofty expectations in terms of new applications and overall productivity, validating predictions of a new paradigm."
263,https://www.forbes.com/sites/bill_stone/2024/06/23/nvidia-takes-the-throne-is-this-a-tech-bubble/,NVIDIA Takes The Throne: Is This A Tech Bubble?,"Jun 23, 2024, 07:00am EDT",Bill Stone,"For a moment last week, NVIDIASPDR Dow Jones Industrial Average ETF Trust (NVDA) surpassed Microsoft (MSFT) as the largest company in the world and S&P 500 index. While it slipped from that perch to end the week, it seemed an appropriate time to revisit whether the large technology stocks, notably NVIDIA, are in the throes of an unsustainable bubble.
The Magnificent 7, consisting of Microsoft (MSFT), Meta Platforms (META), Amazon.com (AMZN), Apple (AAPL), NVIDIA (NVDA), Alphabet (GOOGL), and Tesla (TSLA), have grown to be over 32% of the market capitalization of the S&P 500. Of the Magnificent 7, only Tesla falls outside the ten largest companies in the S&P 500. Seven of the ten largest companies in the S&P 500 are technology companies, including Broadcom (AVGO).
NVIDIA is particularly noticeable given the massive increase in its market capitalization, which now exceeds $3.1 trillion, on the back of the artificial intelligence spending wave. Given this move and its recent short-lived elevation to king of the S&P 500, this analysis will pay extra attention to NVIDIA.
For this analysis, a “bubble” will be defined as a stock price that cannot be justified by the fundamental earnings power of a company. Based on the past twelve months of earnings and consensus earnings estimates for the next year, the price-to-earnings ratios for all the Magnificent 7 stocks are above the S&P 500. While this ratio indicates that these companies have a higher valuation, it doesn’t answer whether other factors justify that premium.
Another way to value companies is by their multiple revenues, which is called the price-to-sales ratio. All the Magnificent 7 are valued at price-to-sales ratios above the S&P 500, with NVIDIA sporting an eye-popping over forty times price-to-sales ratio.
Historical annualized growth in sales over the last three and five years has been impressive for most of the group.
Likewise, historical earnings per share growth has generally been impressive and exceptional in cases like NVIDIA.
The five-year annualized earnings per share growth rate of the Magnificent 7 shows just how unique these companies have been versus the stock market as a whole.
Future earnings are the most important criterion when considering buying or holding a stock. Consensus forecasts from Wall Street analysts show significant optimism for NVIDIA’s earnings growth prospects. Separately, the so-called sustainable growth rate for the group supports the future growth story. The sustainable growth rate theoretically tells how fast the company can grow without borrowing additional funds and using the same capital structure.
Again, the profitability metrics of most of the Magnificent 7 are impressive. Gross margins look at profitability after all direct expenses, defined as the cost of goods sold, have been removed from sales. Operating margins measure how much a company earns on a dollar of sales after paying for the variable expenses or the percentage of sales converted into operating income. NVIDIA converts almost sixty cents of every dollar into operating profit and is at the top of the heap among the Magnificent 7.
Warren Buffett has espoused that a simple high price-to-earnings ratio can be deceiving when analyzing a company with high returns on invested capital (ROIC). He said, “We like stocks that generate high returns on invested capital where there is a strong likelihood that it will continue to do so. For example, the last time we bought Coca-Cola, it was selling at about 23 times earnings. Using our purchase price and today’s earnings, that makes it about 5 times earnings. It’s really the interaction of capital employed, the return on capital, and the future capital generated versus the purchase price today.” To put the return on invested capital figures for the Magnificent 7 into proper perspective, the ROIC for the S&P 500 is around 8%.
Further, most of the companies in the Magnificent 7 produce significant amounts of free cash flow despite investing in growing businesses. The free cash flow margin shows the percentage of revenue converted into free cash flow that the company can use or distribute to shareholders. NVIDIA is exceptional, with almost half of every dollar of sales converted to free cash flow.
Charlie Munger also focused on cash flow rather than other valuation metrics when he said, “We try to stick with companies that are gushing cash flow. We don’t really think of value as low price to book or low price to earnings. We are actually valuing businesses based on cash flows like a private equity investor would. We are valuing businesses based on cash flows.”
Free cash flow yield refers to the free cash flow per share generated from a business as a percentage of its stock price. A free cash flow yield below the S&P 500, like the Magnificent 7 and NVIDIA, can be seen as investors already pricing in superior growth and durability of free cash flow relative to the market.
Investors already appreciate the strong fundamentals of the Magnificent 7, with stock returns far outstripping the S&P 500 in 2023 and 2024 year-to-date.
Despite the massive outperformance of tech stocks, the evidence is almost incontrovertible that the current situation is not a bubble. Large technology stocks generally have exceptional profitability relative to the average stock and should trade at a premium valuation. The rise in the stock prices of the Magnificent 7 has been supported by robust fundamentals and not just a promise of future profitability.
The bullish case on large tech and NVIDIA is bolstered by the rise of artificial intelligence (AI). It seems likely that AI will be a transformational tool for businesses that will generate substantial economic profits. The complex computations needed to support AI are typically done by cloud service providers (CSPs). JPMorgan estimates that tech spending by the top four U.S. CSPs will be around $150 billion in 2024, which is 39% higher than in 2023. NVIDIA should reap significant revenue from this spending as the leading chip provider for AI.
Conversely, the risk of owning these tech darlings has risen, including in NVIDIA. The bullish case is well known, and there is always the possibility that capital spending on AI outstrips the end demand for the services, causing a retrenchment or the technology fails to live up to its promise. Further, the eye-popping profitability of companies like NVIDIA is challenging to maintain long-term as capitalism subjects firms to brutal competition in vying for profits. Investors buy or hold companies for their future profits, not the past. The past is only significant so much as it informs the future prospects of the business. Businesses like many of the Magnificent 7 have shown superior profitability over time, which provides evidence of a competitive advantage that should allow them to reap excess profits. Still, that probability needs to be balanced with the price paid.
The mega-cap technology stocks, represented by the Magnificent 7, continue to produce earnings and free cash flow at impressive rates. These are generally remarkable businesses with outstanding returns on capital, but some selective pruning is likely prudent risk management even with no sign of trouble evident currently. Selective buying within underperforming and less economically sensitive sectors like healthcare and consumer staples can provide an attractive counterweight to the concentration of performance and risk in the technology sector.
Disclosure: Glenview Trust currently has holdings of Microsoft (MSFT), Meta Platforms (META), Amazon.com (AMZN), Apple (AAPL), NVIDIA (NVDA), Broadcom (AVGO), Coca-Cola (KO), and Alphabet (GOOGL) within its recommended investment strategies."
264,https://www.forbes.com/sites/saibala/2024/06/21/nvidias-work-in-healthcare-is-just-getting-started/,Nvidia’s Work in Healthcare Is Just Getting Started,"Jun 21, 2024, 08:30am EDT","Dr. Sai Balasubramanian, M.D., J.D.","Nvidia’s meteoric success has made global headlines in the last few months. One of the primary reasons for this is the rapid growth and skyrocketing demand for the hardware to support advanced artificial intelligence systems—a space that Nvidia has been innovating in for decades.
In congruence with this innovation, Nvidia’s work has helped enable significant progress across numerous industries, and healthcare has been at the forefront of benefitting from this progress. In fact, Nvidia’s healthcare focused work has been ongoing for nearly two decades, especially as the company slowly started transitioning from a traditional graphics card company to focus on advanced computing and hardware. Although not a traditional healthcare company itself, Nvidia has played a pivotal role in the massive changes that the healthcare industry has endured over the last few decades, transitioning from paper records and clipboards to digital records and imaging.
The company has especially focused on the explosion of healthcare data and harnessing this data to gather longitudinal insights, collating it for more advanced applications, and using it to build advanced models to power useful and tangible applications for end-users to actually help drive better outcomes. With regards to this, Kimberly Powell, vice president and general manager of healthcare at Nvidia, explains that the company has focused on a variety of key application areas, including digital surgery, digital biology and digital health.
The umbrella of digital surgery is best explained by the company’s revolutionary work with Holoscan, which is a sensor processing platform that empowers the “software and hardware needed to build AI applications and deploy sensor processing capabilities from edge to cloud.” As the company describes edge computing, “At the edge, IoT and mobile devices use embedded processors to collect data. Edge computing takes the power of AI directly to those devices and processes the captured data at its source—instead of in the cloud or data center.” Using this technology, innovators can develop and deploy medical devices that can bring AI applications and advanced models directly to the patient care setting and operating room. Leveraging multimodal sensors and advanced models, this can aid clinicians in making decisions in real-time.
Similarly, digital biology is a vast ecosystem and encompasses Nvidia’s work with instrumentation companies across a variety of applications including genomics, microscopy, DNA sequencing, computational biology and many more to enable a more insight-driven approach to life sciences and drug discovery. This field is notorious for the incredibly vast amount of data that is often collected and analyzed as a part of the scientific process; the value of Nvidia’s technology is that it empowers the training of foundational models at scale with incredibly high levels of computing power, which enables researchers to create complex models and replicate them to ingest these large amounts of data.
Finally, the rise of digital health has also brought with it the potential for incredible applications in clinical settings. With a variety of artificial intelligence models being deployed in this space, clinicians can now leverage various tools to ease their workflows and augment their day-to-day processes, including the use of enterprise search applications across data sets to employing intelligent AI assistants, which can help document and summarize physician-patient interactions and ease administrative burdens.
Although these three generic categories do not necessarily capture the entirety of the work that Nvidia is doing in the healthcare space, they provide a sample of the numerous applications that have emerged from the company’s progress. Undoubtedly, this market is incredibly competitive, as more innovators are rapidly entering this space. Market behemoths in the healthcare technology and AI space include large technology companies such as Google in addition to the hundreds of smaller startups that are attempting to disrupt this industry.
Despite the incredible progress, however, Powell thoughtfully explains that we have to remain cautious, and that this work has to be approached in a calibrated manner: “Our job is to create the tools and the frameworks to help build and scale the models; we are still learning how to build the models and deploy this technology safely.” Indeed, this learning process will never truly end, especially as the technology rapidly evolves. Nevertheless, if developed in a safe and sustainable manner, this area of innovation has the potential to truly make an impact in healthcare."
265,https://www.forbes.com/sites/dereksaul/2024/06/20/nvidia-stock-endures-277-billion-midday-selloff-as-ai-giant-falls-behind-microsoft/,Nvidia Stock Endures $277 Billion Midday Selloff As AI Giant Falls Behind Microsoft,"Jun 20, 2024, 02:48pm EDT",Derek Saul,"Nvidia stock suffered a rare tumble Thursday, relinquishing the artificial intelligence motor’s briefly held title as the most valuable public company on the planet and exhibiting broader stock indexes’ sensitivity to price movements from Nvidia.
Nvidia’s stock sank by as much as 8% from its early morning all-time high to its afternoon nadir, with its share price down 3.5% on the day to $131 by close, reversing what was as much as a 4% gain shortly after Thursday’s open.
The intraday swing erased $277 billion of Nvidia’s market value, which sank to $3.27 trillion, placing Nvidia behind MIcrosoft after exceeding the market capitalization of the more established technology firm Tuesday.
There was no clear catalyst for the Nvidia selloff, indicating it was likely a matter of investors taking profits after the stock’s 170% rally year-to-date and 800% pop since the beginning of last year.
Nvidia stock is still up 40% over the last month alone.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
Critically, the Thursday dip demonstrated the broader market’s sensitivity to Nvidia stock’s performance, as the S&P 500 turned its peak 0.3% morning gain to a 0.6% loss by around 1:45 p.m., with almost all of the fall explained by Nvidia. Essentially, Nvidia’s intraday slide erased the equivalent of Coca-Cola, the S&P’s 27th most valuable company with a roughly $270 billion market cap, from the index.
It is highly normal for a stock to decline after a sharp increase like Nvidia just had. Broad index funds have benefited far more from Nvidia’s inclusion than they’ve been hurt, with Nvidia’s $2 trillion market cap added in 2024 accounting for a third of the market cap-weighted S&P’s 15% year-to-date gain. Nvidia, which produces a majority of the semiconductor technology most coveted for generative AI applications, reported a sixfold increase in earnings in its most recent quarter as customers like Microsoft and Google shell out for the technology.
$345,000. That’s about how much $10,000 invested in Nvidia exactly five years ago would be worth today. The same amount invested in the S&P would be worth about $20,150.
"
266,https://www.forbes.com/sites/roberthart/2024/06/19/chip-stock-rally-continues-wednesday-after-ai-boom-catapults-nvidia-to-worlds-most-valuable-company/,Chip Stock Rally Continues Wednesday After AI Boom Catapults Nvidia To World’s Most Valuable Company,"Jun 19, 2024, 07:15am EDT",Robert Hart,"Chip stocks jumped in out-of-hours trading on Wednesday as chipmaking giant Nvidia continued Tuesday’s market rally that saw it dethrone Microsoft as the world’s most valuable public company and boost CEO Jensen Huang’s net worth.
Nvidia shares were up around 0.6% to more than $136 during out of hours trading Wednesday morning, when key U.S. markets are closed to observe Juneteenth.
The bump comes after Nvidia stock reached an all time high Tuesday, trading up at $135.58 on market close, a jump of more than 3.5%.
Shares for other chip stocks also rose overnight alongside Nvidia’s rally, predominantly in Asia, where some of the world’s largest tech manufacturers are based, including some of Nvidia’s biggest competitors.
Shares of chip rivals like Taiwan Semiconductor Manufacturing Company, TSMC, another big beneficiary of the AI boom, surged more than 4% by market close in Taipei, with Hong Kong-listed Chinese manufacturers Hua Hong Semiconductor and Semiconductor Manufacturing International Corp also climbing by 0.4% and 1%.
South Korean chipmakers Samsung Electronics and SK Hynix respectively jumped by as much as 3% and 7% during trading hours, though gains atrophied as the day progressed and they were respectively 1.75% up and 0.43% down by market close in Seoul.
Shares for U.S. chipmaking giants remained largely flat during out of hours trading Wednesday, with Arm Holdings (ARM) and Micron Technology (MU), which climbed nearly 9% and 4% on Tuesday, up by a modest 0.34% and 0.81%, respectively and Intel, which was down more than 1% Tuesday, relatively unmoved.
Shares for Hon Hai Technology Group, more widely known as Foxconn, also jumped following the Nvidia rally. The Taipei-listed stock closed more than 2% higher at market close on Wednesday. The Taiwanese giant has partnered with Nvidia to build a specialized set of AI data centers using the U.S. behemoth’s chips to develop a variety of AI applications including robotics platforms, electric vehicles and large language models (LLMs).
Nvidia on Tuesday claimed the title of the world’s most valuable public company, overtaking tech giant Microsoft just weeks after it snatched second place from iPhone maker Apple. The achievement caps Nvidia’s rise from a respected maker of video game hardware to the purveyor of one of tech’s hottest commodities: specialized AI chips. The pivot has seen Nvidia dramatically transform its fortunes in a matter of years, growing from a market capitalization of around $16 billion in 2016 to just under $800 billion towards the end of 2021 before dropping to around $300 billion in 2022 amid poor demand for gaming chips and a wider rout on U.S. stock markets. The boom in generative artificial intelligence, kickstarted by the release of OpenAI’s ChatGPT in late 2022, has helped propel Nvidia to new heights as demand for its specialized AI chips soared. Since then, the company’s value has multiplied from around $400 billion in late 2022 to $1 trillion by mid 2023, $2 trillion by early 2024 and $3 trillion by the middle of 2024 on a near vertical climb. Nvidia shares are up more than 170% this year alone and its stratospheric growth shows little sign of stopping as tech firms duel it out to dominate the nascent market for AI.
The AI boom has not only catapulted Nvidia into a rare position of wealth and power. CEO Jensen Huang, who cofounded the chipmaker in 1993, has also seen his wealth grow astronomically in recent years. After the recent bump in Nvidia stock, Forbes estimates Huang is worth $118.7 billion, up $4 billion on Tuesday alone. He is now the 11th richest person in the world, ahead of Indian billionaire Mukesh Ambani and approximately $12 billion away from entering Forbes’ ranking of the world’s top ten richest people. He trails Microsoft billionaires Steve Balmer, who owns the Los Angeles Clippers, and Bill Gates, respectively worth $130.6 billion and $133.5 billion.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
"
267,https://www.forbes.com/sites/antoniopequenoiv/2024/06/18/nvidia-now-worlds-most-valuable-company-topping-microsoft-and-apple/,Nvidia Now World’s Most Valuable Company—Topping Microsoft And Apple,"Jun 18, 2024, 02:00pm EDT",Antonio Pequeño IV,"Chip designer and artificial intelligence juggernaut Nvidia narrowly topped Microsoft’s market capitalization Tuesday to become the world’s most valuable public company, less than two weeks after it surpassed Apple’s market cap following months of strong stock gains.
Nvidia shares traded up more than 3% Tuesday afternoon at around $135.65, while Microsoft traded down a fraction of a percent at $447.58.
The stock performances sent Nvidia’s market cap up to $3.33 trillion and Microsoft’s to $3.32 trillion.
Nvidia’s stock is up about 12% since its recent stock split, which brought its price down from $1,210 per share to about $120 per share.
Nvidia began the year with a $1.2 trillion market value, less than half of Microsoft and Apple’s market cap at the time.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
177%. That’s how much Nvidia’s market cap has spiked since the start of the year, when shares traded at $48.17 (adjusted for stock split).
We estimate Nvidia CEO Jensen Huang’s net worth at $118.8 billion, making him the 11th-richest person in the world—the highest ranking he’s secured on Forbes’ real-time billionaires list.
Nvidia’s milestone comes amid an artificial intelligence craze that has made its chips and graphics processing units increasingly valuable to fellow tech companies like Microsoft, Meta, Amazon and Alphabet. Microsoft is estimated to make up 15% of Nvidia’s revenue, according to Bloomberg supply chain data, and Nvidia is estimated to control between 70% and 95% of the AI chip market, CNBC reported, citing Mizuho Securities. Nvidia, which reported $8.4 billion in net income in its 2023 fiscal year, shows little to no signs of slowing down, as analysts forecast its net income to surpass Apple’s in just four years.
Nvidia Shares Now Trading At Just $120 After Stock Split In Wake Of Monstrous Run (Forbes)
Nvidia Tops Apple’s Market Cap, Becoming The World’s 2nd Most Valuable Company (Forbes)
Can Apple Stock Go Much Higher? Analysts Don't Think So: Here's Why (Forbes)"
268,https://www.forbes.com/sites/tylerroush/2024/06/18/nvidias-jensen-huang-soars-to-119-billion-net-worth-as-company-becomes-worlds-most-valuable/,Nvidia’s Jensen Huang Soars To $119 Billion Net Worth—As Company Becomes World’s Most Valuable,"Jun 18, 2024, 03:31pm EDT",Ty Roush,"Nvidia chief executive Jensen Huang is now the 11th-richest person in the world after his net worth increased by more than $4 billion on Tuesday, scoring his highest ranking yet on Forbes’ real-time billionaire list, as the chip designer rode an artificial intelligence-driven surge to become the world’s most valuable public company.
Huang’s net worth increased to around $119 billion as of Tuesday afternoon as Nvidia shares traded up more than 3% to $135.70, ranking him ahead of Mukesh Ambani, India’s richest person, and behind former Microsoft CEO Steve Ballmer.
Huang—who owns a 3% stake in Nvidia—started the year with a net worth of $77 billion, according to Forbes’ estimates, before the company’s market cap increased by 177% to $3.33 trillion.
Nvidia said in a securities filing last month Huang would sell 600,000 shares of the company through March 2025, which would net him just over $81.4 million at the company’s current share price.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
$114 billion. That’s how much Huang’s net worth has increased since 2019, when Forbes estimated him to be the 546th-richest person in the world. Huang was worth $21 billion last year, ranking him as the world’s 76th-richest person.
Huang has served as Nvidia’s chief executive and president since cofounding the company in 1993. Nvidia went public in 1999 and has surged in trading in recent years as it dominates the market for the chips needed to power artificial intelligence software, most recently surpassing the market cap of Microsoft and Apple. The company recently executed a stock split—when a company increases the number of shares without diluting the stock’s value—that cut Nvidia’s share price below $130 after trading above $1,200."
269,https://www.forbes.com/sites/phoebeliu/2024/06/16/these-10-nvidia-shareholders-have-gotten-36-billion-richer-in-a-month/,These 10 Nvidia Shareholders Have Gotten $36 Billion Richer In A Month,"Jun 16, 2024, 06:30am EDT","he next industrial revolution has begun,” Nvidia cofounder and CEO Jensen Huang said last month, during Nvidia’s quarterly earnings call. “Companies and countries are partnering with Nvidia to shift the trillion-dollar installed base of traditional data centers to accelerated computing and build a new type of data center, AI factories, to produce a new commodity, artificial intelligence.","While Huang is the best known shareholder of Nvidia, there are plenty of others who have personally benefited from the rising stock. Among the biggest gainers are five other top executives including Chief Financial Officer Colette Kress and its general counsel, Tim Teter, plus four independent board members who together own more than $10 billion worth of stock.
One of those board members is billionaire Mark Stevens, who first got involved with Nvidia in 1993, the same year the fledgling chip company was founded. He was a newish partner at Sequoia when the firm’s founder, Don Valentine, got a call from LSI Logic founder Wilfred “Wilf” Corrigan. “There’s this kid who works at LSI and I’m sorry to see him go. You guys should look at him,” Corrigan told Valentine, according to Stevens. Valentine and Stevens met with Huang, who pitched them on the idea of a company designing 3D graphics chips for PC games. “And we thought, intriguing,” says Stevens. Sequoia put $1 million into the company, part of an investment that valued the firm at just $7 million, according to PitchBook. Stevens, who had worked at Intel and was Sequoia’s young semiconductor expert, joined the Nvidia board. He left in 2006 at a time when Sequoia partners were stepping off public company boards to focus on private companies; in 2008, Stevens began to wind down at Sequoia and Huang invited him to rejoin the board. Largely because of Sequoia’s early investment, Stevens now owns $5 billion worth of Nvidia shares. But he says he doesn’t serve on the board for the money. “I love our board. I think it might be the best board in America.”
Current venture capitalist and former tech executive Brooke Seawell was convinced by another tech executive, Harvey Jones, to join Nvidia’s board in 1997. The two men got to know each other while Seawell was Senior Vice President of Finance & Operations at Synopsys, where Jones was CEO. Nvidia “was in the graphics semiconductor space, which was a hyper-competitive space … and the CEO was barely over 30. He hadn’t even been a VP before he founded the company,” Seawell recalls. “I looked at it and said, ‘Boy, this doesn’t seem likely.’” However, Jones, who’d joined the board in 1993, was persuasive, and Nvidia was in search of someone with expertise in finance and taking a company public, which Seawell had done at Synopsys. Seawell now owns $700 million of its shares.
Relatively speaking, the directors and executives at Nvidia own a lot of stock. At Microsoft, the most valuable U.S. company right now (with a $3.3 trillion market cap), directors and executive officers together own less than 1% of the software giant’s shares, per its latest proxy. The same is true at Apple, which has the second largest market cap, at $3.3 trillion. At Nvidia, directors and executive officers owned 4.23% of shares as of March 25, per its proxy filing. Altogether, that’s more than $130 billion worth of shares, though the vast majority of that is in the hands of Huang. One big reason for the shareholding difference is that, unlike Nvidia, none of the founders of Microsoft or Apple still serve on the board or as senior executives.
Benjamin Hermalin, a professor of finance at U.C. Berkeley, says via email that there are two broad effects of executives holding large amounts of stock, as Huang does. “First, managerial stock ownership makes management more concerned with the firm's value (stock price), which is a positive for the other shareholders,” all other things being equal. But it could also make the team too cautious, knowing that so much of their money is tied up in the stock.
So far there are few signs that Huang is becoming risk averse.
Meanwhile, it’s pretty easy to see now why these board members as well as key insiders held onto so much stock, at least until these record highs. But earlier on, it wasn’t so obvious. Nvidia’s first chip, released in 1995, had failed. Partway through developing its second chip, Huang and team decided the architecture was all wrong and they stopped work on it, according to a podcast from VC firm Sequoia. “Between 1993 and 1997 we almost went bankrupt three times,” says board member Stevens. “One of the things that Jensen said was ‘We’re only 30 days from going out of business.’”
Nvidia turned things around by working in reverse on its third attempt to design a chip, tackling the software first, Huang explains on the Sequoia podcast. It moved quickly due to evaporating funding, and its next chip was a success. In the early 2000s it developed the world’s first graphics processing unit, or GPU, which became hugely popular for rendering images in video games. In the past decade or so, Nvidia found that AI researchers in both the corporate and academic worlds were using its GPUs to run neural networks–a foundation of AI–and to create home-grown supercomputers. That helped fuel the decision to bet on AI computing, before there was a demonstrated market. That bet has paid off in spades.
One person who missed out on billions is Huang’s cofounder and former chief technology officer, Curtis Priem—who had the same number of shares as Huang when Nvidia went public in 1999, but left the company more than 20 years ago. Had he held onto his stock, he could have been a centibillionaire. Priem either sold or donated all of his Nvidia shares by 2007, he told Forbes last year, and is funding a quantum computer worth triple his current estimated net worth of under $50 million. It’s not clear what cofounder and former vice president of engineering Chris Malachowsky has done with his Nvidia shares since coming off the company’s proxy filings two decades ago. He hasn’t been required to disclose his Nvidia sales or holdings since 2002, when he held 773 million shares (adjusted for splits), but still works at Nvidia as a fellow. Attempts to reach Malachowsky were unsuccessful.
The company didn’t miss a beat after the cofounders left their executive roles. Seawell, who’s been on the Nvidia board for nearly 27 years, credits Huang’s leadership for steering the business so steadily. “AI has been a concept for many, many decades. But Jensen saw that with enough compute power, it could become practical and real,” says Seawell. Huang spends part of his time not just on the technology, but focusing on teamwork and helping employees do work they are passionate about, which in turn has led to very low employee turnover—just 3% a year, per Seawell, compared to 15% at most Silicon Valley companies.
Says Seawell: “Jensen will be the first person to say we’ve had lots of ups and downs, and we’ve made tons of mistakes. But we’ve learned from our mistakes, and we constantly get better.”
Huang has come a long way from cleaning toilets in a rural Kentucky boarding school while he was a student. Even so, if he could do it all over again, he wouldn’t build Nvidia because he didn’t realize how hard it could be, he told the Acquired podcast in February. He says he’s always working, around the clock and seven days a week. “I sit through movies, and I don’t remember them because I’m thinking about work,” he said in an interview with Stripe cofounder Patrick Collison last month, during which he also mentioned that he doesn’t believe in one-on-one meetings or in firing employees. In March, Huang adopted a new trading plan to sell six million Nvidia shares, less than 1% of his stake, in the next year.
Longtime venture capitalist Stevens was a partner at Sequoia Capital when the firm invested in PayPal, LinkedIn, Google and, of course, Nvidia—back in 1993, when Nvidia was founded and the entire company was worth just $7 million, according to PitchBook. He served on Nvidia’s board from 1993 to 2006, and then rejoined again two years later. Stevens, who sold more than $120 million worth of shares in the last month, also owns a minority stake in the Golden State Warriors, where he is a member of the ownership’s executive board. He left Sequoia in 2021, and now primarily invests through his own firm, S-Cubed Capital.
Like Stevens, Coxe was a VC, representing early investor Sutter Hill Ventures. He too joined Nvidia’s board the year the company was founded. While he left Sutter in 2020, he never left Nvidia, and is now entering his fourth decade as a director. Coxe is the third-largest insider shareholder at the firm. He has also been a shareholder at software firm Snowflake, which Sutter Hill backed.
Jones, who joined Nvidia’s board with Stevens and Coxe in 1993, has been an entrepreneur, high tech executive and venture investor for more than three decades. He cofounded tech companies Daisy Systems in 1981 and Tensilica in 1997—and served as CEO of electronic design automation company Synopsys, which is now worth more than $80 billion. These days, he is primarily an investor through his firm, Square Wave Ventures.
Nvidia’s executive vice president and chief financial officer, Kress joined the tech outfit Nvidia in 2013, when revenue was just $4.3 billion. She has since helped oversee one of the biggest revenue ramp-ups in history: in the year through January, Nvidia had $60.9 billion in revenue, 15 times what it was a decade ago. Kress reportedly joined Nvidia after the endorsement of her two young sons, 8 and 10 at the time, who knew the company for designing the chips that powered their favorite computer games. (Each of her sons now has 400 shares to their name.) Prior to Nvidia, Kress spent three years as CFO of a Cisco division and 13 years at Microsoft.
Seawell joined Nvidia’s board back in 1997 when he was executive vice president at NetDynamics, an application server software company acquired in 1998 by Sun Microsystems. He pivoted to venture capital in 2000, first as a partner at Technology Crossover Ventures and later at New Enterprise Associates. Over his long career—25 years as an operating executive and nearly as many as a VC, he’s been involved in 12 public offerings including Nvidia and data visualization company Tableau, as well as 13 acquisitions. Seawell is also an avid cyclist who has taken two dozen cycling vacations around the world. According to him, “It’s an in-depth way to experience a place or region.”
Longtime Nvidia employee Puri is now an executive vice president responsible for the company’s worldwide field operations including its go-to-market strategy. Before joining Nvidia in 2005, he spent 22 years at Sun Microsystems—where Nvidia cofounders Priem and Malachowsky also worked before leaving to create Nvidia.
Nvidia’s general counsel joined in 2017 and has already amassed Nvidia shares worth more than a quarter of billion dollars. Previously, he was a partner at law firm Cooley, where he worked on high-profile patent and tech cases representing companies including Qualcomm and Apple. Before heading to law school at Stanford, he worked as an engineer at Lockheed Missiles and Space Company. He has only disposed of shares as gifts or to cover taxes owed for the vesting of restricted stock units.
She joined Nvidia in 2007 and is now executive vice president of operations, which includes overseeing Nvidia’s 1.25-million-square-foot headquarters in Santa Clara. Shoquist has sold a bigger percentage of her shares relative to her current holdings than any of the other top Nvidia holders. She previously worked at JDS Uniphase, Coherent and Quantum.
An independent Nvidia board member since 2005, Perry has held a wide variety of titles in various industries, including general counsel, chief financial officer and executive vice president of operations at public biotech giant Gilead Sciences. He also had a stint at law firm Cooley, was briefly an entrepreneur-in residence at Third Rock Ventures and was CEO of biopharma firm Aerovance. He currently consults for various companies and nonprofit organizations.
Update, June 17: The story has been updated to clarify Chris Malachowsky’s current role with Nvidia."
270,https://www.forbes.com/sites/investor-hub/article/nvidia-vs-amd-vs-intel-best-ai-stock-to-buy/,Nvidia Vs. AMD Vs. Intel: Which AI Stock Is Best As Competition Heats Up?,"Jun 12, 2024, 04:10pm EDT",Sasirekha Subramanian,"In the artificial intelligence stock universe, this is clearly no three-way battle. Nvidia is the undisputed AI leader commanding more than 90% market share in data-center GPUs and more than 80% market share in AI processors. At best, Advanced Micro Devices (AMD) and Intel (INTC) are actively competing with AI chips to seek positioning as viable alternatives for Nvidia’s H100 (the company’s graphics processing unit). But Nvidia is well ahead in the AI game, already evolving to a more sophisticated H200 and the new Blackwell platform later this year. AMD and Intel may have plenty of catching up to do. The more critical threat to Nvidia is the attack on the monopoly of CUDA, its proprietary software stack that allows developers to leverage the parallel processing capabilities of Nvidia GPUs to accelerate machine learning workloads.
From a stock price perspective, Nvidia is knocking it out of the park. Shares of Nvidia have run up more than 200% in the past year positioning the AI chip giant as the third largest company in the U.S. with a $2.97 trillion market cap, trailing only Apple’s $3.17 trillion and Microsoft’s $3.21 trillion market values. AMD stock, not quite in the leagues of Nvidia, has nevertheless fared well with a nearly 25% rally in the past year reaching a market cap of $257 billion that is 2x of Intel’s. The laggard among the three is Intel with its shares down 6% on the year and down 40% from its December highs mainly due to its weaker-than-expected second-quarter outlook. This article endeavors to provide insights on the following questions.
The brain trust at Forbes has run the numbers, conducted the research, and done the analysis to come up with some of the best places for you to make money in 2024. Download Forbes' most popular report, 12 Stocks To Buy Now.
Intel shares are down more than 30% in the past five years vs. AMD’s nearly 400% climb and Nvidia’s whopping 3,000-plus percent rally. Intel stock is facing challenges from the “technology gap that was created by over a decade of underinvestment,” to quote Intel CEO Patrick Gelsinger. AMD has been a primary beneficiary of Intel’s manufacturing missteps in the past.
Intel’s problems started with missing the boat on the 10nm and 7nm processes in chip manufacturing. Processors made using smaller but advanced nm (nanometer) processes are typically faster, perform better and more power efficient.
Two companies that flourished from Intel’s manufacturing fumbles include the Taiwan Semiconductor Manufacturing Company (TSMC) and AMD. While TSMC cruised through the 10nm and 7nm processes, AMD, a fabless semi, grew its share of X86 server CPU market from almost zero to 23.9% through the first quarter of 2024.
Intel missed out on the mobile revolution as well. The iPhone could have had an Intel chip, but today about 99% of premium smart phones are powered by an Arm-based chip. That was a costly mistake, because Apple later stopped using Intel chips in its computers, too, starting in 2020 and transitioned to its own Arm-based chips, breaking a 15-year partnership with Intel. For reference, Apple Macs represent roughly 10% of global PC market share. Intel’s loss was Arm’s gain.
Arm captured 9% of the overall CPU server market in 2023, even as Intel continues to dominate with a 61% market share. Arm uses the RISC architecture vs. Intel’s X86 instruction set that is used by most PCs. Arm-based chips use less power vs. X86-based chips and lately Arm chips have experienced a significant rise in adoption. Arm architecture is at the core of both Amazon Web Services' custom server Graviton chips and Qualcomm’s flagship Snapdragon chips.
It appears so. Here’s why. Nvidia is cutting out Intel entirely from its latest “Blackwell” GPU. Two Nvidia B100 GPUs are paired with one Arm-based processor. For reference, AI-oriented GPU-based servers often leverage multiple Nvidia GPUs, sometimes eight or more, alongside an Intel CPU to facilitate parallel processing, essential for AI tasks such as deep learning and neural network training. Nvidia’s latest Grace Hopper Superchip combines its own GPUs with Arm’s high-performance Neoverse cores.
Arm-based chips are powering Microsoft’s surface laptops that are shipping on June 18, These laptops are equipped with Qualcomm’s Snapdragon X Elite or Plus chip to compete more effectively against Apple’s MacBook laptops.
Google’s first custom Arm-based CPUs–the Axion Processors–designed for the data center, will be available to Google Cloud customers later this year. Google says Axion processors will deliver 30% better performance than the fastest general-purpose Arm-based processors available in the cloud and deliver up to 50% better performance and up to 60% better energy-efficiency than comparable current-generation x86-based CPUs.
It should be noted that Intel has a lower market cap vs. even Arm.
Last week, Intel began to ship the first of its next generation Xeon server processors–a Xeon 6 “efficiency”-model (E-core) designed for public and private clouds where power efficiency and performance are critical. The more powerful “performance” version (P-core) of the Xeon 6– designed to run computationally intensive AI models–is slated to arrive in the third quarter.
A lot hinges on the Xeon 6 chips for Intel in its attempts to reclaim data center market share for x86 chips from AMD. A Reuters report citing data from Mercury Research states that “Intel's share of the data center market for x86 chips has declined 5.6 percentage points over the past year to 76.4%, with AMD now holding 23.6%.”
Intel notes that its Xeon 6 P-core processors will perform AI inferencing 3.7 times better than AMD EPYC processors, while Xeon 6 E-core processors will provide 1.3 times better performance per watt over AMD EPYC chips on media transcoding workloads.
The Xeon 6 ‘efficiency’ model has a 144-core count giving it a lead over AMD’s 4th generation EPYC processors with up to 128-core count. An increasing core count means superior performance as multiple cores enable parallel processing.
However, AMD is not resting on its laurels. AMD’s fifth generation EPYC processors, code-named Turin, will feature up to 192 cores and arrive in the second half of this year. In turn, Intel is planning to release a 288 e-core of the Xeon 6 early next year.
Intel is pricing its Gaudi 2 and Gaudi 3 AI chips much cheaper than Nvidia’s H100 chips. Intel claims that the new Gaudi 3 accelerator delivers ""50% on average better inference and 40% on average better power efficiency"" vs. Nvidia’s H100 at ""a fraction of the cost.” The Gaudi 3 will be widely available in the third quarter.
A Gaudi 3 accelerator kit, which includes eight AI chips, is priced at $125,000, and the previous generation Gaudi 2 costs $65,000. The pricing of the Gaudi 3 accelerator kit appears comparable with AMD’s flagship AI accelerators–the Instinct MI300 lineup, also pitted directly against Nvidia’s H100 GPUs. Reportedly, an Instinct MI300X GPU sells for approximately $15,000.
AMD’s MI300X GPU, which has been around longer, could not dent the demand for Nvidia’s H100 AI GPUs that reportedly cost between $30,000 and $40,000, about 2x its prices. So, it is uncertain if the low prices of Gaudi 3 will make any sizable impact on H100 demand, but it should be noted that Gaudi 3 has gained support from major players like Dell, HPE, Lenovo, Supermicro, Asus, Gigabyte and QCT. AMD expects to launch the MI350 series of chips next year. The MI350 is based on an entirely new architecture, and expected to have 35x better inference capabilities.
Probably in a bid to neutralize the cost advantage touted by competition, Nvidia has signaled solid returns on investment (ROI) of 5x to 7x for customers spending on Nvidia infrastructure. In fact, Nvidia is its own best competition, as it shifts to a new “one-year rhythm” to release new chip architecture, marking a significant acceleration from its two-year cycle.
In a question to how Nvidia customers, who have spent billions of dollars on existing products, would respond to its newer offerings that quickly surpass the capabilities of existing ones and outpace the rate at which the value of the existing products depreciates, Huang suggested that performance-averaging will be the smart way for businesses to deal with “a whole bunch of chips coming at them” when making and saving money are immediate priorities and time is of essence.
Nvidia has deflated concerns of any demand slowdown as it transitions from its current Hopper AI platform to the more advanced next generation Blackwell system. Blackwell has an inference capability that is 30x of Hopper’s, while consuming 25x less cost and energy. So, the analysts were worried if customers would hold off on Hopper orders because of the upcoming Blackwell launch. However, Nvidia said it witnessed increasing demand for Hopper through the quarter (which is after it announced Blackwell) and expects demand to outstrip supply for some time as the transition happens. Besides, Blackwell systems are designed to be backward-compatible, making the transition easy for customers. The demand for both Hopper and Blackwell platforms is well ahead of supply and is expected to continue well into the next year.
Intel expects about $500 million in Gaudi 3 sales this year, while AMD sees about $3.5 billion in annual AI chip sales and Nvidia’s data center business with its AI GPUs is estimated to generate a whopping $57 billion in sales in the second half of the year.
If Nvidia’s GPUs continue to be in overwhelming demand, it is because of its CUDA software stack that allows developers to leverage the parallel processing capabilities of Nvidia GPUs for accelerating machine learning workloads.
Nvidia was ready with the battle-seasoned CUDA years before the boom in deep learning, giving it the early mover advantage. The expansive libraries and tool sets built on CUDA and the integrated native support for CUDA GPU acceleration from major learning frameworks such as TensorFlow, PyTorch, Caffe, Theano and MXNet set the ball rolling. CUDA became the golden standard for GPU acceleration and became deeply ingrained into all aspects of the AI ecosystem.
CUDA alternatives like AMD’s MIOpen, Intel’s oneAPI and even vendor-agnostic frameworks like OpenCL have stumbled due to limited user adoption stemming from the inadequate tooling and support compared to CUDA. Migrating sophisticated neural network codebases from CUDA to alternate programming paradigms continues to pose a solid challenge.
Despite the attempts to unseat CUDA going south, Nvidia has never been negligent about competition. On the contrary, it ensures its market dominance, by constantly evolving its CUDA capabilities and high-performance libraries to accelerate various aspects of deep learning workflows on Nvidia GPUs. Nvidia’s partnerships with the likes of Berkeley university and Facebook help optimize popular deep learning models on CUDA. Besides, Nvidia is the darling of a risk-averse enterprise clientele that would prefer a proven technology as the CUDA.
The efforts to reduce reliance on Nvidia and democratize access to non-CUDA-centric acceleration is gaining momentum.
AMD is taking aim at NVDA’s dominance, by leveraging its open-source ROCm framework that competes directly with the de-facto CUDA standard. ROCm is supported by Google’s open-source machine learning framework -TensorFlow, while PyTorch, another major framework, has introduced initial native AMD GPU integration on an experimental basis to reduce the CUDA lock-in.
Other initiatives from PyTorch include Layer-wise Adaptive Rate Scaling (LARS) to aid in scaling deep learning tasks across diverse hardware platforms. A unified memory allocator in PyTorch 1.11 brings performance improvements to AMD GPUs and Apple M1 chips with unified memory architectures, while the graph mode execution backend introduced in PyTorch 1.5 extends support to workflows on non-Nvidia hardware like Intel integrated GPUs and budget AMD cards with typically lesser memory capacities.
OpenAI’s heavy investment in CUDA/ROCm portability layers like Triton also aims to reduce reliance on Nvidia.
Intel is investing heavily to render oneAPI as a reliable alternative to CUDA. While CUDA is not disappearing overnight, the momentum in shift towards CUDA alternatives underscores the reality that the era of proprietary AI hardware stacks may not last forever.
Stop chasing shadows in the market. Forbes' expert analysts have pinpointed the 12 superstars poised to ignite returns in 2024. Don't miss out—download 12 Stocks To Buy Now and claim your front-row seat to the coming boom.
Sovereign AI: Nvidia expects its Sovereign AI revenue to approach the high single-digit billions this year, from nothing last year, by helping jumpstart the AI ambitions of nations across the world.
Automotive vertical: Automotive is expected to be Nvidia’s largest enterprise vertical within the data center segment this year, driving a multibillion revenue opportunity across on-prem and cloud consumption.
Blackwell platform: The next generation Blackwell platform, which enables real-time generative AI on trillion-parameter large language models, is in full production with shipments slated to begin in the second quarter, and ramp in the third quarter with data centers standing up for customers in the fourth quarter. Nvidia expects to see a lot of Blackwell revenues this year.
Spectrum-X: In the first quarter, Nvidia started shipping its new Spectrum-X Ethernet networking solution that enables Ethernet-only data centers to accommodate large-scale AI. Spectrum-X is ramping in volume with multiple customers, and should ramp to a multibillion-dollar product line within a year.
Intel is hoping that 2024 would be the trough for operating losses in its struggling Foundry business, which deepened its operating loss to $7 billion in 2023 from a loss of $5.2 billion in 2022 on a 31% Y-o-Y revenue drop to $18.9 billion. Intel expects the Foundry business to break even midway between the current quarter and the end of 2030, and to drive considerable earnings growth over time.
In March, Intel was awarded up to $8.5 billion in direct funding, and the option to receive federal loans of up to $11 billion, under the CHIPS Act that aims to build semi fabs on U.S. soil, to protect against a supply crunch if China ever invaded Taiwan. The proposed funding will help Intel advance its commercial semiconductor projects in Arizona, New Mexico, Ohio and Oregon, while supporting its plans to invest more than $100 billion in the U.S. over five years to expand U.S. chipmaking capacity and capabilities and accelerate AI technologies.
It was almost a cinch that Intel would be a key beneficiary of the CHIPS Act, because it operates factories, or fabs that manufacture chips, in addition to designing processors. AMD and Nvidia, are fabless, and only design the chips that are manufactured by TSMC. Intel hopes to get a solid piece of the contract manufacturing business, as it attempts to position its fabs for making AI chips for rival semi companies, as well as its own.
For the first quarter, Intel reported a 10% drop in revenue for its foundry business, and operating losses of $2.5 billion. But Intel expects quarter-over-quarter improvement in its foundry business until 2030.
Intel sees the foundry segment achieving 40% non-GAAP gross margins and 30% operating margins by the end of 2030, while it plans to steer the Intel Products business towards a 60% gross margin and 40% operating margin.
After lagging TSMC for many years, Intel finally expects to return to process technology leadership by 2025 with Intel 18A, and its five-nodes-in-four-years (5N4Y) process roadmap on track. The 18A is Intel’s next-generation technology to manufacture 1.8 nm chips. This will be equivalent to TSMC’s proposed 2nm around the same time-frame.
However, TSMC is refuting Intel’s claim by saying that its N3P process will maintain technical superiority over Intel’s sub-2nm 18A. N3P is tracking ahead to be production ready in the second half of this year and will be on the market earlier. TSMC noted that the N3P process will match Intel's 18A node in power, performance, and density despite the difference in size (of 3nm versus 1.8nm). TSMC also says that it will enjoy the early mover advantage that will offer it a technical superiority over the 18A because the N3P nodes will come at a lower cost with proven prowess.
TSMC’s arguments about early-to-market advantages somewhat resonate with the comments from Nvidia founder and CEO Jensen Huang about the strategic imperative of staying ahead in the AI race rather than aiming for merely incremental improvements, when he posed the question, “do you want to be repeatedly the company delivering groundbreaking AI or the company delivering 0.3% better?
Intel has mostly been a laggard and is now playing catch-up, vs. enjoying an early mover advantage. However, this may not signify much, when Microsoft–the largest U.S. company is making a bet on Intel’s 18A process to manufacture a forthcoming in-house designed chip. As of May 30, Intel noted that it had 6 Intel 18A external foundry customers and a lifetime deal value of greater than $15 billion.
The same Microsoft that eagerly embraced Intel’s 18A, said the new “Copilot+ PC” AI features for its Windows 11 will require at least 40 TOPS (trillion operations per second), which implied that it could not run on Intel’s Meteor Lake hardware that delivered 11.5 TOPS. Microsoft chose Qualcomm’s Snapdragon X Elite that offers a Neural processing Unit (NPU) with 45 TOPS.
The Windows Copilot+ PCs help users to be more productive and creative. For reference, you can quickly retrieve information by typing in cues, quickly find, create, summarize and analyze information without opening multiple apps and files and convert your words into a PowerPoint presentation with visuals.
With the next generation Lunar Lake, Intel is moving past its Meteor Lake limitations. The chip giant is promising 50% faster graphics performance on Lunar Lake compared to Meteor Lake. The Lunar Lake will have a NPU that delivers 48 TOPS. This implies that it could outperform Qualcomm’s Snapdragon X Elite.
However, Qualcomm’s Snapdragon X Elite-powered Copilot+ PCs will arrive on June 18, giving Qualcomm a head-start over Lunar Lake, which is expected to be launched in the third quarter. Meanwhile, AMD’s Ryzen AI 300 mobile SoC sets a new benchmark with 50 TOPS (above Microsoft’s Copilot+ requirement), and notebooks based on it will debut in July. AMD says it is working with Microsoft to meet the new Copilot+ standards.
After years of being a laggard, Intel stock offers a contrarian opportunity with its new products, processes, support from the U.S. government and major tech companies, and a projected turnaround for its Foundry business. The former chip legend has strived to make up for a lost decade via investments and strategic partnerships in its foundry services. Intel’s willingness to open up its fabs for third-party customers will not only generate new revenues, but position it on the good side of the U.S. government that wants its silicon chips homegrown and the benefits could reflect as further tax incentives, fundings and federal loans. However, strong execution of strategic priorities will be key for potential upside. With a multi-year execution cycle still ahead, risks include any delays in launch timelines and Intel continuing to cede the head-start advantage to rivals. Key watchpoints on progress will include quarterly earnings reports and product launches going as planned.
Intel stock valuation: INTC trades at a one-year forward price/earnings of 16x, (based on its 2025 EPS estimate of $1.98). The year 2025 is taken as reference, as Intel expects to return to process technology leadership next year. Assuming a conservative multiple rerating to 19x (below INTC’s 5-year average p/e multiple of 23x), we arrive at a stock price target of around $37 that represents about 20% upside from current stock price levels.
Despite the stunning 3,000+% rally in the past five years, the Nvidia stock has more steam left. Dethroning Nvidia will be a herculean challenge for competition, which at best can likely position itself as a viable alternative and collectively claim approximately 20% to 25% of market share in the AI hardware landscape. As long as Nvidia evolves and enhances its CUDA moat, it has nothing much to be concerned about. CUDA is its biggest strength and weakness. If the CUDA monopoly is broken, the competitive edge can unravel quickly. Efforts of its customers-cum-rivals are already underway to reduce the CUDA lock-in. The consolation is Jensen Huang is well aware of the situation and he is not going to sit back and watch his lifetime work sink into oblivion. In any case, displacing CUDA is more than an overnight process. Expansion beyond cloud service platforms to multiple multibillion-dollar verticals, including consumer Internet companies, and enterprise, Sovereign AI, automotive and healthcare customers will inspire the next wave of growth for Nvidia. That said, no stock will have a straight upward trajectory and will provide buying opportunities along its journey and that’s true of the Nvidia stock as well.
Nvidia Stock Valuation: NVDA trades at a one-year forward price/earnings of 34x, (based on its 2025 EPS estimates of $3.55). Assuming a conservative multiple rerating to 40x (below the 5-year average p/e multiple of 47x), we arrive at a stock price target of $142 that represents roughly 17% upside from current stock price levels.
AMD stock is a key AI bet, as tech giants and AI frameworks strive to break the CUDA dominance. AMD’s open-source ROCm pitted against the CUDA de-facto standard is supported by Google, PyTorch, OpenAI and more. AMD has perfected the art of being a runner-up after several years of vying with Intel for X86 CPU server market share, even as it continues to evolve and compete against Intel, Nvidia and Qualcomm in several aspects of AI. The AMD stock is 30% off its 52-week highs reached in March this year. The selloff creates a buying opportunity.
AMD Stock Valuation: AMD trades at a one-year forward price/earnings of 29x, (based on its 2025 EPS estimate of $5.54). Assuming a conservative multiple rerating to 35x (below the 5-year average p/e multiple of 43x), we arrive at a stock price target of nearly $194 that represents 20+% upside from current stock price levels.
Two other stocks outside of the trio–Nvidia, AMD and Intel–include TSMC (TSM) and Arm (ARM). Both stocks appear well positioned to benefit from the heating AI chip battles, especially TSMC.
Shares of Nvidia, AMD and Intel likely offer 17% to 20% upside potential from current price levels. TSMC and Arm also appear well positioned to benefit from the heating AI chip battles.
Please note that I am not a registered investment advisor and readers should do their own due diligence before investing in this or any other stock. I am not responsible for the investment decisions made by individuals after reading this article. Readers are asked not to rely on the opinions and analysis expressed in the article and encouraged to do their own research before investing.
Which company is best for long-term AI investments?
Contract chip manufacturer Taiwan Semiconductor Manufacturing Company Limited (TSM) appears well positioned to benefit from the ongoing AI boom, as it manufactures AI chips for major technology companies including the AI bellwether Nvidia.
How does AMD compare to Nvidia in AI technology?
Nvidia is the undisputed AI leader commanding more than 90% market share in data-center GPUs and more than 80% market share in AI processors. AMD, although not in the leagues of Nvidia, is upping its game with new AI products.
Is Intel a good AI pick?
After years of being a laggard, Intel stock offers a contrarian opportunity with its new products, processes, support from the U.S. government and major tech companies,  and a projected turnaround for its Foundry business
Is investing in Nvidia, AMD, or Intel risky?
Investing in any stock is risky, especially in technology stocks that pose a high risk/reward.  Nvidia, AMD or Intel stock is no exception. However, these stocks seem positioned to benefit from strong AI tailwinds.
The brain trust at Forbes has run the numbers, conducted the research, and done the analysis to come up with some of the best places for you to make money in 2024. Download Forbes' most popular report, 12 Stocks To Buy Now."
271,https://www.forbes.com/sites/dereksaul/2024/06/10/nvidia-shares-now-trading-at-just-120-after-stock-split-in-wake-of-monstrous-run/,Nvidia Shares Now Trading At Just $120 After Stock Split In Wake Of Monstrous Run,"Jun 10, 2024, 09:31am EDT",Derek Saul,"Nvidia’s share price is at its lowest level since late 2022 after the company executed its previously announced stock split, a procedural move symbolic of the artificial intelligence sensation and semiconductor chip designer’s astonishing stock market run.
Monday was Nvidia’s first day of trading after the company split its shares on a 10-for-1 basis based on Friday’s closing share price of $1,210, meaning shareholders were awarded 10 shares for each of their existing shares, not diluting any equity.
Nvidia shares opened Monday at about $120 per share after a modest dip, but are still within 5% of its intraday, split-adjusted high of $125.59 recorded last week.
The stock split, a routine move for companies whose share prices rose significantly, comes after Nvidia’s stock market surge as investors poured into the firm arguably most well-positioned to profit off of the AI revolution.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
Nvidia’s stock traded Monday at the same unadjusted share price it did in October 2022, meaning 10 shares of the company 20 months ago is worth the same as one share now after the company exploded from a sub-$300 billion market valuation to almost $3 trillion, making Nvidia the third-largest company in the world.
The self-appointed “engine of AI,” Nvidia most notably designs the hardware which stores the immense data loads needed for generative AI and the graphics processing units (GPUs) which bring machine learning computing to life. Over the last decade, Nvidia went from a little-discussed Silicon Valley company worth around $10 billion to the talk of Wall Street, as its profits exploded due to its market dominance in AI chips, with the likes of Amazon, Apple and Microsoft among its big-ticket customers. Thanks largely to the eye popping rally over the last two years with the AI boom, Nvidia stock’s long-term chart looks downright silly, with split-adjusted shares trading at less than $1 as recently as 2016. Nvidia was among the 10 most expensive S&P 500 components by share price at Friday’s close, but is now in line with the S&P median of $118 per share. Nvidia has almost single handedly boosted the S&P from its 2022 nadir to its record highs this year due to its market-beating returns at its massive valuation.

"
272,https://www.forbes.com/sites/petercohan/2024/06/09/nvidia-stock-could-rise-10-fold-on-new-10-billion-growth-vector/,Nvidia Stock Could Rise 10-Fold On New $10 Billion Growth Vector,"Jun 09, 2024, 12:48pm EDT",Peter Cohan,"Nvidia stock has soared to about $1,200 a share — up 287% since the chip designer’s boffo May 2023 earnings report kindled generative AI fever.
With the stock dropping to about $120 per share Monday — as the company’s 10-for-1 split goes into effect — will its price ever return to $1,200?
Here are four reasons that could happen by 2026 — the first is new, and the last three are still valid possibilities since my May Forbes post:


One other risk is some business leaders’ inconsistent attitude toward generative AI.
How so? CEOs host competing fears. They are afraid of being left behind the generative AI boom even as the potential for AI hallucinations — returning false information in response to user prompts — could savage their company’s reputations.
This tension could make it difficult for them to implement high payoff generative AI applications, according to my new book, Brain Rush: How to Invest and Compete in the Real World of Generative AI.
Without that, demand for Nvidia’s technology could be difficult to sustain.
Were Nvidia stock — in an optimistic scenario — to keep rising at the 287% annual rate it enjoyed between May 2023 and last Friday, the company’s post-split shares could top $1,200 sometime in 2026, according to my analysis.
Here is a new source of growth to fuel that rise: Governments in Asia, the Middle East, Europe and the Americas are buying GPUs en masse as they build domestic computing facilities for artificial intelligence, noted the Journal.
Countries’ desire to develop sovereign AI by training large language models in their own language with citizens’ data is driving this demand. Underlying this imperative is “a quest for more strategic self-reliance amid rising tensions between the U.S. and China,” the Journal wrote.
Nvidia expects sovereign AI spending to boost its 2024 revenue by $10 billion, the company said last month.
If more countries feel the urge to splurge on bespoke generative AI capabilities, such spending could help Nvidia to diversify its revenue sources. “The question has been, how can they continue this momentum?” Angelo Zino, an analyst at CFRA Research, told the Journal. “Sovereign AI is a new lever out there in terms of generating higher revenue.”
In addition to sovereign AI demand for its chips, other drivers of Nvidia’s growth that I cited in a May Forbes post include:


Based on my interviews with dozens of business leaders, generative AI in companies is caught in a bipolar battle, Brain Rush noted.
Peer pressure forces CEOs to tell Wall Street how generative AI will transform their business. That pressure is reflected in a record level of mentions of the term “AI” in investor conference calls.
How so? A review of all the S&P 500 conference call transcripts from March 15 through May 23 counted 199 uses of the term “AI” — “well above the 5-year average of 80 and the 10-year average of 50,” according to FactSet.
At the same time, CEOs fear generative AI hallucinations could threaten their company’s reputation.
This fear is based in reality. For instance, Google’s AI advised people to add glue to pizza, Forbes careers contributor Jack Kelly noted. And Air Canada's AI chatbot made up a refund policy for a customer — and a Canadian tribunal forced the airline to issue a real refund based on its AI-invented policy, Wired reported.
This inconsistent battle has significant implications for business. Of 200 to 300 generative AI experiments the typical large company is undertaking, a mere 10 to 15 have been rolled out internally, and perhaps one or two have been released to customers. That’s according to Liran Hason, CEO of Aporia, a Manhattan-based startup offering guardrails to protect companies from AI hallucinations who spoke with me in a June 3 interview.
Unless high-payoff applications emerge from this process of generative AI experimentation, the wave of demand for Nvidia’s GPUs could taper off over the long run.
In the meantime, business and political leaders’ fear of falling behind in the generative AI race could drive high demand for Nvidia’s chips — and the company’s stock."
273,https://www.forbes.com/sites/bethkindig/2024/06/07/prediction-nvidia-stock-will-reach-10-trillion-market-cap-by-2030/,Here’s Why Nvidia Stock Will Reach $10 Trillion Market Cap By 2030,"Jun 07, 2024, 09:15am EDT",Beth Kindig,"Nvidia has a market cap of $3 trillion today. We believe Nvidia will reach a $10 trillion market cap by 2030 or sooner through a rapid product road map, it’s impenetrable moat from the CUDA software platform, and due to being an AI systems company that provides components well beyond GPUs, including networking and software platforms.
In 2021, I published an analysis on Forbes “Here’s Why Nvidia Will Surpass Apple’s Valuation in 5 Years” that stated: “Nvidia has a market cap of roughly $550 billion compared to Apple’s nearly $2.5 trillion. We believe Nvidia can surpass Apple by capitalizing on the artificial intelligence economy, which will add an estimated $15 trillion to GDP.”
Yesterday, Nvidia officially surpassed Apple in market cap, which means I delivered  on my prediction 2 years early.
This lends itself to the question, what do I foresee next for Nvidia, and how am I approaching this heavy hitter in AI. My firm champions full transparency by issuing trade alerts for every buy and sell we make; thus, I’ve included at the end a transparent discussion on how my firm is managing our position today.
But first, I unpack why I believe Nvidia can achieve an astonishing $10 trillion market cap by 2030. As you’ll see from the key points to my thesis, there is a bull case where a $10T market cap estimate in a little over six years’ time is not high enough.
On June 2nd, Jensen Huang made a very important statement about the future of AI that answers quite succinctly why Nvidia is on the verge of becoming the World’s Most Valuable Company:
“The days of millions of GPU data centers are coming. And the reason for that is very simple. Of course, we want to train much larger models. But very importantly, in the future, almost every interaction you have with the Internet or with a computer will likely have a generative AI running in the cloud somewhere. And that generative AI is working with you, interacting with you, generating videos or images or text or maybe a digital human. And so you're interacting with your computer almost all the time, and there's always a generative AI connected to that. Some of it is on-prem, some of it is on your device and a lot of it could be in the cloud [...]  And so the amount of generation we're going to do in the future is going to be extraordinary.” - Jensen Huang, CEO of Nvidia, Computex keynote
Today, there are tens-of-thousands of GPUs in data centers. By end of 2025, there will be hundreds-of-thousands of GPUs in data centers. Due to the market’s forward-looking nature, 2025 is getting close to being fully priced in. Here is a slide of what this looks like from the perspective of scaling the ethernet networking to support a million-plus GPU cluster.
Here’s what we know about Big Tech’s purchases, thus far. Microsoft is reportedly looking to triple its GPU supply to 1.8 million GPUs this year to meet elevated demand for Azure, while Meta has disclosed its GPU orders with an announcement for 150,000 H100s last year and 350,000 H100s or H100-equivalents this year. Musk announced that X’s 100,000 H100 cluster would be online in a few months and hinted at a possible 300,000 B200 GPU purchase.
According to Next Platform, Meta has roughly 600,000 GPUs deployed including previous generations, such as Ampere. This could include some from AMD, although AMD is more likely to ramp in 2025 and beyond. Right now, Nvidia has a $100 billion run rate on its data center compared to AMD’s $4 billion, therefore, any portion of GPUs from AMD is nominal as it stands for 2024.
If we look closer at semantics, Huang used the word “millions” and not the singular word “million,” and “data centers” rather than the singular “data center.” Therefore, my firm is making the assumption that companies like Meta will grow their data center GPUs by a minimum of 233% from 600K to 2M by 2030.
Broadcom shares a similar view, noting that management expects million-GPU clusters by 2027, compared to clusters with tens of thousands of GPUs today. This is even more bullish than Jensen Huang’s comments. Coming back to Meta, even with 600,000 H100 equivalents, it’s building clusters of 24,000 GPUs. In order to see singular clusters scale to the hundreds of thousands and millions, as Broadcom is predicting, we would need to see GPU shipments far in excess of those levels. This alone could get us to $10 trillion market cap based off Big Tech’s data centers, and we have not factored in the enterprise. The enterprise includes companies like the Fortune 500 or Global 2000 that build on-premise AI systems.
We can cross-examine this by looking at comments by CEOs, such as Lisa Su who stated AI accelerators will reach $400 billion by 2027. Nvidia has over 95% market share of data center GPUs but with custom silicon ASICs and more GPUs coming online, this is closer to 80% market share of AI accelerators.
If this estimate materializes, Nvidia’s data center segment will be at $320 billion in 2027, up from data center run rate of $90 billion today, with consensus at roughly $145 billion data center segment by end of calendar year 2025 (consensus is total revenue of $157.51, deducting for other segments).
In my analysis last month on the Blackwell architecture, I made the argument these estimates are too low and that my firm expects we will see a $200 billion data center segment by end of CY2025 propelled forward by the B100, B200 and GB200, including the following points: “Taiwan Semi’s CoWos capacity, which is essential for Blackwell’s architecture, is estimated to rise to 40,000 units/month by the end of 2024, which is more than a 150% YoY increase from ~15,000 units/month at the end of 2023. Applied Materials has boosted its forecast for HBM packaging revenue from a prior view for 4X growth to 6X growth this year.”
Data center segment for Nvidia of $320 billion by 2027 would result in 260% growth for Nvidia’s DC from where it stands today and up 120% from DC revenue estimates for end of CY2025. Using Lisa Su’s prediction, there would still be another three years to achieve the additional 120% needed to reach $10 trillion.
Industry analysts have a high-30 percent CAGR for AI accelerators through 2030 ranging from 36.6% to 37.4%. If we round this up to a 40-percent CAGR for Nvidia, then it’s not out of the question that Nvidia ends the decade with $800 billion from AI systems. That would be 450% growth from $145 billion at end of CY2025. This is the most bullish case scenario, which is why my current prediction is a bit more tame (for now) at predicting 233% growth by 2030.
Valuation is one of the most important points that confuses many investors (and short sellers) on why Nvidia’s stock continues to extend. We’ve called the valuation eerily low as most hypergrowth stocks would trade well above historical averages after a 500% move in 18 months. However, due to the 600% increase in earnings and 400% increase in revenue, the stock has remained well below its historical averages, while in fact, trading near October 2022 levels. To put this in perspective, on a forward PE basis, Nvidia was more expensive at the start of 2023 than it is today. Currently, it is trading at a forward P/E ratio of 44 compared to 62 in January 2023. You can view a clip here where I stated the stock was trading eerily low. This is still true today.
Many investors are surprised that Nvidia has surpassed Apple, and will pass Microsoft any day now to become the world’s most valuable company. Really, a gaming company? All of this from GPUs?
I want to make it abundantly clear that from a technological standpoint Nvidia has run circles around the FAANGs over the past 8 years. Apple has sat stagnant while Nvidia is in its Steve Jobs-era. What has resulted is that Nvidia is no longer a GPU company; it’s an AI systems company. The best ten or fifteen minutes an investor can spend in today’s market is understanding what exactly Nvidia accomplished to get to $3T, otherwise, it will not be clear how we can get to $10T.
Below, I take you through the key points from each generation, including the moment Nvidia transitioned from being a GPU chip company and a gaming company to become the AI systems company that is powering a $15 trillion economy.
For ease of reading, I’ve bolded key takeaways and also underlined the not-to-miss points:
In 2016, Pascal featured 7.2 billion transistors and increased CUDA cores compared to the previous generation, Maxwell. CUDA cores are parallel processors that can perform complex calculations and execute tasks on graphics cards much faster than a central processor. Parallel computing is at the heart of why Nvidia transitioned from gaming to AI, as GPUs can execute multiple tasks at the same time (concurrently). Each generation increases CUDA cores, which helps to accelerate what workloads are possible. CUDA cores distribute compute across thousands of cores to train large scale neural networks and can process big data at exponential rates.
Pascal was built on TSMC’s 16nm process and Samsung’s 14nm FinFET process with 16-bit floating precision, plus NVLink bi-directional interconnect to scale multiple GPUs for applications. TSMC’s CoWoS packaging was used to support high-bandwidth memory (HBM2).
Volta was built on a 12nm FinFET process with 32GB of HBM2, 900GB of bandwidth and 21 billion transistors. The breakthrough here was the introduction of Tensor cores for AI, machine learning and deep learning.
Tensor cores handle tensor and matrix operations, resulting in higher performance for neural networks. Tensor cores are capable of mixed-precision calculations, which contributes a significant amount to the “1,000 times increase in AI compute” quoted by Nvidia this past weekend. For example, switching from a 32-bit floating point to a 16-bit floating point can significantly increase training speed by requiring less memory and speeding up data transfer operations.
Due to introducing Tensor cores, Volta was the officially the first AI accelerator in history as it was designed for large scale training and connected up to eight GPUs. With Tensor Cores, Nvidia combined the benefits of parallel process and general-purpose compute from CUDA cores (which distributes tasks across thousands of cores) with the specialized acceleration offered from the matrix computations from Tensor Cores.
NVLink also saw an upgrade to 2.0 in this generation for higher data transfer rates.
Volta with Tensor Cores was launched in 2017 and further developed with two more releases launched in 2018. My firm began covering Nvidia’s AI thesis around this time, stating CUDA created an impenetrable moat for data center GPUs.
In 2019, Volta’s AI capabilities prompted me to say on my premium stock research site: “To be bold - I believe Nvidia will be one of the world’s most valuable companies by 2030. The research below organizes my investment thesis for the GPU-powered cloud and why I believe Nvidia will emerge as a clear leader.”
That premium research note was written on September 17th 2019 when Nvidia was at a $110 billion valuation.
Pictured Above: Y-charts, the market cap of Nvidia when I first stated it would become the world’s most valuable company at $110.3B compared to a $3T market cap today, for a return of 2,600% in less than five years.
Turing was built on the 12nm FinFET process with upgraded HBM2 memory (GDDR6) for higher bandwidth and 8-bit floating precision. Nvidia’s T4 GPUs delivered up to 40 times more performance than CPUs and are capable of real-time inference due to exponentially better throughput.
The architecture expanded to include more CUDA cores, second generation Tensor cores and the newly introduced RT Cores for real-time ray tracing. RT cores provide a boost to gaming and introduced professional visualization. The RTX platform was invented by Nvidia to “physically simulate light behavior in the world” and combines RT cores for ray tracing with Tensor Cores for AI.
For more information on ray tracing and RT cores, you can read my previous coverage on Omniverse here, or watch a 1-hour video where I interview Richard Kerris of Nvidia on the simulation platform.
If Tensor cores made Volta the first AI accelerator, then Ampere was the architecture that marked the moment Nvidia would no longer be considered a cyclical, gaming stock. I began to call Nvidia “secular” with this release and it’s when I doubled down on my conviction by taking my thesis from behind the paywall to the public, stating Nvidia would Surpass Apple in 5 Years. Nvidia not only became secular in revenue, but it’s secular-level gains have surpassed the world’s most celebrated software companies (every single one of them) since Ampere.
In fact, as one of the leading investors in semiconductors on record, I can assure you semiconductors have gone through a deep, cyclical trough industry-wide over the past 8 or so quarters while Nvidia powered higher with historical beats/raises. By providing in-demand AI systems, Nvidia has become decoupled from consumer spending and macro.
Pictured Above: Nvidia outperforms secular software and did not participate in the steep, cyclical trough over the past eight quarters like its semiconductor peers.
The A100 was built on TSMC’s advanced 7nm FinFET process node with 54 billion transistors. The third-gen Tensor cores featured new mixed-precision calculations, such as Tensor Float (TF32) and Floating Point 64 (FP64) with TF32 delivering up to 20X faster speeds for AI. By using automatic mixed precision, FP16 can be utilized for an additional 2X performance. Nvidia calls this the sparsity feature, which doubles throughput, runs 10X faster than the V100, and is 20X faster with sparsity.
What was special about the A100 is that it unified training and inference on a single chip, whereas in the past Nvidia was mainly used for training. With the specs described above, the A100 also offered a 20x performance boost.
As a multi-instance GPU, the A100 can make one GPU look like up to 7 GPUs for optimal utilization. This is key for cloud service providers, such as Amazon’s AWS, Google Cloud and Microsoft Azure, as it increased GPU instances by 7X.
The A100 was the first architecture where Nvidia was no longer simply a GPU chip company, but rather it marked the moment Nvidia became an AI systems company. The A100 offers the ability to scale-up multiple GPUs for one giant GPU using components such as third-gen NVLink to double GPU-to-GPU bandwidth, NVSwitch which is leveraged for fast data transfers, plus InfiniBand and SmartNICs following the Mellanox acquisition.
For more information on TSMC’s process nodes, reference the analysis “TSMC: April Sales Soar from Advanced Nodes.” The Mellanox acquisition was covered in-depth for my premium readers at time of acquisition here.
Hopper is when Wall Street became aware of Nvidia’s AI story. As you can see in this timeline, it was quite late for the Street to finally discover Nvidia is a promising AI stock!
The H100 GPUs and the DGX H100 server pods and super pods solved an important bandwidth issue and sped up algorithms by offering dynamic programming on GPUs to break down problems to simpler subproblems. The GPUs also boost bandwidth by 3X with SHARP in-networking computing and Infiniband Switches, and the H100 can leverage NVLink to connect eight H100s into one giant GPU for 640 billion transistors, 32 petaflops, 640GB of HBM3, and 24 terabytes per second of memory bandwidth.
The H100 has about 50% more memory and interface bandwidth than the A100. Memory later got a big boost in Blackwell, shipping this year.
The H100 stands apart with the leap in performance of 3X more performance than the A100 and is up to 6X faster. The A100 lacked support for FP8 compute at default whereas the H100 leverages a transformer engine to switch between FP8 and FP16, depending on the workload.
According to Nvidia, the H100 delivers 9X more throughput in AI training, and 16X to 30X more inference performance. The company also states in HPC application-specific workloads, the H100 is 7X faster. The goal of the H100 was not only to add more transistors and make the H100 faster, but to also offer function-specific optimizations. This is achieved through the transformer engine.
Although there are many highlights to consider with the H100, the biggest breakthrough was the transformer engine as it allowed generative AI to come to market. Transformers helped to define generative AI as the neural-network models apply self-attention to detect how data elements in a series influence and depend on one another.
Prior to transformer models, labeled datasets had to be used to train neural networks. Transformer models eliminate this need by finding patterns between elements mathematically, which substantially opens up what datasets can be used and how quickly.
The “T” in Chat-GPT stands for transformer and it was the H100 that created the GenAI breakthrough moment.
Blackwell is the architecture that I stated on Fox Business News will deliver the “ultimate fireworks by the end of this year.” In the analysis Blackwell and the $200B Data Center, I stated: “Blackwell is for the trillion+ parameter era of generative AI. The architecture is designed to support the largest language models today and is future-proofed […]”
The full analysis is worth a read as it spells out how Nvidia will drive growth through the end of 2025 and why I think current data center estimates are too low. In fact, I wrote that prior to the last earnings report and analysts are already proving me correct as FY2026 (ending Jan 2026) have been revised up by a whopping $20 billion since I wrote that only three weeks ago!
Pictured Above: Seeking Alpha, on May 23rd FY2026 revenue was estimated at $125 billion, it is now at $145 billion for an increase of $20 billion on the data center. This means that within three weeks, my prediction (that was written prior to earnings) for 60% higher data center revenue is quickly materializing, as in the last three brief weeks, the consensus has been revised so rapidly, the difference is only 38% now. On Bloomberg Asia, I also discussed why investors should pay close attention to intra-quarter revisions, which is exactly the reason the price moved in the past three weeks.
Unlike previous generations where the V100, A100 and H100 were the show-stoppers, it will be the GB200 and B200 that creates the biggest leap generationally. Therefore, I want to emphasize that I said the fireworks would come at the end of the year and into early 2025. The fireworks begin when the GB200 NVL36/NVL72 ships in late 2024 and then they continue with the B200 GPUs in early 2025.
The B200 GPU chipset due in Q1 of next year will deliver a 2.5X training improvement and 5X inference improvement over the H100. This is due to the B200 having 208 billion transistors compared to the H100’s 80 billion transistors.
The B200 will also have 20 petaflops of FP4 compared to the H100’s 4 petaflops of FP8 reaching 32 petaflops of FP8 in the DGX H100 systems. The difference is that the smaller bit size allows for an economical way to achieve more speed when giving up a small amount of accuracy doesn’t make a critical difference. As discussed, this also helps in the face of a slowing Moore’s Law. The B200 will have a second-generation transformer engine that supports 4-bit floating point (FP4) with the goal of doubling the performance and size of models the memory can support while maintaining accuracy.
The second-generation transformer engine in the Blackwell architecture will offer FP4. This is helpful because AI models are moving toward neural nets that lean on the lowest precision and yet still yield an accurate result. In this case, 4 bits double the throughput of 8-bit units, compute faster and more efficiently, and they require less memory and memory bandwidth.
TheGB200 NVL72 will deliver real-time trillion-parameter LLM inference, 4X LLM training, 25X energy efficiency, and 18X data processing. The GB200 will provide 4X faster training performance than the H100 HGX systems and will include a second-generation transformer engine with FP4/FP6 Tensor core. As stated above, the 4nm process integrates two GPU dies connected with 10 TB/s NVLink with 208 billion transistors.
NVLink Switch is a major component to the Blackwell upgrade. Fifth-generation NVLink enables multi-GPU communication at high speed, reaching 1.8 TB/s bidirectional throughput or 14X the bandwidth of PCIe for a single GPU.
Takeaway: Blackwell is the architecture that will make trillion+ parameter models possible, up from billion parameter models today.
If you’re exhausted reading that, imagine producing it in 8 brief years. Per the Computex keynote, from Pascal to Blackwell, the AI systems delivered “1,000 times increase in AI compute,” while simultaneously decreasing the “energy per token by 45,000X.”
Now, imagine cutting the time in half by producing four generations of AI systems in 4 years instead of 8 years.
In the analysis “Nvidia Q1 Earnings Preview: Blackwell and the $200B Data Center,” I stated that “should [the CUDA] moat become breached, the company’s rapid product road map is the first line of defense,” and later I also stated: ""The product road map is the single most important thing investors should be focused on. A good chunk of the AI accelerator story is understood at this point. What is not understood is how aggressive Nvidia is becoming by speeding up to a one-year release cycle for its next generation of GPUs instead of a two-year release cycle.""
After writing that, I realized it would be impossible to ask investors to focus on the upcoming road map if we did not look more closely at the road map that got us to $3 trillion. By now, it should be crystal clear that Nvidia is not a cyclical GPU chip company, rather it’s a secular AI systems and software platform company that has a near-monopoly in building supercomputers for the $15 trillion AI economy. If you are still not convinced that Nvidia is more than a GPU company, perhaps these two pictures can help.
Here’s a Blackwell GPU chip and a Hopper GPU chip — can easily fit in your hand.
Here’s what AI factories look like (or what I’m calling AI systems):
This past weekend, Nvidia announced the names of future generations: Blackwell Ultra, Rubin, and Rubin Ultra. The specifics of these future generations will be revealed at future GTC conferences.
Here is what you keep an eye out for in future generations:


Some of you reading this own Nvidia, and others do not. For those who do not own the stock, the most important question is not what market cap will Nvidia have by 2030, but rather, where is the stock going in the near-term.
My firm is an actively managed portfolio that publishes our trades in real-time. However, we are not financial advisors and each investor must decide for themselves whether to buy or sell a stock. What my firm does is simply state when we are buying or selling for unrivaled transparency. You will be hard pressed to find anyone else publish every single trade in real-time outside of professional fund managers (who are required to do so).
Since I first began covering Nvidia publicly in 2018, my firm has issued 9 buy alerts under $200 and we have been taking nominal profits along the way. We plan to take profits again in the $1225 to $1315 range. Nvidia is trading in this potential topping zone, at time of writing. Once price moves below $1035, it will signal that the anticipated reversal is underway. Once this happens, our process allows us to get more precise with identifying buy targets. Until then, we have a general range between $920 - $715. Keep in mind, this range can shift once a reversal is identified.
For some stocks, we get more aggressive and would try to time a buy in the lower range of the target zone, which would be around $715 for NVDA. However, due to the strength of its thesis, we will likely buy at the upper end of that target around $920.
If you had bought Nvidia January 1st 2022 instead of October 18th of 2022, your returns would be 387% instead of 1,034%. Therefore, 230% returns by 2030 would be phenomenal, but when entering at lower prices, the total return can multiply. For example, let’s say an investor can buy the stock at $900. In this hypothetical situation, the returns would be 350% compared to 230%. This is simple in concept yet is challenging to execute.
As of now, Nvidia stock should be watched closely between $1225 to $1315. It’s crystal clear that Nvidia owns the AI market, yet the stock will need the broad market to be aligned for its phenomenal run to continue. We’ve been tracking the fading Mag 7 since early March. At this point, the Mag 7 had become the Mag 4, when we stated…
“when the cycle leaders start to underperform, it tends to mark the start of a trend change. The FAANGs have been the undoubted leaders of this bull run, and we are now seeing them start to trend lower against the indexes.”
After the rally we saw this week, it’s worth noting that Nvidia is the only stock in the Mag 7 that is making new all-time highs. Amazon, Alphabet and Meta are making lower highs as of today.
Until we see more market leaders breakout, Nvidia remains the last one standing. Therefore, if Nvidia cannot break above the $1225 range, then the market is communicating that Nvidia’s  weaker peers may be influencing its price action. We’ve stated many times that Nvidia is a buy on the dips (as opposed to a buy on breakouts), specifically as “we brace for Blackwell by the end of the year.”
What’s worth noting is that while SPX, NDX and NVDA are making new highs, almost every other major index (RUT, DJI, NYA, RSP, XLF, XHB, to name a few), including the Mag 6, are not.
For Nvidia to continue moving up in a straight line means the stock will have to operate in a vacuum. This is unlikely, and thus we are waiting for the next dip before we buy again. Our current target, once again, is in the $920 - $715 range, although depending on market dynamics this could shift. We update our premium research members with real-time trade alerts and weekly webinars.
The boldest prediction I have made on Nvidia was to state in an analysis to my premium research members in September of 2019: “To be bold - I believe Nvidia will be one of the world’s most valuable companies by 2030. The research below organizes my investment thesis for the GPU-powered cloud and why I believe Nvidia will emerge as a clear leader.”
The world’s most valuable company at that time was Apple hovering at a $1 trillion market cap compared to Nvidia’s $110 billion market cap. As many fierce critics pointed out to me, I was not only predicting that Nvidia would skyrocket but that Apple and every other FAANG would falter. This was a challenging prediction to make as many things had to line up: 1) Nvidia must blow the doors off, and 2) every FAANG would have to plateau.
Here is what happened next:
All said and done, I will keep the 2030 deadline for the $10 trillion market cap, although I suspect, as with my other predictions, it will be delivered to you sooner.
Every Thursday at 4:30 pm Eastern, the I/O Fund team holds a webinar for premium members to discuss how to navigate the broad market, as well as various stock entries and exits. Beth Kindig offers weekly deep dives including lesser-known AI stocks, plus the team offers trade alerts and an automated hedging signal. The I/O Fund team is one of the only audited portfolios available to individual investors. Learn more here.
Resources:


If you would like notifications when my new articles are published, please hit the button below to ""Follow"" me.
Please note: The I/O Fund conducts research and draws conclusions for the company’s portfolio. We then share that information with our readers and offer real-time trade notifications. This is not a guarantee of a stock’s performance and it is not financial advice. Please consult your personal financial advisor before buying any stock in the companies mentioned in this analysis. Beth Kindig and the I/O Fund own shares in NVDA at the time of writing and may own stocks pictured in the charts."
274,https://www.forbes.com/sites/jackkelly/2024/06/07/us-regulators-are-looking-into-microsoft-nvidia-and-openai/,"U.S. Regulators Are Looking Into Microsoft, Nvidia And OpenAI","Jun 07, 2024, 10:12am EDT",Jack Kelly,"Regulators are looking for a balance between encouraging the development of cutting-edge artificial intelligence and mitigating the potential risks in the United States.
Regulating agencies are concerned about potential harmful biases, the lack of transparency in the algorithms and security risks, as well as preventing the monopolization of this critical technology.
The U.S. Department of Justice and the Federal Trade Commission reached an agreement on Thursday that paves the way for potential antitrust investigations into the leading positions held by Microsoft, OpenAI and Nvidia in the AI industry, the New York Times reported.
Under this deal, the DOJ will investigate Nvidia's dominance in AI chips, while the FTC will probe Microsoft's partnership with OpenAI and the substantial investments Microsoft has made in the AI startup. The probes are driven by fears that the immense market power and partnerships of AI leaders could stifle competition, innovation and consumer choice in this transformative technology space.
“Technological advances can deliver critical innovation—but claims of innovation must not be cover for lawbreaking,” said FTC chair Lina Khan in a joint statement of regulatory bodies last year. “There is no AI exemption to the laws on the books, and the FTC will vigorously enforce the law to combat unfair or deceptive practices or unfair methods of competition.”
The Biden administration has previously accused tech giants Apple and Amazon of engaging in anticompetitive behavior and has argued that Microsoft's acquisition of Activision Blizzard would lead to a monopoly in the gaming market.
There is scrutiny over Microsoft's multibillion-dollar investment in OpenAI and its partnership integrating OpenAI's technology into Microsoft's products. The FTC is investigating if this deal amounted to an undisclosed acquisition that should have undergone antitrust review.
Regulators are wary of a few big companies controlling key elements of the AI ecosystem, including high-performance Nvidia chips, cloud computing, data for training AI models and consumer-facing applications. They aim to prevent this concentration of power.
There are concerns that the AI industry is progressing rapidly without sufficient oversight or accountability mechanisms in place. Regulators want to ensure compliance with antitrust laws as the technology evolves.
Additionally, AI systems can perpetuate biases present in the data they're trained on, leading to unfair outcomes in areas like job applications or loan approvals. Regulators must ensure that AI is developed and used fairly.
Moreover, AI systems are also vulnerable to hacking or manipulation, which could pose a grave cybersecurity threat.
While the investigations aim to promote competition, they also risk disrupting the momentum of AI innovation, if not carefully balanced. The impact will depend on the specific antitrust remedies pursued and how the AI ecosystem adapts to potential shifts in market dynamics and regulations.
If the probes result in dismantling the AI firms or issuing large fines or restrictions, those consequences could hamper the tech industry’s ability to invest heavily in research and development. The immense computing power, data resources and talent pools of these tech giants have been key drivers of recent AI breakthroughs. Curbing their dominance may slow the rapid pace of AI advancement.
Major tech companies have been able to attract top AI researchers and engineers with lucrative compensation and resources. Antitrust actions that diminish their market power could make it harder to acquire and retain this critical AI talent against rivals or startups.
Investigations into partnerships, like Microsoft and OpenAI, could reshape the AI landscape. Collaborations that enable the sharing of models, data and expertise may be impacted, potentially fragmenting the AI ecosystem.
On the positive side, curbing the dominance of a few players could open up opportunities for new AI startups and companies to emerge. Increased competition may spur innovation from different sources rather than it being concentrated among the giants.
Antitrust actions could limit the market power of dominant AI companies, creating a more level playing field. This could open up opportunities for startups to compete more effectively and gain market share.
Smaller companies might gain better access to essential resources, such as data and computing power, if antitrust measures prevent large firms from monopolizing these assets. This could lower barriers to entry and foster innovation among smaller players.
A more competitive market could attract venture capital and other forms of investment to AI startups, as investors seek to capitalize on new opportunities created by the breakup or regulation of larger companies.
The probes signal heightened regulatory scrutiny on AI that could lead to new rules and compliance requirements. This regulatory uncertainty may make AI companies more cautious about partnerships, data practices and market behavior in the near term.
Given AI's importance for national security and economic competitiveness, there are concerns that weakening U.S. tech giants could undermine the nation's AI leadership relative to China. Policymakers will need to balance antitrust objectives with strategic interests."
275,https://www.forbes.com/sites/dereksaul/2024/06/05/nvidia-stock-hits-new-all-time-high-ahead-of-10-for-1-split/,Nvidia Stock Hits New All-Time High Ahead Of 10-For-1 Split,"Jun 05, 2024, 10:49am EDT",Derek Saul,"Nvidia shares just hit their highest price ever, but they are about to get a whole lot cheaper, and, no, that's not some bold prognostication for the world’s hottest stock: the artificial intelligence semiconductor chip designer is on the cusp of executing a stock split announced last month.
Nvidia shares jumped 3% to a new record of $1,199 on Wednesday, expanding its market capitalization to a whopping $2.95 trillion after Bank of America analysts upped its price target for Nvidia to a Wall Street-leading $1,500 per share, implying 25% further upside for the stock.
The rally comes just before Nvidia’s stock will split 10-for-1 after Friday’s market close, a move announced May 22 as part of its blowout quarterly earnings report, with shares trading at their new post-split price beginning next Monday.
That means Nvidia shareholders will receive nine additional shares for each share they owned prior to the split, which will cut the value of each share to a tenth of its prior value.
The split will trim Nvidia’s share price from its almost $1,200 share price Monday to around $120.
That’s pending any further movements for Nvidia’s stock, which is extremely price sensitive for a company of its size.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
Stock splits do not affect a company’s market capitalization or dilute the value of its shares, as the issuance of new shares is directly proportional. Historically, companies executed stock splits to make the purchase of its shares more accessible to investors, but it’s a more procedural move with today’s fractional trading, most directly assisting companies in awarding equity awards to its employees. Nvidia was the eighth company to announce a stock split this year, according to Bank of America research, and follows big technology peers Alphabet, Amazon and Tesla in splitting its shares over the last two years. Nvidia’s split comes after its share price exploded as investors flooded into the stock amid the generative AI explosion, as Nvidia is the primary seller of the semiconductor graphics processing units (GPUs) powering AI. Stocks tend to outperform the broader market following splits, returning 18% in the 12-month period after the move dating back to 2010, according to Bank of America, outstripping the S&P 500’s 13% gain.
Even after the 10-for-1 split, Nvidia’s stock will be more expensive than it was just four years ago, when it traded at $88 per share. Nvidia’s market value has skyrocketed from about $220 billion to almost $3 trillion over the four-year stretch, now trailing only Microsoft and Apple for the mantle of the world’s biggest company.
If Nvidia’s lower share price will lead to its inclusion in the Dow Jones Industrial Average. The Dow, one of the three most tracked American stock indexes, moves with its 30 constituents’ share prices, not market value, often leading it to exclude companies with pricier tickers. The Dow’s exclusion of Nvidia has contributed to its underperformance compared to the market cap-weighted S&P and Nasdaq, with the Dow’s 17% gain over the last 12 months trailing the S&P’s 25% and Nasdaq’s 27%.

"
276,https://www.forbes.com/sites/antoniopequenoiv/2024/06/05/nvidia-tops-apples-market-cap-becoming-the-worlds-2nd-most-valuable-company/,"Nvidia Tops Apple’s Market Cap, Becoming The World’s 2nd Most Valuable Company","Jun 05, 2024, 05:24pm EDT",Antonio Pequeño IV,"Chip designer Nvidia surpassed Apple’s market capitalization Wednesday for the first time ever, becoming the 2nd most valuable company in the world after a year of white-hot stock gains charged by an artificial intelligence craze.
Nvidia’s market cap reached past Apple’s at market close and now boasts a whopping $3.012 trillion market cap compared to Apple’s $3.003 trillion.
Nvidia, which surpassed the likes of Amazon and Google earlier this year, still trails behind Microsoft’s market cap of $3.1 trillion.
The chip maker recorded its highest share price ever Wednesday, closing up more than 5% at $1,224.40 and continuing its gains in after-hours trading by a fraction of a percent.
Nvidia’s milestone comes after the company started the year with $1.2 trillion in market value, less than half of Apple’s $3 trillion market value at the time.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
80%. That is the estimated share of semiconductor chips used to power generative AI technology that are designed by Nvidia, according to Goldman Sachs research.
Nvidia’s stock has more than doubled in price this year, trading up more than 147% since the start of January, when it traded around $481 per share. The chip maker’s success in the semiconductor market has been boiled down to its unique, fast-produced chips capable of powering artificial intelligence products. It has several big-name customers including Apple, Meta and Microsoft, the latter of which is estimated to account for 15% of Nvidia’s revenue. Nvidia’s lion’s share of AI-powering chips also gives it significant future earnings potential. The company’s stock will split 10-for-1 after market close on Friday, making its pricey stock far more accessible for investors.
S&P 500 Hits Record High After Apple And Nvidia Join $3 Trillion Market Cap Club (Forbes)
Apple Stock Hits $3 Trillion Valuation As AI iPhone Hype Builds (Forbes)
Nvidia Stock Hits New All-Time High Ahead Of 10-For-1 Split (Forbes)"
277,https://www.forbes.com/sites/karlfreund/2024/06/03/nvidia-matures-its-ai-while-amd-plays-catchup-at-computex/,"Nvidia Matures Its AI, While AMD Plays Catchup At Computex","Jun 03, 2024, 05:30pm EDT",Karl Freund,"Which is better?  Well Elon Musk said Sunday on X that his xAI start-up will build computing infrastructure to train AI using 100,000 of Nvidia’s H100 chips, which he said would be online “in a few months.”  He also said that the “Next big step would probably be ~300k B200s with CX8 networking next summer.”  At perhaps $40,000 (a low estimate) each, that would be $12B.
Of course, not everyone is so bullish on AI. An article Sunday in a major media platform says, in brief, that “the pace of innovation in AI is slowing, its usefulness is limited, and the cost of running it remains exorbitant.” The author goes on to build his case that “AI is in a massive bubble, a ton of startups will fail, a lot of the value of big tech companies is going to be destroyed, and this level of revenue for NVIDIA is not sustainable.”
I’m not going to argue his points, but I will say that Nvidia’s announcements at Computex are intended to broaden the value AI can deliver to more customers using more applications with faster, more affordable hardware. All this will increase Nvidia’s wallet share of what the company says will become a multi-trillion dollar market.
While the black-leather-clad Tech Superstar did not announce any breakthrough products, he did paint a picture of a maturing ecosystem of partners and developers that will take AI to the next level, enabling PCs, robots, industrial applications, and collaboration platforms that will undeniably improve productivity. And improved productivity leads to improved profits.
Nvidia CEO Jensen Huang’s speech before the Computex event in Taiwan laid out new software, new partners, and plans for post-Blackwell hardware.  In his keynote, Mr. Huang announced that the company is enabling the next industrial revolution, that the company’s ecosystem is broadening to reach new users and use cases. While Jensen did not directly address affordability, the new Broadwell architecture will reduce the cost of AI processing by over ten fold,  and cut the amount of energy required to power it.
“The next industrial revolution has begun. Companies and countries are partnering with NVIDIA to shift the trillion-dollar traditional data centers to accelerated computing and build a new type of data center — AI factories — to produce a new commodity: artificial intelligence,” said Huang. “From server, networking and infrastructure manufacturers to software developers, the whole industry is gearing up for Blackwell to accelerate AI-powered innovation for every field.”
For starters, Nvidia wants everyone to realize that the “AI PC” is not a new thing. They have been shipping GeForce RTX for over six years, enabling over 100 million PCs to run over 500 AI games and applications.
Next, Nvidia is doubling down on NIMS, the Nvidia Inference Micro Services it announced at GTC in March. NIMS not only makes it easier to deploy AI in minutes instead of months, they form building blocks from which customers can create new applications and solve new problems. If adopted, NIMS will help accelerate innovation and speed time to value.
Nvidia announced that NIM is now generally available for downloading on ai.nvidia.com and from Hugging Face, and is being integrated into tools for developers and infrastructure. Nvidia also announced that NIMS are now free for developers and researchers. Deploying NIMS in production requires an AI Enterprise license at $4500 per GPU
Huang pointed to the new vendors supporting Nvidia GPUs, with nearly twenty vendors featuring the Blackwell-Grace designs, and over 20 OEMs now supporting the Spectrum X networking solutions, which is now available.  Nvidia also introduced a new Grace-Blackwell design, called the GB200 NVL2 for mainstream LLMs, providing a more affordable entry-point for Blackwell development and adoption.
There was a lot of emphasis on robotic factories, and Nvidia highlighted four customer deployments, including Apple manufacturing partner Foxconn.
As for new hardware, Jensen announced that the Blackwell successor, called Rubin,   should ship in 2026. No details were provided. This would also be paired with the second generation Nvidia Arm CPU, called Vera.  Again, no details on Vera were announced.
In summary, Nvidia announced a slew of new technologies and partnerships, including:


Nvidia’s increased pace of development, from a 2-year cadence to a yearly calendar,  is forcing its rivals to react. AMD CEO Lisa Su announced an entire portfolio of new and upcoming products, from PCs to Laptops, to the edges, and data center CPUs and GPus.
Dr. Su announced a new Instinct GPU roadmap, with the MI325X accelerator by the end of this year. Notably, it will support 288GB of HBM3E fast memory. Following that, the MI350 series will debut in 2025, and is expected to perform up to 35 times better in inference compared with the current MI300 series. That will be followed by the MI400 series in 2026. The bottom line, at best, the MI350 will compare with the Nvidia Blackwell GPU about one year after Blackwell NVL72 is expected to ship. At which point Nvidia will be moving on to Rubin.
One piece of technology caught my attention: Block FP16, which can double the performance of floating point math without the quantization work required to use FP8. Looks very promising.  And the 5th-gen EPYC, which now commands over 30% share of server CPUs,  looks incredible.
While AMD has come a long way with its GPU for AI, the company is still trying to just catch up with Nvidia hardware. Never mind the massive gap in the ecosystem Nvidia enjoys.  Yes, AMD sees adding more HBM as a differentiator, but thats not enough to compete with Nvidia’s system-level differentiation.  Dr. Su did talk up the Nvidia-free Ultra Accelerator Link consortium, but there is no evidence it will compete with NVLink 5.0, which enables the incredible performance promised with Blackwell.
Meanwhile, Nvidia is driving AI adoption with advancements built on top of CUDA in robotics, manufacturing, 3D collaboration, drug discovery, healthcare, gaming, and enterprise applications.
The hardware battle has just begun, but we don’t see anything outside Google TPU that can really compete with Nvidia.
Disclosures: This article expresses the author's opinions and
should not be taken as advice to purchase from or invest in the companies mentioned. Cambrian-AI Research is fortunate to have many, if not most, semiconductor firms as our clients, including Blaize, BrainChip, Cadence Design, Cerebras, D-Matrix, Eliyan, Esperanto, GML, Groq, IBM, Intel, NVIDIA, Qualcomm Technologies, Si-Five, SiMa.ai, Synopsys, Ventana Microsystems, Tenstorrent and scores of investment clients. We have no investment positions in any of the companies mentioned in this article and do not plan to initiate any in the near future. For more information, please visit our website at https://cambrian-AI.com."
278,https://www.forbes.com/sites/investor-hub/article/nvidia-stock-earnings-stock-split-what-next-ai-investing/,"What’s Next For AI Investing: Taking Cues From Nvidia’s Earnings, Stock Split","May 29, 2024, 10:00am EDT",Sasirekha Subramanian,"Nvidia’s (NVDA) stock is holding its ground above $1,000, after the artificial intelligence (AI) bellwether reported yet another blowout quarter, set the stage for record future performance with a solid second-quarter outlook and announced a 10-for-1 stock split. Nvidia’s blockbuster earnings report clearly signals that the AI spending boom shows no signs of abatement. Even as Nvidia continues its meteoric rise pushing the boundaries, the stock is not the sole beneficiary of the AI growth narrative. The entire AI stock universe is firing on all cylinders, outperforming the U.S. and global indexes by more than 30% since the start of 2023. Although the outsized rally reminds investors of the dotcom bubble of the late 1990s (which burst in March 2000), the current valuations of AI leaders are far more reasonable and based on solid earnings growth expectations, unlike the stretched valuations and speculation-led investing frenzy that characterized the tech bubble era. On the other hand, is this truly the beginning of an AI-inspired “next industrial revolution” as extolled by CEO Jensen Huang? The article examines for clues in Nvidia’s first quarter earnings report.
With a $2.77 trillion valuation, Nvidia is the third most valuable company in the U.S. trailing Microsoft and Apple. Originally popular for its chips used in video games, NVDA exploded to prominence after a pivot to the data center business in the last few years and the emergence of ChatGPT, which highlighted the huge potential of generative AI.
Nvidia designs high-performance chips called graphics processing units (GPUs) or accelerators to run large computers that process data and power generative AI. While Nvidia has a near monopoly on AI chips with a market share exceeding 90%, its true competitive advantage stems from the proprietary CUDA software stack that has been optimized for wide ranging AP applications and workloads.
CUDA’s centrality to Nvidia cannot be overstated. For reference, AMD’s flagship AI accelerators–the Instinct MI300 lineup to accelerate AI in the data center market is pitted directly against Nvidia’s H100 and H200 GPUs. Reportedly, an Instinct MI300X GPU sells for approximately $15,000, while Nvidia’s H100 AI GPUs cost between $30,000 and $40,000. However, Nvidia’s GPUs are still in overwhelming demand because of CUDA. AMD is taking aim at NVDA’s dominance, by leveraging its open-source ROCm framework that competes directly with the de-facto CUDA standard. But NVIDIA
SPDR Dow Jones Industrial Average ETF Trust
 is fighting back with its relentless innovation that makes it difficult for developers to migrate away from CUDA.
Intel is playing catchup with its new Gaudi 3 chip, which will compete with Nvidia's prior generation H100 processor. Intel claims that the new Gaudi 3 accelerator delivers ""50% on average better inference and 40% on average better power efficiency"" vs. Nvidia’s H100 at ""a fraction of the cost.” The company’s large customers Meta, Microsoft and Google also have their own in-house AI chips.
Even as the battle for AI chip supremacy heats up, dethroning Nvidia will be a herculean challenge for competition, which at best can position itself as a viable alternative and collectively claim 10% to 12% of market share.
Bearing testimony to this, Nvidia reported record first-quarter data center revenue of $22.6 billion (representing about 87% of total revenues), up 23% sequentially and up 427% year over year, thanks to continued strong demand for the Nvidia Hopper GPU computing platform, with large cloud providers (including Amazon, Microsoft, Google and Oracle) contributing mid-40s percentage of data center revenue. Other than large cloud providers, data center startups like CoreWeave, buy Nvidia’s GPUs and rent these to customers on an hourly basis.
Probably in a bid to neutralize the cost advantage touted by competition, Nvidia signaled solid returns on investment (ROI) of 5x to 7x for customers spending on Nvidia infrastructure. For every $1 spent on Nvidia AI infrastructure, cloud providers have an opportunity to earn $5 in GPU instant hosting revenue over four years, the company said in its earnings conference call. The returns get even better with the company’s recent H200 hardware–an investment of $1 by an API provider in NVIDIA’s HDX H200 powering Meta Platforms’ Llama 3 AI Large Language model (LLM) can return $7 in revenues over the next four years.
The brain trust at Forbes has run the numbers, conducted the research, and done the analysis to come up with some of the best places for you to make money in 2024. Download Forbes' most popular report, 12 Stocks To Buy Now.
Profit: Net income for the quarter was $14.88 billion, or $5.98 per share, vs. year-ago $2.04 billion, or 82 cents. Adjusted earnings of $6.12 per share topped the $5.59 per share consensus. Nvidia beat EPS estimates in the last four quarters.
Revenue: Nvidia posted record first-quarter revenue of $26 billion, up 18% sequentially and up 262% year over year, and well above its guidance of $24 billion and ahead of the consensus estimate of $24.65 billion. Nvidia has topped revenue estimates in the last four quarters. For the second quarter, Nvidia targets a new all-time high revenue of $28 billion, plus or minus 2%. Analysts expect $26.6 billion in sales.
No demand slowdown on Blackwell transition
Nvidia deflated concerns of any demand slowdown as it transitions from its current Hopper AI platform to the more advanced next generation Blackwell system. Blackwell has an inference capability that is 30x of Hopper’s, while consuming 25x less cost and energy. So, the analysts were worried if customers would hold off on Hopper orders because of the upcoming Blackwell launch. However, Nvidia said it witnessed increasing demand for Hopper through the quarter (which is after it announced Blackwell) and expects demand to outstrip supply for some time as the transition happens. Besides, Blackwell systems are designed to be backward-compatible, making the transition easy for customers. The demand for both Hopper and Blackwell platforms is well ahead of supply and expected to continue well into the next year.
Blackwell To Bring In More Growth
The Blackwell platform is in full production with shipments slated to begin in the second quarter, and ramp in the third quarter with data centers standing up for customers in the fourth quarter. Nvidia expects to see a lot of Blackwell revenues this year.
Customers Should “Performance Average” As Nvidia Accelerates To A New “One-Year Rhythm”
Nvidia’s founder and CEO Jensen Huang said that there is another chip in the cards after Blackwell and Nvidia is on a new “one-year rhythm” to release new chip architecture, which marks a significant acceleration from its two-year cycle.
Huang signaled that staying ahead in the AI race was a strategic imperative rather than aiming for merely incremental improvements. He stated, “do you want to be repeatedly the company delivering groundbreaking AI or the company delivering 0.3% better? And that's the reason why this race, as in all technology races, the race is so important.”
In a question to how customers who have spent billions of dollars on existing products would respond to the evolving landscape of newer offerings that quickly surpass the capabilities of existing ones and outpace the rate at which the value of the existing products depreciates, Huang suggested that performance-averaging will be the smart way for businesses to deal with “a whole bunch of chips coming at them” when making and saving money are immediate priorities and time is of essence.
Nvidia’s 10-for-1 stock split would render its shares more accessible to a broader investor community, including the company’s employees. Shareholders of Nvidia’s common stock as of the close of market on June 6, will receive nine additional shares for each share owned. The additional shares will be distributed after the close of market on June 7 and trading will commence on a split-adjusted basis at market open on June 10.
With the stock split, NVDA may be targeting an inclusion in the prestigious Dow 30 index, following Amazon’s lead. The Dow 30 index is price-weighted, meaning stocks with higher prices have a greater influence on the index. So, the Dow 30 does not typically include very high-priced stocks, as price changes in these stocks could skew the index and render it a less reliable gauge of overall market performance.
The AI leader also raised its dividend by 150% to 10 cents from 4 cents. The new dividend of $0.01 on a post-split basis will be paid on June 28 to all shareholders of record on June 11. Nvidia stock goes ex-dividend on June 10 when it commences its split-adjusted trading. So, the possibility of a stock rally until the ex-dividend date is likely. A stock split only changes the stock price and the share count, but does not affect the total value of the investor’s holdings. So, for investors interested in the dividend and the Nvidia growth story, NVDA offers a buying opportunity ahead of the stock split.
Overall, the median price target for Nvidia on Wall Street rose around $150 to $1,250, after Nvidia posted stellar first-quarter earnings.
Cantor Fitzgerald analyst C.J. Muse, who rates the Nvidia stock at “overweight,” raised his price target on the stock by $200 to $1,400, based on Blackwell being on track for revenues in the second half and the clear line of sight for sequential revenue growth through the October/January quarters.
Morgan Stanley analyst Joseph Moore, affirmed his buy rating on Nvidia and raised the price target from $1,000 to $1,160, citing the company’s strong growth even through product transition. He noted that Nvidia was the clearest way to get exposure to the AI sector.
Bank of America analyst Vivek Arya, who has a Buy rating on Nvidia, raised the price target from $1,100 to $1,320. He noted that Nvidia generates more than 50 cents in free cash flow for every dollar in sales, allowing for potentially higher shareholder returns.
Stop chasing shadows in the market. Forbes' expert analysts have pinpointed the 12 superstars poised to ignite returns in 2024. Don't miss out—download 12 Stocks To Buy Now and claim your front-row seat to the coming boom.
Nvidia founder and CEO Jensen Huang is upbeat about the prospects for AI, and the role of Nvidia in the long runway for AI growth, as the computing paradigm shifts from “instruction-driven software to intention-understanding models.”
“The next industrial revolution has begun—companies and countries are partnering with Nvidia to shift the trillion-dollar installed base of traditional data centers to accelerated computing and build a new type of data center —AI factories—to produce a new commodity: artificial intelligence,"" said Jensen Huang. ""AI will bring significant productivity gains to nearly every industry and help companies be more cost- and energy-efficient while expanding revenue opportunities."" His comments are somewhat in line with expert estimates for AI spending to rise to $1 trillion between 2023 and 2030.
The next phase of the AI growth story will go beyond the cloud, according to Huang. Generative AI will expand well beyond cloud service platforms to multiple multibillion-dollar verticals, including consumer Internet companies, and enterprise, Sovereign AI, automotive and healthcare customers.
With the rise in global data flows, Sovereign AI is becoming a critical consideration for nations. Sovereign AI refers to a nation's capabilities to produce artificial intelligence using its own infrastructure, data, workforce and business networks to navigate the complex landscape of data privacy and security. Nations procure and operate Sovereign AI clouds in collaboration with state-owned telecommunication providers or utilities–or rely on local cloud partners to provide a shared AI computing platform for public and private sector use.
Nvidia expects its Sovereign AI revenue to approach the high single-digit billions this year, from nothing last year, by helping jumpstart the AI ambitions of nations across the world.
The company expects automotive to be its largest enterprise vertical within the data center segment this year, driving a multibillion revenue opportunity across on-prem and cloud consumption. For the first quarter, Nvidia’s automotive revenue was $329 million, up 17% sequentially and up 11% year over year. The sequential growth was driven by the ramp of AI cockpit solutions with global OEM customers and strength in self-driving platforms.
Tesla’s latest autonomous driving software FSD (full self-driving) system version 12 is powered by Nvidia’s chips. As the FSD-12 technology uses video learning and generative AI to predict the path and drive the car, it needs to process huge amounts of video data. As part of this initiative, Nvidia supported Tesla's expansion of its FSD training AI cluster to 35,000 Hopper H100 GPUs.
Xiaomi’s first electric vehicle, the SU7 sedan is built on the Nvidia Drive Orin, which is the company’s AI car computer for software-defined autonomous vehicle fleets. Nvidia Drive Thor, the successor to Orin, has already procured design wins with EV makers, including BYD, XPeng, GAC's Aion Hyper and Neuro. Thor is slated for production in vehicles in 2025.
Strong and accelerating demand for generative AI training and inference should propel data center growth. Training should continue to scale as AI models learn to be multimodal, understanding text, speech, images, video and 3D and learn to reason and plan. Nvidia’s Hopper platform continues to drive exemplary data center growth for the company. The incredible complexity of growing inference workloads, powered about 40% of Nvidia’s data center revenues in the trailing four quarters.
The next generation Blackwell platform, which enables real-time generative AI on trillion-parameter large language models, is in full production.
In the first quarter, Nvidia started shipping its new Spectrum-X Ethernet networking solution that enables Ethernet-only data centers to accommodate large-scale AI. Spectrum-X is ramping in volume with multiple customers, and should ramp to a multibillion-dollar product line within a year.
Nvidia Inference Microservices (NIM) is the new software offering that delivers enterprise-grade optimized generative AI to run on CUDA everywhere, from the cloud to on-prem data centers to RTX AI PCs.
With all the advantages mentioned above, Nvidia seems poised for the next wave of AI growth.
Investor sentiment appears positive towards Nvidia, AI and AI-related stocks. In the past month, NVDA shares have gained about 32% and the Global X Artificial Intelligence & Technology ETF (exchange traded fund) that tracks the performance of the Artificial Intelligence and Big Data Index, by using full replication technique, is up slightly more than 6%. It should be noted that the Artificial Intelligence and Big Data Index, which holds 85 AI-related stocks, outperformed both the S&P 500 and the MSCI World indexes (designed to track broad global equity-market performance) by more than 30% since the beginning of 2023.
Goldman Sachs says that investors are already looking beyond Nvidia, at IT services and software stocks that build generative AI advancements into their product offerings. The next phase of growth should come from AI infrastructure companies, positioned to benefit from the buildout of AI-related infrastructure. This includes semiconductor companies, cloud providers, computer/network equipment makers, data center real estate investment trusts (REITs), utilities, and cyber security providers.
Less availability or lack of high-quality data to train an AI model, and the high cost of developing and implementing AI solutions appear to be major roadblocks for AI. For Nvidia particularly, its major clients turning into competitors poses the biggest challenge besides the high price of its GPUs vs. competition.
Nvidia appears well poised to ride the next wave of AI growth with relentless product innovation and new business verticals. Investors interested in Nvidia’s dividend can consider buying shares before the stock splits. As to what’s next in AI investing, a good watch point would be AI infrastructure companies, positioned to benefit from the buildout of AI-related infrastructure. This includes semiconductor companies, cloud providers, computer/network equipment makers, data center real estate investment trusts (REITs), utilities, and cyber security providers. Any slowdown in demand for its AI chips or a drop in AI spending by the big tech will be red flags to watch out for in NVIDIA’s future earnings reports.
How does Nvidia’s performance impact the AI industry?
Nvidia’s performance is widely interpreted as the barometer of the nascent AI economy, as the computing paradigm shifts from instruction-driven software to intention-understanding AI models.
What key factors should investors consider regarding Nvidia’s earnings reports?
Investors should look for clues of any slowdown in AI spending by the large-cap technology sector that drives more than 40% of Nvidia’s data center revenues. A drop in demand for the company’s AI chips is another critical watchpoint.
What is investor sentiment towards Nvidia and the AI industry?
Investor sentiment appears positive towards Nvidia and AI-related stocks. In the past month, NVDA shares have gained about 32% and the Global X Artificial Intelligence & Technology ETF, which tracks the performance of the Artificial Intelligence and Big Data Index, is up slightly more than 6%.
The brain trust at Forbes has run the numbers, conducted the research, and done the analysis to come up with some of the best places for you to make money in 2024. Download Forbes' most popular report, 12 Stocks To Buy Now."
279,https://www.forbes.com/sites/dereksaul/2024/05/29/how-big-can-nvidia-get-as-it-threatens-apple-and-a-3-trillion-market-cap/,How Big Can Nvidia Get As It Threatens Apple And A $3 Trillion Market Cap?,"May 29, 2024, 04:00pm EDT",Derek Saul,"Just a decade after being valued at less than 2% of Apple, artificial intelligence sweetheart Nvidia is within striking distance of surpassing the iPhone maker as the world’s second-most valuable company, and even Microsoft as the largest company on earth, a remarkable rise for a chip designer with a small consumer-facing footprint.
Nvidia’s market capitalization of $2.8 trillion is just 3% shy of Apple’s $2.9 trillion valuation and 11% shy of Microsoft’s $3.2 trillion.
That’s a stunning turn from the end of 2014, when the market valued Nvidia at just $11 billion, Apple at $643 billion and Microsoft at $382 billion.
Having already surpassed other more established technology titans Google and Amazon earlier this year in market cap amid its 130% year-to-date stock rally, Nvidia would need just another $35 increase in its share price to surpass Apple by total value and a $140-per-share rally to surpass Microsoft.
If recent performance is any indication, that’s a doable feat, as Nvidia’s share price has increased by $70 or more in two of the last four trading days alone.
There are plenty of believers on Wall Street who think Nvidia can surge past a $3 trillion valuation, with about a third of analyst price targets tracked by FactSet above the $1,220 per share Nvidia stock needs to become the third-ever company to ever crack that $3 trillion threshold.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
If Nvidia stock were to perform anything close to its recent market-leading returns in the future, it would become by far the most valuable company in the world—a perhaps foolish bet considering though the most bullish mainstream analysts only forecast 23% upside for Nvidia’s stock over the next 12 months, though it’s a fun thought exercise. If Nvidia stock were to match its 195% return over the last 12 months from now through May 2025, it would achieve a market cap of $8.4 trillion, a valuation equivalent to the combined market values of Apple, Microsoft and Google.
Nvidia is the biggest player in the hottest market on the planet right now, designing an estimated 80% of the semiconductor chips powering generative AI technology, according to Goldman Sachs research. That gives Nvidia immense future earnings potential, especially considering its immense pricing power in dangling its technology to its chip-hungry customers like Apple and Microsoft. The early impact of the AI revolution on Nvidia’s bottom line is apparent, as its $54 billion in earnings before interest, taxes, depreciation and amortization (EBITDA) over its four most recent quarters is six times higher than its $10 billion EBITDA for 12-month period ending in April 2023. Meanwhile, Apple has dealt with stalling profit growth, with its free cash flow down by 20% in its most recent quarter. More colorfully, if you’re to believe we’re in the early stages of a “new industrial revolution” thanks to AI, as Nvidia’s CEO Jensen Huang analogized on his company’s earnings call last week, investing in Nvidia is akin to investing in a company who built crucial components in nearly all steam engines in the 1700s. “We are at the beginning stages of what we refer to as the Mother of All Cycles,” according to Rosenblatt analyst Hans Mosesmann, who has had a $1,400 price target for Nvidia since February, when its stock traded at below $800.
This week, Huang became one of just 16 people in the world to own a fortune of over $100 billion, according to Forbes’ calculations. It’s a remarkable rise for Huang, a majority of whom’s net worth is tied to his 3.7% stake in Nvidia, considering he was worth just $1.7 billion as of Forbes’ 2016 ranking of the 400 richest Americans.
"
280,https://www.forbes.com/sites/dereksaul/2024/05/28/nvidia-ceo-huang-briefly-a-100-billion-man-as-ai-firm-closes-in-on-apple/,Nvidia CEO Huang Briefly A $100 Billion Man As AI Firm Closes In On Apple,"May 28, 2024, 04:29pm EDT",Derek Saul,"Nvidia stock’s rally to yet another major milestone Tuesday made its CEO Jensen Huang a centibillionaire for part of Tuesday afternoon, as the artificial intelligence technology architect closes in on Apple to become the second-most valuable company in the world.
Huang, Nvidia’s co-founder and largest individual shareholder with a 3.7% stake, became $6.6 billion richer Tuesday, with his net worth hitting above $100 billion for the first time, becoming one of only 16 people in the world worth over $100 billion, according to Forbes’ estimates (Huang’s net worth ended the day just below $100 billion, at $99.9 billion).
The record-setting day for Huang’s fortune unsurprisingly came as Nvidia’s stock soared to an all-time high, surging past a $1,100 share price for the first time as shares rose 7% to $1,141.
Tuesday’s rally tacked on about $190 billion to Nvidia’s market capitalization, which shot up to $2.8 billion.
That’s just $108 billion shy of Apple’s $2.9 trillion market value and less than $400 billion shy of Microsoft’s $3.2 trillion market cap, a stunning transformation from Nvidia’s sub-$100 billion valuation just five years ago to becoming the leader of Wall Street’s next frontier.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
More than $600 billion. That’s how much market cap Nvidia has gained just over the last month, mostly since releasing its latest batch of quarterly earnings last Wednesday. That gain, accomplished over just three trading sessions, is larger than the entire market values of all but 11 public companies globally, outranking those of Tesla, JPMorgan Chase and Walmart.
“Every company that wants to do something with AI has to pay for Nvidia's expensive GPU chips,” Yardeni Research founder Ed Yardeni wrote in a Tuesday note to clients, summarizing the lucrative stranglehold Nvidia enjoys in the rapidly growing graphics processing unit (GPU) technology space. Nvidia designs a reported of the GPU semiconductor chips necessary to power AI.
Based in Santa Clara, Nvidia is the quintessential Silicon Valley firm, tracing its roots to the humble origins to a conversation at a Denny’s diner in 1993, keeping its founding CEO Huang, perhaps best known publicly for his patented black leather jacket look, at the helm for its entire 31-year existence. Huang is still $105 billion poorer than the world’s richest person Bernard Arnault and $100 billion poorer than the richest person in the U.S., Jeff Bezos. Nvidia’s market value, fame and financial performance have all grown exponentially over the past two years as public interest in AI boomed. Nvidia’s profit of $15 billion during its first full quarter of 2024 was about 40 times higher than its net income during 2019’s comparable period and even seven times greater than last year’s comparable profit."
281,https://www.forbes.com/sites/garthfriesen/2024/05/25/ai-demand-boosts-nvidia-earnings-challenges-commodities-and-utilities/,Nvidia’s AI Demand Is Booming But Utilities Are Struggling To Keep Up,"May 25, 2024, 08:00am EDT",Garth Friesen,"Nvidia stock shares surged more than 10% following the company's latest earnings report, which exceeded investor expectations and demonstrated the ongoing high demand for its artificial intelligence chips. Notably, the company's data center revenue soared by an impressive 427% during Q1 of fiscal year 2025, which ended on April 28.
While Nvidia benefits from the exponential growth in AI, other areas of the economy associated with the boom — such as certain commodities and electric utilities — are struggling to keep pace with the expansion.
Demand for Nvidia's graphic processing units is driven by the major cloud services providers —Amazon Web Services, Microsoft Azure, Google Cloud, and Oracle Cloud. According to Nvidia, these clients accounted for approximately 45% of Nvidia's $22.56 billion in data center sales in Q1 of FY 2025. ""The demand for GPUs in all the data centers is incredible — we're racing every single day,"" Nvidia CEO Jensen Huang said in a call with investors following the earnings release.
These cloud services giants are in an AI arms race, investing hundreds of billions of dollars in new data centers. According to an analysis from JP Morgan made available to institutional clients in February, global data center annual capital expenditure is expected to rise from around $300 billion in 2024 to more than $500 billion by 2027.
Data centers are electricity hogs, and the availability of power is a major bottleneck to their global expansion.  Aging and inefficient electrical grids are not able to scale to keep up with demand. The rapid growth of data centers coincides with the increased adoption of electric vehicles, driving power demand far beyond expectations.
Electricity consumption from data centers and artificial intelligence could double by 2026, according to a report from the International Energy Agency. The IEA predicts that “after globally consuming an estimated 460 terawatt-hours in 2022, electricity consumption from data centers could reach more than 1,000 TWh in 2026.” This doubling of demand is roughly equivalent to the entire electricity consumption of some industrialized countries, such as Japan.
Existing U.S. electrical utility companies are expected to step up. Nine of the top 10 said data centers were a primary source of customer growth, leading many of them to increase their forecasts for capital expenditure plans and demand, according to a Reuters analysis of Q1 earnings reports. Yet many of these companies require infrastructure investments to materially increase their output.
The growth of the grid means that utilities and their customers are scrambling to purchase all types of electrical equipment. One company benefiting from utility company investment spending is Eaton. According to the company's website, it aims to ""future-proof infrastructure, optimize distributed generation, automate processes and reduce risk."" Eaton stock has responded to the expansion of the grid. Its stock is up 42% so far this year.
A larger electrical grid also means more demand for natural gas to power the utilities. According to Goldman SachsGoldman Sachs' report published in April, natural gas is expected to supply 60% of the power demand growth from AI and data centers, while renewables will provide the remaining 40%. Fortunately, the U.S. is blessed with plenty of domestic natural gas to feed the anticipated increase in demand.
However, some critical metals needed in the build out are harder to source. The exponential growth of AI computing capacity in the coming decade has a direct impact on the amount of copper required to support this growth. According to a Man Institute report, “the potential incremental copper demand from the U.S. data center buildout could range between 0.5% and 1.5% of global copper demand. Although this might seem insignificant, even a 1% shortage could plunge the market into a significant deficit.”
The copper market is responding to the anticipated imbalance between supply and demand. Copper prices have soared 22% in the last three months. Similar to the situation with the electrical grid, additional supply is not poised to meet demand. Aging mines, lack of exploration success, environmental and water issues, and heightened political risk in key producing regions are challenging supply.
More AI demand means more need for data centers, which in turn means more need for electricity. More electricity means greater demand for copper and natural gas. Certain commodities and utilities are riding the wave of the AI boom, along with Nvidia. Nvidia's earnings suggest AI growth remains robust, and the stress on the infrastructure to support that growth will only increase."
282,https://www.forbes.com/sites/petercohan/2024/05/25/nvidia-stock-tops-1000-blackwell-could-aid-10-fold-rise-by-2026/,"Nvidia Stock Tops $1,000 As Blackwell Could Aid Tenfold Rise By 2026","May 25, 2024, 02:51pm EDT",Peter Cohan,"About a year ago, Nvidia released an earnings report that captured the attention of investors around the world. Since then, the chip design giant’s stock has soared 248%.
On May 23, Nvidia realized my April prediction in a Forbes post — a strong quarterly earnings report could propel the company’s stock price past $1,000 a share.
Nvidia — which contracts out chip manufacturing — not only beat expectations and raised guidance. The company also announced a 10-for-1 stock split, bought back billions of dollars’ worth of stock, and considerably boosted its dividend.
When the split sends Nvidia’s stock price down to around $100 a share, I expect the company could sustain the high growth that sent its stock price soaring for years to come.
Here are three reasons its shares could again top $1,000 post-split:


Nvidia faced an exceptionally high growth hurdle as the anniversary of its mind-blowing earnings report released in May 2023 neared. That announcement, which covered Q1 of the company's 2024 fiscal year, featured a 54% better-than-expected growth forecast.
Below are the key investor expectations for Nvidia in the most recent quarter ending April 28, 2024 (Q1 of fiscal year 2025) and how much the company beat those ambitious targets.


Nvidia also decided to take a bow for investors such as Warren Buffett who like stock buybacks and dividends.
To that end, the chip designer announced a 10-for-1 stock split effective June 7, bought back $7.7 billion of the company’s shares, and more than doubled its per-share dividend from four cents to 10 cents based on the current share count, noted the Wall Street Journal.
“We are fundamentally changing how computing works and what computers can do,” Nvidia CEO Jensen Huang said in a conference call with analysts. “The next industrial revolution has begun.”
The most important question a CEO must answer is: Whither future growth? In my book, Disciplined Growth Strategies, I described a canvas on which leaders can paint their future growth trajectories. This canvas consists of five dimensions of growth — new or current customer groups, products, geographies, capabilities, and culture.
In high-tech markets, business leaders must place the right bets on future growth opportunities before their rivals beat them to the punch. Since technology product life cycles are short and rivals are quick to copy innovations, such bets are vital for sustaining rapid growth.
For Nvidia, the key dimensions of growth are customer group and product. Here are the company’s key bets on each dimension:


Under Huang’s leadership, Nvidia’s share of the AI chip industry has reached somewhere between 80% and 95%, I wrote in a February 2024 article on Forbes.
Signs of Nvidia’s market power include:


The biggest risk to Nvidia’s continuing stock market rise is the company’s dependence on Huang — who at some point will no longer be the company’s CEO.
As I described in my forthcoming book, Brain Rush, Huang is the rare CEO — accounting for fewer than 0.4% of founders who can turn an idea into a public company worth billions (or in Nvidia’s case, trillions) of dollars.
He has done a great job in five of the six key leadership tasks that distinguish such leaders:


The final leadership task is preparing the next CEO. Is Huang developing a successor who could do the CEO job as well or better than he is doing it now? An opinion column in TechNewsWorld argued Huang would keep serving as CEO for a decade and gradually replace himself with “digital Jensen Huangs.” I can’t help but wonder whether this is a joke.
Nvidia officially considers succession planning to be important. For example, it operates a management development and exposure program involving more than “60 senior managers working directly with the CEO to execute corporate strategies,” according to the company’s 2024 annual report. Nvidia also offers executive development programs including “ training courses, mentoring, peer coaching, and ongoing feedback.”
In the year since Nvidia’s boffo Q1 FY 2024 report released in May 2023, the company’s stock has risen 248%.
Nvidia stock — after splitting 10-for-1 early in June — could rise from $100 to $1,000 by 2026. This optimistic scenario assumes Nvidia keeps beating growth expectations and raising its forecasts — resulting in a 248% annual increase in the company’s stock price over the next two years.
Optimism is certainly justified, according to one analyst. “I say it's the most important company in the world, as far as innovation,” Jim Roppel, founder of The Roppel Report, told Investor’s Business Daily.
“Nvidia has managed to leapfrog from one tech trend to another, beginning with video games, then moving to things like automotive applications and now generative AI,” he added.
Generative AI’s productivity benefits could crimp inflation — sustaining strong demand for Nvidia’s products. “The major innovation cycle is healthy and thriving,” he told IBD. “I really think this golden goose is ripping. The golden goose is working overtime here.”
Another analyst is less sanguine. Nvidia stock could drop unless it beats expectations by at least $1.5 billion in future quarters, according to a May 20 client note by Susquehanna analyst Christopher Rolland that appeared in a Journal report.
Another risk to investors is: “Moves from competitors—including in-house chip efforts at its own largest customers,” the Journal noted.
Someday the Nvidia bears will be right. That day could be a decade away."
283,https://www.forbes.com/sites/stevendesmyter/2024/05/23/nvidia-and-the-diversification-paradox/,NVIDIA And The Diversification Paradox,"May 23, 2024, 12:31pm EDT",Steven Desmyter,"How diversification is the market's end-game
Anyone who knows anything about investment will tell you that diversification is a sine qua non of portfolio construction. Richard Barclay spoke about diversification as the “only free lunch” in finance at our European Investment Symposium last week. There is, though, a stark divide between the theoretical benefit of diversification and the brutal facts of the last few years of heavily concentrated market leadership. As I posted the other day, NVIDIANVIDIASPDR Dow Jones Industrial Average ETF Trust has been responsible for fully a quarter of the S&P’s c. 11.5% YTD performance.
I write this in the aftermath of another NVIDIA earnings beat. It feels like the company did just enough – and not much more – to push the prospect of what must surely be an eventual disappointment three months down the line. This morning saw the stock up 8% or so, moving through $1000, but there was a lack of the wow factor that has surrounded previous releases. That “just enough” felt like it wasn’t enough to give bulls a reason to push the stock dramatically higher (remember it jumped 25% post the Q2 results last year), but nor is there much for bears to build a case around. If nothing else, the 10-to-1 stock split should be good for retail flow…
My colleague Sumant Wahi has an impressive pedigree when it comes to speaking about this market. Prior to his investment career, he worked at the cutting edge of the semiconductor industry. I sat down with him soon after NVIDIA released its numbers to chew over the details. Sumant pointed me towards an interesting observation that goes beyond the numbers.
Jensen Huang is developing an increasingly rigorous – not to say desperate – focus on monetization. He spoke about the fact that NVIDIA is providing clients with complete systems solutions, building out AI factories. He spoke about the wave of Gen AI startups that are being unleashed – 15-20,000, he said – all of whom would have voracious demand for chips. It all felt like an attempt to pre-empt questions (which definitely came once the sell side were let loose on the earnings call) about the possibility of waning demand, over-ordering, chips sitting idle in data centres. Decoding the dynamic between analysts and management, the message was clear: how would this look different if it was a bubble? What is the AI killer app (something I’ve written about before)? How does the company continue its remorseless raising of the bar?
You might have seen this graphic before – it’s out of date already (the market cap is actually more than $2.5T post-results). But Sumant used it to make a different point. He sees significantly greater value in the companies on the right-hand side of the chart, and indeed in companies too small even to feature on the scale. The competition is coming for NVIDIA.
All this brings us back to the question of diversification.
Markets are self-regulating entities. While NVIDIA will do everything it can to ensure its position of leadership, it is inevitable that others will begin to chip away at its hegemony. It’s how markets work, it’s the central dynamic of competition, it’s one of the reasons that diversification is such a powerful tool. Nimble specialists will find ways of improving or undercutting or disrupting NVIDIA’s position. People have been holding up Cisco as a warning for NVIDIA to heed; IBMIBM is perhaps more relevant just now. IBM totally dominated the personal computing market in the early years. It grew cumbersome and bloated, and MicrosoftMicrosoft and AppleApple profited.
It’s a powerful, practical example of the sometimes abstract theory of diversification, further evidence that you shouldn’t put all your eggs in one basket."
284,https://www.forbes.com/sites/roberthart/2024/05/23/nvidia-shares-soar-after-stellar-earnings-along-with-chip-stocks/,Nvidia Shares Soar After Stellar Earnings—Along With Chip Stocks,"May 23, 2024, 06:14am EDT",Robert Hart,"Chip stocks climbed in out-of-hours trading on Wednesday and Thursday after chipmaker Nvidia smashed Wall Street’s expectations with its latest earnings report, continuing a period of extraordinary growth as booming interest in artificial intelligence propels the tech sector to new heights.
Shares for chipmaking giants Nvidia climbed nearly 8% during after-hours trading on Wednesday, peaking above $1,000 per share for the first time.
While these gains have pared a little in the hours since Nvidia’s earnings report, shares for the California-based company were still up by more than 6% at the time of writing on Thursday morning.
Nvidia shares are currently trading at around $1,013, putting the chipmaker on track to surpass the $1,000 milestone and hit an all-time high when markets open on Thursday.
Nvidia is not the only chipmaker benefiting from intense interest in artificial intelligence and shares for other companies producing crucial infrastructure to build, train and run AI models also soared on Thursday.
This includes shares for Taiwan Semiconductor Manufacturing Company, Arm Holdings, Dell Technologies and Super Micro Computer Inc., which were all up between 3% and 6% during pre-market trading.
Shares for Advanced Micro Devices (AMD) and ASML Holdings also climbed, with both rising nearly 2% on Thursday.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
Nvidia released its hotly anticipated earnings report for the first quarter of 2024 on Wednesday afternoon. Nvidia smashed Wall Street’s expectations and the first three months of the year marked the company’s most profitable quarter ever. Respectively, profits and sales were up 628% and 268% compared to the same time period last year, Nvidia said, reporting $6.12 earnings per share and $26 billion in sales for the three-month period ending April 30.
The release of OpenAI’s generative AI chatbot ChatGPT in 2022 ignited a global race among tech companies to build and deploy ever more advanced AI systems. The race has spurred stellar demand for the kinds of advanced computer chips required to maintain, run and develop these AI systems and Nvidia, formerly known for its gaming hardware, is one of the world’s leading beneficiaries for this demand and it has become a bellwether for interest in the sector. The AI boom has rapidly catapulted Nvidia from a successful but limited hardware producer worth around $360 billion at the end of 2022 to a global titan rivaling the likes of Microsoft and Apple and today the company has a market capitalization of more than $2 trillion.
Nvidia announced a 10-for-1 stock split to make buying whole shares more accessible to investors and employees. This split is due to happen on June 7."
285,https://www.forbes.com/sites/dereksaul/2024/05/23/nvidia-ceo-jensen-huangs-net-worth-shoots-to-90-billion-now-among-top-20/,Nvidia CEO Jensen Huang’s Net Worth Shoots To $90 Billion—Now Among Top 20,"May 23, 2024, 10:02am EDT",Derek Saul,"Nvidia CEO and cofounder Jensen Huang became a much richer man Thursday as shares of his semiconductor chip company that’s become the poster child of the artificial intelligence movement surged to a fresh all-time high, placing him among the world’s Top 20 wealthiest, according to Forbes calculations.
Huang’s net worth grew by almost $8 billion Thursday to $91 billion, according to Forbes’ calculations, as Nvidia’s stock rose by 9% to well over $1,000 per share.
A majority of Huang’s net worth comes from his 3.5% stake in Nvidia, the company he cofounded Nvidia in 1993 and has led as chief executive ever since.
Huang, who is by far the largest individual shareholder in the company, leapfrogged Indian mogul Gautam Adani to become the 17th-richest person in the world Thursday, having surpassed Walmart heirs Alice, Jim and Rob Walton earlier this week to vault into the top 20.
It’s a remarkable rise in riches for Huang, who was worth just $4.7 billion as of Forbes’ 2020 billionaires list and $13.5 billion at the beginning of last year, a wealth blossoming attributable to Nvidia stock’s meteoric rise as investors grew infatuated with the Silicon Valley firm’s exponential earnings growth.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
Nvidia designs the highly coveted semiconductors used to power and store data for generative AI, with Huang boasting Wednesday demand far outstrips supply for the companies’ graphics processing units as the company’s earnings power proves robust. Its stock’s rally comes after the company reported blowout quarterly earnings Wednesday afternoon, highlighted by a more than 600% annual increase in profits. The third-most valuable company in the world, Nvidia added $220 billion in market value on Thursday, growing to a $2.6 trillion market capitalization, a sum larger than the combined valuations of Berkshire Hathaway, JPMorgan Chase and Tesla. Nvidia has handsomely rewarded long-term shareholders like Nvidia, with shares returning 21,500% over the last decade, 2,500% over the last five years and 115% this year alone. Even after the valuation spike, Nvidia’s growth story “is clearly nowhere near its end, and likely nowhere near its peak,” according to Bernstein analysts led by Stacy Rasgon.
$8.7 million. That’s how much Huang will receive in quarterly dividend payments next month after Nvidia upped its dividend payout by 150% to $0.10 per share before the stock split. That’s chump change compared to the roughly $175 million in quarterly dividends paid out to Meta CEO Mark Zuckerberg.
“The Godfather of AI Jensen and Nvidia delivered another masterpiece quarter,” gushed Wedbush’s Dan Ives in a note to clients, serving among a chorus of reverential Wall Street analysts toward Huang and Nvidia.
"
286,https://www.forbes.com/sites/dereksaul/2024/05/22/nvidia-earnings-stock-rallies-as-ai-giant-reports-600-profit-explosion/,"Nvidia Earnings: Stock Rallies As AI Giant Reports 600% Profit Explosion, 10-For-1 Stock Split","May 22, 2024, 04:23pm EDT",Derek Saul,"Nvidia again shattered Wall Street forecasts in its anxiously awaited earnings report Wednesday afternoon, sending shares of the chip designer and artificial intelligence top dog toward a record high.
Nvidia reported $6.12 earnings per share and $26 billion of sales for the three-month period ending April 30, shattering mean analyst forecasts of $5.60 and $24.59 billion, according to FactSet.
Nvidia’s profits and revenues skyrocketed by 628% and 268% compared to 2023’s comparable period, respectively.
This was Nvidia’s most profitable and highest sales quarter ever, topping the quarter ending this January’s record $12.3 billion net income and $22.1 billion revenue.
Driving the numerous superlatives for Nvidia’s financial growth over the last year is unsurprisingly its AI-intensive datacenter division, which raked in $22.6 billion of revenue last quarter, a 427% year-over-year increase and a whopping 20 times higher than the $1.1 billion the segment brought in in 2020.
Nvidia also announced it will conduct a 10-for-1 stock split June 7, which would trim its share price from about $950 to $95 while maintaining the company’s total valuation, enabling investors and employees to more affordably purchase whole shares.
Nvidia’s stock popped 4% immediately after the release, sitting at what would be an all-time high in regular trading hours.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
High volatility for Nvidia’s stock was expected, as options trading priced in a roughly 8% move in either direction after earnings. Nvidia, which designs a majority of the semiconductor chips powering generative AI technology, has arguably been the biggest winner of the AI boom over the last two years, with the likes of Amazon and Microsoft among its top customers. Nvidia is the third most valuable company in the world with a market capitalization of about $2.3 trillion, a far cry from its sub-$400 billion market value at the end of 2022. Though a slowdown from the fairly unprecedented financial growth Nvidia experienced in recent quarters, analysts expect Nvidia to continue to grow at a strong pace, with consensus estimates projecting Nvidia’s revenues to expand by about 90% in its fiscal year ending in Jan. 2025, with the $112 billion in forecasted sales more than four times higher than the 2022-23 fiscal year’s $27 billion.
Nvidia has almost single-handedly lifted the American stock market from its 2022 doldrums to today’s record levels. Its 490% total return over the last 18 months is far better than the average S&P 500 stock’s 13% return over the period, with the S&P up over 36% during that stretch. Even after Nvidia’s valuation exploded, there aren’t many on Wall Street who find Nvidia overvalued, as not one analyst tracked by FactSet has a sell rating on the stock, with the average price target of $1,039 per share pricing in about 10% upside from Wednesday’s closing price.
Nvidia’s rise to become a $2 trillion company came as its once bread-and-butter video game business fell into a rut. Sales for Nvidia’s gaming unit are about 25% lower than they were two years ago, accounting for just a tenth of overall revenues last quarter, a far cry from the over 40% gaming revenue mix Nvidia had for each of 2020 to 2022’s first calendar quarters. In short, Nvidia has largely shifted its focus away from cheaper direct-to-consumer graphics processing units (GPUs) often coveted by gamers and instead redirected attention toward supplying AI-focused GPUs to big-ticket corporate clients."
287,https://www.forbes.com/sites/bethkindig/2024/05/22/nvidia-q1-earnings-preview-blackwell-and-the-200b-data-center/,Nvidia Q1 Earnings Preview: Blackwell And The $200B Data Center,"May 22, 2024, 05:58am EDT",Beth Kindig,"Nvidia’s management team will focus on the H200 in the upcoming earnings call, but make no mistake, we will end this year in full-on Blackwell territory. The new architecture is at the forefront of training and inference for trillion+ parameter models. More than five years ago, I called CUDA the moat for Nvidia’s AI data center story, yet should that moat become breached, the company’s rapid product road map is the first line of defense.
Nvidia is the world’s leading GPU design company, which bears reminding since such little emphasis in Wall Street is placed on what the designs intend to solve. For those paying close attention, there are clues that the company’s fast and furious data center growth will see a second wind with Blackwell.
Last quarter in fiscal Q4, Nvidia reported growth of 265%. Last quarter is likely to be peak growth for the company. We pointed this out three months ago when our analysis stated: “Even if we see a beat and raise, the slowing growth in the second half will be hard to overcome due to high comps. As mentioned in the introduction, Nvidia will begin to lap some stellar quarters come the October CY2024 quarter as the growth in October of CY2023 was 205.5% YoY.”
At time of writing, the revenue estimates for Nvidia point to growth of 242%. A beat/raise this quarter is not likely to flow through to a higher growth rate in H2 compared to what we saw in Q4 and what we will see in Q1. Therefore, even if Q1 inches slightly past fiscal Q4 tomorrow evening, we have hit peak growth.
Typically, a growth investor should be cautious when a company hits its peak growth rate after a drastic rise in the stock price. Here is a chart we published three months ago updated with current estimates:
However, Nvidia’s margins and earnings expansion are creating an outlier of a stock. There are rumors Blackwell GPUs will be priced starting at $30,000 to $40,000 but will have more expensive memory components with HBM3e. As long as margins remain within range, this will not be consequential considering Nvidia is posting organic growth.
This is drastically different than a stock that relies on growth at any cost, which is where rapid growth is bought rather than earned. The quality of Nvidia’s growth is much better than what tech investors are used to, and this is predominately why Nvidia stock is resilient (within reason; there will always be selloffs in the market). As supply/demand becomes more balanced, it will be Nvidia’s aggressive product road map, which in many cases is designed to compete with themselves, that will keep pricing power stable, starting with Blackwell.
For example, there are recent reports that AWS is pausing orders on Hopper GPUs in anticipation of Blackwell GPUs. The market may interpret this as weakness, but this is actually a sign of immense strength. Nvidia needs to pass the baton from the H100s and H200s to the Blackwell architecture for the stock price to extend. We are less concerned with what happens in the immediate-term, and in fact, the I/O Fund has stated a few times that Nvidia is a buy on dips, implying the stock won’t go up forever. Instead, we are encouraged to see early signs of a careful transition to the next architecture to help inform our next buy.
There is nothing quite like rapid earnings revisions intra-quarter to determine the quality of a position. For example, consider that Nvidia sold off directly after the November report, yet has gone up a rapid 91% since. The earnings revisions are why Nvidia is so strong intra-quarter:


Below, we discuss why margins, cash flow and strong earnings support our decision to buy on dips. However, equally as important, there is also a decent probability that FY2026 and FY2027 revenue estimates are too low. The most bullish analyst from KeyBanc is calling for a $200 billion data center segment by 2025. HSBC believes Nvidia’s FY26 revenue could be as high as $196 billion, which implies about a $192 billion data center segment. Loop Capital foresees a $150 billion data center segment as soon as this year, while Wells Fargo has estimates for a $150 billion data center segment by 2027. The exact timing from these analysts has a range, but the conclusion is very similar.
Let’s breakdown the weight of those comments with some back-of-the-napkin math, which shows that analysts are currently estimating about $122.4B in data center revenue for FY2026 (calendar year 2025). This is about 65% lower than the more bullish analyst estimates of $200 billion in data center revenue.


These are the current estimates, yet if the analysts are correct, then the far right of the graph will end in $50B quarterly revenue. The difference between the current consensus and this much higher trajectory can be summarized in one word: Blackwell.
There are additional data points in the supply chain and on the demand side that support Blackwell seeing an increase in orders over Hopper. For example, Taiwan Semi’s CoWos capacity, which is essential for Blackwell’s architecture, is estimated to rise to 40,000 units/month by the end of 2024, which is more than a 150% YoY increase from ~15,000 units/month at the end of 2023. Applied Materials has boosted its forecast for HBM packaging revenue from a prior view for 4X growth to 6X growth this year. According to Wells Fargo, Taiwanese export data rose 360% year-over-year and 33% quarter-over-quarter, and is often correlated to Nvidia data center revenue.
Note: It’s important to remember this is not earnings call on what will happen tomorrow evening as the revenue will be reported when it ships to the customer. However, it helps to consider there are directionally bullish data points should the market sell off following the report and provide us a lower entry.
Notably, the premiere component for the H200 and Blackwell is HBM3e memory, which is currently supply constrained. Samsung and SK Hynix are both re-allocating ~20% of DRAM production capacity to HBM to meet high demand, while HBM4 roadmaps are being accelerated.
CEOs of major companies in AI acceleration are in agreement the total addressable market is much, much larger than today’s market size. Lisa Su of AMD has stated the AI chip market will reach $400B by 2027. Intel’s CEO has stated AI chips will become a $1T opportunity by 2030, which is almost twice the size of the entire chip industry in 2023.
Big Tech capex is supporting this growth. Our firm has been especially strong on correlating capex to AI investments for our paid research members, where we held a 1-hour webinar in April discussing our expectations that capex increases in support of AI stocks. We followed this up with free analysis in our newsletter that tracked a 35% YoY increase to $200 billion across Big Tech companies. A disproportionate amount of this will go to Nvidia.
We’re closely tracking Big Tech’s capex plans for 2024 and how this will flow downstream to AI hardware companies. The I/O Fund had a 45% allocation to AI going into 2023, one of the highest on record. Today, the AI allocation is higher with many lesser-known names. Learn more here.
A curveball in the report could be higher than expected China revenue due to China-specific GPUs, such as the H20. Similar to Big Tech in the United States, China’s main players are stockpiling GPUs to secure their lead in AI.
Regarding China, last quarter, the following was stated: “Growth was strong across all regions except for China, where our Data Center revenue declined significantly following the U.S. government export control regulations imposed in October. Although we have not received licenses from the U.S. government to ship restricted products to China, we have started shipping alternatives that don't require a license for the China market. China represented a mid-single-digit percentage of our Data Center revenue in Q4, and we expect it to stay in a similar range in the first quarter.”
The product road map is the single most important thing investors should be focused on. A good chunk of the AI accelerator story is understood at this point. What is not understood is how aggressive Nvidia is becoming by speeding up to a one-year release cycle for its next generation of GPUs instead of a two-year release cycle.
This means Nvidia is competing with itself by putting Blackwell dangerously close to Hopper’s product cycle. This move is bold, it’s daring, and it’s absolutely necessary.
Here is the very ambitious eight month schedule Nvidia has set for itself:


The Blackwell architecture remains on 4nm dies, similar to the Hopper architecture. What is different is that Blackwell has 2 reticle-sized GPU dies. Reticle size refers to the limit in the chip surface that can be exposed by a single mask. The limit is set by the lithography equipment. At one point it was expected Blackwell would be on 3nm dies, yet due to reasons unknown, Nvidia is moving forward with 4nm. Since Nvidia is not able to offer a more advanced process node, the company is instead doubling the silicon. The Blackwell architecture is rumored to be priced between $30,000 to $40,000, which is higher than the H100’s reported $25,000 cost. This is competitive considering B200 will offer nearly 30X better performance (benchmarks are provided by Nvidia).
The B100 is a replacement chip, which means customers can remove the H100 and place the B100 in the same rack. The B100 is air-cooled and doubles NVLink speeds from the H100 and H200. The B100 is will ship in Q3 and provide upgrades to memory from 80GB in the H100, 141GB in the H200 to 192GB in the B100.
The B200 GPU chipset due in Q1 of next year will deliver a 2.5X training improvement and 5X inference improvement over the H100. This is due to the B200 having 208 billion transistors compared to the H100’s 80 billion transistors.
The B200 will also have 20 petaflops of FP4 compared to the H100’s 4 petaflops of FP8 reaching 32 petaflops of FP8 in the DGX H100 systems. The difference is that the smaller bit size allows for an economical way to achieve more speed when giving up a small amount of accuracy doesn’t make a critical difference. This also helps in the face of a slowing Moore’s Law. Following the release of the Hopper H100, Intel released Gaudi2 which supports FP8. About two years back, chip makers Graphcore, AMD and Qualcomm pushed for an industry-standard for floating point format FP8. However, the recent B200 will have a second-generation transformer engine that supports 4-bit floating point (FP4) with the goal of doubling the performance and size of models the memory can support while maintaining accuracy.
Part of the secret sauce of the H100 is the transformer engine. The A100 lacked support for FP8 compute at default whereas the H100 leveraged a transformer engine to switch between FP8 and FP16, depending on the workload. The second-generation transformer engine in the Blackwell architecture will offer FP4. This is helpful because AI models are moving toward neural nets that lean on the lowest precision and yet still yields an accurate result. In this case, 4 bits double the throughput of 8-bit units, compute faster and more efficiently, and they require less memory and memory bandwidth.
The main feature from the Transformer Engine is the ability to choose what precision is needed for each layer in the neural network at each step, transitioning between 4-bits, 8-bits, 16-bits, or 32-bits. The H100 is able to do matrix math with two forms of 8-bit numbers with either 5-bits as the exponent or 4-bits as the exponent: E5M2 and E4M3. This is important because the E4M3 may be favored for back propagation while E5M2 may be favored for inferencing.
Building on the first-gen transformer engine, the B200’s second-gen transformer engine will support double the compute and model sizes with new 4-bit floating point AI inference capabilities.
According to the current product road map, the GB200 will be released before the B200 GPUs. The real fireworks will begin with the GB200 NVL36/NVL72 systems in late 2024 and then continue with the B200 GPUs in early 2025.
The GB200 Grace Blackwell chip connects two Blackwell Tensor core GPUs with the Nvidia Grace CPU. The GB200 NVL 72 rack-scale exascale supercomputer, connects 36 Grace CPUs with 72 Blackwell GPUs in a rack-scale design with liquid cooling. We’ve written in-depth about liquid cooling for our premium research members, learn more here.
According to HSBC, the average sales price of NVL36/NVL72 server rack will be $1.8 million and $3 million, respectively. Notably, its expected the GB200 systems will have strong margins due to using an in-house CPU.
Here are the stats provided from Nvidia on how it will compare:


Source: Nvidia, the GB200 System due to ship in Q4 this year
The GB200 will provide 4X faster training performance than the H100 HGX systems and will include a second-generation transformer engine with FP4/FP6 Tensor core. As stated above, the 4nm process integrates two GPU dies connected with 10 TB/s NVLink with 208 billion transistors.
NVLink Switch is a major component to the Blackwell upgrade. Fifth-generation NVLink enables multi-GPU communication at high speed, reaching 1.8 TB/s bidirectional throughput or 14X the bandwidth of PCIe for a single GPU.
For the NVL72 systems, NVLink Switch can reach 130 TB/second, which is “more than the aggregate bandwidth of the internet.” Therefore, it’s the compute and the communication capabilities of the upcoming GB200 release that are important to consider. The 72 GPUs in the NVL72 can be used as a single accelerator for 1.4 exaflops of AI compute power.
To scale up a model, AI departments utilize a Mixture of Experts (MoE) approach. MoE distributes a computational load across “multiple experts” (or neural networks) and trains across thousands of GPUs using what is called model and pipeline parallelism. This enables more compute-efficient pretraining yet the parameters still need to be loaded in RAM, so the memory requirements remain high.
For inference, GB200 will deliver “a 30X speedup” for 1 trillion­­+ parameter models by leveraging FP4 precision and fifth-generation NVLink. This is what that the leap in real-time throughput for inference looks like for a 1.8 trillion parameter model:
Source: Nvidia Blog
Blackwell is for the trillion+ parameter era of generative AI. The architecture is designed to support the largest language models today and is future-proofed with the GB200 NVL72 rack-scale solution, which is an exascale computer that contains up to 5,000 NVLink cables that total 2 miles. You also have to consider that AMD was coming to market in the first release with nearly 2X memory as the H100. Nvidia is remaining competitive with HBM3e and soon HBM4 to help models run in memory.
The GB200 also has a new decompression engine that allows GPUs to process and decompress compressed data sets to speed up database queries. Coupled with 8 TB/s of high memory bandwidth and high speed NVLink, the GB200 systems deliver up to 18X faster database queries. In addition to this, there is up to 13X faster physics-based simulations compared to CPUs and 22X faster simulations for computational fluid dynamics (CFD).
High bandwidth memory (HBM) offers higher bandwidth, capacity, performance, and lower power by vertically stacking up to twelve DRAM memory chips to shorten how far data has to travel, while also allowing for smaller form factors. Stacked memory chips are connected through something called “through silicon vias” or TSVs. HBM is increasingly being used to power machine learning, high performance data centers, and more recently, generative AI models.
CoWoS (chip-on-wafer-on-substrate) architecture refers to 3D stacking of memory and processor modules layer by layer to create chiplets. The architecture leverages through-silicon vias (TSVs) and micro-bumps for shorter interconnect length and reduced power consumption compared to 2D packaging.
The advanced CoWoS packaging that is needed to combine logic system-on-chip (SoC) with high bandwidth will take longer, and thus, it’s expected that Blackwell will be able to fully ship by Q4 this year or Q1 next year. How management guides for this will be up to them, but commentary should be fairly informative by Q3 time frame.
GPUs will move from 8Hi configurations to 12Hi HBM3e configurations by 2025. These upgrades are needed to train and deploy large models with trillions of parameters in the near future. What Nvidia’s product road map intends to accomplish is a way forward for real-time inference that is computationally efficient, cost-effective and energy efficient.
My firm has covered HBM3e in the past when we stated in a premium research report six months ago:
The recent surge in generative AI and AI GPUs, spurred by the success of OpenAI’s ChatGPT and development of hundreds of other large language models, are forecast to bring about a new DRAM market, underpinned by high-bandwidth memory (HBM) and DDR5
[…] HBM3 and HBM3e are becoming the next battleground for memory chip manufacturers as well as AI chip design companies, especially Nvidia and AMD, who are pushing the boundaries with the amount of memory bandwidth in each GPU.
AMD’s competing GPUs, the MI300 series, substantially boosted memory and bandwidth relative to the H100, utilizing Samsung’s HBM3. The MI300A is shipping with 128GB HBM3 memory while the MI300x ships with 192GB memory and 5.2 TB/s of bandwidth – that’s 1.6x more bandwidth and 2.4x more HBM3 density than Nvidia’s H100.
Nvidia is rapidly moving forward with its GPU roadmap, as it aims to launch its next-gen H200 and B100 GPUs next year followed by the X100 GPU in 2025 – each GPU will accelerate AI inference times along an exponential curve, thus creating a need for more memory and more bandwidth.”
Now that we’ve touched base on the importance of Blackwell, let’s get prepped for this evening. Here is what analysts are expecting:
Revenue:


EPS:
In Nvidia’s case, top line growth is flowing through to bottom line growth disproportionately.


As the story for Nvidia unfolds over the next few years, keep an eye on margins as software will begin to positively impact the company with higher margins. The company is expected to end the year with $2 billion in software revenue.
In the near-term, and especially for this earnings report, it’s likely that analysts ask about the costs associated with HBM3e as memory components are increasing in costs. TrendForce has reported that HBM3 prices have risen 5-fold since 2023. HBM3e prices will be even higher than HBM3. Analysts may also ask about the yield issues that major memory suppliers SK Hynix, Micron, and Samsung are reported to be facing, given the complexities in the manufacturing process for HBM3e and its longer production cycle. For our premium members, we’ve discussed what stocks will benefit from this leading trend in 2024.


Last quarter, Nvidia reported operating cash flow of $11.5 billion for a margin of 52%. The free cash flow of $11.2 billion represents a margin of 50.7%. The fiscal year free cash flow of $26.9 billion was more than 7 times higher than the fiscal year 2023 free cash flow of $3.75 billion.
The data center segment reported revenue of $18.4 billion for growth of 409% YoY and was up 29% QoQ. Nvidia’s tough comps kick in with the Q2 July quarter when the company reported DC revenue of $10.3 billion for growth of 171%, and thus the guide is key. Management will not guide to DC specifically but it’ll be easy enough for analysts to read through the lines that any beat/raise on Q2 is likely coming from the DC segment.
The CFO mentioned in the earnings call that 40% of the revenue came from inference in the past year. “Fourth quarter data center growth was driven by both training and inference of generative AI and large language models across a broad set of industries, use cases and regions. The versatility and leading performance of our data center platform enables a high return on investment for many use cases, including AI training and inference, data processing and a broad range of CUDA accelerated workloads. We estimate in the past year approximately 40% of data center revenue was for AI inference.”
Gaming revenue of $2.8 billion was up 56% YoY and was flat QoQ. Nvidia has fared better than gaming peers due to the timing of the RTX 4000 Series, which I covered in a previous editorial: “Nvidia Stock: Evidence Gaming has Bottomed and Why It’s Important.” With that said, management guided for a seasonal decline in gaming.


As stated on Making Money with Charles Payne today, the upcoming earnings report is only one piece to the story, whereas the ultimate fireworks will be when the Blackwell architecture begins to ship Q3-Q4. The product road map is communicating that AI accelerators are secular; not cyclical.
We will see peak growth this quarter – even if we get that beat that Nvidia is becoming known for, H2 will certainly see a slowdown. This is normally a great jumping off point for investors but those who stick with Nvidia will be rewarded for a few reasons:


Nvidia has sold off 10% or greater about 9 times since the 2022 low. We see any dips as buying opportunities as we brace for Blackwell toward the end of this year.
The I/O Fund we had five positions with returns over 100% and seven positions beat the Nasdaq in 2023. This contributed to cumulative returns of 131% since May of 2020. For more in-depth research from Beth, including 15-page+ deep dives on the stock positions that the I/O Fund owns, take advantage of our biggest sale of the year in honor of our four-year anniversary and subscribe here.
If you would like notifications when my new articles are published, please hit the button below to ""Follow"" me.
Please note: The I/O Fund conducts research and draws conclusions for the company’s portfolio. We then share that information with our readers and offer real-time trade notifications. This is not a guarantee of a stock’s performance and it is not financial advice. Please consult your personal financial advisor before buying any stock in the companies mentioned in this analysis. Beth Kindig and the I/O Fund own shares in NVDA at the time of writing and may own stocks pictured in the charts."
288,https://www.forbes.com/sites/dereksaul/2024/05/21/nvidia-stock-hits-record-high-ahead-of-blockbuster-earnings/,Nvidia Stock Hits Record High Ahead Of Blockbuster Earnings,"May 21, 2024, 04:09pm EDT",Derek Saul,"Nvidia stock closed at a new all-time high Tuesday ahead of its hotly anticipated earnings report due this week, as the top investor pick for artificial intelligence grapples with sky-high expectations.
Nvidia shares reversed their morning slide and ended Tuesday trading up nearly 1% at $954.
That bests the stock’s previous, highest-ever close of $950 per share set in March, with the swing likely resulting from repositioning ahead of its earnings report slated for Wednesday afternoon.
Nvidia shares are up 25% over the last month alone and 93% in 2024, extending 2023’s 240% runup as investors can’t seem to get enough of the AI dominator.
Options trades indicate the market expects Nvidia’s stock to move about 8.5% in either direction following its Wednesday report, according to Bank of America.
That’s a move which would add or knock off some $200 billion from Nvidia’s $2.3 trillion market  capitalization, a hard-to-grasp sum for a company valued at $200 billion total just four years ago.
Nvidia investors’ moods this week will likely depend on the company’s ability to deliver on the stratospheric forecasts for its first full quarter of 2024, as Wall Street anticipates Nvidia to expand revenue by 240% to $24.6 billion and net income by 540% to $13.1 billion, according to year-over-year consensus analyst projections compiled by FactSet.
Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here.
“Expectations aren’t exactly low,” remarked Raymond James analyst Srini Pajjuri in a note to clients, summarizing the massive bar facing Nvidia this week."
289,https://www.forbes.com/sites/investor-hub/article/nvidia-nvda-stock-earnings-preview/,Nvidia Stock Earnings Preview: What Investors Need To Know,"May 14, 2024, 01:36pm EDT",Catherine Brock,"Nvidia (NVDA), the darling of AI and technology investors, will report results from its first quarter of fiscal year 2025 on May 22. The details of that release will either support or undercut Nvidia's massive $2.2 trillion valuation and the investment community's obsession with AI stocks.
Let's establish the context for this much-anticipated earnings release. We'll cover why investors are excited about NVDA's first quarter, key metrics to watch, challenges and opportunities for the chip designer and analyst expectations for the near and long-term.
Nvidia has been working on AI solutions since 2012. In that year, the company launched an AI application that could classify images. Called AlexNet, the application was trained in days and ran on two of Nvidia's programmable GPUs. The realization that GPUs could accelerate machine learning versus regular processors shaped a new strategic direction for Nvidia. That's essentially when the company began its quest to become a leader in AI solutions.
Eleven years later in 2023, ChatGPT sparked a race to launch generative AI solutions. From the big tech companies to niche software providers, seemingly everyone started working to get their generative AI tools into the hands of eager customers.
Nvidia, now the dominant leader in AI hardware and software, has benefited directly from the AI application race. The company's fiscal year 2024 earnings releases show an increasing demand for its AI products. By year-end, Nvidia's AI division, called Data Center, had grown its annual revenue 217% from the prior year. Company-wide revenue grew 126%.
The growth fueled a tripling of the NVDA stock price between May 2023 and May 2024. That value spike prompted rumors of an upcoming Nvidia stock split and divided many investors into two camps. Some say Nvidia is fairly valued. Others argue the stock is overpriced and the opportunity for investors to benefit from Nvidia's AI success has passed.
This first quarter earnings release could quiet the overvaluation argument or turn it up. More specifically, Nvidia's recent performance will speak to the company's ongoing growth potential and the runway for value creation in AI generally.
The brain trust at Forbes has run the numbers, conducted the research, and done the analysis to come up with some of the best places for you to make money in 2024. Download Forbes' most popular report, 12 Stocks To Buy Now.
The metrics to watch in NVDA's first quarter earnings report include:


Nvidia will face these challenges going forward:


Nvidia's largest opportunity lies in product development. The company can leverage and extend its dominant position by continuing to roll out faster and more powerful solutions.
Fortunately for shareholders, Nvidia has a knack for self-improvement. In March, the company announced its upcoming, next-generation AI offering, Blackwell GPUs. The new GPUs are said to be twice as powerful for training AI models versus Nvidia's prior-generation chip.
Analysts expect Nvidia to report an adjusted, diluted EPS of $5.57 for the quarter ending on April 28, 2024. That expectation has increased by $0.05 in the last 30 days and by $0.70 in the last 90 days. A $5.57 EPS number would translate to 411% quarter-over-quarter growth and 8% sequential growth.
Notably, two key Nvidia suppliers, Taiwan Semiconductor and SK Hynix, both recently reported surprisingly strong numbers–prompting a small rally on NVDA stock. TSM announced 60% revenue growth in April, while SK Hynix posted 144% quarter-over-quarter sales growth.
It's also worth pointing out that Nvidia has delivered positive revenue and earnings surprises in seven of the most recent eight quarters. So it is likely that the official revenue guidance of $24 billion was a conservative view when Nvidia published it in February.
The consensus price target for NVDA stock is $999, representing 12.5% upside. That target already assumes Nvidia will have a positive earnings result for the first quarter.
Longer term, analysts believe Nvidia's fiscal year 2025 EPS will come in around $24.84—roughly double the fiscal year 2024 result. Like the expectation for the first quarter, that estimate has been trending up over the last three months.
The EPS expectation for the following year is $29.44. That's a more tempered growth expectation relative to the current fiscal year, but still a significant prediction. The implications are that AI demand has a runway and Nvidia will continue to dominate in this space.
Investors are eagerly awaiting Nvidia's first quarter results to gauge the stock's valuation and the growth potential in AI investing going forward. The likely outcome is a positive earnings surprise, but whether it's enough to fuel another NVDA rally remains to be seen.
What key factors may drive Nvidia's earnings?
Ongoing demand for AI chips and related solutions will be a key driver for Nvidia's first quarter earnings.
What are analyst expectations for Nvidia's earnings?
Analysts expect Nvidia to exceed its earnings guidance with an adjusted, diluted EPS of roughly $5.57 per share.
How may Nvidia's earnings impact the stock market?
Nvidia is the third largest company in the S&P 500, comprising over 5% of the index. If Nvidia reports a very strong or very weak quarter, it could spark a corresponding trend in tech stocks and the market as a whole.


Stop chasing shadows in the market. Forbes' expert analysts have pinpointed the 12 superstars poised to ignite returns in 2024. Don't miss out—download 12 Stocks To Buy Now and claim your front-row seat to the coming boom."
290,https://www.forbes.com/sites/digital-assets/2024/05/11/exponential-baby-navigating-the-ai--convergence-of-tech-with-nvidia/,Exponential Baby! Navigating The AI  Convergence Of Tech With Nvidia,"May 11, 2024, 09:00am EDT",Sandy Carter,"The word  'exponential' is one of the most abused in the English language, but in the world of technology it retains its true meaning of constantly accelerating change. This exponential growth isn’t confined to isolated pockets or a handful of industries; it’s systemic, engulfing every technology-dependent industry. Which is to say, practically all of them. But today we have a massive set of technology converging all at the same time including AI, Quantum, Spatial, Blockchain, and more. All of these technologies underpin the surging wave that is shaping the new norm of our era - a period I like to call 'Exponential Baby.' If we are to seize the opportunities and avoid the dangers of this growth, we need a similar rapid augmentation in our understanding of these technologies.
The world of Artificial Intelligence (AI) provides a clear illustration of this growth pattern. According to Gartner, a staggering 80% of enterprises anticipate increasing their use of generational AI by 2024. These aren’t incremental upticks; these are massive leaps in adoption. NVIDIA stands as a testament to this movement, now valued as a two-trillion dollar company  largely due to their pivotal role in powering AI operations. Their success isn't a solo journey – they’re marching alongside giants like Microsoft, Meta, Oracle, and Amazon. Their soaring chipset shipments is just one indicator of the growth trajectory we’re on.
Jensen Huang, CEO of NVIDIA, has expressed significant insights on the growth of AI and its implications. He acknowledges a ""major second wave"" of AI, propelled by the recognition of the need for sovereign AI that reflects the unique cultural and linguistic characteristics of each region, and the widespread adoption of AI across various industries​​.
Huang sees NVIDIA's advanced computing platforms as instrumental in driving this growth, noting that ""NVIDIA are going to bring a ton of technology to networking"" and that the exponential increase in compute throughput will span nearly 1,000 times in just a couple of years​​. This rapid development is not just contained within AI but extends to areas like inference computing, which now exceeds all cloud CPUs in aggregate, demonstrating NVIDIA's significant role in fueling AI-powered consumer services​​.
Huang's vision is both a lesson and warning for every business that uses technology: as we move into the twin  Ages of AI and Decentralization, every technological development is interlinked and contributes to an overarching acceleration and intensification of AI capabilities and applications across the globe. This has, needless to say, fundamental implications for long-cherished organizational mindsets and business models.
For example, take AI’s contribution to productivity is a narrative that disrupts traditional labor conversations. We once fretted about AI’s potential to displace blue-collar jobs. Yet, reality portrays a more complex canvas where 46% of office legal work is automated. In a mere year, AI-driven automation spiked by 25%. And when ChatGPT arrived, it exemplified how rapidly AI could capture public attention and utility, amassing a user base that other platforms took decades to build.
But just as we begin to comprehend AI’s impact on every role, department and organization, the technology makes another exponential leap.
Alongside the explosive growth in AI, we're also witnessing a remarkable convergence in the field of spatial computing. This encompasses augmented reality (AR), virtual reality (VR), and other technologies that enable digital and physical realities to coalesce. The pace of change is staggering. Companies like NVIDIA are leveraging their expertise in AI to push the boundaries of spatial computing, making it faster and more integrated into our daily lives.
This convergence is not just technical but experiential, dramatically altering how we interact with data, entertainment, and each other. Innovations in hardware, such as more powerful GPUs, and software, like sophisticated spatial algorithms, are changing the nature of digital interactions, which are fast becoming as intuitive and natural as physical ones. As spatial computing becomes more mainstream, its integration with AI will only deepen, paving the way for fully immersive experiences that are seamlessly integrated into various sectors—from gaming and entertainment to education and telepresence.
How are enterprises implementing AI?  Scale AI just released their ""AI Readiness"" report where they surveyed 1800+ practitioners building or applying AI solutions in enterprises. Programming is the most popular use case, hence Venture Capitalist (VC) are leading very large rounds in companies like Cognition, Augment and Codeium.  Codeium was recently recognized by Forbes as part of their AI 50 list, the only AI code assistant to be included.
As Varun Mohan, the CEO of Codeium, told me, “To deliver a product that is truly valuable to a developer in a professional setting, we've realized that more than just the state of the art LLMs, you need widespread platform availability, agnostic integrations across internal knowledge stores, and adherence to enterprise security and compliance standards, especially while case law is still being developed. Today, most tools provide targeted acceleration but we envision a world where AI will help solve the hardest problems in all parts of the software development cycle.""
Using AI in coding, particularly at Codeium, exemplifies its ability to enhance personalization and efficiency. AI coding assistants excel by integrating deep contextual insights, such as the specific third-party libraries and internal documentation relevant to a developer's project. ""Context is king,"" as noted by one expert, highlighting that the more contextual data provided, the more accurate and useful the AI's output. This personalized approach not only streamlines the coding process but also ensures that the solutions are uniquely tailored to the developer's needs, significantly boosting productivity.
The implication of this trend for businesses and individuals is profound. It means not only rethinking user interfaces but also reimagining how we collaborate and interact in workspaces that are no longer bound by geography. The convergence of AI and spatial computing will revolutionize industries, leading to an acceleration in remote work capabilities, training procedures, and even altering the landscape of social interactions. Leaders and organizations that understand and leverage this convergence will likely be at the forefront of the next wave of digital transformation.
In addition to AI, cryptocurrency's rebound speaks volumes about the fast-changing landscape of asset valuation. Crypto has risen from its ashes to become a leading asset class, ballooning in value at an astonishing pace, with prices now driven by hugely increased utility, rather than just speculation. The chart below from Bitwise Asset Management with data from Bloomberg Data shows that Bitcoin has led in performance 10 of the last 13 years.
Perhaps the most mind-boggling of all trends is the anticipated growth in data. Over the next decade, data will burgeon by approximately 660 zettabytes. To put that into perspective, it equates to around 610 more 128GB iPhones for every person on the planet.
Dealing with this data deluge is a challenge for the tech industry, but leaders in every sector must keep pace, too. How can they navigate the convergence of technology and the exponential growth in its capability, and what lessons must they learn to stay ahead of the game?
The challenge posed by this  whirling vortex of growth lies not in riding the wave but in steering through it effectively. Change management becomes pivotal. It's about helping teams navigate through the innovation influx while fostering an environment ripe for adaptation and learning. Empathy becomes as crucial a tool as any software – understanding that while machines may not experience job insecurity, humans certainly do.
It’s encouraging to see stories of dental hygienists who proactively educate themselves in AI, recognizing that these skills are integral to their evolving roles. These actions epitomize the mindset shift required for the workforce of tomorrow. In the workplace of the future, leadership and lessons can come from the unlikeliest places, rather than the hierarchical, top-down models to which we’ve become long-accustomed. Education must flow in both directions, with teams and leaders embracing dialogue, openness, and a willingness to experiment.
What's evident is that the pace of technological change isn't going to plateau – it’s set to accelerate. The exponential trend is here to stay, and it's not exclusive to AI. The ripple effects are felt across blockchain, machine learning, and IoT, to name a few. With each converging technology, new possibilities and challenges emerge.
Navigating these trends necessitates a leadership style that's dynamic, inclusive, and forward-thinking. Change management, empathy, and education form the triad of essential skills. The goal is to cultivate a culture where innovation is not just reacted to but anticipated and embraced.
‘Exponential Baby’ isn't just a trend; it's a paradigm shift in the technological and corporate landscape. As AI continues to evolve, and data swells to previously unimaginable levels, leaders must adapt their strategies to keep pace. This era is a test of our agility, our foresight, and our willingness to engage with technology not as a tool, but as an integral part of our evolving humanity.
We stand at the threshold of the exponential age – an era of immense potential, profound change, and boundless opportunity. As leaders, educators, and individuals, our task is to harness this potential, surf the wave of change, and emerge resilient and ready for the world of tomorrow."
291,https://www.forbes.com/sites/investor-hub/article/missed-nvidia-stock-price-rally-check-these-ai-stocks/,Missed Nvidia’s Stock Price Rally? Check Out These 7 AI Stocks Instead,"Apr 26, 2024, 08:59am EDT",Catherine Brock,"A year ago, you could have bought semiconductor stock Nvidia (NVDA) for about $277 per share. Today, the price is above $800. Investors, eager to capitalize on the company's momentum in the artificial intelligence (AI) space, have driven NVDA up some 188%.
AI has been a topic of interest for investors since ChatGPT took the technology mainstream in late-2022. Since then, Nvidia has monetized AI more dramatically than any other public company. That's great news for those who bought Nvidia stock before the end of 2023. The rest of us, sadly, missed the rally.
Fortunately, the AI revolution is just getting started. Let's get motivated for AI investing redemption with a quick look at Nvidia's recent trajectory, followed by an introduction to seven top AI stocks with double-digit upside.
Nvidia's amazing rally over the past year had several catalysts. To start, the company reported four consecutive earnings surprises, ranging from 17% to 44%. Nvidia also grew its EPS from $0.88 in the April, 2023 quarter to $4.91 in the January, 2024 quarter.
The tech stock tripled in 2023, and then hit its steepest climb in January 2024. In that month, Nvidia announced graphics cards designed for use in ""AI-ready"" laptops and released its AI Workbench developer toolkit to beta. AI Workbench streamlines the creation and deployment of AI applications. Analysts responded to the new product splash by increasing their NVDA price targets and ratings.
By the time Nvidia announced 486% growth in its adjusted, diluted EPS between the fourth quarters of 2023 and 2024, the stock price had already eclipsed $700. After the last earnings release, NVDA climbed to $950 before moderating back to about $800.
Even at $800, the prevailing opinion is that the stock has already priced in investors' strong expectations for future AI-related growth.
The brain trust at Forbes has run the numbers, conducted the research, and done the analysis to come up with some of the best places for you to make money in 2024. Download Forbes' most popular report, 12 Stocks To Buy Now.
Nvidia may have been the first to capitalize on AI enthusiasm, but it won't be the last. The table below introduces seven stocks that are well-positioned to benefit from growing AI adoption.
Table data sources: Morningstar, Tradingview.
Here's a look at TSM by the numbers:
Taiwan Semiconductor is the world's largest contract manufacturer of semiconductor chips.
TSM enjoys an almost unshakeable market share, owning roughly 61% of global semiconductor production. Its customer list includes several companies with strong AI exposure in their own right, including Nvidia, Apple
  ﻿
  Apple﻿
  0.0%
, Advanced Micro Devices
  ﻿
  Advanced Micro Devices﻿
  0.0%
 and Qualcomm
  ﻿
  Qualcomm﻿
  0.0%
.
As the world collectively demands better, faster computing chips, TSM will benefit. The company is gearing up for future growth with production capacity expansion efforts in Japan and the United States.
TSM pays a reasonable dividend yield of 1.3%, which is uncommon among AI stocks. You can also find TSM on my list of best stocks for 2024.
Here's a look at TCEHY by the numbers:
China-based Tencent Holdings operates a collection of businesses that provide online advertising, streaming entertainment, fintech and business services.
Tencent debuted its generative AI product, called Hunyuan, last year. Like ChatGPT, Hunyuan is a general-purpose technology, but the company is positioning it as a platform for business-related AI applications.
In fiscal year 2023, Tencent grew revenues by 10% and operating profit by 24%. The company has momentum in the gaming side of its business, but an upgrade to its AI-powered ad tech platform was also a factor in the revenue growth.
Tencent pays a modest dividend and regularly repurchases its shares.
Here's a look at NOW by the numbers:
ServiceNow develops and maintains a cloud-based workflow automation platform that businesses use to manage technology, operations, customer experience and human resources.
ServiceNow is delivering generative AI capabilities to its business customer base within its core Now platform. Those capabilities include the automation of repetitive tasks, identification of similarities across open tickets, work routing, recommendations for knowledge base content improvements and more.
Those features, collectively called Now Assist, are resonating with customers. In the quarter after Now Assist launched, ServiceNow reported 25.5% growth in subscription revenues and 15% growth in its customer base. It was the largest net increase in annual contract value associated with any product release in the company's history.
Stop chasing shadows in the market. Forbes' expert analysts have pinpointed the 12 superstars poised to ignite returns in 2024. Don't miss out—download 12 Stocks To Buy Now and claim your front-row seat to the coming boom.
Here's a quick look at SNPS by the numbers:
Synopsys makes software plus related tools and services used in the design and testing of silicon chips.
Synopsys' software stack for chip design, called Synopsys.ai, uses generative AI to streamline the design process. At the company's annual user group conference, CEO Sassine Ghazi said Synopsys.ai usage was growing rapidly on the strength of positive customer outcomes. The platform is delivering up to 10 times faster turnaround times plus a 10%-plus increase in performance, power and area—key variables in the optimization of chip design.
In its most recent earnings release, SNPS reported record revenue of $1.65 billion, representing a 21% gain over the prior-year result.
Here's a quick look at CRWD by the numbers:
Crowdstrike delivers AI-powered cybersecurity solutions and workflow automation, primarily on a subscription basis, for businesses in the U.S. and around the world.
Crowdstrike went public in 2019 and has only recently delivered GAAP profits. When the company reported positive EPS in the quarters ending April and July 2023, analysts and investors were pleasantly surprised.
For the fourth quarter of fiscal year 2024, Crowdstrike reported 33% increases in total revenues and subscription revenues and a 34% increase in annual recurring revenue. Momentum in recurring revenue bodes well for the future, given the company's 78% gross margin on subscriptions.
Crowdstrike is guiding to roughly $3.9 billion in revenues and $3.77 to $3.97 in adjusted, diluted net income per share for fiscal year 2025. Those numbers translate to revenue growth of about 27% and earnings growth of at least 22%.
Here's a quick look at MRVL by the numbers:
Marvell Technology designs integrated semiconductors for use in data centers across multiple end markets.
For its fiscal year 2024, Marvell reported declines in revenue and non-GAAP EPS of 7% and 29%, respectively. The company also reported GAAP net losses for the fourth quarter and year. Soft demand in the consumer, carrier infrastructure and enterprise networking markets was a driving factor. That's the bad news.
The good news is twofold. First, prior to fiscal year 2024, MRVL had been growing revenues, adjusted EPS and Ebitda. And second, in fiscal year 2024, MRVL grew its AI business substantially. The company's AI revenue in the fourth quarter eclipsed $200 million, which was about 10% of total company revenues—up from 3% in the prior year.
The bullish view is that MRVL will turn around its income statement performance as AI becomes a larger piece of the business and the conditions in other end markets improve.
Here's a quick look at DRKTF by the numbers:
Darktrace, like Crowdstrike, provides AI-powered cybersecurity solutions. Darktrace and Crowdstrike Falcon are often viewed as competing platforms, but they also integrate with one another.
Darktrace reported 27.4% revenue growth and 8,939% net profit growth in the first half of its fiscal year 2024 versus the year-ago period. Leadership cites a strategy change plus improved economic conditions as key contributors.
Thanks to double-digit growth in its annual recurring revenue, Darktrace also raised its revenue outlook for the full fiscal year by 50 basis points. The new expectation is a range of 23.5% to 25%. Going forward, the company will benefit from its 89% gross margin coupled with multiyear contracts and a flexible cost structure.
These seven stocks directly benefit from increased AI adoption. TSM and MRVL provide hardware that enables AI, while the rest deliver AI benefits to their customers. All seven companies are loved by analysts, have double-digit upside in share price and show positive momentum in their AI-related operations, if not company-wide.
Nvidia proved itself to be a wealth-building AI stock in the span of about 12 months. It's likely a few other AI players will experience their own wild price rallies, particularly as more businesses realize quantifiable benefits related to their AI implementations. The fun thing is that even a modest AI position coupled with a rally can deliver a nice bump to your net worth.
The brain trust at Forbes has run the numbers, conducted the research, and done the analysis to come up with some of the best places for you to make money in 2024. Download Forbes' most popular report, 12 Stocks To Buy Now."
292,https://www.forbes.com/sites/janakirammsv/2024/04/26/nvidias-acquisition-of-runai-emphasizes-the-importance-of-kubernetes-for-generative-ai/,Nvidia’s Acquisition Of Run:ai Emphasizes The Importance Of Kubernetes For Generative AI,"Apr 26, 2024, 01:41am EDT",Janakiram MSV,"Nvidia announced that it’s acquiring Run:ai, an Israeli startup that built a Kubernetes-based GPU orchestrator. While the price is not disclosed, there are reports that it is valued anywhere between $700 million and $1 billion.
The acquisition of Run:ai highlights Kubernetes' growing importance in the generative AI era. This makes Kubernetes the de facto standard for managing GPU-based accelerated computing infrastructure.
Run:ai is a Tel Aviv, Israel-based AI infrastructure startup founded in 2018 by Omri Geller (CEO) and Dr. Ronen Dar (CTO). It has created an orchestration and virtualization platform tailored to the specific requirements of AI workloads running on GPUs, which efficiently pools and shares resources. Tiger Global Management and Insight Partners led a $75 million Series C round in March 2022, bringing the company's total funding to $118 million.
Unlike CPUs, GPUs cannot be easily virtualized so that multiple workloads can use them at the same time. Hypervisors like VMware's vSphere and KVM enabled the emulation of multiple virtual CPUs from a single physical processor, giving workloads the illusion that they were running on a dedicated CPU. When it comes to GPUs, they cannot be effectively shared across multiple machine learning tasks, such as training and inference. For example, researchers cannot use half of a GPU for training and experimentation while using the other half for another machine learning task. Similarly, they cannot pool multiple GPUs to make better use of the available resources. This poses a huge challenge to enterprises running GPU-based workloads in the cloud or on-premises.
The problem described above extends to containers and Kubernetes. If a container requires a GPU, it will effectively consume 100% of the GPU if it is not used to its full potential. The shortage of AI chips and GPUs exacerbates the problem.
Run:ai saw an opportunity to effectively solve this problem. They used Kubernetes' primitives and proven scheduling mechanisms to create a layer that allows enterprises to use only a fraction of the available GPU or pool multiple GPUs. This resulted in better utilization of GPUs, delivering better economics.
Here are five key features of Run:ai platform:


Notably, Run:ai is not an open-source solution, even though it is based on Kubernetes. It provides customers with proprietary software that must be deployed in their Kubernetes clusters together with a SaaS-based management application.
Nvidia’s acquisition of Run:ai strategically positions the company to strengthen its leadership in the AI and machine learning sectors, especially in the context of optimizing GPU utilization for these technologies. Here are the primary reasons why Nvidia pursued this acquisition:
Enhanced GPU Orchestration and Management: Run:ai’s advanced orchestration tools are pivotal for managing GPU resources more efficiently. This capability is critical as the demand for AI and machine learning solutions continues to rise, requiring more sophisticated management of hardware resources to ensure optimal performance and utilization.
Integration with Nvidia's Existing AI Ecosystem: By acquiring Run:ai, Nvidia can integrate this technology into its existing suite of AI and machine learning products. This enhances Nvidia’s overall product offerings, allowing for better service to customers who rely on Nvidia’s ecosystem for their AI infrastructure needs. Nvidia, HGX, DGX and DGX Cloud customers will gain access to Run:ai’s capabilities for their AI workloads, particularly for generative AI workloads.
Expansion of Market Reach: Run:ai’s established relationships with key players in the AI space, including their prior integration with Nvidia’s technologies, provide Nvidia with an expanded market reach and the potential to serve a broader array of customers. This is particularly valuable in sectors that are rapidly adopting AI technologies but face challenges in resource management and scalability.
Innovation and Research Development: The acquisition enables Nvidia to harness the innovative capabilities of Run:ai’s team, known for their pioneering work in GPU virtualization and management. This could lead to further advancements in GPU technology and orchestration, keeping Nvidia at the forefront of technological developments in AI.
Competitive Advantage in a Growing Market: As enterprises increase their investment in AI and machine learning, effective GPU management becomes a competitive advantage. Nvidia’s acquisition of Run:ai ensures it remains competitive against other tech giants venturing into the AI hardware and orchestration space.
By acquiring Run:ai, Nvidia not only enhances its product capabilities but also solidifies its position as a leader in the AI infrastructure market, ensuring it stays ahead of the curve in technology innovations and market demands.
Nvidia’s acquisition of Run:ai is significant for the Kubernetes and cloud-native ecosystems for several reasons:
Enhanced GPU Orchestration in Kubernetes: The integration of Run:ai's advanced GPU management and virtualization capabilities into Kubernetes will allow for more dynamic allocation and efficient utilization of GPU resources across AI workloads. This aligns with Kubernetes' capabilities in handling complex, resource-intensive applications, particularly in AI and machine learning, where efficient resource management is critical​​.
Advancements in Cloud-Native AI Infrastructure: By leveraging Run:ai's technology, Nvidia can further enhance the Kubernetes ecosystem's ability to support high-performance computing (HPC) and AI workloads. This synergy between Nvidia’s GPU technology and Kubernetes will likely lead to more robust solutions for deploying, managing and scaling AI applications in cloud-native environments​.
Wider Adoption and Innovation: The acquisition could drive broader adoption of Kubernetes in sectors that are increasingly reliant on AI, such as healthcare, automotive and finance. The ability to efficiently manage GPU resources in these sectors can lead to faster innovation and deployment cycles for AI models​​.
Impact on Kubernetes Maturity: The integration of Nvidia and Run:ai technologies with Kubernetes underlines the platform's maturity and readiness to support advanced AI workloads, reinforcing Kubernetes as the de facto system for modern AI and ML deployments. This could also encourage more organizations to adopt Kubernetes for their AI infrastructure needs​.
Nvidia’s move to acquire Run:ai not only strengthens its position in the AI and cloud computing markets but also enhances the Kubernetes ecosystem's capacity to support the next generation of AI applications, benefiting a wide range of industries."
293,https://www.forbes.com/sites/robertpearl/2024/04/17/nvidias-ai-bot-outperforms-nurses-heres-what-it-means-for-you/,"Nvidia’s AI Bot Outperforms Nurses, Study Finds. Here’s What It Means.","Apr 17, 2024, 04:30am EDT","Robert Pearl, M.D.","Soon after Apple released the original iPhone, my father, an unlikely early adopter, purchased one. His plan? “I’ll keep it in the trunk for emergencies,” he told me.
He couldn’t foresee that this device would eventually replace maps, radar detectors, traffic reports on AM radio, CD players, and even coin-operated parking meters—not to mention the entire taxi industry.
His was a typical response to revolutionary technology. We view innovations through the lens of what already exists, fitting the new into the familiar context of the old.
Generative AI is on a similar trajectory. In my new book, ChatGPT, MD, I envisioned AI tools becoming hubs of medical expertise for doctors and patients within the next five years.
I feared my optimism about the technology was too ambitious, but by the time my book published last week, I realized my estimated timeline might have been too conservative.
In March, Nvidia stunned the tech and healthcare industries with a flurry of headline-grabbing announcements at its 2024 GTC AI conference. The most striking being a collaboration between Nvidia and Hippocratic AI to launch generative AI “agents.”
According to company-released data, the AI bots are 16% better than nurses at identifying a medication’s impact on lab values, 24% more accurate detecting toxic dosages of over-the-counter drugs, and 43% better at identifying condition-specific negative interactions from OTC meds. All that at $9 an hour compared to the $39.05 median hourly pay for U.S. nurses. These AI nurse-bots are designed to make new diagnoses, manage chronic disease, and give patients a detailed but clear explanation of clinicians’ advice.
These rapid developments suggest we are on the cusp of a technology revolution, one that could reach global ubiquity far faster than the iPhone. Here are three major implications for patients and medical practitioners:
The human brain can easily predict the rates of arithmetic and geometric growth.
But even the most astute minds struggle to grasp the implications of continuous, exponential growth. And that’s what we’re witnessing with generative AI.
Imagine, for example, a pond with just one lily pad. Assuming the number of lilies will double every night, the entire pond will be covered in just 50 days. Yet, on day 43, you would see only 1% of the pond’s surface covered. It seems hard to believe that just seven days later, the lily pads will completely fill the pond.
Experts project that AI’s computational progress will double roughly every year, if not faster. But even with conservative projections, ChatGPT and similar AI tools are poised to be 32 times more powerful in five years and over 1,000 times more powerful in a decade. That’s equivalent to your bicycle traveling as fast as a car and then, shortly after, a rocket ship.
This rate of advancement proves challenging for both healthcare providers and patients to understand, but it means that now is the time to prepare for what’s coming.
When assessing the transformative potential of generative AI in healthcare, it’s crucial not to let past failures, such as IBM’s Watson, cloud our expectations. IBM set out ambitious goals for Watson, hoping it would revolutionize healthcare by assisting with diagnoses, treatment planning and interpreting complex medical data for cancer patients.
I was highly skeptical at the time, not because of the technology itself, but because Watson relied on data from electronic medical records, which lack the accuracy needed for  “narrow AI” to make reliable diagnoses and recommendations.
In contrast, generative AI leverages a broader and more useful array of sources. It not only pulls from published, peer-reviewed medical journals and textbooks but also will be able to integrate real-time information from global health databases, ongoing clinical trials and medical conferences. It will soon incorporate continuous feedback loops from actual patient outcomes and clinician input. This extensive data integration will allow generative AI to continuously stay at the forefront of medical knowledge, making it fundamentally different from its predecessors.
That said, generative AI will require a couple more generational upgrades before it can be widely used without direct clinician oversight. Still, Nvidia’s bold entry into healthcare signals a long-overdue willingness among tech companies to navigate the legal and regulatory hurdles of healthcare. Once an AI clinician chatbot is available, multiple other companies will quickly follow.
Just as my father never imagined his iPhone (stored in the trunk) would evolve into an essential tool for navigating life, many Americans struggle to envision the transformative impact that generative AI will have on healthcare.
The concept of accessing medical advice and expertise continuously, affordably, reliably, and conveniently around the clock represents such a departure from current healthcare models that it’s easy for our minds to dismiss it as far-fetched. Yet it’s becoming increasingly clear that these capabilities are not just possible, but likely and even imminent.
Daily, I receive feedback from both clinicians and patients who have interacted with generative AI tools. The majority report that the responses, particularly when prompted effectively, align closely with clinician recommendations. This is a testament to the evolving accuracy and reliability of generative AI in healthcare settings, and it promises a revolution in medical care delivery in the near future.
A decade from now, we will look back at today’s skepticism in much the same way I think about my dad’s initial underestimation of his iPhone. We are on the cusp of a major shift, where generative AI will become as integral to healthcare as smartphones have become to daily life."
294,https://www.forbes.com/sites/petercohan/2024/04/11/why-nvidia-stock-could-top-1000-a-share-after-may-earnings-report/,"Why Nvidia Stock Could Top $1,000 A Share After May Earnings Report","Apr 11, 2024, 08:41am EDT",Peter Cohan,"Nvidia’s boffo quarterly earnings report last April set in motion a global fever for generative AI stocks.
As the anniversary of that report — which featured unexpectedly high demand by cloud service providers for Nvidia’s GPU  — approaches, has Nvidia stock peaked? Or could the stock rise to a new high on May 22, when the company delivers its fiscal year 2025 first-quarter results?
While possible rival products and high investor expectations could drive down Nvidia’s stock price, I favor the view of optimists such as Morgan Stanley analyst Joseph Moore, who raised to $1,000 his price target on the GPU maker’s stock, according to InvestorPlace.
Here are the reasons for optimism:


The biggest obstacle to Nvidia’s stock price topping $1,000 is the company’s recent success. How so? Every quarter in which Nvidia exceeds expectations and raises guidance, investors expect even more of the same for the next quarter.
Analysts expect the company to keep doing that for at least the next year or two. If they are right, Nvidia’s stock will keep rising.
If a company exceeds Wall Street expectations for sales and earnings in the most recent three months and raises guidance for the future, the stock usually rises. If the company falls short of those results, the stock falls.
Bearing that rule in mind, the 236% pop in Nvidia’s stock price in 2023 and its 79% rise so far in 2024 do not come as a shock. According to Nasdaq, Nvidia — whose fiscal year 2024 ended in January — has surprised on the upside in each of the four most recent quarters:


These numbers reveal a disturbing trend for Nvidia bulls: The extent to which Nvidia’s earnings exceed investor expectations is steadily dropping. If the recent trend continues — an 8 percentage-point-per-quarter drop in Nvidia’s beat — Nvidia could fall short of what Wall Street expects later this year.
Underlying Nvidia’s stock price increase is soaring demand for the company’s GPUs. Demand for Nvidia’s AI-focused chips soared 410% to $18.4 billion in the January 2024-ending quarter. Big customers include:


Nvidia’s gross margins have also increased. In the fourth quarter of 2024, Nvidia’s gross margins improved 12 percentage points to 76% and are expected to rise to 77% in Nvidia’s first quarter of FY 2025, noted TheStreet.com.
Nvidia’s combination of chips and developer software constitute barriers to entry which are difficult for rivals to surmount. Over the last 15 years, Nvidia’s share of the AI chip industry has reached somewhere between 80% and 95%, as I wrote in my February 2024 Forbes article.
As I said then, signs of Nvidia’s market power include:


Here are Nvidia’s most important sources of competitive advantage:


To be sure, rivals such as AMD, Intel, and Google are announcing efforts aimed at winning market share from Nvidia, according to InvestorPlace.
While Nvidia’s competitive advantages are formidable, the company has been considered vulnerable in the market for chips that perform inferencing — enabling AI chatbots to respond to user prompts, according to the Wall Street Journal.
Last month, Nvidia announced Blackwell GPU, a new chip architecture for inferencing. These so-called B100 chips work more than two times faster than the company’s current offerings and Nvidia expects to charge 40% more — between $30,000 to $40,000 per unit — than it does for the H100, according to  TheStreet.com.
Analysts reacted positively to the B100 news. The UBS analyst Tim Arcuri said Nvidia’s new chips, “will reassert its undisputed technical lead in performance,” while the Bank of AmericaBAC securities analyst Vivek Arya said in a note to clients featured in the Journal, the B100 and other recent developments, “continue to fundamentally widen Nvidia’s competitive moat.”
Nvidia stock has lost about 8% of its value since peaking in late March.
When Nvidia reports fiscal first quarter 2025 earnings the GPU designer’s stock price will soar or plunge depending on whether its results and forecast are better than consensus estimates.
Here are the key numbers investors are looking for from Nvidia, according to TheStreet.com:


In February, Nvidia guided investors to expect 300% revenue growth to $24 billion in Q1 2025 — $2.4 billion more than the analysts’ consensus, I wrote in February.
Will Nvidia raise its Q2 2025 revenue growth guidance above 300%? Analysts have lower expectations for Nvidia’s FY 2025 revenue growth and gross margins. They forecast Nvidia revenue will grow 82% to $11 billion while gross margins will “stabilize at 75.83%,” according to TheStreet.com.
Has Nvidia stock passed its peak? The average price target of 41 analysts suggests 13.7% upside with a 12-month price target of $989, according to TipRanks.
Many analysts expect Nvidia stock to rise:


Not all analysts are bullish. “Although NVDA (Neutral-rated) should deliver a spectacular 2024 (and perhaps into 2025), we continue to believe recent trends set up a significant cyclical downturn by 2026,” D.A. Davidson analysts said in the note Tuesday featured in a CNBC report.
“A combination of shrinking models, more steady growth in demand, maturing hyperscaler investments, and increased reliance by their largest customers on their own chips do not bode well for NVDA’s out years,” CNBC noted."
295,https://www.forbes.com/sites/stevemcdowell/2024/04/09/intels-challenges-nvidia-with-gaudi-3-ai-accelerator/,Intel Challenges Nvidia With Gaudi 3 AI Accelerator,"Apr 09, 2024, 11:43pm EDT",Steve McDowell,"In a move that directly challenges Nvidia in the lucrative AI training and inference markets, Intel announced its long-anticipated new Intel Gaudi 3 AI accelerator at its Intel Vision event.
The new accelerator offers significant improvements over the previous generation Gaudi 3 processor, promising to bring new competitiveness to training and inference for LLMs and multimodal models.
Gaudi 3 dramatically increases AI compute capabilities, delivering substantial improvements over Gaudi 2 and competitors, particularly in processing BF16 data types, which are crucial for AI workloads.
Manufactured using a 5nm process technology, Gaudi 3 incorporates significant architectural advancements, including more TPCs and MMEs. This provides the computing power necessary for the parallel processing of AI operations, significantly reducing training and inference times for complex AI models.
Gaudi 3 expands its hardware capabilities with more Matrix Math Engines and Tensor Cores than its predecessor, Gaudi 2. Specifically, it increases from 2 to 4 MMEs and 24 to 32 TPCs, bolstering its processing power for AI workloads.
The new accelerator boasts an FP8 precision throughput of 1835 TFLOPS, doubling the performance of Gaudi 2. It also significantly enhances BF16 performance, although specific throughput figures for this improvement were not disclosed.
It has 128GB of HBMe2 memory, offering 3.7TB/s of memory bandwidth and 96MB of onboard static RAM. This massive memory capacity and bandwidth supports processing large datasets efficiently, which is crucial for training and running large AI models.
High-speed, low-latency networking is critical when building clusters of accelerators to solve large training tasks. While Nvidia is building its accelerators using proprietary interconnects like its NVLInk, Intel is all-in on standard ethernet-based networking.
Gaudi 3 reflects this, featuring twenty-four 200Gb Ethernet ports, significantly enhancing its networking capabilities. This ensures scalable and flexible system connectivity, allowing for the efficient scaling of AI compute clusters without being locked into proprietary networking technologies.
Intel’s Gaudi 3 AI accelerator shows robust performance improvements across several key areas relevant to AI training and inference tasks, particularly for LLMs and multimodal models.
Intel projects that Gaudi 3 will significantly outperform competing products like Nvidia's H100 and H200 in training speed, inference throughput, and power efficiency for various parameterized models.
Intel also predicts Gaudi 3 will deliver an average 50% faster training time and superior inference throughput and power efficiency against leading competitors for several parameterized models. This includes a greater inference performance advantage on longer input and output sequences.
Intel's Gaudi 3 AI accelerator is a strategic move by Intel to gain a greater position in the supply-hungry AI accelerator market, directly challenging Nvidia to address the burgeoning demand for advanced AI compute solutions.
Intel built a compelling solution, bringing substantial performance improvements over Gaudi 2 and delivering a solution that will challenge the market. The 4x AI compute for BF16, 1.5x increase in memory bandwidth, and 2x networking bandwidth improvements position the Gaudi 3 as a powerful solution for the needs of next-generation AI applications.
Intel's emphasis on open community-based software and industry-standard Ethernet networking addresses critical market needs for flexibility and scalability without vendor lock-in. This approach differentiates Intel from Nvidia and aligns with the broader industry trend toward open standards and interoperability.
Intel's partnerships with Dell Technologies, HPE, Lenovo, and Supermicro for the Gaudi 3 rollout set Intel up for success. If Intel can deliver the accelerators to the market on schedule and the promised performance claims hold, then Intel is poised to realize significant growth in the accelerator market. The same is also true for AMD and its MI300x accelerator.
Gaudi 3 isn’t just about the current generation of AI accelerators but also sets the stage for Intel's next-generation GPU, Falcon Shores. By integrating the Intel Gaudi and Intel Xe IP with a single GPU programming interface, Falcon Shores is expected to further Intel's capabilities in AI and HPC.
The launch of the Gaudi 3 AI accelerator is a significant milestone for Intel, highlighting its technological advancements, strategic market positioning, and commitment to addressing the evolving needs of the AI industry.
By offering substantial performance improvements, embracing open standards, and establishing strategic OEM partnerships, Intel is challenging the status quo in the AI accelerator market and positioning itself as a leader in the next wave of AI infrastructure."
296,https://www.forbes.com/sites/antonyleather/2024/04/09/nvidia-rtx-5090-and-5080-expected-to-launch-late-2024/,Nvidia RTX 5090 And 5080 Expected To Launch Late 2024,"Apr 09, 2024, 01:16pm EDT",Antony Leather,"Nvidia is likely aiming for a late 2024 release of its next generation RTX 5000 'Blackwell' gaming graphics cards, specifically the GeForce RTX 5090 and 5080. The news of a Q4 launch comes from a regular Nvidia leaker as well as other online sources.
In addition, website UDN also claims (via Wccftech) that Nvidia board partners such as MSI and Gigabyte are expecting a Q4 launch for the RTX 5000 series, landing between October and December. This would also tie in with launches of the previous RTX 3000 Ampere and RTX 4000 Ada Lovelace GPUs, which both launched at similar times of the year, spaced at two-year intervals - again lighting up perfectly with a 2024 launch for RTX 5000.
Other than performance rumors, there hasn't been much information about the RTX 5000-series, with all the buzz around the recent launches of Nvidia's RTX 4000-Super series such as the RTX 4080 Super, except for a supposed 70 percent performance increase for the RTX 5090 over the RTX 4090
FEATURED | Frase ByForbes™
Unscramble The Anagram To Reveal The Phrase
Pinpoint By Linkedin
Guess The Category
Queens By Linkedin
Crown Each Region
Crossclimb By Linkedin
Unlock A Trivia Ladder
This would likely translate into an early 2025 launch for mid-range models such as the RTX 5070 and RTX 5060, although Nvidia hasn't let any information about naming schemes or specifications slip yet. Encouragingly for the PC market, the board partners had favorable things to say about Nvidia's biennial launch schedule, with refreshes in-between such as the RTX Super series earlier in 2024, and were also predicting decent growth in GPU sales this year too, despite prices rises.
The RTX 5000-series is expected to use GDDR7 memory and a 3nm manufacturing process. The RTX 4000 Super-series cards were praised for not adding significant price rises into the mix, but the sources at UDN claim analysts expect the average selling price for graphics card products to increase.
Another reason for manufacturers to be optimistic about sales later in 2024 is that AMD and Intel are both introducing new ranges of desktop processors, with Intel launching an entirely new platform too. These kinds of launches often result in more extensive purchases as enthusiasts upgrade their PCs, especially in the lead-up to Christmas, which is exactly when all three manufacturers' products should land.
That said, while AMD is expected to release it's highly-anticipated Zen 5 CPUs, we may not see competition from the company against Nvidia's high-end products, namely the RTX 5080 and RTX 5090, with AMD reportedly focussing on the more popular mid-range instead - something it's done several times in the past. Given the vast majority of sales are in this area, it makes sense, even though it means we'll likely see an Nvidia monopoly, with potential high prices for the RTX 5080 and RTX 5090 as a result.
It's possible we'll know more at this year's Computex trade show in Taiwan with keynotes expected from PC hardware manufacturers such as AMD. I'll be reporting from the event so make sure you follow me here on Forbes using the blue button below, on Facebook or Twitter for the latest PC hardware news and reviews."
297,https://www.forbes.com/sites/forbesbooksauthors/2024/04/02/nvidias-prescription-for-the-future-transforming-healthcare-with-ai/,NVIDIA’s Prescription for the Future: Transforming Healthcare With AI,"Apr 02, 2024, 04:49pm EDT",Rajeev Ronanki,"In the dynamic arena of healthcare technology, NVIDIA stands as a beacon of progress, guided by Jensen Huang's visionary leadership. With the recent unveiling of AI microservices, the introduction of a groundbreaking AI chip, the anticipation surrounding a next-generation robot, and a series of strategic partnerships, NVIDIA is sculpting a future where healthcare is transformed through the power of AI.
These innovations, driven by Huang's belief in the power of humility and innovation, signify a leap towards a more efficient, personalized, and interconnected healthcare ecosystem. By seamlessly integrating advanced AI into the fabric of healthcare, from diagnostics to patient care, NVIDIA is not merely responding to current trends; it's pioneering a future where technology and human-centric care coalesce to improve lives globally.
NVIDIA's recent launch of generative AI microservices is setting the stage for a seismic shift in healthcare. With over two dozen new microservices now available, NVIDIA is enabling healthcare enterprises worldwide to harness the latest advances in generative AI across any cloud platform.
This suite, featuring optimized NVIDIA NIM™ AI models and workflows, is revolutionizing healthcare by offering advanced imaging, natural language and speech recognition, as well as digital biology generation capabilities. Such tools are crucial in screening for trillions of drug compounds, with the potential to significantly advance medicine. Furthermore, by facilitating the early detection of diseases through improved patient data gathering and enhancing digital assistants for healthcare providers, NVIDIA is not just redefining precision in diagnostics and personalization in patient care; it's accelerating the pace of drug discovery at an unprecedented scale.
Separately, organizations like Abridge seek to greatly improve the future of healthcare documentation. By automating the conversion of clinical conversations into structured notes, their technology aims to drive down the administrative burden on clinicians, potentially saving them hours each day. With its capacity to swiftly process and summarize key medical information from dialogues, even in challenging acoustic environments, we are looking at impacting administrative burden—perhaps the biggest driver responsible for clinical burnout today.
At the heart of NVIDIA's technological revolution in healthcare is the Blackwell B200 GPU, heralded as the world's most potent chip for AI. This groundbreaking GPU is engineered to offer up to 20 petaflops of FP4 horsepower—an astounding 2.5 times increase from its H100 Hopper GPU, which serves as a crucial component for getting us where we are today with generative AI.
Such computational power is pivotal for healthcare organizations, enabling the real-time processing and analysis of extensive medical datasets. This capability is critical for developing more informed, faster clinical decisions, ultimately improving patient outcomes and operational efficiency. It’s also important to consider the impact toward payment innovation across healthcare, where, according to Turquoise Health, there are 630 Terabytes (TB) or 78 billion health records worth of data in machine readable files held by health plans alone.
The Blackwell B200 GPU is a manifestation of NVIDIA's foresight and Huang's leadership ethos, aiming to democratize access to advanced AI analytics in healthcare. It promises a future where AI-accelerated healthcare solutions are within reach for all, ensuring that cutting-edge technology can drive innovations in patient care, diagnostic accuracy, and treatment efficacy.
Huang notes, “Building foundation models for general humanoid robots is one of the most exciting problems to solve in AI today.”
And with the introduction of Project GR00T, NVIDIA ventures into the realm of embodied AI, anticipating a future where humanoid robots become integral to healthcare delivery. These next-generation robots are being designed to understand and execute tasks with a level of dexterity and empathy previously unseen, embodying Huang's vision of a future where technology seamlessly integrates with humanity to enhance healthcare services.
GR00T robots are poised to assist in various healthcare settings, from surgical assistance to patient care, embodying the AI potential to work alongside human professionals harmoniously. This initiative marks a significant step towards humanizing robotic assistance in healthcare, where robots are seen as partners in delivering care, thereby elevating the quality and efficiency of patient interactions.
NVIDIA's foray into humanoid robotics with Project GR00T illustrates a commitment to pushing the boundaries of AI and robotics, ensuring that technological advancements serve to complement and augment human efforts in healthcare, rather than replace them.
NVIDIA is not working in isolation. Its collaborations with renowned companies are testament to its commitment to transforming healthcare:
The strategic alliance announced between Hippocratic AI and NVIDIA is catalyzing a revolution in healthcare communication, merging AI's computational prowess with the nuanced art and science linked to “empathy inference.” This collaboration—enabling conversations of a low-latency nature—leverages NVIDIA’s AI platform to bring to life “healthcare agents” capable of engaging with patients in real-time conversations that were once the exclusive domain of human caregivers.
Demonstrating the power of this partnership, user tests have shown a remarkable increase in patient trust and comfort—a 5-10% rise in emotional connection with every half-second reduction in response time. The decreased time goes beyond efficiency; and serves as a catalyst for what may help further innovation in patient experience.
Empowered by NVIDIA’s advanced H100 Tensor Core GPUs and the innovative NVIDIA Avatar Cloud Engine, these AI agents have surpassed industry standards, outperforming notable models like GPT-4 and LLaMA2 70B Chat in safety benchmarks. Such achievements underscore the transformative potential of integrating state-of-the-art AI with healthcare, promising not just to fill the gaps in staffing shortages but to forge a new paradigm in patient engagement.
Recently, Microsoft announced a partnering with NVIDIA to further AI, cloud, and accelerated computing. In harnessing Microsoft Azure's vast capabilities alongside NVIDIA's cutting-edge AI and computing platforms, this partnership appears well-rooted for transformation across life sciences and patient care.
From accelerating drug discovery through AI-powered analyses to enhancing diagnostics with advanced imaging technology, the potential benefits appear to be immense. This strategic alliance not only democratizes access to groundbreaking healthcare solutions but also paves the way for more personalized, efficient, and accessible patient care. This AI-driven collaboration exemplifies how collaborative technological advancement sets the stage for driving future positive impacts on healthcare outcomes globally—a forward-thinking approach to addressing the complexities of modern medicine.
In healthcare and other highly regulated environments, NVIDIA recognizes security is a crucial aspect. The landscape of security is evolving rapidly, with the rise of ransomware attacks and supply chain attacks. As a result, we are witnessing the rise of governance and recommendations such as Software Bill of Materials and Zero Trust Architectures. Progress is being made at the silicon level to secure hardware with trusted suppliers and verification. An example of this is HPE’s silicon root of trust, which offers protection against firmware attacks.
Additionally, there will be an increase in computations running on secure enclaves to safeguard privacy and ensure that sensitive data remains harder to access.
As we stand at the threshold of a new frontier in healthcare, the journey led by pioneers like NVIDIA serves as a testament to the boundless potential of artificial intelligence. Yet, the true essence of this transformation lies not just in the technology itself, but in our collective will to harness it responsibly, with humans always in the loop. NVIDIA's role in this paradigm shift reminds us of the power of collaboration and the importance of keeping human empathy at the core of innovation.
As we venture further into this era of AI-enhanced healthcare, let us be inspired to embrace these advancements not merely as technological triumphs but as tools to deepen our connection to one another, improve patient care, and enrich the human experience. In doing so, we chart a course toward a future where technology and humanity converge in harmony, driving forward a healthcare revolution that is inclusive, compassionate, and transformative."
298,https://www.forbes.com/sites/jackkelly/2024/04/01/i-hope-suffering-happens-to-you-hiring-for-grit-over-pedigree/,‘I Hope Suffering Happens To You’: Nvidia CEO’s Case For Hiring For Grit Over Pedigree,"Apr 01, 2024, 12:00pm EDT",Jack Kelly,"“I wish you ample doses of pain and suffering.”
That is the unconventional message Nvidia CEO and founder Jensen Huang had for students at Stanford University, his alma mater, where he spoke last month at the Stanford Institute for Economic Policy Research.
According to Huang, elite educational pedigrees alone are not necessarily the best predictor of success.
“Greatness is not intelligence. Greatness comes from character. And character isn’t formed out of smart people, it’s formed out of people who suffered,” said the Stanford alum.
Instead, the man who presides over one the world’s most valuable companies in terms of stock market valuation, now looks to find certain traits in job applicants that include resilience, grit and determination, which he contends lead to innovation and success in the workplace.
Counterintuitively, Huang believes that having very high expectations coming out of an Ivy League education can potentially set people up for less resilience when faced with failure or adversity, which are inevitable in any career path.
“Unfortunately, resilience matters in success,” he said to the students. “I don’t know how to teach it to you except for I hope suffering happens to you.”
His comments indicate a view that real-world experience, involving struggles, obstacles and suffering, can instill the resilience needed for long-term success. Therefore, he believes that employers should look beyond just the prestige factor of a university and more closely at the inner drive, mindset and resolution of potential hires.
“To this day I use the phrase ‘pain and suffering’ inside our company with great glee,” said the chief executive. “I mean that in a happy way, because you want to refine the character of your company. You want greatness out of them.”
In 2011, Huang shared a similar sentiment in another speech he gave at Stanford, stating, “Unless you have a tolerance for failure, you will never experiment, and if you don’t ever experiment, you will never innovate. If you don’t innovate, you don’t succeed.”
The Nvidia founder’s statements about valuing resilience over educational pedigrees align with the old Wall Street adage about hiring what they call “PHDs”—the poor, hungry and driven.
The core parallels are prioritizing hunger, grit and motivation over academic credentials alone, believing that candidates who have had to overcome adversity, financial hardships or personal trauma offer invaluable drive and tenacity. Both schools of thought find real-world fight and scrappiness desirable, in addition to raw intellectual ability. These philosophies prioritize the intangibles. There is a skepticism that those who come from privileged backgrounds lack ""hunger"" and may be less resilient and more entitled.
The PHD concept essentially argues that struggle itself, if handled constructively, can be great preparation for thriving in ultra-competitive business environments where stamina and an insatiable work ethic are keys to success.
The idea is harnessing difficult experiences as motivation for growth. With the right mindset, life's challenges can forge very capable professionals. Having  resilience,  enduring trauma and difficulties builds mental toughness.
A person who had to fight for opportunities may have a stronger inner drive and motivation to succeed compared to those for whom things came easily. This hunger can translate to work ethic. Facing adversity often provides deeper empathy and understanding of struggles others face. This can build better emotional intelligence and people skills.
Overcoming adversity requires adaptability and comfort with ambiguity. These mindsets are very valuable in ever-changing work environments. Wrestling with major life challenges can provide a sense of perspective often lacking in those with smoother paths. This can sharpen priorities and decision making.
There are a number of  well-known examples of highly successful companies hiring talented individuals without traditional elite university credentials. These examples show that many transformative technology and business leaders were able to find success by demonstrating qualities like vision, passion, grit and hustle rather than being selected just based on an elite college pedigree. Their companies have embraced similar hiring philosophies.


There are several key indicators that employers can look for in applicants for signs of grit and determination, especially candidates who take ownership of their failures rather than passing blame. Conduct structured interviews that focus on assessing their history of hard work, overcoming obstacles, passion for learning, self-improvement, ability to take criticism and desire to contribute to a team.
Ask questions that reveal candidates' approach toward work, determination to overcome obstacles, coping mechanisms for challenges and their ability to learn from failures. Ask behavioral questions during interviews that focus on their responses to failure and long-term goals.
Evaluate candidates' responses to these questions to gauge their resilience in the face of failure, determination to persist past mistakes, passion for their work, ability to cope with challenges and willingness to take initiative.
Hiring managers want to see applicants that show resilience by acknowledging mistakes and reflecting on how to improve in the future. Individuals who engage in self-reflection on what went wrong and have a clear plan for what to do differently next time exhibit determination to persist past mistakes. This quality highlights their ability to adapt and learn from failures, which are essential traits for grit."
299,https://www.forbes.com/sites/davealtavilla/2024/03/27/nvidia-and-synopsys-punctuate-ai-chip-design-and-acceleration-leadership/,Nvidia And Synopsys Punctuate AI Chip Design And Acceleration Leadership,"Mar 27, 2024, 04:53pm EDT",Dave Altavilla,"Last week in Silicon Valley, there was a meeting of the minds in artificial intelligence and AI-driven chip design. Two industry juggernauts, Nvidia and Synopsys, held conferences that brought developers and tech innovators together in very different, but complementary ways. Nvidia, world renowned for its AI acceleration technologies and silicon platform dominance, and Synopsys, a long-standing industry bellwether in semiconductor design tools, IP and automation, are both capitalizing on the huge market opportunity that is now flourishing for machine learning and artificial intelligence.
Both companies launched a proverbial arsenal of enabling technologies, Nvidia leading the way with its massive AI accelerator chips and Synopsys enabling chip developers the ability to harness AI for many of the laborious steps of chip design and validation processes. In fact, we soon may reach not only an inflection point with AI, but perhaps a kind of “inception” point is on the horizon as well. In other words, sort of like the chicken and the egg, which came first, the AI or the AI chip? It’s the stuff of science fiction for many of you I’m sure, but let’s dig into a couple of what I feel were the highlights of this AI show of force last week.
There’s no question, the folks at Synopsys stepped into a bit of Nvidia’s lime light with respect to the company’s announcements in AI accelerators, which are the de facto standard in the data center currently, at Nvidia’s GPU Technology Conference that took place earlier in the week. However, as Nvidia CEO Jensen Huang pointed out on stage with Synopsys CEO Sassine Ghazi (above), it’s for darn good reason. In short, virtually every chip Nvidia designs and sends to manufacturing is implemented using Synopsys EDA tools for design, verification and then porting to chip fab manufacturing. However, Ghazi’s keynote also touched on a new Synopsys technology called 3DSO.ai, and I was kind of blown away.
Synopsys announced its Design Space Optimization AI tool back in 2021, which enabled a massive speed-up in the process of place and route, or floor planning a chip design. Finding optimal circuit layout and routing in large scale semiconductor designs is both labor-intensive and complex, often leaving performance, power efficiency and silicon cost on the table. Synopsys DSO.ai has the machine tirelessly work on this iterative process, eliminating many manhours of engineering time, speeding time to market with much more optimized chip designs.
Now, Synopsys 3DSO.ai takes this technology to the next level for modern 3D stacked chiplet solutions, with multiple layers of design automation for this new era of the chiplet, in addition to critical design thermal analysis as well. In essence, Synopsys 3DSO.ai is not only playing a sort of Tetris to optimize chip designs, but rather 3D Tetris now, optimizing place and route in three dimensions, and then providing thermal analysis of the design to ensure the physics of it are thermally feasible or optimal. So yeah, it’s like that. AI-powered chip design in 3D — mind officially blown.
For its GPU Technology Conference, Nvidia once again pulled out all the stops, this time selling out the San Jose SAP Center, with a huge crowd of developers, press, analysts and even tech dignitaries like Michael Dell. My analyst biz partner and longtime friend, Marco Chiappetta, covered GTC highlights well here (check out AI NIMs too, very cool), but for me the stars of Jensen Huang’s show were the company’s new Blackwell GPU architecture for AI, Project GR00T for building humanoid robots and another AI-powered chip tool called cuLitho that is now being adopted by Nvidia and TSMC in production environments. The net-net on cuLitho is that designing costly chip mask sets, for patterning these designs on wafers in production, just got a much-needed shot in the arm from machine learning and AI. Nvidia claims its GPUs, in conjunction with its cuLitho models, can deliver up to a 40X lift in chip lithography performance, with a huge power savings over legacy CPU-based servers. And this technology, in partnership with Synopsys on the design and verification side of things, and TSMC on the manufacturing end, is now in full production.
Which brings us to Blackwell. If you thought Nvidia’s Hopper H100 and H200 GPUs were monster AI silicon engines, then Nvidia’s Blackwell is kind of like “releasing the Kraken.” For reference, a single, dual die Blackwell GPU is comprised of some 208 billion transistors, more than 2.5X that of Nvidia’s Hopper architecture. However, those dual GPU clusters function as one massive GPU communicating over Nvidia’s NV-HB1 high bandwidth fabric that offers a blistering 10TB/s of throughput. Couple those GPUs with 192GB of HMB3e memory, with over 8TB/s of peak bandwidth, and we’re looking at double the memory of H100 along with double the bandwidth. Nvidia also co-joins a pair of Blackwell GPUs together with its Grace CPU for a trifecta AI solution it’s calling a Grace Blackwell Superchip, aka GB200.
Configure a rack full of dual GB200 servers with the company’s 5th Gen NVLink technology, that offers 2X the throughput of Nvidia’s previous gen, and you have an Nvidia GB200 NVL72 AI Supercomputer. An NVL72 cluster configures up to 36 GB200 Superchips in this rack, connected via its NVLink spine on the backside. It’s a pretty wild design that’s also comprised of Nvidia BlueField 3 data processing units, and the company claims it’s 30X faster than its previous gen H100 based systems at trillion parameter Large Language Model inferencing. GB200 NVL72 is also claimed to offer 25X lower power consumption and 25X better TCO. The company is also configuring up to 8 racks in a DGX SuperPODs comprised of NVL72 Supercomputers. Nvidia announced a multitude of partners adopting Blackwell, including Amazon Web Services, Dell, Google, Meta, Microsoft, OpenAI and many others. The company is vowing to have these new powerful AI GPU solutions in market later this year.
So while Nvidia is not only leading the charge as the 800 lb gorilla of AI processing, it also appears the company is just getting warmed up.
Another area Nvidia is continuing to accelerate its execution in is robotics, and Jensen Huang’s GTC 2024 robot show highlighting its Project Gr00T (that’s right with two zeros) Foundation Model for humanoid robots, was another wild ride. GR00T, which stands for Generalist Robot 00 Technology is all about training robots for not only natural language input and conversation, but also how to mimic human movements and actions for dexterity, navigating and adapting to a changing world around them. Like I said, science fiction, but Nvidia is apparently poised to make it reality sooner than later, with GR00T.
And really, that’s what impressed me the most from both Nvidia and Synopsys during my time in the valley last week. What previously seemed like nearly unsolvable problems and workloads, are now being solved and executed via machine learning at an ever-accelerating pace. And it’s having a compounding effect, such that bigger and bigger strides are being made year after year. I feel fortunate to be an observer and guide of sorts in this fascinating time of technology, and it’s what gets me up in the morning."
